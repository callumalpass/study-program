[
  {
    "id": "cs203-t1-ex1",
    "subjectId": "cs203",
    "topicId": "cs203-topic-1",
    "type": "written",
    "title": "DFA for Strings Ending in \"ab\"",
    "description": "Design a DFA over alphabet Σ = {a, b} that accepts all strings ending with \"ab\". Provide the formal 5-tuple definition and draw the state diagram.",
    "difficulty": 1,
    "hints": [
      "Think about what information you need to remember",
      "You need to track the last two symbols seen",
      "Consider states: \"nothing relevant\", \"saw a\", \"saw ab\""
    ],
    "solution": "**DFA M = (Q, Σ, δ, q₀, F)**\n\nQ = {q₀, q₁, q₂}\nΣ = {a, b}\nq₀ = q₀ (start state)\nF = {q₂}\n\nTransition function δ:\n| State | a  | b  |\n|-------|----|----|\n| q₀    | q₁ | q₀ |\n| q₁    | q₁ | q₂ |\n| q₂    | q₁ | q₀ |\n\n**State meanings:**\n- q₀: Haven't seen pattern or last was b (not after a)\n- q₁: Last symbol was 'a'\n- q₂: Last two symbols were \"ab\" (accepting)\n\n**Correctness argument:**\n- From any state, seeing 'a' transitions to q₁ (potential start of \"ab\")\n- From q₁, seeing 'b' completes \"ab\", going to accepting q₂\n- From q₂, seeing 'b' breaks the pattern (returns to q₀)\n- From q₂, seeing 'a' could start new pattern (goes to q₁)"
  },
  {
    "id": "cs203-t1-ex2",
    "subjectId": "cs203",
    "topicId": "cs203-topic-1",
    "type": "written",
    "title": "NFA with ε-Transitions",
    "description": "Convert the following NFA with ε-transitions to an equivalent NFA without ε-transitions. The NFA has states {q₀, q₁, q₂}, start state q₀, accepting state q₂, and transitions: δ(q₀, a) = {q₀}, δ(q₀, ε) = {q₁}, δ(q₁, b) = {q₁}, δ(q₁, ε) = {q₂}, δ(q₂, a) = {q₂}.",
    "difficulty": 3,
    "hints": [
      "First compute ε-closure for each state",
      "Then build new transitions by following ε-closures",
      "Any state with accepting state in its ε-closure becomes accepting"
    ],
    "solution": "**Step 1: Compute ε-closures**\n- ε-closure(q₀) = {q₀, q₁, q₂} (follow q₀→q₁→q₂)\n- ε-closure(q₁) = {q₁, q₂} (follow q₁→q₂)\n- ε-closure(q₂) = {q₂}\n\n**Step 2: Determine new accepting states**\nStates with q₂ in their ε-closure: {q₀, q₁, q₂}\nSo F' = {q₀, q₁, q₂}\n\n**Step 3: Compute new transitions**\nFor δ'(q, a), compute ε-closure(δ(ε-closure(q), a)):\n\nδ'(q₀, a) = ε-closure(δ({q₀,q₁,q₂}, a)) = ε-closure({q₀,q₂}) = {q₀,q₁,q₂}\nδ'(q₀, b) = ε-closure(δ({q₀,q₁,q₂}, b)) = ε-closure({q₁}) = {q₁,q₂}\n\nδ'(q₁, a) = ε-closure(δ({q₁,q₂}, a)) = ε-closure({q₂}) = {q₂}\nδ'(q₁, b) = ε-closure(δ({q₁,q₂}, b)) = ε-closure({q₁}) = {q₁,q₂}\n\nδ'(q₂, a) = ε-closure(δ({q₂}, a)) = ε-closure({q₂}) = {q₂}\nδ'(q₂, b) = ε-closure(δ({q₂}, b)) = ε-closure(∅) = ∅\n\n**Resulting NFA without ε-transitions:**\n- States: {q₀, q₁, q₂}\n- Start: q₀\n- Accept: {q₀, q₁, q₂}\n- Transitions as computed above"
  },
  {
    "id": "cs203-t1-ex3",
    "subjectId": "cs203",
    "topicId": "cs203-topic-1",
    "type": "written",
    "title": "Subset Construction",
    "description": "Apply the subset construction algorithm to convert the following NFA to a DFA: States {p, q, r}, start state p, accepting state r, transitions: δ(p, 0) = {p, q}, δ(p, 1) = {p}, δ(q, 0) = {r}, δ(q, 1) = {r}, δ(r, 0) = ∅, δ(r, 1) = ∅.",
    "difficulty": 3,
    "hints": [
      "Each DFA state is a subset of NFA states",
      "Start with the set containing just the NFA start state",
      "A DFA state is accepting if it contains any NFA accepting state"
    ],
    "solution": "**Subset Construction**\n\nStart state: {p}\n\n**Build transitions from reachable states:**\n\nFrom {p}:\n- On 0: δ({p}, 0) = δ(p, 0) = {p, q}\n- On 1: δ({p}, 1) = δ(p, 1) = {p}\n\nFrom {p, q}:\n- On 0: δ(p, 0) ∪ δ(q, 0) = {p, q} ∪ {r} = {p, q, r}\n- On 1: δ(p, 1) ∪ δ(q, 1) = {p} ∪ {r} = {p, r}\n\nFrom {p, q, r}:\n- On 0: δ(p, 0) ∪ δ(q, 0) ∪ δ(r, 0) = {p, q} ∪ {r} ∪ ∅ = {p, q, r}\n- On 1: δ(p, 1) ∪ δ(q, 1) ∪ δ(r, 1) = {p} ∪ {r} ∪ ∅ = {p, r}\n\nFrom {p, r}:\n- On 0: δ(p, 0) ∪ δ(r, 0) = {p, q} ∪ ∅ = {p, q}\n- On 1: δ(p, 1) ∪ δ(r, 1) = {p} ∪ ∅ = {p}\n\n**Resulting DFA:**\n- States: {{p}, {p,q}, {p,q,r}, {p,r}}\n- Start: {p}\n- Accept: {{p,q,r}, {p,r}} (contain r)\n- Transition table:\n| State    | 0       | 1     |\n|----------|---------|-------|\n| {p}      | {p,q}   | {p}   |\n| {p,q}    | {p,q,r} | {p,r} |\n| {p,q,r}  | {p,q,r} | {p,r} |\n| {p,r}    | {p,q}   | {p}   |"
  },
  {
    "id": "cs203-t1-ex4",
    "subjectId": "cs203",
    "topicId": "cs203-topic-1",
    "type": "written",
    "title": "DFA Minimization",
    "description": "Minimize the following DFA using the table-filling algorithm: States {A, B, C, D, E}, alphabet {0, 1}, start A, accept {C, E}. Transitions: δ(A,0)=B, δ(A,1)=C, δ(B,0)=D, δ(B,1)=E, δ(C,0)=B, δ(C,1)=C, δ(D,0)=D, δ(D,1)=E, δ(E,0)=B, δ(E,1)=C.",
    "difficulty": 5,
    "hints": [
      "First mark pairs where one is accepting and one is not",
      "Then iteratively mark pairs whose transitions lead to marked pairs",
      "Unmarked pairs at the end are equivalent"
    ],
    "solution": "**Table-Filling Algorithm**\n\n**Step 1: Initial marking (accepting vs non-accepting)**\nMark pairs (A,C), (A,E), (B,C), (B,E), (D,C), (D,E)\n\nTable after step 1:\n|   | A | B | C | D |\n|---|---|---|---|---|\n| B | - |   |   |   |\n| C | X | X |   |   |\n| D | - | - | X |   |\n| E | X | X | - | X |\n\n**Step 2: Iterative marking**\nCheck unmarked pairs:\n\n(A,B): δ(A,0)=B, δ(B,0)=D → check (B,D)\n       δ(A,1)=C, δ(B,1)=E → check (C,E) unmarked\n(B,D): δ(B,0)=D, δ(D,0)=D → check (D,D) same\n       δ(B,1)=E, δ(D,1)=E → check (E,E) same\n(A,D): δ(A,0)=B, δ(D,0)=D → check (B,D)\n       δ(A,1)=C, δ(D,1)=E → check (C,E) unmarked\n(C,E): δ(C,0)=B, δ(E,0)=B → check (B,B) same\n       δ(C,1)=C, δ(E,1)=C → check (C,C) same\n\nNo new marks in this iteration.\n\n**Step 3: Identify equivalent states**\nUnmarked pairs: (A,B), (A,D), (B,D), (C,E)\nEquivalence classes: {A,B,D}, {C,E}\n\n**Minimal DFA:**\nStates: {ABD, CE}\nStart: ABD\nAccept: {CE}\nTransitions:\n- δ(ABD, 0) = ABD (since B→D, D→D, A→B, all in {A,B,D})\n- δ(ABD, 1) = CE (since A→C, B→E, D→E, all in {C,E})\n- δ(CE, 0) = ABD (since C→B, E→B)\n- δ(CE, 1) = CE (since C→C, E→C)\n\n**Result: 2-state minimal DFA**"
  },
  {
    "id": "cs203-t1-ex5",
    "subjectId": "cs203",
    "topicId": "cs203-topic-1",
    "type": "written",
    "title": "State Elimination to Regular Expression",
    "description": "Convert the following DFA to a regular expression using state elimination: States {q₀, q₁, q₂}, start q₀, accept q₂. Transitions: δ(q₀, a) = q₁, δ(q₀, b) = q₀, δ(q₁, a) = q₁, δ(q₁, b) = q₂, δ(q₂, a) = q₂, δ(q₂, b) = q₂.",
    "difficulty": 5,
    "hints": [
      "Add new start and accept states with ε-transitions",
      "Eliminate intermediate states one by one",
      "When eliminating state q, update all paths that go through q"
    ],
    "solution": "**Step 1: Add new start (s) and accept (f) states**\nAdd ε from s to q₀ and ε from q₂ to f.\n\n**Step 2: Eliminate q₁**\nPaths through q₁:\n- q₀ →a q₁ →b q₂ becomes q₀ →(aa*b) q₂\n- q₁ has self-loop on a, so path is a·a*·b = aa*b\n\nAfter eliminating q₁:\n- s →ε q₀\n- q₀ →b q₀ (self-loop)\n- q₀ →(aa*b) q₂\n- q₂ →(a|b) q₂ (self-loop)\n- q₂ →ε f\n\n**Step 3: Eliminate q₀**\nPaths through q₀:\n- s →ε q₀ →(aa*b) q₂\n- q₀ has self-loop b, so path is ε·b*·(aa*b) = b*aa*b\n\nAfter eliminating q₀:\n- s →(b*aa*b) q₂\n- q₂ →(a|b) q₂\n- q₂ →ε f\n\n**Step 4: Eliminate q₂**\nPath from s to f through q₂:\n- s →(b*aa*b) q₂ →(a|b)* (self-loop) →ε f\n- Result: b*aa*b(a|b)*\n\n**Final Regular Expression: b*aa*b(a|b)***\n\nSimplified: b*a⁺b(a|b)* where a⁺ = aa*\n\nThis matches strings over {a,b} that contain the pattern: some b's, then one or more a's, then a b, then anything."
  },
  {
    "id": "cs203-t1-ex6",
    "subjectId": "cs203",
    "topicId": "cs203-topic-1",
    "type": "written",
    "title": "Product Construction for Intersection",
    "description": "Given two DFAs: M₁ accepts strings over {a,b} with an even number of a's, M₂ accepts strings with an odd number of b's. Construct a DFA accepting the intersection (even a's AND odd b's).",
    "difficulty": 3,
    "hints": [
      "Product construction creates states as pairs (q₁, q₂)",
      "Both components must accept for the product state to accept",
      "Each component transitions independently"
    ],
    "solution": "**DFA M₁ (even a's):**\nStates: {e_a, o_a} (even, odd count of a's)\nStart: e_a, Accept: {e_a}\nδ₁(e_a, a) = o_a, δ₁(e_a, b) = e_a\nδ₁(o_a, a) = e_a, δ₁(o_a, b) = o_a\n\n**DFA M₂ (odd b's):**\nStates: {e_b, o_b} (even, odd count of b's)\nStart: e_b, Accept: {o_b}\nδ₂(e_b, a) = e_b, δ₂(e_b, b) = o_b\nδ₂(o_b, a) = o_b, δ₂(o_b, b) = e_b\n\n**Product DFA M₁ × M₂:**\nStates: {(e_a,e_b), (e_a,o_b), (o_a,e_b), (o_a,o_b)}\nStart: (e_a, e_b)\nAccept: {(e_a, o_b)} (even a's AND odd b's)\n\nTransitions:\n| State      | a          | b          |\n|------------|------------|------------|\n| (e_a, e_b) | (o_a, e_b) | (e_a, o_b) |\n| (e_a, o_b) | (o_a, o_b) | (e_a, e_b) |\n| (o_a, e_b) | (e_a, e_b) | (o_a, o_b) |\n| (o_a, o_b) | (e_a, o_b) | (o_a, e_b) |\n\n**Verification:**\n- \"ab\": (e_a,e_b) →a (o_a,e_b) →b (o_a,o_b) → reject ✓\n- \"b\": (e_a,e_b) →b (e_a,o_b) → accept ✓\n- \"aab\": (e_a,e_b) →a (o_a,e_b) →a (e_a,e_b) →b (e_a,o_b) → accept ✓"
  },
  {
    "id": "cs203-t1-ex7",
    "subjectId": "cs203",
    "topicId": "cs203-topic-1",
    "type": "written",
    "title": "Pumping Lemma Proof",
    "description": "Use the pumping lemma to prove that L = {aⁿbⁿ | n ≥ 0} is not regular.",
    "difficulty": 3,
    "hints": [
      "Assume L is regular with pumping length p",
      "Choose a string in L with length at least p",
      "Show that pumping leads to a string not in L"
    ],
    "solution": "**Proof by contradiction using Pumping Lemma**\n\nAssume L = {aⁿbⁿ | n ≥ 0} is regular.\nThen the pumping lemma applies with some pumping length p.\n\n**Choose string:** s = aᵖbᵖ ∈ L with |s| = 2p ≥ p\n\n**By pumping lemma:** s = xyz where:\n1. |y| > 0\n2. |xy| ≤ p\n3. xyⁱz ∈ L for all i ≥ 0\n\n**Analyze the split:**\nSince |xy| ≤ p and s starts with p a's:\n- x = aʲ for some j ≥ 0\n- y = aᵏ for some k > 0 (since |y| > 0)\n- z = aᵖ⁻ʲ⁻ᵏbᵖ\n\nSo xy consists only of a's.\n\n**Pump with i = 0:**\nxy⁰z = xz = aʲaᵖ⁻ʲ⁻ᵏbᵖ = aᵖ⁻ᵏbᵖ\n\nSince k > 0, we have p - k < p a's but still p b's.\nTherefore aᵖ⁻ᵏbᵖ ∉ L (unequal counts).\n\n**Contradiction:** The pumping lemma guarantees xy⁰z ∈ L, but we showed xy⁰z ∉ L.\n\n**Conclusion:** Our assumption was wrong. L is not regular. ∎"
  },
  {
    "id": "cs203-t1-ex8",
    "subjectId": "cs203",
    "topicId": "cs203-topic-1",
    "type": "written",
    "title": "DFA for Divisibility",
    "description": "Design a DFA that accepts binary strings representing numbers divisible by 3. The input is read most-significant-bit first. For example, \"110\" represents 6 and should be accepted.",
    "difficulty": 5,
    "hints": [
      "Track the remainder when divided by 3",
      "Reading bit b after having value v gives new value 2v + b",
      "New remainder = (2 * old_remainder + bit) mod 3"
    ],
    "solution": "**DFA for divisibility by 3:**\n\n**Key insight:** If current value mod 3 = r, then after reading bit b:\nnew_remainder = (2r + b) mod 3\n\n**States:** {q₀, q₁, q₂} representing remainders 0, 1, 2\n**Start:** q₀ (remainder 0, empty string represents 0)\n**Accept:** {q₀} (remainder 0 means divisible by 3)\n\n**Transition table:**\n| State | 0 | 1 |\n|-------|---|---|\n| q₀    | q₀| q₁|\n| q₁    | q₂| q₀|\n| q₂    | q₁| q₂|\n\n**Derivation:**\nFrom q₀ (r=0): 0→(2·0+0) mod 3=0→q₀, 1→(2·0+1) mod 3=1→q₁\nFrom q₁ (r=1): 0→(2·1+0) mod 3=2→q₂, 1→(2·1+1) mod 3=0→q₀\nFrom q₂ (r=2): 0→(2·2+0) mod 3=1→q₁, 1→(2·2+1) mod 3=2→q₂\n\n**Verification:**\n- \"110\" (6): q₀→1→q₁→1→q₀→0→q₀ ✓ accept (6÷3=2)\n- \"101\" (5): q₀→1→q₁→0→q₂→1→q₂ ✗ reject (5÷3=1 r 2)\n- \"1001\" (9): q₀→1→q₁→0→q₂→0→q₁→1→q₀ ✓ accept (9÷3=3)\n- \"0\" (0): q₀→0→q₀ ✓ accept (0÷3=0)"
  },
  {
    "id": "cs203-t1-ex9",
    "subjectId": "cs203",
    "topicId": "cs203-topic-1",
    "type": "written",
    "title": "Closure Under Reversal",
    "description": "Prove that regular languages are closed under reversal. That is, if L is regular, then Lᴿ = {wᴿ | w ∈ L} is also regular.",
    "difficulty": 3,
    "hints": [
      "Start with a DFA for L",
      "Reverse all transitions",
      "Swap start and accepting states",
      "The result may be an NFA"
    ],
    "solution": "**Theorem:** If L is regular, then Lᴿ is regular.\n\n**Proof:**\nLet M = (Q, Σ, δ, q₀, F) be a DFA for L.\n\nConstruct NFA Mᴿ = (Q', Σ, δ', q₀', F') for Lᴿ:\n\n**Construction:**\n- Q' = Q ∪ {q_new} (add new start state)\n- q₀' = q_new\n- F' = {q₀} (old start becomes sole accept state)\n- δ' defined as:\n  - δ'(q_new, ε) = F (ε-transitions from new start to old accept states)\n  - δ'(q, a) = {p | δ(p, a) = q} (reverse all original transitions)\n\n**Why this works:**\n- Original: M accepts w iff δ*(q₀, w) ∈ F\n- Reversed: There's a path q₀ →w₁ q₁ →w₂ ... →wₙ qₙ with qₙ ∈ F\n- In Mᴿ: Start at q_new, ε-move to qₙ, then follow reversed path to q₀\n- Path in Mᴿ accepts wᴿ = wₙ...w₂w₁\n\n**Formal argument:**\nw ∈ L ⟺ δ*(q₀, w) ∈ F\n     ⟺ there's a path from q₀ to some f ∈ F on w\n     ⟺ there's a reversed path from f to q₀ on wᴿ in Mᴿ\n     ⟺ wᴿ ∈ L(Mᴿ)\n\nSince NFAs recognize exactly regular languages, Lᴿ is regular. ∎"
  },
  {
    "id": "cs203-t1-ex10",
    "subjectId": "cs203",
    "topicId": "cs203-topic-1",
    "type": "written",
    "title": "Myhill-Nerode Equivalence",
    "description": "Use the Myhill-Nerode theorem to show that L = {w ∈ {a,b}* | w has the same number of a's and b's} is not regular by identifying infinitely many equivalence classes.",
    "difficulty": 5,
    "hints": [
      "Find strings x, y where no matter what z you append, xz and yz behave differently",
      "Consider strings aⁿ for different n",
      "Show that aⁱ and aʲ are distinguishable for i ≠ j"
    ],
    "solution": "**Myhill-Nerode Theorem approach**\n\nThe Myhill-Nerode theorem states: L is regular iff it has finitely many equivalence classes under ≡_L, where:\nx ≡_L y ⟺ ∀z: xz ∈ L ⟺ yz ∈ L\n\n**Claim:** For L = {equal a's and b's}, the strings a, aa, aaa, ... are pairwise inequivalent.\n\n**Proof that aⁱ and aʲ are inequivalent for i ≠ j:**\n\nConsider the distinguishing suffix z = bⁱ:\n- aⁱbⁱ has i a's and i b's → aⁱbⁱ ∈ L\n- aʲbⁱ has j a's and i b's → since j ≠ i, aʲbⁱ ∉ L\n\nTherefore aⁱ ≢_L aʲ whenever i ≠ j.\n\n**Conclusion:**\nThe strings {a, aa, aaa, aaaa, ...} form infinitely many distinct equivalence classes under ≡_L.\n\nBy Myhill-Nerode theorem, L has infinitely many equivalence classes.\n\nTherefore L is not regular. ∎\n\n**Alternative view:**\nAny DFA for L would need distinct states for a, aa, aaa, ... since they need different numbers of b's to reach acceptance. Infinitely many states required → not finite → not a DFA."
  },
  {
    "id": "cs203-t1-ex11",
    "subjectId": "cs203",
    "topicId": "cs203-topic-1",
    "type": "written",
    "title": "NFA Design",
    "description": "Design an NFA (using nondeterminism effectively) that accepts strings over {a, b} where the third symbol from the end is \"a\". Explain why this is easier with an NFA than a DFA.",
    "difficulty": 3,
    "hints": [
      "NFA can \"guess\" when it's 3 symbols from the end",
      "Use nondeterminism to try starting the \"last 3\" check at every position",
      "Compare the state count to what a DFA would need"
    ],
    "solution": "**NFA for \"third symbol from end is a\":**\n\n**States:** {q₀, q₁, q₂, q₃}\n**Start:** q₀\n**Accept:** {q₃}\n**Alphabet:** {a, b}\n\n**Transitions:**\n- δ(q₀, a) = {q₀, q₁}  (stay or guess \"this a is third from end\")\n- δ(q₀, b) = {q₀}       (stay, waiting)\n- δ(q₁, a) = {q₂}       (second from end)\n- δ(q₁, b) = {q₂}       (second from end)\n- δ(q₂, a) = {q₃}       (last symbol)\n- δ(q₂, b) = {q₃}       (last symbol)\n\n**How it works:**\n1. q₀ is a \"waiting\" state that loops on any symbol\n2. When reading 'a' in q₀, nondeterministically also go to q₁\n3. q₁, q₂, q₃ count the last 3 positions\n4. Accept iff some branch reaches q₃ exactly at end\n\n**Why NFA is easier:**\n- NFA: 4 states, uses nondeterminism to \"guess\" the position\n- DFA: Must track all possible \"last 3 symbols\" combinations\n  - Need to remember last 3 symbols: 2³ = 8 states\n  - States like {aaa, aab, aba, abb, baa, bab, bba, bbb}\n  - Accept states: {aaa, aab, aba, abb} (third from end is a)\n\n**NFA advantage:** Nondeterminism lets us \"guess\" the right position instead of tracking all possibilities deterministically."
  },
  {
    "id": "cs203-t1-ex12",
    "subjectId": "cs203",
    "topicId": "cs203-topic-1",
    "type": "written",
    "title": "Distinguishing Strings",
    "description": "For the language L = {strings over {0,1} containing \"01\" as substring}, identify which of the following pairs of strings are distinguishable: (ε, 0), (0, 00), (01, 010). Provide distinguishing suffixes.",
    "difficulty": 3,
    "hints": [
      "Two strings x, y are distinguishable if some suffix z puts xz in L and yz out (or vice versa)",
      "Think about what each string \"needs\" to reach acceptance",
      "Consider minimal strings that would complete the pattern"
    ],
    "solution": "**Language:** L = {w ∈ {0,1}* | w contains \"01\"}\n\n**Analysis of pairs:**\n\n**Pair (ε, 0):**\n- ε needs \"01\" appended → \"01\" ∈ L ✓\n- 0 needs \"1\" appended → \"01\" ∈ L ✓\n\nTry z = \"1\":\n- ε·1 = \"1\" ∉ L (no \"01\")\n- 0·1 = \"01\" ∈ L ✓\n\n**Distinguishing suffix: z = \"1\"**\nTherefore ε and 0 are **distinguishable**.\n\n**Pair (0, 00):**\n- 0 needs at least \"1\" to get \"01\"\n- 00 needs at least \"1\" to get \"001\" which contains \"01\"\n\nTry z = \"1\":\n- 0·1 = \"01\" ∈ L ✓\n- 00·1 = \"001\" ∈ L ✓ (contains \"01\")\n\nFor any z: both 0z and 00z contain \"01\" iff z starts with \"1\" or z contains \"01\"\nThis is the same condition!\n\n**No distinguishing suffix exists.**\nTherefore 0 and 00 are **indistinguishable** (equivalent).\n\n**Pair (01, 010):**\nBoth already contain \"01\", so both are in L.\nFor any suffix z:\n- 01·z contains \"01\" → 01z ∈ L\n- 010·z contains \"01\" → 010z ∈ L\n\n**No distinguishing suffix exists.**\nTherefore 01 and 010 are **indistinguishable**.\n\n**Summary:**\n- (ε, 0): Distinguishable by z = \"1\"\n- (0, 00): Indistinguishable\n- (01, 010): Indistinguishable"
  },
  {
    "id": "cs203-t1-ex13",
    "subjectId": "cs203",
    "topicId": "cs203-topic-1",
    "type": "written",
    "title": "Closure Under Complement",
    "description": "Prove that regular languages are closed under complement. Given a DFA M for language L, construct a DFA for L̄ (the complement of L).",
    "difficulty": 1,
    "hints": [
      "The complement accepts exactly what the original rejects",
      "Simply swap accepting and non-accepting states",
      "This only works directly for DFAs, not NFAs"
    ],
    "solution": "**Theorem:** If L is regular, then L̄ = Σ* - L is regular.\n\n**Proof:**\nLet M = (Q, Σ, δ, q₀, F) be a DFA for L.\n\nConstruct DFA M̄ = (Q, Σ, δ, q₀, Q - F) for L̄.\n\n**Construction is trivial:**\n- Same states Q\n- Same alphabet Σ\n- Same transition function δ\n- Same start state q₀\n- **New accepting states:** F̄ = Q - F (complement of original accepting states)\n\n**Correctness:**\nFor any string w ∈ Σ*:\n- Let q = δ*(q₀, w) be the state reached after processing w\n- M accepts w ⟺ q ∈ F\n- M̄ accepts w ⟺ q ∈ (Q - F) ⟺ q ∉ F\n- Therefore: M̄ accepts w ⟺ M rejects w ⟺ w ∈ L̄ ✓\n\n**Important note:**\nThis construction requires a DFA (or complete NFA).\n- In a DFA, every string leads to exactly one state\n- In an NFA, rejecting means \"no accepting path exists\"\n- Simply swapping accept states in an NFA doesn't give the complement!\n\n**Example:**\nIf M accepts L = {strings ending in \"ab\"}\nThen M̄ accepts L̄ = {strings NOT ending in \"ab\"} = {ε, a, b, aa, ba, bb, ...}"
  },
  {
    "id": "cs203-t1-ex14",
    "subjectId": "cs203",
    "topicId": "cs203-topic-1",
    "type": "written",
    "title": "Extended Transition Function",
    "description": "For the DFA M with states {A, B}, start A, accept {B}, and δ(A,0)=B, δ(A,1)=A, δ(B,0)=A, δ(B,1)=B, compute δ*(A, w) for w = 0110 step by step. What language does M accept?",
    "difficulty": 1,
    "hints": [
      "δ* processes one symbol at a time",
      "δ*(q, wa) = δ(δ*(q, w), a)",
      "Look for a pattern in what makes M accept"
    ],
    "solution": "**Extended transition function computation:**\n\n**Definition:** δ*(q, ε) = q, and δ*(q, wa) = δ(δ*(q, w), a)\n\n**Computing δ*(A, 0110):**\n\nStep 1: δ*(A, ε) = A\n\nStep 2: δ*(A, 0) = δ(δ*(A, ε), 0) = δ(A, 0) = B\n\nStep 3: δ*(A, 01) = δ(δ*(A, 0), 1) = δ(B, 1) = B\n\nStep 4: δ*(A, 011) = δ(δ*(A, 01), 1) = δ(B, 1) = B\n\nStep 5: δ*(A, 0110) = δ(δ*(A, 011), 0) = δ(B, 0) = A\n\n**Result:** δ*(A, 0110) = A\n\nSince A ∉ F = {B}, the string \"0110\" is **rejected**.\n\n**Trace summary:**\nA →0→ B →1→ B →1→ B →0→ A (reject)\n\n**Identifying the language:**\nLooking at transitions:\n- From A: 0 goes to B (accept), 1 stays in A\n- From B: 1 stays in B (accept), 0 goes to A\n\nPattern: State B = \"last symbol was 0\" or \"sequence of 1s after a 0\"\nActually: A = even number of 0s, B = odd number of 0s\n\n**M accepts L = {w | w has an odd number of 0s}**\n\nVerification of \"0110\":\n- Number of 0s = 2 (even) → reject ✓"
  },
  {
    "id": "cs203-t1-ex15",
    "subjectId": "cs203",
    "topicId": "cs203-topic-1",
    "type": "written",
    "title": "Concatenation Closure",
    "description": "Prove that regular languages are closed under concatenation. Given NFAs M₁ for L₁ and M₂ for L₂, construct an NFA for L₁L₂.",
    "difficulty": 3,
    "hints": [
      "Run M₁, then at the end switch to M₂",
      "Add ε-transitions from M₁ accept states to M₂ start state",
      "Only M₂ accept states should be accepting in the result"
    ],
    "solution": "**Theorem:** If L₁ and L₂ are regular, then L₁L₂ is regular.\n\n**Proof by construction:**\n\nLet M₁ = (Q₁, Σ, δ₁, q₁, F₁) be an NFA for L₁\nLet M₂ = (Q₂, Σ, δ₂, q₂, F₂) be an NFA for L₂\n\nConstruct NFA M = (Q, Σ, δ, q₀, F) for L₁L₂:\n\n**Construction:**\n- Q = Q₁ ∪ Q₂ (disjoint union)\n- q₀ = q₁ (start of M₁)\n- F = F₂ (only M₂'s accept states)\n- δ defined as:\n  - δ(q, a) = δ₁(q, a) for q ∈ Q₁, a ∈ Σ\n  - δ(q, a) = δ₂(q, a) for q ∈ Q₂, a ∈ Σ\n  - δ(q, ε) = δ₁(q, ε) ∪ {q₂} for q ∈ F₁ (add ε to M₂ start)\n  - δ(q, ε) = δ₂(q, ε) for q ∈ Q₂\n\n**Intuition:**\n1. Start in M₁'s start state\n2. Process input through M₁\n3. When in an accepting state of M₁, can ε-transition to M₂\n4. Continue processing in M₂\n5. Accept when M₂ accepts\n\n**Correctness:**\nw ∈ L₁L₂ ⟺ w = xy where x ∈ L₁ and y ∈ L₂\n         ⟺ M₁ accepts x (reaching F₁) and M₂ accepts y\n         ⟺ M can process x in M₁ part, ε-move to M₂, process y\n         ⟺ M accepts w\n\n**Diagram:**\n```\n[M₁] --ε--> [M₂]\n  ↑           ↓\nstart       accept\n```"
  },
  {
    "id": "cs203-t1-ex16",
    "subjectId": "cs203",
    "topicId": "cs203-topic-1",
    "type": "written",
    "title": "Pumping Lemma for ww",
    "description": "Use the pumping lemma to prove that L = {ww | w ∈ {a,b}*} is not regular.",
    "difficulty": 5,
    "hints": [
      "Choose a specific string in L of length ≥ p",
      "Consider aᵖbᵖaᵖbᵖ",
      "Since |xy| ≤ p, y must be in the first block of a's"
    ],
    "solution": "**Proof that L = {ww | w ∈ {a,b}*} is not regular**\n\nAssume for contradiction that L is regular with pumping length p.\n\n**Choose string:** s = aᵖbᵖaᵖbᵖ\n\nCheck: s = ww where w = aᵖbᵖ, so s ∈ L ✓\nCheck: |s| = 4p ≥ p ✓\n\n**Apply pumping lemma:** s = xyz where |y| > 0, |xy| ≤ p\n\nSince |xy| ≤ p and s starts with p a's:\n- xy consists entirely of a's from the first block\n- x = aⁱ, y = aʲ for some i ≥ 0, j > 0, i + j ≤ p\n- z = aᵖ⁻ⁱ⁻ʲbᵖaᵖbᵖ\n\n**Pump with i = 2:**\nxy²z = aⁱ · aʲ · aʲ · aᵖ⁻ⁱ⁻ʲbᵖaᵖbᵖ\n     = aᵖ⁺ʲbᵖaᵖbᵖ\n\nThis has length 4p + j, which is odd when j is odd... but more importantly:\n\nFor xy²z to be in L, it must equal uu for some u.\n|xy²z| = 4p + j, so |u| = (4p + j)/2 = 2p + j/2\n\nIf j is odd, this isn't an integer, contradiction.\nIf j is even, u would need to be aᵖ⁺ʲ/²bᵖ, but then:\nuu = aᵖ⁺ʲ/²bᵖaᵖ⁺ʲ/²bᵖ ≠ aᵖ⁺ʲbᵖaᵖbᵖ = xy²z\n\n(The first half has p + j/2 a's, but xy²z has p + j a's in first part)\n\n**Contradiction:** xy²z ∉ L but pumping lemma guarantees xy²z ∈ L.\n\n**Conclusion:** L is not regular. ∎"
  },
  {
    "id": "cs203-t2-ex1",
    "subjectId": "cs203",
    "topicId": "cs203-topic-2",
    "type": "written",
    "title": "Basic Regular Expression",
    "description": "Write a regular expression for the language L = {w ∈ {a,b}* | w contains at least two a's}.",
    "difficulty": 1,
    "hints": [
      "Two a's with anything before, between, and after",
      "Use (a|b)* for \"any string\"",
      "Think: stuff, then a, then stuff, then a, then stuff"
    ],
    "solution": "**Regular Expression:** (a|b)*a(a|b)*a(a|b)*\n\n**Alternative forms:**\n- b*ab*a(a|b)*\n- (b|ab*a)*ab*a(a|b)* (more complex)\n\n**Explanation:**\n- (a|b)* matches any string (zero or more of 'a' or 'b')\n- First 'a' ensures at least one a\n- Second 'a' ensures at least two a's\n- Pattern: [anything] a [anything] a [anything]\n\n**Verification:**\n- \"aa\" ∈ L: (a|b)* = ε, first a, (a|b)* = ε, second a, (a|b)* = ε ✓\n- \"bab\" ∉ L: only one a ✗\n- \"abba\" ∈ L: ε, a, bb, a, ε ✓\n- \"baba\" ∈ L: b, a, b, a, ε ✓"
  },
  {
    "id": "cs203-t2-ex2",
    "subjectId": "cs203",
    "topicId": "cs203-topic-2",
    "type": "written",
    "title": "Language from Regular Expression",
    "description": "Describe in English the language defined by the regular expression (0|1)*1(0|1)(0|1). List all strings of length 4 or less in this language.",
    "difficulty": 1,
    "hints": [
      "Read the regex from left to right",
      "The suffix is fixed: 1 followed by two symbols",
      "Enumerate systematically"
    ],
    "solution": "**Language description:**\nAll binary strings where the third-to-last symbol is 1.\n\nOr equivalently: All binary strings ending with \"1\" followed by any two binary symbols.\n\n**Structure:**\n- (0|1)*: any prefix (including empty)\n- 1: third-to-last must be 1\n- (0|1): second-to-last is any bit\n- (0|1): last is any bit\n\n**Strings of length 4 or less:**\n\nLength 3 (minimum): 1xy where x,y ∈ {0,1}\n- 100, 101, 110, 111\n\nLength 4: z1xy where z,x,y ∈ {0,1}\n- 0100, 0101, 0110, 0111\n- 1100, 1101, 1110, 1111\n\n**Complete list:**\n{100, 101, 110, 111, 0100, 0101, 0110, 0111, 1100, 1101, 1110, 1111}\n\n**Total: 12 strings** (4 of length 3, 8 of length 4)\n\nNote: No valid strings of length < 3 exist (need at least 3 symbols for \"1 _ _\" pattern)."
  },
  {
    "id": "cs203-t2-ex3",
    "subjectId": "cs203",
    "topicId": "cs203-topic-2",
    "type": "written",
    "title": "Thompson's Construction",
    "description": "Apply Thompson's construction to build an NFA for the regular expression (ab|b)*. Show the construction step by step.",
    "difficulty": 3,
    "hints": [
      "Build NFAs for primitives first",
      "Combine using standard constructions for |, ·, and *",
      "The result will have many ε-transitions"
    ],
    "solution": "**Thompson's Construction for (ab|b)***\n\n**Step 1: Build NFA for 'a'**\n```\n(1) --a--> (2)\n```\n\n**Step 2: Build NFA for 'b'**\n```\n(3) --b--> (4)\n```\n\n**Step 3: Build NFA for 'ab' (concatenation)**\nConnect NFA(a) to NFA(b):\n```\n(1) --a--> (2) --ε--> (3) --b--> (4)\n```\n\n**Step 4: Build another NFA for 'b'**\n```\n(5) --b--> (6)\n```\n\n**Step 5: Build NFA for 'ab|b' (union)**\nAdd new start (7) with ε to both sub-NFAs, new accept (8):\n```\n        ε--> (1) --a--> (2) --ε--> (3) --b--> (4) --ε--\n       /                                              \\\n(7) --<                                                >-- (8)\n       \\                                              /\n        ε--> (5) --b--> (6) --ε---------------------ε--\n```\n\n**Step 6: Build NFA for (ab|b)* (Kleene star)**\nAdd new start (9) with ε to old start and new accept (10):\n```\n     ε (skip)\n(9) ---------> (10)\n |    ε         ^\n v              | ε (loop back)\n(7) --> [ab|b NFA] --> (8) --ε--> (10)\n                        |\n                        ε--> (7)\n```\n\n**Final NFA:**\n- States: {9, 7, 1, 2, 3, 4, 5, 6, 8, 10}\n- Start: 9\n- Accept: {10}\n- Transitions include ε-transitions for all connections"
  },
  {
    "id": "cs203-t2-ex4",
    "subjectId": "cs203",
    "topicId": "cs203-topic-2",
    "type": "written",
    "title": "State Elimination Details",
    "description": "Use state elimination to convert this DFA to a regex: States {1, 2, 3}, start 1, accept 3. δ(1,a)=2, δ(1,b)=3, δ(2,a)=3, δ(2,b)=1, δ(3,a)=3, δ(3,b)=3. Eliminate state 2 first.",
    "difficulty": 5,
    "hints": [
      "First convert to GNFA with new start and accept",
      "When eliminating state 2, find all paths through it",
      "Update edge labels to include detour through eliminated state"
    ],
    "solution": "**State Elimination Process**\n\n**Step 1: Convert to GNFA**\nAdd start state S (→ε→ 1) and accept state F (3 →ε→ F)\n\nInitial edges:\n- S →ε→ 1\n- 1 →a→ 2\n- 1 →b→ 3\n- 2 →a→ 3\n- 2 →b→ 1\n- 3 →(a|b)→ 3 (combine self-loops)\n- 3 →ε→ F\n\n**Step 2: Eliminate state 2**\nFind paths through state 2:\n- 1 →a→ 2 →a→ 3: path \"aa\"\n- 1 →a→ 2 →b→ 1: path \"ab\" (back to 1)\n\nNo self-loop on state 2, so no iteration.\n\nAfter elimination:\n- S →ε→ 1\n- 1 →b→ 3 (direct)\n- 1 →aa→ 3 (through 2)\n- 1 →ab→ 1 (through 2, back to 1)\n- 3 →(a|b)→ 3\n- 3 →ε→ F\n\nCombine 1→3 edges: 1 →(b|aa)→ 3\n\n**Step 3: Eliminate state 1**\nPaths through 1:\n- S →ε→ 1 →(b|aa)→ 3\n- 1 has self-loop: 1 →ab→ 1\n\nPath S to 3 through 1: ε·(ab)*·(b|aa) = (ab)*(b|aa)\n\nAfter elimination:\n- S →(ab)*(b|aa)→ 3\n- 3 →(a|b)→ 3\n- 3 →ε→ F\n\n**Step 4: Eliminate state 3**\nPath S to F through 3:\n- S →(ab)*(b|aa)→ 3 →(a|b)*→ 3 →ε→ F\n\n**Final regex: (ab)*(b|aa)(a|b)***\n\n**Verification:**\n- \"b\": (ab)⁰·b·(a|b)⁰ ✓\n- \"aa\": (ab)⁰·aa·(a|b)⁰ ✓\n- \"aba\": (ab)¹·a·(a|b)⁰ — but \"aba\" needs checking: 1→a→2→b→1→a→2→a→3 ✓"
  },
  {
    "id": "cs203-t2-ex5",
    "subjectId": "cs203",
    "topicId": "cs203-topic-2",
    "type": "written",
    "title": "Algebraic Simplification",
    "description": "Simplify the following regular expression using algebraic identities: (a|b)*(a|ε)(b|ε) Give the simplified form and list the identities used.",
    "difficulty": 3,
    "hints": [
      "Expand using distributivity",
      "Use (a|b)* properties",
      "a*a = a* · a, and (a|b)*a ⊆ (a|b)*"
    ],
    "solution": "**Simplification of (a|b)*(a|ε)(b|ε)**\n\n**Step 1: Expand (a|ε)(b|ε)**\nUsing distributivity: (r|s)(t|u) = rt|ru|st|su\n\n(a|ε)(b|ε) = ab | a·ε | ε·b | ε·ε\n           = ab | a | b | ε\n\n**Step 2: Substitute back**\n(a|b)*(ab | a | b | ε)\n\n**Step 3: Distribute (a|b)***\n= (a|b)*ab | (a|b)*a | (a|b)*b | (a|b)*ε\n\n**Step 4: Simplify each term**\n- (a|b)*ε = (a|b)* (identity: r·ε = r)\n- (a|b)*a ⊆ (a|b)* (since (a|b)*a matches strings ending in 'a')\n- Similarly (a|b)*b ⊆ (a|b)* and (a|b)*ab ⊆ (a|b)*\n\n**Step 5: Key insight**\n(a|b)* already contains all strings over {a,b}!\n\nSo: (a|b)*ab | (a|b)*a | (a|b)*b | (a|b)* = (a|b)*\n\n(Union with subsets doesn't change the result)\n\n**Final simplified form: (a|b)***\n\n**Identities used:**\n1. Distributivity: r(s|t) = rs|rt\n2. Identity: r·ε = r\n3. Absorption: r | rs = r for any s (when (a|b)*X ⊆ (a|b)*)\n4. Idempotence: r | r = r"
  },
  {
    "id": "cs203-t2-ex6",
    "subjectId": "cs203",
    "topicId": "cs203-topic-2",
    "type": "written",
    "title": "Regex for Fixed Length",
    "description": "Write a regular expression for all binary strings of length exactly 4 that do not contain \"00\" as a substring.",
    "difficulty": 3,
    "hints": [
      "Enumerate patterns or think about \"never two 0s in a row\"",
      "After a 0, must have a 1",
      "Build up valid 4-symbol sequences"
    ],
    "solution": "**Analysis:**\nNeed binary strings of length 4 with no \"00\" substring.\n\n**Approach 1: Systematic enumeration**\nAfter 0, must see 1. After 1, can see 0 or 1.\n\nValid patterns (where 0 can't be followed by 0):\nStarting with 1: 1___ where each position follows the rule\nStarting with 0: 01__ where each position follows the rule\n\n**Building the regex:**\nLet's think of it as: (1|01)* but constrained to length 4.\n\nPossible 4-character strings:\n- 1111 ✓\n- 1110 ✓\n- 1101 ✓\n- 1100 ✗ (has \"00\")\n- 1011 ✓\n- 1010 ✓\n- 1001 ✗\n- 1000 ✗\n- 0111 ✓\n- 0110 ✓\n- 0101 ✓\n- 0100 ✗\n- 0011 ✗\n- 0010 ✗\n- 0001 ✗\n- 0000 ✗\n\nValid: {1111, 1110, 1101, 1011, 1010, 0111, 0110, 0101}\n\n**Regular expression (explicit union):**\n1111 | 1110 | 1101 | 1011 | 1010 | 0111 | 0110 | 0101\n\n**Factored form:**\n= 1(111|110|101|011|010) | 01(11|10|01)\n= 1(1(11|10|01)|01(1|0)) | 01(1(1|0)|01)\n\n**Cleaner factored form:**\n(1|01)(1|01)(1|0)(1|ε) where we ensure length 4...\n\nActually simplest: **1(1|01)*(1|0) constrained to length 4:**\n\n**(11|101|01)(11|10|01) | 1(11|10|01)(1|0)**\n\nOr just enumerate: **1111|1110|1101|1011|1010|0111|0110|0101**"
  },
  {
    "id": "cs203-t2-ex7",
    "subjectId": "cs203",
    "topicId": "cs203-topic-2",
    "type": "written",
    "title": "Decision Problem - Emptiness",
    "description": "Describe an algorithm to decide whether the language of a given DFA is empty. What is the time complexity? Apply it to determine if the DFA with states {1,2,3}, start 1, accept {3}, δ(1,a)=2, δ(1,b)=1, δ(2,a)=2, δ(2,b)=2 has an empty language.",
    "difficulty": 3,
    "hints": [
      "Empty language means no accepting state is reachable",
      "Use graph reachability (BFS or DFS)",
      "The DFA is a directed graph"
    ],
    "solution": "**Algorithm for DFA Emptiness:**\n\n**Input:** DFA M = (Q, Σ, δ, q₀, F)\n**Output:** TRUE if L(M) = ∅, FALSE otherwise\n\n**Algorithm:**\n1. Perform BFS/DFS from start state q₀\n2. Mark all reachable states\n3. Return TRUE if F ∩ (reachable states) = ∅\n4. Return FALSE otherwise\n\n**Pseudocode:**\n```\nfunction isEmpty(M):\n    visited = {q₀}\n    queue = [q₀]\n    while queue not empty:\n        q = queue.dequeue()\n        for each a in Σ:\n            p = δ(q, a)\n            if p not in visited:\n                visited.add(p)\n                queue.enqueue(p)\n    return (F ∩ visited) == ∅\n```\n\n**Time Complexity:** O(|Q| × |Σ|)\n- Visit each state at most once\n- Check |Σ| transitions per state\n- Linear in DFA size\n\n**Application to given DFA:**\n- States: {1, 2, 3}, Start: 1, Accept: {3}\n- δ(1,a)=2, δ(1,b)=1, δ(2,a)=2, δ(2,b)=2\n\n**BFS from state 1:**\n- Initial: visited = {1}, queue = [1]\n- Process 1: δ(1,a)=2, δ(1,b)=1\n  - Add 2: visited = {1,2}, queue = [2]\n- Process 2: δ(2,a)=2, δ(2,b)=2\n  - 2 already visited\n\n**Reachable states: {1, 2}**\n**Accept states: {3}**\n**Intersection: ∅**\n\n**Result: Language is EMPTY (TRUE)**\n\nState 3 is unreachable from the start state, so no string is accepted."
  },
  {
    "id": "cs203-t2-ex8",
    "subjectId": "cs203",
    "topicId": "cs203-topic-2",
    "type": "written",
    "title": "Decision Problem - Universality",
    "description": "Describe an algorithm to decide whether a given DFA accepts all strings (L = Σ*). What is the time complexity?",
    "difficulty": 3,
    "hints": [
      "L = Σ* iff L̄ = ∅",
      "Complement a DFA by swapping accept and non-accept states",
      "Then test for emptiness"
    ],
    "solution": "**Algorithm for DFA Universality:**\n\n**Input:** DFA M = (Q, Σ, δ, q₀, F)\n**Output:** TRUE if L(M) = Σ*, FALSE otherwise\n\n**Key insight:** L(M) = Σ* ⟺ L̄(M) = ∅\n\n**Algorithm:**\n1. Construct complement DFA M̄:\n   - Same states, transitions, and start state\n   - Accept states F̄ = Q - F\n2. Test if L(M̄) = ∅ using emptiness algorithm\n3. Return TRUE if empty (original is universal)\n4. Return FALSE otherwise\n\n**Pseudocode:**\n```\nfunction isUniversal(M):\n    // Construct complement\n    M_bar = (Q, Σ, δ, q₀, Q - F)\n\n    // Test emptiness of complement\n    return isEmpty(M_bar)\n```\n\n**Detailed steps for emptiness check:**\n```\nfunction isUniversal(M):\n    visited = {q₀}\n    queue = [q₀]\n    rejecting_states = Q - F\n\n    while queue not empty:\n        q = queue.dequeue()\n        if q in rejecting_states:\n            // Found reachable rejecting state\n            // So some string is rejected\n            return FALSE\n        for each a in Σ:\n            p = δ(q, a)\n            if p not in visited:\n                visited.add(p)\n                queue.enqueue(p)\n\n    // No rejecting state reachable\n    return TRUE\n```\n\n**Time Complexity:** O(|Q| × |Σ|)\n- Same as emptiness test\n- No actual construction needed\n- Just check if any non-accepting state is reachable\n\n**Correctness:**\n- L(M) = Σ* ⟺ no string is rejected\n- String w is rejected ⟺ δ*(q₀, w) ∉ F\n- ⟺ some non-accepting state is reachable\n- ⟺ complement is non-empty"
  },
  {
    "id": "cs203-t2-ex9",
    "subjectId": "cs203",
    "topicId": "cs203-topic-2",
    "type": "written",
    "title": "Homomorphism on Regular Languages",
    "description": "Define a homomorphism h: {a,b}* → {0,1}* where h(a) = 01 and h(b) = 1. If L is the regular language a*b, what is h(L)? Write a regular expression for h(L).",
    "difficulty": 3,
    "hints": [
      "Apply h to each symbol in strings of L",
      "h extends to strings: h(w₁w₂) = h(w₁)h(w₂)",
      "Think about what strings in a*b look like"
    ],
    "solution": "**Homomorphism h: {a,b}* → {0,1}***\n- h(a) = 01\n- h(b) = 1\n\n**Language L = a*b**\nL = {b, ab, aab, aaab, ...} = {aⁿb | n ≥ 0}\n\n**Computing h(L):**\nh(aⁿb) = h(a)ⁿh(b) = (01)ⁿ1\n\nSo h(L) = {(01)ⁿ1 | n ≥ 0} = {1, 011, 01011, 0101011, ...}\n\n**Regular expression for h(L):**\n**(01)*1**\n\n**Verification:**\n- h(b) = h(a⁰b) = (01)⁰·1 = 1 ✓\n- h(ab) = h(a¹b) = (01)¹·1 = 011 ✓\n- h(aab) = h(a²b) = (01)²·1 = 01011 ✓\n\n**Properties demonstrated:**\n1. Homomorphism maps each symbol to a string\n2. Extends to strings by concatenation\n3. Regular languages are closed under homomorphism\n4. If L is described by regex r, h(L) is described by h(r) where h is applied to each symbol\n\nFor L = a*b:\n- h(a*b) = h(a)*h(b) = (01)*1 ✓"
  },
  {
    "id": "cs203-t2-ex10",
    "subjectId": "cs203",
    "topicId": "cs203-topic-2",
    "type": "written",
    "title": "Inverse Homomorphism",
    "description": "For the same homomorphism h(a)=01, h(b)=1, let L = {011, 0111}. Compute h⁻¹(L) = {w | h(w) ∈ L}.",
    "difficulty": 5,
    "hints": [
      "Find all strings w where h(w) equals something in L",
      "h(w) = 011 means w must expand to 011",
      "Try different combinations of a and b"
    ],
    "solution": "**Inverse Homomorphism h⁻¹(L)**\n\nh(a) = 01, h(b) = 1\nL = {011, 0111}\n\nNeed to find all w ∈ {a,b}* such that h(w) ∈ L.\n\n**For h(w) = 011:**\nNeed h(w) to have length 3 and be \"011\".\n\nPossible factorizations of \"011\":\n- (01)(1) → h(a)h(b) = h(ab), length(ab) = 2 ✓\n- (0)(11) → no single symbol maps to 0 ✗\n- (011) → no single symbol maps to 011 ✗\n- (0)(1)(1) → no single symbol maps to 0 ✗\n\nSo h(ab) = 011 → ab ∈ h⁻¹(L) ✓\n\n**For h(w) = 0111:**\nNeed h(w) to have length 4 and be \"0111\".\n\nPossible factorizations:\n- (01)(1)(1) → h(a)h(b)h(b) = h(abb), check: h(abb) = 01·1·1 = 0111 ✓\n- (01)(11) → h(a)(11), but no symbol maps to 11 ✗\n- (0)(111) → no symbol maps to 0 ✗\n- (0111) → no symbol maps to 0111 ✗\n\nSo h(abb) = 0111 → abb ∈ h⁻¹(L) ✓\n\n**Are there other possibilities?**\n- bb: h(bb) = 11 ≠ 011, 0111 ✗\n- aa: h(aa) = 0101 ≠ 011, 0111 ✗\n- a: h(a) = 01 ≠ 011, 0111 ✗\n- b: h(b) = 1 ≠ 011, 0111 ✗\n- aab: h(aab) = 01011 ≠ 011, 0111 ✗\n- bab: h(bab) = 1011 ≠ 011, 0111 ✗\n\n**h⁻¹(L) = {ab, abb}**"
  },
  {
    "id": "cs203-t2-ex11",
    "subjectId": "cs203",
    "topicId": "cs203-topic-2",
    "type": "written",
    "title": "Regular Expression Equivalence",
    "description": "Prove or disprove: The regular expressions (a|b)* and a*b*a*b* are equivalent (describe the same language).",
    "difficulty": 3,
    "hints": [
      "What languages do they describe?",
      "Is every string in one also in the other?",
      "Try to find a counterexample"
    ],
    "solution": "**Claim: (a|b)* and a*b*a*b* are NOT equivalent.**\n\n**Language of (a|b)*:**\nAll strings over {a,b}, including:\n- ε, a, b, aa, ab, ba, bb, aaa, aba, bab, ...\n- Any combination of a's and b's in any order\n\n**Language of a*b*a*b*:**\nStrings of form: (some a's)(some b's)(some a's)(some b's)\n- ε (all parts empty) ✓\n- a, aa, aaa, ... (first part only) ✓\n- b, bb, ... (second part only) ✓\n- ab, aab, abb, ... ✓\n- ba, bba, ... (third and fourth parts) ✓\n- aba, abab, ... ✓\n\n**Finding a counterexample:**\nConsider \"bab\":\n- In (a|b)*: Yes (b·a·b) ✓\n- In a*b*a*b*: Need to split as aⁱbʲaᵏbˡ\n  - b = a⁰b¹a⁰b⁰? Then \"ab\" part is b, need to place \"ab\" at end...\n  - Actually: a⁰b¹a¹b¹ = \"bab\" ✓\n\nConsider \"abab\":\n- In (a|b)*: Yes ✓\n- In a*b*a*b*: a¹b¹a¹b¹ = \"abab\" ✓\n\nConsider \"baba\":\n- In (a|b)*: Yes ✓\n- In a*b*a*b*: We need aⁱbʲaᵏbˡ = \"baba\"\n  - If i=0: bʲaᵏbˡ = \"baba\", so j=1, then aᵏbˡ = \"aba\"\n  - Then k≥1 (need 'a'), k=1 gives bˡ = \"ba\" - but b* can't produce \"ba\"!\n\n**Counterexample found: \"baba\" ∈ (a|b)* but baba ∉ a*b*a*b***\n\n**Proof that baba ∉ a*b*a*b*:**\nAny string in a*b*a*b* has at most 2 \"runs\" of a's (consecutive a's) and at most 2 \"runs\" of b's, with structure (a-run)(b-run)(a-run)(b-run).\n\n\"baba\" has structure: b, a, b, a = 4 alternations, which requires 2 b-runs with an a-run between them. This pattern (b-a-b-a) cannot fit a*b*a*b*.\n\n**Conclusion: The expressions are NOT equivalent. ∎**"
  },
  {
    "id": "cs203-t2-ex12",
    "subjectId": "cs203",
    "topicId": "cs203-topic-2",
    "type": "written",
    "title": "Kleene Star Properties",
    "description": "Prove the identity (r*)* = r* for regular expressions. That is, show L((r*)*) = L(r*).",
    "difficulty": 3,
    "hints": [
      "Show inclusion in both directions",
      "(r*)* = concatenations of concatenations of L(r) strings",
      "Concatenation is associative"
    ],
    "solution": "**Theorem: (r*)* = r***\n\nLet L = L(r). We show L((r*)*) = L(r*).\n\n**Recall definitions:**\n- L(r*) = L* = {ε} ∪ L ∪ LL ∪ LLL ∪ ... = ⋃_{n≥0} Lⁿ\n- L((r*)*) = (L*)* = ⋃_{n≥0} (L*)ⁿ\n\n**Part 1: L* ⊆ (L*)***\n- L* is the set of all concatenations of L strings\n- (L*)* is the set of all concatenations of L* strings\n- Since ε ∈ L*, we have L* = L* · {ε} ⊆ L* · L* ⊆ (L*)*\n- Therefore L* ⊆ (L*)* ✓\n\n**Part 2: (L*)* ⊆ L***\nLet w ∈ (L*)*.\nThen w = w₁w₂...wₖ where each wᵢ ∈ L*.\n\nEach wᵢ ∈ L* means wᵢ = uᵢ₁uᵢ₂...uᵢₘᵢ where each uᵢⱼ ∈ L.\n\nSo w = u₁₁u₁₂...u₁ₘ₁ u₂₁u₂₂...u₂ₘ₂ ... uₖ₁...uₖₘₖ\n\nThis is a concatenation of strings from L, hence w ∈ L*.\n\nTherefore (L*)* ⊆ L* ✓\n\n**Conclusion: (L*)* = L*, so (r*)* = r* ∎**\n\n**Intuition:**\n- L* = \"any number of L strings concatenated\"\n- (L*)* = \"any number of (any number of L strings) concatenated\"\n- But concatenating concatenations is just concatenation\n- So (L*)* = L*\n\n**Analogous to arithmetic:** ((n×a) + (m×a)) is still a multiple of a."
  },
  {
    "id": "cs203-t2-ex13",
    "subjectId": "cs203",
    "topicId": "cs203-topic-2",
    "type": "written",
    "title": "Decision Problem - Infiniteness",
    "description": "Describe an algorithm to decide whether a given DFA accepts infinitely many strings. What is the key insight?",
    "difficulty": 3,
    "hints": [
      "When can a DFA accept infinitely many strings?",
      "Think about cycles in the transition graph",
      "A cycle reachable from start and reaching accept gives infinity"
    ],
    "solution": "**Algorithm for DFA Infiniteness:**\n\n**Key Insight:**\nL(M) is infinite ⟺ M has a cycle on a path from start to some accepting state.\n\nIf there's such a cycle, we can traverse it any number of times, generating infinitely many accepted strings.\n\n**Algorithm:**\n\n**Input:** DFA M = (Q, Σ, δ, q₀, F)\n**Output:** TRUE if |L(M)| = ∞, FALSE otherwise\n\n1. Find all states reachable from q₀ (forward reachability)\n2. Find all states that can reach some f ∈ F (backward reachability)\n3. Let U = (forward reachable) ∩ (backward reachable) = \"useful\" states\n4. Check if the subgraph induced by U contains a cycle\n5. Return TRUE if cycle exists, FALSE otherwise\n\n**Pseudocode:**\n```\nfunction isInfinite(M):\n    // Step 1: Forward reachability from q₀\n    forward = BFS_forward(q₀)\n\n    // Step 2: Backward reachability to F\n    backward = BFS_backward(F)\n\n    // Step 3: Useful states\n    useful = forward ∩ backward\n\n    // Step 4: Check for cycle in useful subgraph\n    return hasCycle(useful, δ)\n```\n\n**Time Complexity:** O(|Q| × |Σ|)\n- BFS is O(|Q| × |Σ|)\n- Cycle detection is O(|Q| + |E|) = O(|Q| × |Σ|)\n\n**Alternative using pumping:**\nL is infinite ⟺ ∃ string w ∈ L with |Q| ≤ |w| < 2|Q|\n\nThis follows from pumping lemma: if L has strings of all lengths ≥ |Q|, it's infinite.\n\nSo: enumerate/test strings of length |Q| to 2|Q|-1.\nLess efficient but theoretically interesting."
  },
  {
    "id": "cs203-t2-ex14",
    "subjectId": "cs203",
    "topicId": "cs203-topic-2",
    "type": "written",
    "title": "Regex to NFA Size",
    "description": "If a regular expression r has length n (total symbols and operators), how many states does Thompson's construction produce? Give a tight bound.",
    "difficulty": 3,
    "hints": [
      "Count states added for each construction",
      "Base cases add 2 states each",
      "Union adds 2, concatenation adds 0, star adds 2"
    ],
    "solution": "**Thompson's Construction State Count**\n\n**State counts per construction:**\n\n1. **Base case ∅:** 2 states (start, accept)\n2. **Base case ε:** 2 states (start, accept with ε-edge)\n3. **Base case a (symbol):** 2 states (start →a→ accept)\n4. **Union r|s:** 2 new states (new start and accept)\n5. **Concatenation rs:** 0 new states (merge accept of r with start of s)\n6. **Kleene star r*:** 2 new states (new start and accept)\n\n**Counting for expression of length n:**\n\nAn expression of length n has:\n- k symbols (base cases), where k ≤ n\n- Operators: at most n - k operators (union, concat, star)\n\nEach symbol creates 2 states.\nEach union/star creates 2 additional states.\nConcatenation creates 0 additional states.\n\n**Upper bound analysis:**\n- Worst case: all base cases are symbols, rest are | or *\n- Symbols: ≤ n, each adds 2 states\n- Operators | and *: each adds 2 states\n\n**Tight bound: O(n) states**\n\nMore precisely: ≤ 2n states\n\n**Proof:**\n- Expression with n nodes in syntax tree\n- Each leaf (symbol, ε, ∅) contributes 2 states\n- Each internal node (|, ·, *) contributes at most 2 states\n- Total internal nodes ≤ number of leaves - 1\n- So total states ≤ 2 × (leaves) + 2 × (leaves - 1) ≤ 4 × leaves ≤ 2n\n\n**Exact count:** The NFA from Thompson's construction has exactly 2n states where n is the number of nodes in the regex syntax tree (counting each symbol and operator).\n\nFor a regex string of length n, this is **O(n) states**."
  },
  {
    "id": "cs203-t2-ex15",
    "subjectId": "cs203",
    "topicId": "cs203-topic-2",
    "type": "written",
    "title": "Quotient Operation",
    "description": "For languages L₁ and L₂, define L₁/L₂ = {x | ∃y ∈ L₂: xy ∈ L₁} (right quotient). Prove that if L₁ is regular and L₂ is any language, then L₁/L₂ is regular.",
    "difficulty": 5,
    "hints": [
      "Build from a DFA for L₁",
      "What states should be accepting in the quotient DFA?",
      "State q should accept if some y ∈ L₂ leads from q to accepting"
    ],
    "solution": "**Theorem:** If L₁ is regular, then L₁/L₂ is regular for any L₂.\n\n**Proof by construction:**\n\nLet M = (Q, Σ, δ, q₀, F) be a DFA for L₁.\n\nConstruct DFA M' = (Q, Σ, δ, q₀, F') for L₁/L₂ where:\n- F' = {q ∈ Q | ∃y ∈ L₂: δ*(q, y) ∈ F}\n\nThat is, a state is accepting in M' if some string from L₂ leads from it to an accepting state of M.\n\n**Correctness:**\n\nx ∈ L₁/L₂ ⟺ ∃y ∈ L₂: xy ∈ L₁\n           ⟺ ∃y ∈ L₂: δ*(q₀, xy) ∈ F\n           ⟺ ∃y ∈ L₂: δ*(δ*(q₀, x), y) ∈ F\n           ⟺ δ*(q₀, x) ∈ F'\n           ⟺ M' accepts x\n\n**Why this works:**\n- M' has the same states and transitions as M\n- Only the accepting states change\n- A state q is in F' if reaching q gives \"hope\" of accepting with some suffix from L₂\n\n**Key insight:**\nThe quotient construction only changes F, so:\n- Number of states: same as M\n- Transitions: same as M\n- Only accepting states are redefined\n\n**Regularity preserved:**\nEven if L₂ is non-regular (or even non-recursive), F' is still a finite set of states (subset of Q).\n\nThe construction doesn't need to \"compute\" anything about L₂ at runtime - F' is determined statically.\n\n**Example:**\nL₁ = a*b*, L₂ = {aⁿbⁿ | n ≥ 0}\nL₁/L₂ = {x | ∃y ∈ L₂: xy ∈ a*b*}\n      = {aⁱ | ∃aⁿbⁿ: aⁱaⁿbⁿ ∈ a*b*}\n      = a* (since aⁱ⁺ⁿbⁿ ∈ a*b* for any i, n)\n\nL₁/L₂ is regular even though L₂ is not! ∎"
  },
  {
    "id": "cs203-t2-ex16",
    "subjectId": "cs203",
    "topicId": "cs203-topic-2",
    "type": "written",
    "title": "Concatenation Star Identity",
    "description": "Prove or disprove: (rs)* = ε | r(sr)*s for all regular expressions r and s.",
    "difficulty": 5,
    "hints": [
      "Think about what strings are in (rs)*",
      "Think about what strings are in ε | r(sr)*s",
      "Try specific examples first"
    ],
    "solution": "**Claim: (rs)* = ε | r(sr)*s is FALSE in general.**\n\n**Analysis of both sides:**\n\n**Left side: (rs)***\nL((rs)*) = {ε, rs, rsrs, rsrsrs, ...} = {(rs)ⁿ | n ≥ 0}\n\n**Right side: ε | r(sr)*s**\nL(ε | r(sr)*s) = {ε} ∪ L(r(sr)*s)\n               = {ε} ∪ {r·(sr)ⁿ·s | n ≥ 0}\n               = {ε, rs, rsrs, rsrsrs, ...} where each has r at front, s at back\n\nWait, let's compute more carefully:\n- r(sr)⁰s = rs\n- r(sr)¹s = rsrs\n- r(sr)²s = rsrsrs\n\nSo L(ε | r(sr)*s) = {ε, rs, rsrs, rsrsrs, ...}\n\nThis looks the same as (rs)*!\n\n**Let's verify the identity:**\n\nFor n ≥ 1: (rs)ⁿ vs r(sr)ⁿ⁻¹s\n\n(rs)ⁿ = (rs)(rs)...(rs) [n times]\nr(sr)ⁿ⁻¹s = r(sr)(sr)...(sr)s [n-1 copies of sr]\n          = r·s·r·s·r...·s·r·s [this has n r's and n s's alternating]\n          = (rs)(rs)...(rs) [n times] ✓\n\n**The identity appears to be TRUE!**\n\n**Formal Proof:**\n\n**Part 1: (rs)* ⊆ ε | r(sr)*s**\n- ε ∈ (rs)* and ε ∈ {ε | r(sr)*s} ✓\n- (rs)ⁿ for n ≥ 1:\n  - (rs)ⁿ = rs·(rs)ⁿ⁻¹ = r·(s·r)ⁿ⁻¹·s = r(sr)ⁿ⁻¹s ∈ r(sr)*s ✓\n\n**Part 2: ε | r(sr)*s ⊆ (rs)***\n- ε ∈ (rs)* ✓\n- r(sr)ⁿs = r·(sr)ⁿ·s = (rs)·(rs)ⁿ⁻¹·(rs)...\n\nHmm, let me recompute: r(sr)ⁿs with n=1:\nrsrs = r·sr·s. And (rs)² = rsrs ✓\n\n**Conclusion: The identity (rs)* = ε | r(sr)*s is TRUE. ∎**"
  },
  {
    "id": "cs203-t3-ex1",
    "subjectId": "cs203",
    "topicId": "cs203-topic-3",
    "type": "written",
    "title": "Basic CFG Design",
    "description": "Design a context-free grammar for the language L = {aⁿbⁿ | n ≥ 1}. Show derivations for \"ab\" and \"aabb\".",
    "difficulty": 1,
    "hints": [
      "Each \"a\" must be matched with a \"b\"",
      "Think recursively: add one \"a\" and one \"b\" at a time",
      "Need to ensure at least one of each"
    ],
    "solution": "**Context-Free Grammar for L = {aⁿbⁿ | n ≥ 1}**\n\n**Grammar G:**\nS → aSb | ab\n\n**Alternative (using n ≥ 0 base):**\nS → aSb | ε would give {aⁿbⁿ | n ≥ 0}\nFor n ≥ 1: S → aSb | ab\n\n**Derivation of \"ab\":**\nS ⇒ ab\n\n**Derivation of \"aabb\":**\nS ⇒ aSb ⇒ aabb\n\n**Derivation of \"aaabbb\":**\nS ⇒ aSb ⇒ aaSbb ⇒ aaabbb\n\n**Correctness argument:**\n\n**Claim:** L(G) = {aⁿbⁿ | n ≥ 1}\n\n*Part 1: L(G) ⊆ {aⁿbⁿ | n ≥ 1}*\n- Every derivation starts with S\n- S → ab produces a¹b¹ ✓\n- S → aSb adds one 'a' prefix and one 'b' suffix\n- By induction: after k applications of S → aSb followed by S → ab:\n  - Result is aᵏ⁺¹bᵏ⁺¹ ∈ {aⁿbⁿ | n ≥ 1}\n\n*Part 2: {aⁿbⁿ | n ≥ 1} ⊆ L(G)*\n- For any n ≥ 1, apply S → aSb (n-1) times, then S → ab\n- This produces aⁿbⁿ\n\n**Parse tree for \"aabb\":**\n```\n    S\n   /|\\\n  a S b\n    |\n   ab\n```"
  },
  {
    "id": "cs203-t3-ex2",
    "subjectId": "cs203",
    "topicId": "cs203-topic-3",
    "type": "written",
    "title": "Grammar for Palindromes",
    "description": "Design a CFG for the language of all palindromes over {a, b}. Include derivations for \"aba\" and \"abba\".",
    "difficulty": 1,
    "hints": [
      "A palindrome reads the same forwards and backwards",
      "Add the same symbol to both ends",
      "Handle both odd and even length palindromes"
    ],
    "solution": "**CFG for palindromes over {a, b}:**\n\n**Grammar G:**\nS → aSa | bSb | a | b | ε\n\n**Explanation:**\n- S → aSa: add 'a' to both ends\n- S → bSb: add 'b' to both ends\n- S → a: odd-length palindrome with 'a' center\n- S → b: odd-length palindrome with 'b' center\n- S → ε: even-length palindrome (empty center)\n\n**Derivation of \"aba\":**\nS ⇒ aSa ⇒ aba (using S → b)\n\n**Derivation of \"abba\":**\nS ⇒ aSa ⇒ abSba ⇒ abba (using S → ε)\n\n**More examples:**\n- \"aa\": S ⇒ aSa ⇒ aa (S → ε)\n- \"aaa\": S ⇒ aSa ⇒ aaa (S → a)\n- \"baab\": S ⇒ bSb ⇒ baSab ⇒ baab (S → ε)\n\n**Parse tree for \"abba\":**\n```\n      S\n     /|\\\n    a S a\n     /|\\\n    b S b\n      |\n      ε\n```\n\n**Correctness:**\n- Every derivation produces a palindrome (symmetric additions)\n- Every palindrome can be derived:\n  - Empty: use S → ε\n  - Single char: use S → a or S → b\n  - Longer: first and last chars match, recurse on middle"
  },
  {
    "id": "cs203-t3-ex3",
    "subjectId": "cs203",
    "topicId": "cs203-topic-3",
    "type": "written",
    "title": "Leftmost and Rightmost Derivations",
    "description": "For the grammar S → AB, A → aA | a, B → bB | b, give both leftmost and rightmost derivations for \"aabb\".",
    "difficulty": 3,
    "hints": [
      "Leftmost: always expand the leftmost variable",
      "Rightmost: always expand the rightmost variable",
      "Both should produce the same string"
    ],
    "solution": "**Grammar:**\nS → AB\nA → aA | a\nB → bB | b\n\n**Leftmost derivation of \"aabb\":**\n(Always expand the leftmost variable)\n\nS ⇒_lm AB        (S → AB, expand S)\n  ⇒_lm aAB       (A → aA, expand leftmost A)\n  ⇒_lm aaB       (A → a, expand leftmost A)\n  ⇒_lm aabB      (B → bB, expand leftmost B)\n  ⇒_lm aabb      (B → b, expand leftmost B)\n\n**Rightmost derivation of \"aabb\":**\n(Always expand the rightmost variable)\n\nS ⇒_rm AB        (S → AB, expand S)\n  ⇒_rm AbB       (B → bB, expand rightmost B)\n  ⇒_rm Abb       (B → b, expand rightmost B)\n  ⇒_rm aAbb      (A → aA, expand rightmost A, which is the only A)\n  ⇒_rm aabb      (A → a, expand rightmost A)\n\n**Both produce: \"aabb\" ✓**\n\n**Parse tree (same for both):**\n```\n      S\n     / \\\n    A   B\n   /|   |\\\n  a A   b B\n    |     |\n    a     b\n```\n\n**Key insight:**\n- Different derivations, same parse tree\n- Leftmost and rightmost derivations are canonical forms\n- Ambiguity occurs when multiple leftmost (or rightmost) derivations exist for the same string"
  },
  {
    "id": "cs203-t3-ex4",
    "subjectId": "cs203",
    "topicId": "cs203-topic-3",
    "type": "written",
    "title": "Ambiguous Grammar",
    "description": "Show that the grammar S → SS | a | b is ambiguous by finding a string with two different parse trees.",
    "difficulty": 3,
    "hints": [
      "Try a string with at least 3 symbols",
      "Think about different ways to group concatenations",
      "Consider how \"aab\" could be parsed"
    ],
    "solution": "**Proving ambiguity of S → SS | a | b**\n\n**Consider the string \"aab\":**\n\n**Parse Tree 1:** Group as (aa)b\n```\n      S\n     / \\\n    S   S\n   /\\   |\n  S  S  b\n  |  |\n  a  a\n```\nDerivation: S ⇒ SS ⇒ SSS ⇒ aSS ⇒ aaS ⇒ aab\n\n**Parse Tree 2:** Group as a(ab)\n```\n      S\n     / \\\n    S   S\n    |  / \\\n    a S   S\n      |   |\n      a   b\n```\nDerivation: S ⇒ SS ⇒ aS ⇒ aSS ⇒ aaS ⇒ aab\n\n**These are different parse trees for the same string \"aab\".**\n\n**Leftmost derivations:**\n1. S ⇒ SS ⇒ SSS ⇒ aSS ⇒ aaS ⇒ aab\n2. S ⇒ SS ⇒ aS ⇒ aSS ⇒ aaS ⇒ aab\n\n**Wait, these look similar. Let me reconsider the trees:**\n\nActually, for tree 1: S ⇒ SS ⇒ (SS)S ⇒ ... different structure\nFor tree 2: S ⇒ SS ⇒ S(SS) ⇒ ...\n\n**Clearer leftmost derivations:**\nTree 1: S ⇒ SS ⇒ SSS ⇒ aSSS → ... wait, this gets complicated.\n\n**Better approach - use \"ab\":**\n- Tree 1: S → SS → aS → ab (left child is 'a', right child is 'b')\n- Tree 2: This is the only parse for \"ab\" with SS\n\n**For \"aaa\":**\n```\nTree 1:       S           Tree 2:       S\n            / \\                       / \\\n           S   S                     S   S\n          /\\   |                     |   |\\\n         S  S  a                     a  S  S\n         |  |                           |  |\n         a  a                           a  a\n```\n\nThese represent (a·a)·a vs a·(a·a) - different parse trees!\n\n**Conclusion:** The grammar is ambiguous because \"aaa\" (and \"aab\") have multiple distinct parse trees. ∎"
  },
  {
    "id": "cs203-t3-ex5",
    "subjectId": "cs203",
    "topicId": "cs203-topic-3",
    "type": "written",
    "title": "Chomsky Normal Form Conversion",
    "description": "Convert the grammar S → aAB, A → bB | ε, B → b to Chomsky Normal Form.",
    "difficulty": 5,
    "hints": [
      "First eliminate ε-productions",
      "Then eliminate unit productions",
      "Finally convert to binary productions with terminals replaced"
    ],
    "solution": "**Converting to Chomsky Normal Form**\n\n**Original grammar:**\nS → aAB\nA → bB | ε\nB → b\n\n**Step 1: Eliminate ε-productions**\n\nNullable variables: A (since A → ε)\n\nFor each production with A, add version without A:\n- S → aAB becomes S → aAB | aB\n- A → bB stays (no A on right side)\n- B → b stays\n\nRemove A → ε.\n\nAfter Step 1:\nS → aAB | aB\nA → bB\nB → b\n\n**Step 2: Eliminate unit productions**\nNo unit productions (A → B form), so nothing to do.\n\n**Step 3: Convert to CNF**\n\nCNF requires: A → BC or A → a\n\n**Problem productions:**\n- S → aAB (length 3, starts with terminal)\n- S → aB (length 2, has terminal)\n- A → bB (length 2, has terminal)\n\n**Fix 1:** Replace terminals with new variables\n- Create Cₐ → a, C_b → b\n\nAfter replacement:\nS → CₐAB | CₐB\nA → C_bB\nB → b\nCₐ → a\nC_b → b\n\n**Fix 2:** Break long productions into binary\n- S → CₐAB becomes S → CₐD, D → AB\n\n**Final CNF grammar:**\nS → CₐD | CₐB\nD → AB\nA → C_bB\nB → b\nCₐ → a\nC_b → b\n\n**Verification for \"abB\" → \"abb\":**\nS ⇒ CₐD ⇒ aD ⇒ aAB ⇒ aC_bBB ⇒ abBB ⇒ abbB ⇒ abbb\nHmm, let's trace \"ab\":\nS ⇒ CₐB ⇒ aB ⇒ ab ✓"
  },
  {
    "id": "cs203-t3-ex6",
    "subjectId": "cs203",
    "topicId": "cs203-topic-3",
    "type": "written",
    "title": "Pumping Lemma for CFLs",
    "description": "Use the pumping lemma for context-free languages to prove that L = {aⁿbⁿcⁿ | n ≥ 0} is not context-free.",
    "difficulty": 5,
    "hints": [
      "Choose string aᵖbᵖcᵖ where p is the pumping length",
      "For s = uvxyz, vy can't cover all three symbols equally",
      "Pumping will unbalance the counts"
    ],
    "solution": "**Proof that L = {aⁿbⁿcⁿ | n ≥ 0} is not context-free**\n\nAssume for contradiction that L is context-free with pumping length p.\n\n**Choose string:** s = aᵖbᵖcᵖ ∈ L with |s| = 3p ≥ p\n\n**Apply CFL pumping lemma:** s = uvxyz where:\n1. |vy| > 0 (v and y aren't both empty)\n2. |vxy| ≤ p\n3. uvⁱxyⁱz ∈ L for all i ≥ 0\n\n**Key constraint:** |vxy| ≤ p\n\nSince |vxy| ≤ p and s = aᵖbᵖcᵖ, the substring vxy:\n- Can span at most two different symbol types\n- Cannot span all three (a's, b's, and c's)\n\n**Case analysis:**\n\n**Case 1:** vxy is entirely within aᵖ (all a's)\nThen vy consists only of a's.\nPumping: uv²xy²z has more a's than b's or c's.\nSo uv²xy²z ∉ L (unequal counts). Contradiction.\n\n**Case 2:** vxy is entirely within bᵖ (all b's)\nThen vy consists only of b's.\nPumping: uv²xy²z has more b's than a's or c's.\nSo uv²xy²z ∉ L. Contradiction.\n\n**Case 3:** vxy is entirely within cᵖ (all c's)\nThen vy consists only of c's.\nPumping: uv²xy²z has more c's than a's or b's.\nSo uv²xy²z ∉ L. Contradiction.\n\n**Case 4:** vxy spans a's and b's\nThen vy contains only a's and b's (no c's).\nPumping: uv²xy²z has more a's and/or b's, same c's.\nSince |vy| > 0, at least one of a-count or b-count increases.\nBut c-count stays at p.\nSo uv²xy²z ∉ L. Contradiction.\n\n**Case 5:** vxy spans b's and c's\nThen vy contains only b's and c's (no a's).\nPumping increases b's and/or c's, a-count stays at p.\nSo uv²xy²z ∉ L. Contradiction.\n\n**All cases lead to contradiction.**\n\n**Conclusion:** L = {aⁿbⁿcⁿ} is not context-free. ∎"
  },
  {
    "id": "cs203-t3-ex7",
    "subjectId": "cs203",
    "topicId": "cs203-topic-3",
    "type": "written",
    "title": "CFG for Expression Grammar",
    "description": "Design an unambiguous CFG for arithmetic expressions with +, *, and parentheses over variable x. Ensure * has higher precedence than + and both are left-associative.",
    "difficulty": 5,
    "hints": [
      "Use separate non-terminals for different precedence levels",
      "Lower precedence operators at higher levels of the grammar",
      "Left-recursion gives left-associativity"
    ],
    "solution": "**Unambiguous Expression Grammar**\n\n**Grammar G:**\nE → E + T | T\nT → T * F | F\nF → (E) | x\n\n**Explanation:**\n- E (Expression): handles addition (lowest precedence)\n- T (Term): handles multiplication (higher precedence)\n- F (Factor): handles atoms and parentheses (highest precedence)\n\n**Precedence enforcement:**\n- * binds tighter because T appears inside E's production\n- To reach * in E + T, must go through T first\n- Parentheses override precedence by restarting with E\n\n**Left-associativity:**\n- E → E + T makes + left-associative: x+x+x = (x+x)+x\n- T → T * F makes * left-associative: x*x*x = (x*x)*x\n\n**Example derivations:**\n\n**\"x+x*x\" (should be x+(x*x)):**\nE ⇒ E + T ⇒ T + T ⇒ F + T ⇒ x + T ⇒ x + T * F ⇒ x + F * F ⇒ x + x * F ⇒ x + x * x\n\nParse tree groups as x + (x * x) ✓\n\n**\"x+x+x\" (should be (x+x)+x):**\nE ⇒ E + T ⇒ E + T + T ⇒ T + T + T ⇒ x + T + T ⇒ x + x + T ⇒ x + x + x\n\nWait, that derivation doesn't show left-assoc clearly. Correct tree:\nE ⇒ E + T ⇒ (E + T) + T ⇒ ... leftmost E becomes x+x, then +T gives +x\n\n**Parse tree for x+x+x:**\n```\n        E\n       /|\\\n      E + T\n     /|\\  |\n    E + T  x\n    |   |\n    T   x\n    |\n    x\n```\n\nThis groups as ((x)+x)+x = (x+x)+x ✓"
  },
  {
    "id": "cs203-t3-ex8",
    "subjectId": "cs203",
    "topicId": "cs203-topic-3",
    "type": "written",
    "title": "Closure Under Union",
    "description": "Prove that context-free languages are closed under union. Given CFGs G₁ and G₂, construct a CFG for L(G₁) ∪ L(G₂).",
    "difficulty": 3,
    "hints": [
      "Create a new start symbol",
      "Choose which grammar to use",
      "Ensure variable names don't clash"
    ],
    "solution": "**Theorem:** CFLs are closed under union.\n\n**Proof by construction:**\n\nLet G₁ = (V₁, Σ, R₁, S₁) generate L₁\nLet G₂ = (V₂, Σ, R₂, S₂) generate L₂\n\nAssume V₁ ∩ V₂ = ∅ (rename if necessary).\n\n**Construct G = (V, Σ, R, S) for L₁ ∪ L₂:**\n\n- V = V₁ ∪ V₂ ∪ {S} where S is new\n- R = R₁ ∪ R₂ ∪ {S → S₁ | S₂}\n- Start symbol: S\n\n**Correctness:**\n\n**Part 1: L(G) ⊆ L₁ ∪ L₂**\nAny derivation in G starts with S → S₁ or S → S₂.\n- If S → S₁: subsequent derivation uses only R₁ rules, producing string in L₁\n- If S → S₂: subsequent derivation uses only R₂ rules, producing string in L₂\nTherefore any string derived is in L₁ or L₂.\n\n**Part 2: L₁ ∪ L₂ ⊆ L(G)**\n- If w ∈ L₁: S ⇒ S₁ ⇒* w using R₁ rules\n- If w ∈ L₂: S ⇒ S₂ ⇒* w using R₂ rules\nTherefore any string in L₁ ∪ L₂ can be derived in G.\n\n**Example:**\nG₁: S₁ → aS₁b | ε (generates {aⁿbⁿ})\nG₂: S₂ → cS₂ | ε (generates c*)\n\nG: S → S₁ | S₂\n   S₁ → aS₁b | ε\n   S₂ → cS₂ | ε\n\nL(G) = {aⁿbⁿ | n ≥ 0} ∪ {cᵐ | m ≥ 0} ∎"
  },
  {
    "id": "cs203-t3-ex9",
    "subjectId": "cs203",
    "topicId": "cs203-topic-3",
    "type": "written",
    "title": "Non-Closure Under Intersection",
    "description": "Prove that context-free languages are NOT closed under intersection using specific languages.",
    "difficulty": 3,
    "hints": [
      "Use two CFLs whose intersection is not context-free",
      "Consider {aⁿbⁿcᵐ} ∩ {aᵐbⁿcⁿ}",
      "The intersection would require matching all three"
    ],
    "solution": "**Theorem:** CFLs are NOT closed under intersection.\n\n**Proof:**\n\n**Define two context-free languages:**\n- L₁ = {aⁿbⁿcᵐ | n, m ≥ 0}\n- L₂ = {aᵐbⁿcⁿ | n, m ≥ 0}\n\n**Show L₁ is context-free:**\nGrammar for L₁:\nS → AB\nA → aAb | ε\nB → cB | ε\n\nThis generates strings with equal a's and b's, followed by any number of c's.\nL₁ is context-free. ✓\n\n**Show L₂ is context-free:**\nGrammar for L₂:\nS → AB\nA → aA | ε\nB → bBc | ε\n\nThis generates any number of a's, followed by equal b's and c's.\nL₂ is context-free. ✓\n\n**Compute L₁ ∩ L₂:**\nw ∈ L₁ ∩ L₂ ⟺ w ∈ L₁ AND w ∈ L₂\n⟺ w = aⁿbⁿcᵐ AND w = aᵐ'bⁿ'cⁿ'\n⟺ n = n' (number of a's = number where a's match)...\n\nActually: w = aⁱbʲcᵏ where:\n- From L₁: i = j (equal a's and b's)\n- From L₂: j = k (equal b's and c's)\n- Combined: i = j = k\n\n**Therefore: L₁ ∩ L₂ = {aⁿbⁿcⁿ | n ≥ 0}**\n\n**We proved earlier that {aⁿbⁿcⁿ} is NOT context-free.**\n\n**Conclusion:**\nL₁ and L₂ are both context-free, but L₁ ∩ L₂ is not context-free.\n\nTherefore CFLs are not closed under intersection. ∎\n\n**Corollary:** CFLs are not closed under complement either.\n(If closed under complement: L₁ ∩ L₂ = (L̄₁ ∪ L̄₂)̄ using De Morgan's law. Union is closed, so intersection would be closed. Contradiction.)"
  },
  {
    "id": "cs203-t3-ex10",
    "subjectId": "cs203",
    "topicId": "cs203-topic-3",
    "type": "written",
    "title": "CYK Parsing Algorithm",
    "description": "Use the CYK algorithm to determine if \"baaba\" is in the language of the CNF grammar: S → AB | BC, A → BA | a, B → CC | b, C → AB | a.",
    "difficulty": 5,
    "hints": [
      "Build a triangular table bottom-up",
      "Row 1: which variables generate each single symbol",
      "Each cell (i,j): which variables generate substring from i of length j"
    ],
    "solution": "**CYK Algorithm for \"baaba\"**\n\n**Grammar in CNF:**\nS → AB | BC\nA → BA | a\nB → CC | b\nC → AB | a\n\n**String:** b a a b a (positions 1,2,3,4,5)\n\n**Build table T where T[i,j] = variables generating substring starting at i with length j**\n\n**Row 1 (length 1):**\n- T[1,1]: b → {B} (B → b)\n- T[2,1]: a → {A, C} (A → a, C → a)\n- T[3,1]: a → {A, C}\n- T[4,1]: b → {B}\n- T[5,1]: a → {A, C}\n\n**Row 2 (length 2):**\n- T[1,2]: \"ba\" = T[1,1]·T[2,1] = {B}·{A,C}\n  - B·A = BA → A, so A ∈ T[1,2]\n  - B·C = BC → S, so S ∈ T[1,2]\n  - T[1,2] = {A, S}\n\n- T[2,2]: \"aa\" = T[2,1]·T[3,1] = {A,C}·{A,C}\n  - A·A = AA → nothing\n  - A·C = AC → nothing\n  - C·A = CA → nothing\n  - C·C = CC → B, so B ∈ T[2,2]\n  - T[2,2] = {B}\n\n- T[3,2]: \"ab\" = T[3,1]·T[4,1] = {A,C}·{B}\n  - A·B = AB → S,C, so S,C ∈ T[3,2]\n  - C·B = CB → nothing\n  - T[3,2] = {S, C}\n\n- T[4,2]: \"ba\" = T[4,1]·T[5,1] = {B}·{A,C}\n  - B·A = BA → A\n  - B·C = BC → S\n  - T[4,2] = {A, S}\n\n**Row 3 (length 3):**\n- T[1,3]: \"baa\" = T[1,1]·T[2,2] ∪ T[1,2]·T[3,1]\n  - {B}·{B} = BB → nothing\n  - {A,S}·{A,C} = AA,AC,SA,SC → nothing\n  - T[1,3] = {}\n\n- T[2,3]: \"aab\" = T[2,1]·T[3,2] ∪ T[2,2]·T[4,1]\n  - {A,C}·{S,C} = AS,AC,CS,CC → CC → B\n  - {B}·{B} = BB → nothing\n  - T[2,3] = {B}\n\n- T[3,3]: \"aba\" = T[3,1]·T[4,2] ∪ T[3,2]·T[5,1]\n  - {A,C}·{A,S} = AA,AS,CA,CS → nothing\n  - {S,C}·{A,C} = SA,SC,CA,CC → CC → B\n  - T[3,3] = {B}\n\n**Row 4 (length 4):**\n- T[1,4]: \"baab\" = T[1,1]·T[2,3] ∪ T[1,2]·T[3,2] ∪ T[1,3]·T[4,1]\n  - {B}·{B} = BB → nothing\n  - {A,S}·{S,C} = AS,AC,SS,SC → nothing\n  - {}·{B} = nothing\n  - T[1,4] = {}\n\n- T[2,4]: \"aaba\" = T[2,1]·T[3,3] ∪ T[2,2]·T[4,2] ∪ T[2,3]·T[5,1]\n  - {A,C}·{B} = AB,CB → AB → S,C\n  - {B}·{A,S} = BA,BS → BA → A\n  - {B}·{A,C} = BA,BC → BA → A, BC → S\n  - T[2,4] = {S, C, A}\n\n**Row 5 (length 5):**\n- T[1,5]: \"baaba\" = T[1,1]·T[2,4] ∪ T[1,2]·T[3,3] ∪ T[1,3]·T[4,2] ∪ T[1,4]·T[5,1]\n  - {B}·{S,C,A} = BS,BC,BA → BC → S, BA → A\n  - {A,S}·{B} = AB,SB → AB → S,C\n  - {}·{A,S} = nothing\n  - {}·{A,C} = nothing\n  - T[1,5] = {S, A, C}\n\n**S ∈ T[1,5], so \"baaba\" ∈ L(G) ✓**"
  },
  {
    "id": "cs203-t3-ex11",
    "subjectId": "cs203",
    "topicId": "cs203-topic-3",
    "type": "written",
    "title": "Eliminating ε-Productions",
    "description": "Remove ε-productions from the grammar: S → ABC, A → aA | ε, B → bB | ε, C → c.",
    "difficulty": 3,
    "hints": [
      "Find all nullable variables",
      "For each production with nullable variables, add versions without them",
      "Remove the ε-productions themselves"
    ],
    "solution": "**Eliminating ε-Productions**\n\n**Original grammar:**\nS → ABC\nA → aA | ε\nB → bB | ε\nC → c\n\n**Step 1: Find nullable variables**\n- A is nullable (A → ε)\n- B is nullable (B → ε)\n- S is nullable? S → ABC, need A, B, C all nullable. C is not nullable.\n- So nullable = {A, B}\n\n**Step 2: Add productions for each way to omit nullable variables**\n\nFor S → ABC:\n- Keep ABC\n- Omit A: BC\n- Omit B: AC\n- Omit both A and B: C\nNew: S → ABC | BC | AC | C\n\nFor A → aA:\n- Keep aA\n- Omit A: a\nNew: A → aA | a\n\nFor B → bB:\n- Keep bB\n- Omit B: b\nNew: B → bB | b\n\nFor C → c:\n- No nullable variables\nKeep: C → c\n\n**Step 3: Remove ε-productions**\nRemove A → ε and B → ε\n\n**Final grammar (ε-free):**\nS → ABC | BC | AC | C\nA → aA | a\nB → bB | b\nC → c\n\n**Verification:**\nOriginal: S ⇒ ABC ⇒ aABC ⇒ aBC ⇒ abBC ⇒ abC ⇒ abc (using ε twice)\nNew: S ⇒ AC ⇒ aAC ⇒ aC... wait let me redo:\nNew: S ⇒ AC ⇒ aC ⇒ ac (if A → a)\nNew: S ⇒ ABC ⇒ aABC ⇒ aBC ⇒ abC ⇒ abc ✓"
  },
  {
    "id": "cs203-t3-ex12",
    "subjectId": "cs203",
    "topicId": "cs203-topic-3",
    "type": "written",
    "title": "Eliminating Unit Productions",
    "description": "Remove unit productions from: S → A | ab, A → B | a, B → S | bb.",
    "difficulty": 3,
    "hints": [
      "Unit productions are A → B where B is a single variable",
      "Find all unit pairs (A, B) where A ⇒* B using only unit productions",
      "Replace with non-unit productions"
    ],
    "solution": "**Eliminating Unit Productions**\n\n**Original grammar:**\nS → A | ab\nA → B | a\nB → S | bb\n\n**Step 1: Identify unit productions**\n- S → A (unit)\n- A → B (unit)\n- B → S (unit)\n\n**Step 2: Compute unit pairs (transitive closure)**\nUnit pairs (X, Y) where X ⇒* Y using only unit productions:\n\nReflexive: (S,S), (A,A), (B,B)\n\nDirect unit productions:\n- S → A: (S, A)\n- A → B: (A, B)\n- B → S: (B, S)\n\nTransitive:\n- (S, A) and (A, B) → (S, B)\n- (A, B) and (B, S) → (A, S)\n- (B, S) and (S, A) → (B, A)\n- Continue: (S, B) and (B, S) → (S, S) already have\n- (B, A) and (A, B) → (B, B) already have\n\n**All unit pairs:** {(S,S), (S,A), (S,B), (A,A), (A,B), (A,S), (B,B), (B,S), (B,A)}\n\n**Step 3: For each unit pair (X, Y), add X → α for each non-unit Y → α**\n\nNon-unit productions:\n- S → ab\n- A → a\n- B → bb\n\nFor (S, S): S → ab (already there)\nFor (S, A): S → a\nFor (S, B): S → bb\nFor (A, A): A → a (already there)\nFor (A, B): A → bb\nFor (A, S): A → ab\nFor (B, B): B → bb (already there)\nFor (B, S): B → ab\nFor (B, A): B → a\n\n**Step 4: Remove unit productions**\n\n**Final grammar:**\nS → ab | a | bb\nA → a | bb | ab\nB → bb | ab | a\n\n**Verification:**\nOriginal: S ⇒ A ⇒ B ⇒ bb\nNew: S ⇒ bb ✓"
  },
  {
    "id": "cs203-t3-ex13",
    "subjectId": "cs203",
    "topicId": "cs203-topic-3",
    "type": "written",
    "title": "Grammar for Matching Brackets",
    "description": "Design a CFG for strings of properly nested brackets using [ and ]. Include strings like \"[]\", \"[[]]\", \"[][]\", \"[[][]]\".",
    "difficulty": 1,
    "hints": [
      "Each [ must have a matching ]",
      "Brackets can be nested or sequential",
      "Think recursively: balanced string = [ balanced ] or concat of balanced"
    ],
    "solution": "**CFG for Properly Nested Brackets**\n\n**Grammar G:**\nS → [S] | SS | ε\n\n**Alternative (often cleaner):**\nS → [S]S | ε\n\n**Explanation:**\n- S → ε: empty string is balanced\n- S → [S]: a pair of brackets around a balanced string\n- S → SS: concatenation of two balanced strings\n- Combined [S]S handles both nesting and sequence\n\n**Derivations:**\n\n**\"[]\":**\nS ⇒ [S] ⇒ [] (using S → ε)\n\n**\"[[]]\":**\nS ⇒ [S] ⇒ [[S]] ⇒ [[]]\n\n**\"[][]\":**\nS ⇒ SS ⇒ [S]S ⇒ []S ⇒ [][S] ⇒ [][]\n\nOr with S → [S]S:\nS ⇒ [S]S ⇒ []S ⇒ [][S]S ⇒ [][ε]ε ⇒ [][]\n\n**\"[[][]]\":**\nS ⇒ [S] ⇒ [SS] ⇒ [[S]S] ⇒ [[]S] ⇒ [[][S]] ⇒ [[][][]]\n\nHmm, that's [[][]], let me redo:\nS ⇒ [S] ⇒ [SS] ⇒ [[S]S] ⇒ [[][S]] ⇒ [[][][]]...\n\nFor exactly \"[[][]]\":\nS ⇒ [S] ⇒ [SS] ⇒ [[S][S]] ⇒ [[][]] ✓\n\n**Parse tree for \"[[]][]\":**\n```\n       S\n      /|\n     S S\n    /|  |\\\n   [ S ] []\n     |\n    [S]\n     |\n     ε\n```\n\nUsing S → [S]S version:\n```\n         S\n        /|\\\\\n       [ S ] S\n         |   |\\\\\n        [S]S [ S ] S\n         | |   |   |\n         ε ε   ε   ε\n```"
  },
  {
    "id": "cs203-t3-ex14",
    "subjectId": "cs203",
    "topicId": "cs203-topic-3",
    "type": "written",
    "title": "Greibach Normal Form",
    "description": "Describe Greibach Normal Form (GNF) and explain why every CFG without ε (except possibly S → ε) can be converted to GNF.",
    "difficulty": 5,
    "hints": [
      "GNF requires all productions to start with a terminal",
      "This is useful for PDA construction",
      "Conversion involves left-recursion elimination"
    ],
    "solution": "**Greibach Normal Form (GNF)**\n\n**Definition:**\nA CFG is in Greibach Normal Form if every production has the form:\nA → aα\nwhere a ∈ Σ (terminal) and α ∈ V* (string of variables)\n\nSpecial case: S → ε is allowed only if S doesn't appear on any right-hand side.\n\n**Properties of GNF:**\n1. Every production starts with exactly one terminal\n2. Followed by zero or more variables\n3. Reading one terminal = one derivation step\n4. Natural correspondence with PDAs (one input symbol per transition)\n\n**Why every ε-free CFG can be converted to GNF:**\n\n**Sketch of conversion algorithm:**\n\n1. **Start with CNF** (or any ε-free form)\n\n2. **Order variables:** A₁, A₂, ..., Aₙ\n\n3. **Transform productions so Aᵢ → Aⱼγ only when j > i:**\n   - If Aᵢ → Aⱼγ with j < i: substitute Aⱼ's productions\n   - If Aᵢ → Aᵢγ (left recursion): eliminate using new variable\n\n4. **Eliminate left recursion:**\n   For Aᵢ → Aᵢα₁ | ... | Aᵢαₘ | β₁ | ... | βₖ (βⱼ don't start with Aᵢ):\n   Replace with:\n   - Aᵢ → β₁ | ... | βₖ | β₁Bᵢ | ... | βₖBᵢ\n   - Bᵢ → α₁ | ... | αₘ | α₁Bᵢ | ... | αₘBᵢ\n\n5. **After step 4:** All productions Aᵢ → Aⱼγ have j > i\n\n6. **Back-substitute:** Starting from Aₙ (highest), substitute downward so all productions start with terminals\n\n**Example:**\nA → Aa | Ab | c\nEliminate left recursion:\nA → c | cB\nB → a | b | aB | bB\n\nNow all productions for A start with terminal 'c'. ✓\n\n**Key insight:** Left recursion elimination and systematic substitution guarantee we can always make productions start with terminals.\n\n**GNF is useful for:**\n- PDA construction (one-to-one with derivation steps)\n- Proving |derivation| = |string| for non-ε strings\n- Parsing algorithms"
  },
  {
    "id": "cs203-t3-ex15",
    "subjectId": "cs203",
    "topicId": "cs203-topic-3",
    "type": "written",
    "title": "Inherent Ambiguity",
    "description": "Explain what it means for a context-free language to be inherently ambiguous. Give an example of such a language.",
    "difficulty": 5,
    "hints": [
      "Inherent ambiguity is about the language, not a specific grammar",
      "No unambiguous grammar exists for the language",
      "Classic example involves matching two different things"
    ],
    "solution": "**Inherent Ambiguity**\n\n**Definition:**\nA context-free language L is **inherently ambiguous** if every CFG G with L(G) = L is ambiguous.\n\nIn other words, no unambiguous grammar exists for L.\n\n**Key distinction:**\n- Ambiguous grammar: some string has multiple parse trees\n- Inherently ambiguous language: EVERY grammar for it is ambiguous\n\n**Classic example:**\nL = {aⁿbⁿcᵐdᵐ | n, m ≥ 1} ∪ {aⁿbᵐcᵐdⁿ | n, m ≥ 1}\n\n**Why L is inherently ambiguous:**\n\nConsider strings where n = m, e.g., a²b²c²d² = \"aabbccdd\"\n\nThis string is in L because:\n1. It's in {aⁿbⁿcᵐdᵐ} with n = 2, m = 2\n2. It's in {aⁿbᵐcᵐdⁿ} with n = 2, m = 2\n\nAny grammar for L must handle both cases. For strings where n = m, the grammar cannot \"know\" which pattern applies, leading to two fundamentally different derivations.\n\n**Proof sketch (Ogden's lemma based):**\n- Use Ogden's lemma (marked pumping lemma) to show that any grammar must have productions that can derive both patterns independently for certain strings.\n- The \"overlap\" strings (n = m) will always have multiple derivations.\n\n**Another example:**\nL = {aⁱbʲcᵏ | i = j or j = k}\n\nStrings with i = j = k are in L via both conditions, causing inherent ambiguity.\n\n**Implications:**\n- Some CFLs have no unambiguous grammar\n- LR/LL parsing cannot work for such languages\n- Inherently ambiguous languages still have deterministic recognizers (parsers), just not unambiguous ones\n\n**Contrast with removable ambiguity:**\nL = {all strings over {a,b}} has:\n- Ambiguous grammar: S → SS | a | b\n- Unambiguous grammar: S → aS | bS | ε\n\nSo this L is NOT inherently ambiguous."
  },
  {
    "id": "cs203-t3-ex16",
    "subjectId": "cs203",
    "topicId": "cs203-topic-3",
    "type": "written",
    "title": "Closure Under Kleene Star",
    "description": "Prove that context-free languages are closed under Kleene star. Given a CFG G, construct a CFG for L(G)*.",
    "difficulty": 3,
    "hints": [
      "L* = {ε} ∪ L ∪ LL ∪ LLL ∪ ...",
      "Add a new start symbol",
      "Allow repeating derivations from the original grammar"
    ],
    "solution": "**Theorem:** CFLs are closed under Kleene star.\n\n**Proof by construction:**\n\nLet G = (V, Σ, R, S) be a CFG for L.\n\nConstruct G' = (V', Σ, R', S') for L*:\n\n**Construction:**\n- V' = V ∪ {S'} where S' is a new start symbol\n- R' = R ∪ {S' → SS' | ε}\n\n**Alternative (cleaner) construction:**\n- R' = R ∪ {S' → S S' | ε}\n\n**Correctness:**\n\n**Part 1: L(G') ⊆ L***\nAny derivation in G' looks like:\nS' ⇒ SS' ⇒ w₁S' ⇒ w₁SS' ⇒ w₁w₂S' ⇒ ... ⇒ w₁w₂...wₖS' ⇒ w₁w₂...wₖ\n\nWhere each wᵢ ∈ L (derived using original rules from S).\nSo the final string is w₁w₂...wₖ ∈ Lᵏ ⊆ L*.\n\nAlso S' ⇒ ε gives ε ∈ L⁰ ⊆ L*.\n\n**Part 2: L* ⊆ L(G')**\nFor any w ∈ L*:\n- If w = ε: S' ⇒ ε ✓\n- If w = w₁w₂...wₖ where each wᵢ ∈ L:\n  S' ⇒ SS' ⇒ w₁S' ⇒ w₁SS' ⇒ w₁w₂S' ⇒ ... ⇒ w₁...wₖS' ⇒ w₁...wₖ ✓\n\n**Example:**\nG: S → ab generates L = {ab}\n\nG' for L*:\nS' → SS' | ε\nS → ab\n\nDerivation of \"abab\":\nS' ⇒ SS' ⇒ abS' ⇒ abSS' ⇒ ababS' ⇒ abab ✓\n\nDerivation of ε:\nS' ⇒ ε ✓\n\n**Result:** L(G') = L* = {ε, ab, abab, ababab, ...} ∎"
  },
  {
    "id": "cs203-t4-ex1",
    "subjectId": "cs203",
    "topicId": "cs203-topic-4",
    "type": "written",
    "title": "PDA for Equal a's and b's",
    "description": "Design a PDA that accepts L = {w ∈ {a,b}* | w has equal numbers of a's and b's}. Use acceptance by empty stack.",
    "difficulty": 3,
    "hints": [
      "Use stack to track the \"imbalance\" between a's and b's",
      "Push for one symbol, pop for the other",
      "What if we see a \"b\" when expecting \"a\" on stack?"
    ],
    "solution": "**PDA for equal a's and b's (empty stack acceptance)**\n\n**Intuition:**\n- Track excess a's or excess b's on stack\n- Use different stack symbols for each case\n- Accept when balanced (stack empty)\n\n**PDA M = (Q, Σ, Γ, δ, q₀, Z₀, F):**\n- Q = {q}\n- Σ = {a, b}\n- Γ = {A, B, Z₀}\n- q₀ = q\n- Z₀ = Z₀ (initial stack symbol)\n- F = ∅ (accept by empty stack)\n\n**Transitions:**\n\nStart: pop Z₀ if balanced, or begin tracking\n- δ(q, ε, Z₀) = {(q, ε)} — accept empty string\n- δ(q, a, Z₀) = {(q, A)} — first symbol is a\n- δ(q, b, Z₀) = {(q, B)} — first symbol is b\n\nTracking excess a's (A on stack):\n- δ(q, a, A) = {(q, AA)} — more a's, push A\n- δ(q, b, A) = {(q, ε)} — b cancels an a\n\nTracking excess b's (B on stack):\n- δ(q, b, B) = {(q, BB)} — more b's, push B\n- δ(q, a, B) = {(q, ε)} — a cancels a b\n\n**Trace for \"abba\":**\n(q, abba, Z₀)\n⊢ (q, bba, A) — read a, push A\n⊢ (q, ba, ε) — read b, pop A, stack empty!\nNeed to handle empty stack mid-computation...\n\n**Revised design with bottom marker:**\n\n- δ(q, a, Z₀) = {(q, AZ₀)}\n- δ(q, b, Z₀) = {(q, BZ₀)}\n- δ(q, a, A) = {(q, AA)}\n- δ(q, b, A) = {(q, ε)}\n- δ(q, b, B) = {(q, BB)}\n- δ(q, a, B) = {(q, ε)}\n- δ(q, ε, Z₀) = {(q, ε)} — accept when only Z₀ remains\n\n**Trace for \"abba\":**\n(q, abba, Z₀) ⊢ (q, bba, AZ₀) ⊢ (q, ba, Z₀) ⊢ (q, a, BZ₀) ⊢ (q, ε, Z₀) ⊢ (q, ε, ε) Accept! ✓"
  },
  {
    "id": "cs203-t4-ex2",
    "subjectId": "cs203",
    "topicId": "cs203-topic-4",
    "type": "written",
    "title": "PDA for Palindromes",
    "description": "Design a PDA for L = {wwᴿ | w ∈ {a,b}*}, palindromes of even length. Use acceptance by final state.",
    "difficulty": 3,
    "hints": [
      "Push first half, match second half",
      "Need to guess where the middle is",
      "Use nondeterminism to guess the midpoint"
    ],
    "solution": "**PDA for even-length palindromes {wwᴿ}**\n\n**Strategy:**\n1. Push symbols for the first half\n2. Nondeterministically guess when we're at the middle\n3. Pop and match for the second half\n\n**PDA M = (Q, Σ, Γ, δ, q₀, Z₀, F):**\n- Q = {q₀, q₁, q₂}\n- Σ = {a, b}\n- Γ = {a, b, Z₀}\n- Start: q₀\n- F = {q₂}\n\n**States:**\n- q₀: pushing (first half)\n- q₁: popping and matching (second half)\n- q₂: accept state\n\n**Transitions:**\n\nIn q₀ (pushing first half):\n- δ(q₀, a, Z₀) = {(q₀, aZ₀)} — push a\n- δ(q₀, b, Z₀) = {(q₀, bZ₀)} — push b\n- δ(q₀, a, a) = {(q₀, aa)} — push a\n- δ(q₀, a, b) = {(q₀, ab)} — push a\n- δ(q₀, b, a) = {(q₀, ba)} — push b\n- δ(q₀, b, b) = {(q₀, bb)} — push b\n- δ(q₀, ε, Z₀) = {(q₁, Z₀)} — guess middle (empty w case)\n- δ(q₀, ε, a) = {(q₁, a)} — guess middle\n- δ(q₀, ε, b) = {(q₁, b)} — guess middle\n\nIn q₁ (matching second half):\n- δ(q₁, a, a) = {(q₁, ε)} — match a\n- δ(q₁, b, b) = {(q₁, ε)} — match b\n- δ(q₁, ε, Z₀) = {(q₂, Z₀)} — done, accept\n\n**Trace for \"abba\":**\n(q₀, abba, Z₀)\n⊢ (q₀, bba, aZ₀)\n⊢ (q₀, ba, baZ₀)\n⊢ (q₁, ba, baZ₀) — guess middle here\n⊢ (q₁, a, aZ₀) — match b\n⊢ (q₁, ε, Z₀) — match a\n⊢ (q₂, ε, Z₀) — accept ✓"
  },
  {
    "id": "cs203-t4-ex3",
    "subjectId": "cs203",
    "topicId": "cs203-topic-4",
    "type": "written",
    "title": "Converting Acceptance Modes",
    "description": "Given a PDA that accepts by final state, describe how to construct an equivalent PDA that accepts by empty stack.",
    "difficulty": 5,
    "hints": [
      "Add a new bottom-of-stack marker",
      "When original would accept, pop everything",
      "Need to handle both acceptance and emptying"
    ],
    "solution": "**Converting Final State PDA to Empty Stack PDA**\n\n**Given:** PDA P = (Q, Σ, Γ, δ, q₀, Z₀, F) accepting by final state\n\n**Construct:** PDA P' = (Q', Σ, Γ', δ', q'₀, X₀, ∅) accepting by empty stack\n\n**Construction:**\n\n**New components:**\n- Q' = Q ∪ {q'₀, q_empty} (add new start and emptying state)\n- Γ' = Γ ∪ {X₀} (add new bottom marker)\n- Start state: q'₀\n- No accepting states (accept by empty stack)\n\n**Transitions:**\n\n**1. Setup:** Push original start config on stack\nδ'(q'₀, ε, X₀) = {(q₀, Z₀X₀)}\n\n**2. Original transitions:** Keep all of P's transitions\nFor all (q, a, Y) → (p, γ) in P:\n  add δ'(q, a, Y) = {(p, γ)} (plus whatever was there)\n\n**3. Enter emptying mode:** When P would accept\nFor all q ∈ F and all Y ∈ Γ':\n  add (q_empty, ε) to δ'(q, ε, Y)\n\n**4. Empty the stack:** Pop everything in emptying mode\nFor all Y ∈ Γ':\n  δ'(q_empty, ε, Y) = {(q_empty, ε)}\n\n**Why it works:**\n\n- X₀ ensures we don't accidentally empty during computation\n- Original computation proceeds normally\n- When original P reaches accepting state, we can ε-transition to q_empty\n- q_empty pops everything including X₀\n- Empty stack = acceptance\n\n**Correctness argument:**\n- If w is accepted by P (ends in state q ∈ F with some stack γX₀):\n  P' can reach (q, ε, γX₀), then (q_empty, ε, γX₀), then empty the stack\n- If P' accepts w (empty stack):\n  Must have gone through q_empty, which requires visiting some q ∈ F\n  Therefore P accepts w\n\n**Reverse construction (empty stack → final state) is similar:**\n- Detect when original would empty (can't directly detect, so use bottom marker)\n- Transition to accepting state when bottom marker is about to be popped"
  },
  {
    "id": "cs203-t4-ex4",
    "subjectId": "cs203",
    "topicId": "cs203-topic-4",
    "type": "written",
    "title": "PDA Computation Trace",
    "description": "For the PDA with δ(q₀, a, Z) = {(q₀, AZ)}, δ(q₀, b, A) = {(q₁, ε)}, δ(q₁, b, A) = {(q₁, ε)}, δ(q₁, ε, Z) = {(q₂, Z)}, start q₀, stack symbol Z, accept {q₂}. Trace the computation on \"aabb\".",
    "difficulty": 1,
    "hints": [
      "Write out each configuration (state, remaining input, stack)",
      "Stack is written with top on the left",
      "Follow the unique path (this PDA is deterministic)"
    ],
    "solution": "**PDA Trace for \"aabb\"**\n\n**PDA definition:**\n- δ(q₀, a, Z) = {(q₀, AZ)} — push A\n- δ(q₀, b, A) = {(q₁, ε)} — pop A, change state\n- δ(q₁, b, A) = {(q₁, ε)} — continue popping A's\n- δ(q₁, ε, Z) = {(q₂, Z)} — accept when only Z left\n\n**Configuration notation:** (state, remaining input, stack)\nStack written top-first: \"AZ\" means A is on top, Z at bottom\n\n**Trace:**\n\n```\nStep 0: (q₀, aabb, Z)\n        Read 'a', see Z: apply δ(q₀, a, Z) = {(q₀, AZ)}\n\nStep 1: (q₀, abb, AZ)\n        Read 'a', see A: no transition for (q₀, a, A)!\n```\n\n**Wait - there's no transition for (q₀, a, A)!**\n\nLet me check: the PDA only has δ(q₀, a, Z), not δ(q₀, a, A).\n\n**This PDA does not accept \"aabb\".**\n\nLet's try \"abb\":\n\n```\nStep 0: (q₀, abb, Z)\n        Read 'a', see Z: δ(q₀, a, Z) = (q₀, AZ)\n\nStep 1: (q₀, bb, AZ)\n        Read 'b', see A: δ(q₀, b, A) = (q₁, ε)\n\nStep 2: (q₁, b, Z)\n        Read 'b', see Z: no transition for (q₁, b, Z)!\n```\n\n**\"abb\" also not accepted.**\n\nLet's try \"ab\":\n\n```\nStep 0: (q₀, ab, Z)\n        Read 'a', see Z: δ(q₀, a, Z) = (q₀, AZ)\n\nStep 1: (q₀, b, AZ)\n        Read 'b', see A: δ(q₀, b, A) = (q₁, ε)\n\nStep 2: (q₁, ε, Z)\n        ε-transition, see Z: δ(q₁, ε, Z) = (q₂, Z)\n\nStep 3: (q₂, ε, Z)\n        Accept! (q₂ ∈ F and input consumed)\n```\n\n**\"ab\" is accepted!**\n\nThis PDA recognizes {aⁿbⁿ} but seems to only work for n=1 based on the given transitions. For n>1, we'd need δ(q₀, a, A) = {(q₀, AA)}."
  },
  {
    "id": "cs203-t4-ex5",
    "subjectId": "cs203",
    "topicId": "cs203-topic-4",
    "type": "written",
    "title": "CFG to PDA Conversion",
    "description": "Convert the CFG S → aSb | ε to an equivalent PDA using the standard construction. Show the transitions.",
    "difficulty": 3,
    "hints": [
      "Use a 3-state construction: start, loop, accept",
      "Push start symbol, then simulate derivation",
      "Variables: expand by pushing production RHS",
      "Terminals: match against input"
    ],
    "solution": "**CFG to PDA Conversion**\n\n**Given CFG:**\nS → aSb | ε\n\n**Standard construction produces PDA M = (Q, Σ, Γ, δ, q₀, Z₀, F):**\n\n- Q = {q₀, q₁, q₂}\n- Σ = {a, b}\n- Γ = {S, a, b, Z₀}\n- Start: q₀\n- F = {q₂}\n\n**States:**\n- q₀: initial state\n- q₁: main processing loop\n- q₂: accept state\n\n**Transitions:**\n\n**Setup:**\nδ(q₀, ε, Z₀) = {(q₁, SZ₀)}  — push start symbol S\n\n**For variable S on stack, apply productions:**\nδ(q₁, ε, S) = {(q₁, aSb), (q₁, ε)}\n- First option: S → aSb, push \"aSb\" (b first, then S, then a on top)\n- Second option: S → ε, pop S\n\n**For terminals on stack, match against input:**\nδ(q₁, a, a) = {(q₁, ε)}  — match a\nδ(q₁, b, b) = {(q₁, ε)}  — match b\n\n**Accept:**\nδ(q₁, ε, Z₀) = {(q₂, Z₀)}  — accept when stack has only Z₀\n\n**Trace for \"aabb\":**\n```\n(q₀, aabb, Z₀)\n⊢ (q₁, aabb, SZ₀)      — push S\n⊢ (q₁, aabb, aSbZ₀)    — expand S → aSb\n⊢ (q₁, abb, SbZ₀)      — match a\n⊢ (q₁, abb, aSbbZ₀)    — expand S → aSb\n⊢ (q₁, bb, SbbZ₀)      — match a\n⊢ (q₁, bb, bbZ₀)       — expand S → ε\n⊢ (q₁, b, bZ₀)         — match b\n⊢ (q₁, ε, Z₀)          — match b\n⊢ (q₂, ε, Z₀)          — accept ✓\n```\n\n**Note:** Stack shows rightmost symbol at bottom. When pushing \"aSb\", we push b, S, a in that order so a is on top."
  },
  {
    "id": "cs203-t4-ex6",
    "subjectId": "cs203",
    "topicId": "cs203-topic-4",
    "type": "written",
    "title": "PDA to CFG Conversion",
    "description": "Explain the construction for converting a PDA to an equivalent CFG. What do the variables in the constructed grammar represent?",
    "difficulty": 5,
    "hints": [
      "Variables are triples [p, A, q] representing \"from p to q while net-popping A\"",
      "If PDA goes from p to q while removing A from stack, variable [p,A,q] derives input consumed",
      "Productions capture how transitions affect the stack"
    ],
    "solution": "**PDA to CFG Conversion**\n\n**Given:** PDA P = (Q, Σ, Γ, δ, q₀, Z₀, ∅) accepting by empty stack\n\n**Construct:** CFG G = (V, Σ, R, S)\n\n**Variable Interpretation:**\nVariables have form [p, A, q] meaning:\n\"Starting in state p with A on top of stack, the PDA can reach state q having exactly popped A (and anything pushed then popped in between).\"\n\n**Construction:**\n\n**Variables:**\nV = {[p, A, q] | p, q ∈ Q, A ∈ Γ} ∪ {S}\n\n**Start symbol:**\nS with production S → [q₀, Z₀, q] for each q ∈ Q\n\n**Productions from transitions:**\n\n**Case 1:** δ(p, a, A) contains (r, ε) — pop A directly\nAdd: [p, A, r] → a\n\n**Case 2:** δ(p, a, A) contains (r, B₁B₂...Bₖ) — replace A with B₁...Bₖ\nFor all choices of states q₁, q₂, ..., qₖ₊₁ where qₖ₊₁ is the \"final\" state:\nAdd: [p, A, qₖ₊₁] → a [r, B₁, q₁] [q₁, B₂, q₂] ... [qₖ₋₁, Bₖ, qₖ₊₁]\n\n**Intuition for Case 2:**\n- Start in p with A on stack\n- Read a, replace A with B₁B₂...Bₖ, go to state r\n- From r, pop B₁, ending in state q₁\n- From q₁, pop B₂, ending in state q₂\n- ...\n- From qₖ₋₁, pop Bₖ, ending in final state\n\n**Example:**\nIf δ(p, a, A) = {(r, BC)}:\n[p, A, q₂] → a [r, B, q₁] [q₁, C, q₂] for all q₁, q₂ ∈ Q\n\n**Why it works:**\n- [p, A, q] derives exactly strings that take PDA from (p, w, Aγ) to (q, ε, γ) for any γ\n- The construction captures all possible state sequences\n- Nondeterminism in PDA becomes nondeterminism in grammar (multiple productions)\n\n**Correctness:**\nS ⇒* w ⟺ (q₀, w, Z₀) ⊢* (q, ε, ε) for some q ⟺ P accepts w\n\n**Size:** O(|Q|³|Γ||R|) variables and productions in worst case"
  },
  {
    "id": "cs203-t4-ex7",
    "subjectId": "cs203",
    "topicId": "cs203-topic-4",
    "type": "written",
    "title": "DPDA Limitations",
    "description": "Prove that the language L = {wwᴿ | w ∈ {a,b}*} cannot be recognized by a DPDA (deterministic pushdown automaton).",
    "difficulty": 5,
    "hints": [
      "DPDA must know when to switch from pushing to popping",
      "For wwᴿ, the middle is not marked",
      "Contrast with wcwᴿ which IS recognizable by DPDA"
    ],
    "solution": "**Proof that {wwᴿ} is not a DCFL (not recognizable by DPDA)**\n\n**Language:** L = {wwᴿ | w ∈ {a,b}*} (even-length palindromes)\n\n**Key insight:** A DPDA must deterministically decide when it's at the middle of the string, but there's no marker to indicate this.\n\n**Proof (informal):**\n\nConsider the strings a²ⁿb²ⁿ for various n.\nAt position n (after reading aⁿ), a DPDA processing aⁿb...bⁿaⁿ must:\n- Either commit to \"we're at the middle\" (start matching)\n- Or continue pushing (we're still in first half)\n\n**Problem:** The same prefix aⁿ could be:\n- The first half of aⁿaⁿ (middle after aⁿ)\n- The first quarter of aⁿaⁿaⁿaⁿ... (middle much later)\n\nA DPDA must make the same decision for the same configuration.\n\n**Formal argument using prefix property:**\n\nL is prefix-free at no length. For any w₁ ∈ L:\n- Consider w₁ = u₁u₁ᴿ\n- There exists w₂ = u₁u₁ᴿu₂u₂ᴿ ∈ L where w₁ is a proper prefix of w₂\n\nWhen a DPDA finishes reading w₁, it must accept (if w₁ ∈ L).\nBut then for w₂ = w₁u₂u₂ᴿ, after reading w₁ prefix, the DPDA already accepted!\n\nDCFLs have the property that we can detect acceptance at end of input.\n\n**Closure argument:**\n- DCFLs are closed under complement\n- If {wwᴿ} were DCFL, so would its complement\n- The complement of {wwᴿ} is not even context-free!\n  (Actually this argument doesn't work since complement of CFL might not be CFL)\n\n**Better argument - Pumping for DCFLs:**\nThere's a stronger pumping lemma for DCFLs showing that palindromes cannot be DCFL.\n\n**Contrast with wcwᴿ:**\nL' = {wcwᴿ | w ∈ {a,b}*} IS a DCFL:\n- Push until seeing c\n- After c, pop and match\n- The marker c provides deterministic information about the middle\n\n**Conclusion:** {wwᴿ} requires nondeterminism to guess the middle, so it's not a DCFL. ∎"
  },
  {
    "id": "cs203-t4-ex8",
    "subjectId": "cs203",
    "topicId": "cs203-topic-4",
    "type": "written",
    "title": "PDA for Unequal Strings",
    "description": "Design a PDA for L = {aⁱbʲ | i ≠ j}. Hint: This is the union of {aⁱbʲ | i > j} and {aⁱbʲ | i < j}.",
    "difficulty": 5,
    "hints": [
      "Handle i > j and i < j separately",
      "Use nondeterminism to guess which case",
      "For i > j: push a's, pop for b's, must have a's left"
    ],
    "solution": "**PDA for {aⁱbʲ | i ≠ j}**\n\n**Strategy:**\nL = {aⁱbʲ | i > j} ∪ {aⁱbʲ | i < j}\n\nUse nondeterminism to guess which case at the start.\n\n**PDA M = (Q, Σ, Γ, δ, q₀, Z₀, F):**\n- Q = {q₀, q_more_a, q_match_a, q_more_b, q_match_b, q_accept}\n- Σ = {a, b}\n- Γ = {A, B, Z₀}\n- F = {q_accept}\n\n**Branch 1: i > j (more a's than b's)**\n\nStart: guess this case\n- δ(q₀, ε, Z₀) = {(q_more_a, Z₀), (q_more_b, Z₀)}\n\nIn q_more_a: push all a's\n- δ(q_more_a, a, Z₀) = {(q_more_a, AZ₀)}\n- δ(q_more_a, a, A) = {(q_more_a, AA)}\n\nSwitch to matching when b's start, but ensure extra a's exist:\n- δ(q_more_a, b, A) = {(q_match_a, ε)}\n\nIn q_match_a: pop A's for b's\n- δ(q_match_a, b, A) = {(q_match_a, ε)}\n\nAccept when b's done but A's remain:\n- δ(q_match_a, ε, A) = {(q_accept, A)}\n- δ(q_accept, ε, A) = {(q_accept, ε)} — clean up\n\n**Branch 2: i < j (more b's than a's)**\n\nIn q_more_b: push all a's\n- δ(q_more_b, a, Z₀) = {(q_more_b, AZ₀)}\n- δ(q_more_b, a, A) = {(q_more_b, AA)}\n\nStart matching b's:\n- δ(q_more_b, b, A) = {(q_match_b, ε)}\n- δ(q_more_b, b, Z₀) = {(q_match_b, BZ₀)} — no a's case\n\nIn q_match_b: pop A's, then start pushing B's for excess b's\n- δ(q_match_b, b, A) = {(q_match_b, ε)}\n- δ(q_match_b, b, Z₀) = {(q_match_b, BZ₀)} — all a's matched, start counting excess\n- δ(q_match_b, b, B) = {(q_match_b, BB)}\n\nAccept when done reading and B's on stack (excess b's):\n- δ(q_match_b, ε, B) = {(q_accept, B)}\n\n**Trace for \"aab\" (i > j):**\n(q₀, aab, Z₀) ⊢ (q_more_a, aab, Z₀) ⊢ (q_more_a, ab, AZ₀) ⊢ (q_more_a, b, AAZ₀)\n⊢ (q_match_a, ε, AZ₀) ⊢ (q_accept, ε, AZ₀) ✓"
  },
  {
    "id": "cs203-t4-ex9",
    "subjectId": "cs203",
    "topicId": "cs203-topic-4",
    "type": "written",
    "title": "Two-Stack PDA Power",
    "description": "Explain why a PDA with two stacks is equivalent to a Turing machine in computational power.",
    "difficulty": 5,
    "hints": [
      "A TM tape can be split at the head position",
      "One stack holds left part, other holds right part",
      "Moving the head = transferring between stacks"
    ],
    "solution": "**Two-Stack PDA = Turing Machine Power**\n\n**Theorem:** A PDA with two stacks can simulate any Turing machine.\n\n**Intuition:**\nA TM has a tape that extends infinitely in both directions from the head.\nTwo stacks can represent:\n- Stack 1: tape contents to the LEFT of the head\n- Stack 2: tape contents to the RIGHT of the head (including current cell)\n\n**Simulation:**\n\n**TM Configuration:** ...□ a b c [d] e f □ ...\n                     ←left→ ^head ←right→\n\n**Two-stack representation:**\n- Stack 1 (left of head): top→ c b a □ ... ←bottom\n- Stack 2 (right of head): top→ d e f □ ... ←bottom\n- Current symbol: top of Stack 2\n\n**Simulating TM operations:**\n\n**1. Read current symbol:**\nRead top of Stack 2 (the cell under the head)\n\n**2. Write symbol:**\nPop Stack 2, push new symbol to Stack 2\n\n**3. Move head RIGHT:**\nPop Stack 2 (current cell), push it to Stack 1\nNow top of Stack 2 is new current cell\n\n**4. Move head LEFT:**\nPop Stack 1 (cell to left), push to Stack 2\nNow top of Stack 2 is new current cell\n\n**Handling tape boundaries:**\nIf a stack is empty when we need to pop, push a blank (□) to the other stack first (extending the tape).\n\n**State simulation:**\nTwo-stack PDA states correspond directly to TM states.\n\n**Why single-stack PDA is weaker:**\n- Single stack gives LIFO access only\n- Can't \"see\" both sides of current position\n- Can only match nested/recursive patterns, not arbitrary tape manipulation\n\n**Corollary:**\n- Two-stack PDA can recognize non-context-free languages\n- Two-stack PDA can recognize {aⁿbⁿcⁿ}\n- Two-stack PDA faces undecidability (halting problem)\n\n**Hierarchy:**\nDFA < PDA (1 stack) < 2-stack PDA = TM\n\nThe jump from 1 to 2 stacks crosses the computability boundary from CFLs to recursively enumerable languages."
  },
  {
    "id": "cs203-t4-ex10",
    "subjectId": "cs203",
    "topicId": "cs203-topic-4",
    "type": "written",
    "title": "PDA for Dyck Language",
    "description": "The Dyck language D₂ consists of balanced strings of two types of brackets: () and []. Design a PDA for D₂.",
    "difficulty": 3,
    "hints": [
      "Push opening brackets, pop and match closing brackets",
      "Different stack symbols for different bracket types",
      "Accept when input is consumed and stack has only the bottom marker"
    ],
    "solution": "**PDA for Dyck Language D₂**\n\n**Language:** Balanced strings over {(, ), [, ]}\n\n**Examples:**\n- \"()\" ✓, \"[]\" ✓, \"([])\" ✓, \"()[()]\" ✓\n- \"(\" ✗, \"[(])\" ✗, \"([)]\" ✗\n\n**PDA M = (Q, Σ, Γ, δ, q₀, Z₀, F):**\n- Q = {q, q_acc}\n- Σ = {(, ), [, ]}\n- Γ = {P, B, Z₀}  (P for (, B for [)\n- Start: q\n- F = {q_acc}\n\n**Transitions:**\n\n**Push opening brackets:**\n- δ(q, (, Z₀) = {(q, PZ₀)}\n- δ(q, (, P) = {(q, PP)}\n- δ(q, (, B) = {(q, PB)}\n- δ(q, [, Z₀) = {(q, BZ₀)}\n- δ(q, [, P) = {(q, BP)}\n- δ(q, [, B) = {(q, BB)}\n\n**Pop and match closing brackets:**\n- δ(q, ), P) = {(q, ε)}  — match ( with )\n- δ(q, ], B) = {(q, ε)}  — match [ with ]\n\n**Accept when balanced:**\n- δ(q, ε, Z₀) = {(q_acc, Z₀)}\n\n**Note:** No transitions for mismatches like δ(q, ), B) or δ(q, ], P) — these cause rejection.\n\n**Trace for \"([()])\":**\n```\n(q, ([()])], Z₀)\n⊢ (q, [()]), PZ₀)      — push (\n⊢ (q, ()]), BPZ₀)      — push [\n⊢ (q, )]), PBP Z₀)     — push (\n⊢ (q, ]), BPZ₀)        — match )\n⊢ (q, ), PZ₀)          — match ]\n⊢ (q, ε, Z₀)           — match )\n⊢ (q_acc, ε, Z₀)       — accept ✓\n```\n\n**Trace for \"([)]\" (mismatched):**\n```\n(q, ([)], Z₀)\n⊢ (q, [)], PZ₀)        — push (\n⊢ (q, )], BPZ₀)        — push [\n⊢ STUCK!               — see ), top is B, no matching transition\n```\n\nComputation blocks, string rejected. ✓"
  },
  {
    "id": "cs203-t4-ex11",
    "subjectId": "cs203",
    "topicId": "cs203-topic-4",
    "type": "written",
    "title": "PDA with Multiple Stack Symbols",
    "description": "Design a PDA that accepts {aⁿbᵐcⁿ⁺ᵐ | n, m ≥ 0}. The number of c's equals the sum of a's and b's.",
    "difficulty": 5,
    "hints": [
      "Push something for each a and each b",
      "Pop one symbol for each c",
      "Can use the same or different stack symbols"
    ],
    "solution": "**PDA for {aⁿbᵐcⁿ⁺ᵐ | n, m ≥ 0}**\n\n**Analysis:**\n- Read a's, push a marker for each\n- Read b's, push a marker for each\n- Read c's, pop one marker for each\n- Accept when stack has only bottom marker (counts match)\n\n**Simple approach:** Use same symbol X for both a's and b's\n\n**PDA M = (Q, Σ, Γ, δ, q₀, Z₀, F):**\n- Q = {q_a, q_b, q_c, q_acc}\n- Σ = {a, b, c}\n- Γ = {X, Z₀}\n- Start: q_a\n- F = {q_acc}\n\n**Transitions:**\n\n**Reading a's (push X for each):**\n- δ(q_a, a, Z₀) = {(q_a, XZ₀)}\n- δ(q_a, a, X) = {(q_a, XX)}\n\n**Transition to reading b's:**\n- δ(q_a, b, Z₀) = {(q_b, XZ₀)}  — no a's, start b's\n- δ(q_a, b, X) = {(q_b, XX)}    — done with a's, start b's\n- δ(q_a, ε, Z₀) = {(q_b, Z₀)}   — no a's, maybe no b's\n\n**Reading b's (push X for each):**\n- δ(q_b, b, Z₀) = {(q_b, XZ₀)}\n- δ(q_b, b, X) = {(q_b, XX)}\n\n**Transition to reading c's:**\n- δ(q_b, c, X) = {(q_c, ε)}     — start c's, pop first X\n- δ(q_b, ε, Z₀) = {(q_acc, Z₀)} — no b's or c's, accept\n\n**Reading c's (pop X for each):**\n- δ(q_c, c, X) = {(q_c, ε)}\n\n**Accept when c's done and stack is empty of X's:**\n- δ(q_c, ε, Z₀) = {(q_acc, Z₀)}\n\n**Trace for \"aabccc\" (n=2, m=1, n+m=3):**\n```\n(q_a, aabccc, Z₀)\n⊢ (q_a, abccc, XZ₀)       — push for first a\n⊢ (q_a, bccc, XXZ₀)       — push for second a\n⊢ (q_b, ccc, XXXZ₀)       — push for b\n⊢ (q_c, cc, XXZ₀)         — pop for first c\n⊢ (q_c, c, XZ₀)           — pop for second c\n⊢ (q_c, ε, Z₀)            — pop for third c\n⊢ (q_acc, ε, Z₀)          — accept ✓\n```\n\n**Trace for \"c\" (n=0, m=0, need 0 c's, not 1):**\n```\n(q_a, c, Z₀)\n⊢ (q_b, c, Z₀)             — ε-move, no a's\n  No transition for (q_b, c, Z₀)!\n  STUCK — rejected ✓\n```"
  },
  {
    "id": "cs203-t4-ex12",
    "subjectId": "cs203",
    "topicId": "cs203-topic-4",
    "type": "written",
    "title": "ε-Transitions in PDAs",
    "description": "Explain the role of ε-transitions in PDAs. Can every PDA be converted to an equivalent PDA with no ε-transitions on input?",
    "difficulty": 3,
    "hints": [
      "ε-transitions allow stack manipulation without reading input",
      "Consider what would happen without them",
      "Think about NPDAs vs DPDAs"
    ],
    "solution": "**Role of ε-Transitions in PDAs**\n\n**ε-transitions in PDAs:** Transitions of the form δ(q, ε, A) = {...}\n- Can be taken without consuming any input symbol\n- Allow stack manipulation independent of input\n\n**Key uses of ε-transitions:**\n\n**1. Nondeterministic guessing:**\n- Guess where the middle of a palindrome is\n- Choose between different parsing strategies\n- δ(q, ε, A) = {(p₁, γ₁), (p₂, γ₂)} — choose nondeterministically\n\n**2. Stack setup/cleanup:**\n- Initialize stack before processing\n- Clean up stack before accepting\n- δ(q₀, ε, Z₀) = {(q₁, SZ₀)} — push start symbol\n\n**3. Mode changes:**\n- Switch from \"pushing phase\" to \"matching phase\"\n- Change states based on stack, not input\n- δ(q_push, ε, X) = {(q_pop, X)} — switch to popping mode\n\n**Can we eliminate ε-transitions?**\n\n**For NPDAs:** Generally NO, not in a straightforward way.\n\n**Problem:** ε-transitions allow the PDA to examine and modify the stack without reading input. This is essential for:\n- Simulating CFG derivations (expand variables before matching)\n- Making decisions based on stack contents alone\n\n**Simulation attempt:**\nIf we try to fold ε-transitions into regular transitions, we face issues:\n- Don't know when an ε-loop terminates\n- May need unbounded ε-moves between inputs\n- Stack can change arbitrarily on ε-moves\n\n**However:** We can convert to a PDA where ε-transitions don't change the stack \"net effect\" per input symbol, but some ε-moves may remain.\n\n**For DPDAs:** Different constraints apply, and ε-transitions are more limited.\n\n**Key theorem:** NPDAs (even with ε-transitions) recognize exactly the CFLs. The ε-transitions add convenience but not power — they're part of the standard model.\n\n**Contrast with NFAs:**\n- NFAs: ε-transitions can be eliminated (subset construction preserves this)\n- NPDAs: ε-transitions are more intrinsic due to stack operations"
  },
  {
    "id": "cs203-t4-ex13",
    "subjectId": "cs203",
    "topicId": "cs203-topic-4",
    "type": "written",
    "title": "PDA Instantaneous Description",
    "description": "Define the instantaneous description (ID) of a PDA and the yields relation (⊢). Show how to formalize PDA computation.",
    "difficulty": 1,
    "hints": [
      "ID captures the complete state of a PDA at any moment",
      "Similar to TM configurations",
      "⊢ shows one computation step"
    ],
    "solution": "**Instantaneous Description (ID) of a PDA**\n\n**Definition:**\nAn instantaneous description (ID) or configuration of a PDA is a triple:\n\n**(q, w, γ)**\n\nwhere:\n- q ∈ Q is the current state\n- w ∈ Σ* is the remaining (unread) input\n- γ ∈ Γ* is the current stack contents (top on left)\n\n**The Yields Relation ⊢**\n\n**(q, aw, Aβ) ⊢ (p, w, γβ)** if (p, γ) ∈ δ(q, a, A)\n\nThis means: In state q, reading input symbol a (or ε), with A on top of stack, the PDA can transition to state p, consuming a, and replacing A with γ.\n\n**Variants:**\n- **(q, w, Aβ) ⊢ (p, w, γβ)** if (p, γ) ∈ δ(q, ε, A) — ε-transition\n\n**Reflexive-transitive closure:**\n- **⊢*** : zero or more steps\n- **(q, w, γ) ⊢* (p, w', γ')** means the PDA can reach (p, w', γ') from (q, w, γ)\n\n**Acceptance definitions:**\n\n**By final state:**\nL(M) = {w | (q₀, w, Z₀) ⊢* (q, ε, γ) for some q ∈ F, γ ∈ Γ*}\n\n**By empty stack:**\nN(M) = {w | (q₀, w, Z₀) ⊢* (q, ε, ε) for some q ∈ Q}\n\n**Example computation:**\n\nPDA: δ(q₀, a, Z) = {(q₀, AZ)}, δ(q₀, b, A) = {(q₁, ε)}, δ(q₁, ε, Z) = {(q₂, Z)}\n\nInput: \"ab\"\n\n```\n(q₀, ab, Z)\n  ⊢ (q₀, b, AZ)      by δ(q₀, a, Z) = {(q₀, AZ)}\n  ⊢ (q₁, ε, Z)       by δ(q₀, b, A) = {(q₁, ε)}\n  ⊢ (q₂, ε, Z)       by δ(q₁, ε, Z) = {(q₂, Z)}\n```\n\nSince (q₀, ab, Z) ⊢* (q₂, ε, Z) and q₂ ∈ F, string \"ab\" is accepted."
  },
  {
    "id": "cs203-t4-ex14",
    "subjectId": "cs203",
    "topicId": "cs203-topic-4",
    "type": "written",
    "title": "DCFL Closure Properties",
    "description": "Which closure properties do deterministic context-free languages (DCFLs) have that general CFLs don't? Explain.",
    "difficulty": 3,
    "hints": [
      "Consider complement closure",
      "Think about what determinism guarantees",
      "DCFLs are recognized by DPDAs"
    ],
    "solution": "**DCFL Closure Properties**\n\n**Recall:**\n- DCFLs = languages recognized by deterministic PDAs\n- DCFLs ⊊ CFLs (proper subset)\n\n**Key property: DCFLs are CLOSED under complement**\n\n**Why CFLs are NOT closed under complement:**\nIf CFLs were closed under complement, they'd be closed under intersection:\nL₁ ∩ L₂ = complement(complement(L₁) ∪ complement(L₂))\n\nBut we know CFLs aren't closed under intersection (example: {aⁿbⁿcᵐ} ∩ {aᵐbⁿcⁿ} = {aⁿbⁿcⁿ}).\n\n**Why DCFLs ARE closed under complement:**\n\nFor a DPDA M recognizing L:\n- At each step, exactly one transition applies (determinism)\n- M processes all of input and either accepts or rejects\n- To recognize L̄: swap accepting and rejecting outcomes\n\n**Technical details:**\n1. First convert DPDA to one that always reads entire input\n2. For accept by final state: F' = Q - F\n3. Must handle cases where DPDA gets stuck or loops\n\n**Formal complement construction:**\nGiven DPDA M for L, construct DPDA M' for L̄:\n- Make M read all input (add error state if needed)\n- Swap accept/reject (complement accepting states)\n- Handle end-of-input ε-moves carefully\n\n**Other DCFL closure properties:**\n\n**Closed under:**\n- Complement ✓ (as shown above)\n- Intersection with regular languages ✓\n- Inverse homomorphism ✓\n\n**NOT closed under:**\n- Union ✗ (L₁ ∪ L₂ may require nondeterminism to choose)\n- Intersection ✗\n- Concatenation ✗\n- Kleene star ✗\n\n**Example of union failure:**\nL₁ = {aⁿbⁿ | n ≥ 0} and L₂ = {aⁿb²ⁿ | n ≥ 0}\nBoth are DCFLs, but L₁ ∪ L₂ is not (would need to guess which pattern).\n\n**Significance:**\nComplement closure makes DCFLs \"nicer\" for some applications (parsing). It's a key distinguishing property from general CFLs."
  },
  {
    "id": "cs203-t4-ex15",
    "subjectId": "cs203",
    "topicId": "cs203-topic-4",
    "type": "written",
    "title": "LR Parsing and DPDAs",
    "description": "Explain the connection between LR parsing and deterministic pushdown automata.",
    "difficulty": 5,
    "hints": [
      "LR parsers are essentially DPDAs",
      "The stack holds parser states and grammar symbols",
      "Shift = push, Reduce = pop and push"
    ],
    "solution": "**LR Parsing and DPDAs**\n\n**Key insight:** LR parsers ARE DPDAs (with lookup tables for efficiency).\n\n**LR Parser Components:**\n1. **Input:** String to parse\n2. **Stack:** Parser states + grammar symbols\n3. **Parse table:** ACTION and GOTO tables\n4. **Output:** Sequence of reductions (derivation)\n\n**Correspondence with DPDAs:**\n\n| LR Parser | DPDA |\n|-----------|------|\n| States (in table) | States Q |\n| Stack | Stack Γ |\n| ACTION[s,a] = shift | δ(s, a, X) = (s', YX) push |\n| ACTION[s,a] = reduce | δ(s, ε, ...) pop and push |\n| Lookahead | Reading input |\n| Accept | Reaching accept state |\n\n**LR Parsing Actions:**\n\n**Shift:**\n- Read input symbol a\n- Push new state s' onto stack\n- Like DPDA reading and pushing\n\n**Reduce by A → β:**\n- Pop |β| symbols from stack (the handle)\n- Push A (the non-terminal)\n- GOTO gives new state\n- Like DPDA ε-transitions modifying stack\n\n**Why DPDA is appropriate:**\n1. LR(k) grammars have the \"deterministic\" property\n2. At each step, the parser knows exactly what to do\n3. No backtracking needed\n4. Stack holds sufficient context for decisions\n\n**LR(k) Languages = DCFLs:**\n\n**Theorem:** A language is DCFL if and only if it has an LR(k) grammar for some k.\n\nThis establishes:\n- Every LR-parsable language has a DPDA\n- Every DPDA language has an LR grammar (after end-marker)\n\n**Practical implications:**\n- Most programming languages are designed to be LR(1)\n- Parser generators (yacc, bison) build DPDAs from grammars\n- Conflicts (shift-reduce, reduce-reduce) indicate grammar is not LR\n\n**Non-LR example:**\nPalindromes {wwᴿ} are CFL but not DCFL, so no LR grammar exists.\nThis is why natural language parsing is harder!"
  },
  {
    "id": "cs203-t4-ex16",
    "subjectId": "cs203",
    "topicId": "cs203-topic-4",
    "type": "written",
    "title": "PDA for Prefix-Free Language",
    "description": "A language L is prefix-free if no string in L is a prefix of another. Show that if L is a CFL and prefix-free, then L is a DCFL.",
    "difficulty": 5,
    "hints": [
      "With prefix-freeness, we know acceptance immediately at end of string",
      "Don't need to look ahead or guess",
      "Construct a DPDA from any PDA for L"
    ],
    "solution": "**Theorem:** If L is context-free and prefix-free, then L is deterministic context-free.\n\n**Prefix-free definition:**\nL is prefix-free if for all w₁, w₂ ∈ L: w₁ is not a proper prefix of w₂.\n\n**Proof Idea:**\n\n**Why prefix-freeness helps:**\n\nConsider an NPDA M for L. When M accepts string w:\n- Some computation path reaches acceptance\n- Other paths may continue or reject\n\n**Key observation:**\nIf L is prefix-free and w ∈ L, then no extension wv (v ≠ ε) is in L.\n\nThis means: Once we've read w ∈ L, we don't need to consider accepting longer strings.\n\n**Construction sketch:**\n\nGiven NPDA M for L, construct DPDA M':\n\n1. **Determinize carefully:**\n   - At each step, track set of possible configurations\n   - This would normally cause exponential blowup\n\n2. **Use prefix-freeness:**\n   - If any configuration in the set would accept, we can accept immediately\n   - No need to continue reading for alternative acceptances\n   - Once we accept, we know no extension is in L\n\n3. **Handle ambiguity:**\n   - Different paths reaching different configurations\n   - Since L is prefix-free, at most one path leads to acceptance at each input length\n   - Can resolve nondeterminism \"lazily\"\n\n**More formal approach:**\n\nUse the fact that:\n- Prefix-free CFLs have unambiguous grammars\n- These correspond to grammars where end-of-string is unambiguous\n- Such grammars are essentially LR\n\n**Alternative proof using complement:**\n\n1. L is CFL and prefix-free\n2. L$ (with end marker $) is DCFL (can detect end of accepting string)\n3. L$ being DCFL implies L is DCFL (remove end marker)\n\n**Significance:**\n- Prefix-freeness removes the \"guessing\" problem\n- Many practical languages are designed to be prefix-free\n- Tokenization often produces prefix-free token sequences\n\n**Counterexample without prefix-freeness:**\n{wwᴿ} is CFL but not prefix-free (\"ε\" is prefix of \"aa\") and not DCFL."
  },
  {
    "id": "cs203-t5-ex1",
    "subjectId": "cs203",
    "topicId": "cs203-topic-5",
    "type": "written",
    "title": "TM for Palindrome Detection",
    "description": "Design a Turing machine that accepts palindromes over {a, b}. Describe the high-level algorithm and key states.",
    "difficulty": 3,
    "hints": [
      "Compare first and last symbols",
      "Mark or erase matched symbols",
      "Repeat until string is empty or mismatch found"
    ],
    "solution": "**Turing Machine for Palindromes**\n\n**High-level algorithm:**\n1. Read first symbol, remember it, mark it (or erase)\n2. Scan right to find last symbol\n3. Compare with remembered symbol\n4. If match: mark last symbol, go back to start, repeat\n5. If mismatch: reject\n6. If only blanks/marks remain: accept\n\n**States:**\n- q₀: start state\n- q_a: remembered first symbol is 'a'\n- q_b: remembered first symbol is 'b'\n- q_left: scanning left to find start\n- q_accept: accepting state\n- q_reject: rejecting state\n\n**Key transitions:**\n\n**From q₀ (read first symbol):**\n- δ(q₀, a) = (q_a, X, R) — mark 'a' as X, remember 'a'\n- δ(q₀, b) = (q_b, X, R) — mark 'b' as X, remember 'b'\n- δ(q₀, X) = (q₀, X, R) — skip marked symbols\n- δ(q₀, □) = (q_accept, □, R) — empty/all marked, accept\n\n**Scan right (q_a or q_b):**\n- δ(q_a, a) = (q_a, a, R) — keep going right\n- δ(q_a, b) = (q_a, b, R)\n- δ(q_a, □) = (q_check_a, □, L) — found end, go back to check\n\n**Check last symbol (q_check_a):**\n- δ(q_check_a, a) = (q_left, X, L) — match! mark it\n- δ(q_check_a, b) = (q_reject, b, R) — mismatch, reject\n- δ(q_check_a, X) = (q_check_a, X, L) — skip marks (single char palindrome)\n\n**Scan back left:**\n- δ(q_left, a) = (q_left, a, L)\n- δ(q_left, b) = (q_left, b, L)\n- δ(q_left, X) = (q₀, X, R) — found marked start, restart\n\n**Example trace for \"aba\":**\n```\nq₀: [a]ba□ → q_a: X[b]a□ → X[b]a□ → Xb[a]□ → Xba[□]\nq_check_a: Xb[a]□ → q_left: X[b]X□ → [X]bX□\nq₀: X[b]X□ → q_b: XX[X]□ → XXX[□]\nq_check_b: XX[X]□ → (only marks, no unmatched)\nq_accept ✓\n```"
  },
  {
    "id": "cs203-t5-ex2",
    "subjectId": "cs203",
    "topicId": "cs203-topic-5",
    "type": "written",
    "title": "TM Configuration Notation",
    "description": "Define the configuration (instantaneous description) of a Turing machine. Write the sequence of configurations for a TM that erases its input, starting with \"ab\".",
    "difficulty": 1,
    "hints": [
      "Configuration shows tape contents, head position, and state",
      "Use notation like \"abq₀cd\" meaning head at position before c, in state q₀",
      "A simple erasing TM writes blanks moving right"
    ],
    "solution": "**Turing Machine Configuration**\n\n**Definition:**\nA configuration (or instantaneous description) of a TM captures the complete machine state:\n- Current state\n- Tape contents\n- Head position\n\n**Notation:** αqβ where:\n- q is the current state\n- α is the tape content to the LEFT of the head\n- β is the tape content from the head position onward\n- First symbol of β is under the head\n- Blanks on ends can be omitted\n\n**Example:** \"abq₃cd\" means:\n- State: q₃\n- Tape: ...□abcd□...\n- Head is positioned at 'c'\n- α = \"ab\", β = \"cd\"\n\n**TM to erase input:**\nStates: q₀ (erasing), q_acc (accept)\nTransitions:\n- δ(q₀, a) = (q₀, □, R) — erase a\n- δ(q₀, b) = (q₀, □, R) — erase b\n- δ(q₀, □) = (q_acc, □, R) — done, accept\n\n**Configuration sequence for input \"ab\":**\n\n```\nq₀ab      Initial: head at 'a', state q₀\n□q₀b      After erasing 'a', move right\n□□q₀□     After erasing 'b', move right\n□□□q_acc  Read blank, accept\n```\n\nOr in cleaner notation:\n```\n[a]b → q₀ reads 'a', writes □, moves R\n□[b] → q₀ reads 'b', writes □, moves R\n□□[□] → q₀ reads □, transitions to q_acc\n```\n\n**Key points:**\n- Configuration uniquely determines future computation\n- Sequence of configurations is the computation history\n- Accepting computation ends in q_accept configuration\n- Rejecting computation ends in q_reject configuration\n- Some computations never halt (loop forever)"
  },
  {
    "id": "cs203-t5-ex3",
    "subjectId": "cs203",
    "topicId": "cs203-topic-5",
    "type": "written",
    "title": "Multi-tape TM Simulation",
    "description": "Explain how a single-tape TM can simulate a k-tape TM. What is the time overhead of this simulation?",
    "difficulty": 5,
    "hints": [
      "Encode all k tapes on one tape",
      "Use special markers for tape boundaries and head positions",
      "Each step of k-tape TM requires scanning the single tape"
    ],
    "solution": "**Simulating k-tape TM with Single-tape TM**\n\n**Encoding k tapes on one tape:**\n\nFor k-tape TM M with tapes T₁, T₂, ..., Tₖ:\n\n**Format:** #T₁#T₂#...#Tₖ#\n\n**Mark head positions:** Use dotted symbols\n- If Tᵢ has \"abc\" with head on 'b': encode as \"aḃc\"\n- ḃ indicates head position on tape i\n\n**Example (2 tapes):**\nT₁: ab with head on 'a' → ȧb\nT₂: cd with head on 'd' → cd̊\n\nSingle tape: #ȧb#cd̊#\n\n**Simulation algorithm:**\n\n**To simulate one step of M:**\n\n1. **Scan right:** Find all marked (dotted) symbols\n   - Record symbol under each head in finite state\n   - This requires O(n) where n is used tape length\n\n2. **Determine transitions:** Based on state and k symbols\n   - Look up M's transition: (q, a₁, ..., aₖ) → (q', b₁, ..., bₖ, D₁, ..., Dₖ)\n\n3. **Update tape:** Scan again to:\n   - Write new symbols at head positions\n   - Move head markers according to Dᵢ\n   - May need to shift content if tape boundary reached\n\n4. **Return to left:** Scan back to the leftmost #\n\n**Time analysis:**\n\n**Single step of M:** O(n) time on single-tape simulator\n- Must scan entire tape to find/update all heads\n- n = total symbols across all tapes\n\n**If M runs in T(n) steps:**\n- After T(n) steps, tape length is at most T(n) · k (each step adds at most k symbols)\n- Each step costs O(T(n) · k) on simulator\n\n**Total simulation time:**\nT(n) steps × O(T(n)) per step = **O(T(n)²)**\n\n**Space:** Only O(T(n)) — linear in original space\n\n**Theorem:**\nAny k-tape TM running in time T(n) can be simulated by a single-tape TM in time O(T(n)²).\n\n**Significance:**\n- Multi-tape TMs are at most polynomially faster\n- Same languages are recognized (equivalent power)\n- Quadratic overhead is acceptable for theoretical purposes\n- In practice, multi-tape gives more efficient algorithms"
  },
  {
    "id": "cs203-t5-ex4",
    "subjectId": "cs203",
    "topicId": "cs203-topic-5",
    "type": "written",
    "title": "Universal Turing Machine",
    "description": "Describe the Universal Turing Machine (UTM). What does it take as input, and how does it operate?",
    "difficulty": 5,
    "hints": [
      "UTM takes encoded TM + input as its input",
      "It simulates any TM on any input",
      "Like an interpreter for TM programs"
    ],
    "solution": "**The Universal Turing Machine (UTM)**\n\n**Definition:**\nA Universal Turing Machine U is a specific TM that can simulate any other TM M on any input w.\n\n**Input to U:** ⟨M, w⟩\n- Encoded description of TM M\n- Input string w\n\n**Output:** U accepts ⟨M, w⟩ iff M accepts w\n\n**Encoding a TM:**\nEncode M = (Q, Σ, Γ, δ, q₀, q_accept, q_reject):\n- States: q₁, q₂, ... (numbered)\n- Tape symbols: 0, 1, B (fixed small alphabet)\n- Transitions: List of tuples δ(qᵢ, aⱼ) = (qₖ, aₗ, D)\n- Use delimiter to separate components\n\n**How U operates:**\n\n**Setup (3 tapes conceptually):**\n1. Tape 1: Description of M (⟨M⟩)\n2. Tape 2: Simulated tape of M (initially w)\n3. Tape 3: Current state of M (initially q₀)\n\n**Simulation loop:**\n1. Read current state from Tape 3\n2. Read symbol under simulated head from Tape 2\n3. Search Tape 1 for matching transition δ(state, symbol)\n4. If found: Execute transition\n   - Write new symbol on Tape 2\n   - Move simulated head on Tape 2\n   - Update state on Tape 3\n5. If M in q_accept: U accepts\n6. If M in q_reject: U rejects\n7. Loop\n\n**Key properties:**\n\n**Universality:** U can compute anything computable\n- Any TM can be simulated by U\n- U is a \"programmable computer\"\n\n**Self-reference:** U can simulate itself\n- U on ⟨U, ⟨M, w⟩⟩ simulates U simulating M on w\n\n**Efficiency:** U is slower than direct simulation\n- O(T(n) log T(n)) overhead typically\n- But polynomial slowdown, not exponential\n\n**Existence:** Turing proved UTM exists\n- Can be built with fixed finite number of states\n- Shows TMs can interpret other TMs\n\n**Historical significance:**\n- Foundation of stored-program computers\n- Separation of \"hardware\" (U) from \"software\" (⟨M⟩)\n- Basis for undecidability proofs (halting problem)"
  },
  {
    "id": "cs203-t5-ex5",
    "subjectId": "cs203",
    "topicId": "cs203-topic-5",
    "type": "written",
    "title": "Decidable vs Recognizable",
    "description": "Explain the difference between decidable (recursive) and Turing-recognizable (recursively enumerable) languages. Give an example of each.",
    "difficulty": 3,
    "hints": [
      "Decidable: TM always halts with yes/no answer",
      "Recognizable: TM halts and accepts for yes, may loop for no",
      "Decidable ⊂ Recognizable"
    ],
    "solution": "**Decidable vs Turing-Recognizable Languages**\n\n**Decidable (Recursive) Languages:**\n\n**Definition:** L is decidable if there exists a TM M such that:\n- For w ∈ L: M accepts w (halts in q_accept)\n- For w ∉ L: M rejects w (halts in q_reject)\n- M halts on ALL inputs\n\n**Key property:** Always get a definite yes/no answer.\n\n**Turing-Recognizable (R.E.) Languages:**\n\n**Definition:** L is Turing-recognizable if there exists a TM M such that:\n- For w ∈ L: M accepts w (halts in q_accept)\n- For w ∉ L: M either rejects OR loops forever\n\n**Key property:** May never get an answer for non-members.\n\n**Relationship:**\nDecidable ⊊ Recognizable ⊊ All languages\n\nEvery decidable language is recognizable (a decider is a special recognizer), but not vice versa.\n\n**Examples:**\n\n**Decidable languages:**\n- All regular languages\n- All context-free languages\n- {aⁿbⁿcⁿ | n ≥ 0} (TM can count and compare)\n- {⟨M⟩ | M is a valid DFA encoding}\n- {⟨G, w⟩ | CFG G generates string w}\n\n**Recognizable but NOT decidable:**\n- A_TM = {⟨M, w⟩ | TM M accepts w}\n  - Recognizable: simulate M on w, accept if M accepts\n  - Not decidable: can't detect if M loops forever\n\n- HALT = {⟨M, w⟩ | TM M halts on w}\n  - Recognizable: simulate M, accept if M halts\n  - Not decidable: halting problem\n\n**Not even recognizable:**\n- Ā_TM = {⟨M, w⟩ | TM M does NOT accept w}\n- {⟨M⟩ | M accepts all strings}\n- Complements of recognizable-but-not-decidable languages\n\n**Key theorems:**\n1. L is decidable ⟺ both L and L̄ are recognizable\n2. If L is recognizable but not decidable, then L̄ is not recognizable"
  },
  {
    "id": "cs203-t5-ex6",
    "subjectId": "cs203",
    "topicId": "cs203-topic-5",
    "type": "written",
    "title": "TM for String Comparison",
    "description": "Design a TM that accepts L = {w#w | w ∈ {a,b}*}. The input consists of two copies of the same string separated by #.",
    "difficulty": 3,
    "hints": [
      "Match symbols one at a time across the #",
      "Mark matched symbols on both sides",
      "Accept if all symbols match"
    ],
    "solution": "**TM for {w#w | w ∈ {a,b}*}**\n\n**Algorithm:**\n1. Mark first unmatched symbol in first w\n2. Remember it, scan past # to second w\n3. Find corresponding position, compare\n4. If match: mark it, go back to step 1\n5. If mismatch: reject\n6. If both sides fully matched: accept\n\n**States:**\n- q₀: start, looking for next symbol to match\n- q_a: carrying 'a' to compare\n- q_b: carrying 'b' to compare\n- q_back: scanning back to left side\n- q_check: verifying all matched\n- q_accept, q_reject\n\n**Key transitions:**\n\n**Starting from left (q₀):**\n- δ(q₀, a) = (q_a, X, R) — mark 'a' as X, remember 'a'\n- δ(q₀, b) = (q_b, X, R) — mark 'b' as X, remember 'b'\n- δ(q₀, X) = (q₀, X, R) — skip marked symbols\n- δ(q₀, #) = (q_check, #, R) — done with left side\n\n**Scanning right (q_a):**\n- δ(q_a, a) = (q_a, a, R)\n- δ(q_a, b) = (q_a, b, R)\n- δ(q_a, #) = (q_a', #, R) — crossed #, now find first unmarked\n- δ(q_a', X) = (q_a', X, R) — skip marks on right side\n- δ(q_a', a) = (q_back, X, L) — found 'a', match! mark it\n- δ(q_a', b) = (q_reject, b, R) — mismatch\n- δ(q_a', □) = (q_reject, □, R) — second w shorter\n\n**Scanning back left (q_back):**\n- δ(q_back, X) = (q_back, X, L)\n- δ(q_back, a) = (q_back, a, L)\n- δ(q_back, b) = (q_back, b, L)\n- δ(q_back, #) = (q_back, #, L)\n- δ(q_back, □) = (q₀, □, R) — reached left end, restart\n\n**Final check (q_check):**\n- δ(q_check, X) = (q_check, X, R) — skip marks\n- δ(q_check, □) = (q_accept, □, R) — all matched!\n- δ(q_check, a) = (q_reject, a, R) — unmatched in second w\n- δ(q_check, b) = (q_reject, b, R)\n\n**Trace for \"ab#ab\":**\n```\nq₀: [a]b#ab → q_a: X[b]#ab → Xb[#]ab → Xb#[a]b\nq_a': Xb#X[b] → reject? No wait...\n```\n\nNeed to track positions properly. The key is matching 1st symbol of left with 1st unmarked of right."
  },
  {
    "id": "cs203-t5-ex7",
    "subjectId": "cs203",
    "topicId": "cs203-topic-5",
    "type": "written",
    "title": "Church-Turing Thesis",
    "description": "State the Church-Turing thesis and explain why it cannot be formally proven. Give three pieces of evidence supporting it.",
    "difficulty": 3,
    "hints": [
      "The thesis is about the informal notion of \"algorithm\"",
      "It connects formal TMs to intuitive computability",
      "Evidence comes from equivalent models"
    ],
    "solution": "**The Church-Turing Thesis**\n\n**Statement:**\nEvery function that is intuitively computable (by an algorithm) is computable by a Turing machine.\n\nOr equivalently:\nThe informal notion of \"effective procedure\" or \"algorithm\" is exactly captured by the formal notion of Turing machine.\n\n**Why it cannot be formally proven:**\n\n1. **One side is informal:** \"Intuitive computability\" is not a mathematical definition\n   - We can't prove something about an informal concept\n   - Any formal definition would just be another formal model\n\n2. **It's a definition/thesis, not theorem:**\n   - Proposes that TMs correctly model computation\n   - Like defining \"line\" in geometry—can't prove a definition\n\n3. **Could potentially be refuted:**\n   - If someone found an \"algorithmic\" process not TM-computable\n   - No such counterexample has ever been found\n\n**Evidence Supporting the Thesis:**\n\n**1. Equivalent formal models all compute the same functions:**\n- Lambda calculus (Church, 1936)\n- General recursive functions (Gödel-Kleene)\n- Post systems\n- Markov algorithms\n- Register machines\n- RAM model\n- Modern programming languages\n\nAll define exactly the same class of computable functions!\n\n**2. Robustness to variations:**\n- Multi-tape TMs\n- Nondeterministic TMs\n- Two-way infinite tape\n- Multiple heads\n- Multi-dimensional tape\n\nAll equivalent to basic TM model.\n\n**3. No counterexamples found:**\n- 80+ years of effort\n- Every proposed \"algorithm\" has been shown TM-computable\n- Physical computation seems bounded by TM power\n\n**4. Natural problems:**\n- No natural problem is known to be \"computable but not TM-computable\"\n- Suggests TMs capture something fundamental\n\n**Variations:**\n- **Physical Church-Turing:** No physical device computes more than TM\n- **Extended (Strong):** Includes efficiency claims (polynomial overhead)\n\n**Challenges:**\n- Quantum computing (same power, different efficiency?)\n- Hypercomputation proposals (generally rejected)"
  },
  {
    "id": "cs203-t5-ex8",
    "subjectId": "cs203",
    "topicId": "cs203-topic-5",
    "type": "written",
    "title": "Nondeterministic TM",
    "description": "Define nondeterministic Turing machines. Prove they are equivalent in power to deterministic TMs.",
    "difficulty": 5,
    "hints": [
      "NTM can have multiple transitions from same configuration",
      "Accepts if ANY computation path accepts",
      "DTM can simulate by searching all paths"
    ],
    "solution": "**Nondeterministic Turing Machines (NTMs)**\n\n**Definition:**\nAn NTM is like a DTM except δ is a relation, not function:\nδ ⊆ (Q × Γ) × (Q × Γ × {L, R})\n\nAt each step, multiple transitions may be applicable.\n\n**Acceptance:**\nAn NTM accepts input w if there EXISTS a sequence of choices (a computation path) that leads to q_accept.\n\n**Visualization:**\nComputation is a tree:\n- Root: initial configuration\n- Branches: different nondeterministic choices\n- Accept if any leaf is accepting\n\n**Theorem: NTMs and DTMs are equivalent in power.**\n\n**Proof:**\n\n**Direction 1: DTM → NTM**\nEvery DTM is already an NTM (with exactly one choice at each step).\n\n**Direction 2: NTM → DTM**\nGiven NTM N, construct DTM D that simulates all possible computation paths.\n\n**Simulation strategy (BFS):**\n\n**Three-tape DTM D:**\n1. Tape 1: Input (read-only)\n2. Tape 2: Current simulation tape\n3. Tape 3: Address of current path in computation tree\n\n**Address encoding:**\n- If N has at most b choices at any step\n- Path after k steps encoded as string from {1,2,...,b}ᵏ\n- \"231\" means: first choice 2, second choice 3, third choice 1\n\n**D's algorithm:**\n1. Initialize Tape 3 to empty (root of tree)\n2. Copy input to Tape 2\n3. Simulate N using choices from Tape 3\n   - If path leads to accept: D accepts\n   - If path exhausted or rejects: go to step 4\n4. Generate next address (BFS order)\n5. Goto step 2\n\n**Why BFS (not DFS):**\n- DFS might go down infinite branch\n- BFS ensures every finite accepting path is found\n- First try all length-1 paths, then length-2, etc.\n\n**Correctness:**\n- If N accepts w: some finite path accepts, D finds it\n- If N doesn't accept w: D never accepts\n\n**Time complexity:**\nIf N accepts in t steps with branching factor b:\nD runs in O(bᵗ) time (exponential)\n\n**Key insight:**\nNTMs don't compute more functions than DTMs, but may be exponentially faster. This exponential gap is central to P vs NP."
  },
  {
    "id": "cs203-t5-ex9",
    "subjectId": "cs203",
    "topicId": "cs203-topic-5",
    "type": "written",
    "title": "TM Encoding",
    "description": "Describe how to encode a Turing machine as a string over {0,1}. Why is this encoding important?",
    "difficulty": 3,
    "hints": [
      "Need to encode states, alphabet, and transitions",
      "Use binary for numbers",
      "Enables TMs to take other TMs as input"
    ],
    "solution": "**Encoding Turing Machines as Binary Strings**\n\n**Why encode TMs?**\n- TMs can take other TMs as input\n- Enables universal computation\n- Required for diagonalization proofs\n- Foundation for undecidability results\n\n**TM Components to encode:**\nM = (Q, Σ, Γ, δ, q₀, q_accept, q_reject)\n\n**Encoding scheme:**\n\n**1. Number the states:** q₁, q₂, ..., qₙ\n- q₁ = start state\n- q₂ = accept state\n- q₃ = reject state\n\n**2. Number tape symbols:** a₁, a₂, ..., aₘ\n- a₁ = 0 (input alphabet)\n- a₂ = 1 (input alphabet)\n- a₃ = □ (blank)\n- a₄, ... = other work symbols\n\n**3. Encode transitions:**\nδ(qᵢ, aⱼ) = (qₖ, aₗ, D) encoded as (i, j, k, l, d)\nwhere d = 1 for L, 2 for R\n\n**4. Binary representation:**\n- Integers in binary\n- Use separator (like 00) between numbers\n- Use different separator (like 000) between transitions\n\n**Example encoding:**\nδ(q₁, 0) = (q₂, 1, R) becomes: 1 00 1 00 10 00 10 00 10\n(state 1, symbol 1, state 2, symbol 2, direction 2)\n\n**Full encoding ⟨M⟩:**\n- Number of states (in unary: 1ⁿ for n states)\n- Separator\n- List of all transitions\n- Separators between transitions\n\n**Properties of encoding:**\n\n**1. Computable:** Given M, we can compute ⟨M⟩\n**2. Decodable:** Given ⟨M⟩, we can reconstruct M\n**3. Checkable:** Can verify if string is valid encoding\n**4. Prefix-free (if designed carefully):** No encoding is prefix of another\n\n**Standard notation:**\n- ⟨M⟩: encoding of TM M\n- ⟨M, w⟩: encoding of TM M with input w\n- Often use: ⟨M, w⟩ = ⟨M⟩#w where # separates\n\n**Applications:**\n1. **Universal TM:** Takes ⟨M, w⟩, simulates M on w\n2. **Diagonalization:** Enumerate TMs as ⟨M₁⟩, ⟨M₂⟩, ...\n3. **Rice's theorem:** Reason about TMs as strings\n4. **Undecidability:** Define languages over TM descriptions"
  },
  {
    "id": "cs203-t5-ex10",
    "subjectId": "cs203",
    "topicId": "cs203-topic-5",
    "type": "written",
    "title": "Linear Bounded Automata",
    "description": "Define Linear Bounded Automata (LBA). What class of languages do they recognize? How do they relate to TMs?",
    "difficulty": 3,
    "hints": [
      "LBA = TM with restricted tape",
      "Tape limited to input length",
      "More powerful than PDA, less than TM"
    ],
    "solution": "**Linear Bounded Automata (LBA)**\n\n**Definition:**\nAn LBA is a nondeterministic TM where the tape head cannot move beyond the original input boundaries.\n\n**Formal definition:**\n- Same as NTM but with restriction\n- Tape cells: only those containing input (plus end markers)\n- Head cannot write on or move past end markers\n\n**Notation:**\nInput w is placed between end markers: ¢w$\nHead stays within these bounds.\n\n**Language class:**\nLBAs recognize exactly the **context-sensitive languages (CSLs)**.\n\n**Context-Sensitive Grammars:**\nProductions of form αAβ → αγβ where |γ| ≥ 1\n(Can only expand, never shrink — except S → ε if S doesn't appear on right)\n\n**Examples of CSLs (recognized by LBAs):**\n- {aⁿbⁿcⁿ | n ≥ 0}\n- {ww | w ∈ Σ*}\n- {aⁿ | n is prime}\n\n**Hierarchy:**\nRegular ⊂ Context-Free ⊂ Context-Sensitive ⊂ Recursive ⊂ R.E.\nDFA/NFA  ⊂ PDA        ⊂ LBA               ⊂ Decider  ⊂ TM\n\n**Key properties:**\n\n**1. Space bound:** O(n) space on input of length n\n**2. Decidability:**\n   - Membership in L(LBA) is decidable\n   - Only finitely many configurations for length-n input\n   - Number of configurations: |Q| × n × |Γ|ⁿ (finite but exponential)\n\n**3. Emptiness undecidable:**\n   - \"Is L(LBA) = ∅?\" is undecidable\n   - Even though membership is decidable\n\n**4. Deterministic LBA:**\n   - Open problem: Does DLBA = NLBA?\n   - Equivalent to CSL = DCSL question\n\n**Why important:**\n- Models computation with linear memory\n- Many natural languages are context-sensitive\n- Shows hierarchy between CFLs and decidable languages\n- Demonstrates that space bounds affect power\n\n**Comparison:**\n| Feature | PDA | LBA | TM |\n|---------|-----|-----|-----|\n| Memory | Stack | Linear | Infinite |\n| Languages | CFL | CSL | R.E. |\n| Membership | Decidable | Decidable | Undecidable |\n| Emptiness | Decidable | Undecidable | Undecidable |"
  },
  {
    "id": "cs203-t5-ex11",
    "subjectId": "cs203",
    "topicId": "cs203-topic-5",
    "type": "written",
    "title": "TM for Unary Multiplication",
    "description": "Design a TM that computes multiplication of two unary numbers. Input: 1ⁿ#1ᵐ, Output: 1ⁿᵐ.",
    "difficulty": 5,
    "hints": [
      "Multiplication = repeated addition",
      "For each 1 in first number, copy second number",
      "Use markers to track progress"
    ],
    "solution": "**TM for Unary Multiplication**\n\n**Input:** 1ⁿ#1ᵐ (n ones, separator #, m ones)\n**Output:** 1ⁿᵐ (n×m ones)\n\n**Algorithm:**\nFor each 1 in the first group:\n  - Copy all 1s from the second group to the output area\nMark/erase processed 1s from first group\n\n**Tape layout:**\nInitial: 1ⁿ#1ᵐ\nWorking: X...X#1ᵐ=1ᵐ...1ᵐ\nFinal: 1ⁿᵐ (after cleanup)\n\n**States:**\n- q₀: start\n- q_copy: copying phase, remember we need to copy second group\n- q_mark: mark a 1 from second group\n- q_paste: add 1 to output\n- q_return: return to second group\n- q_reset: reset second group marks, back to first group\n- q_clean: final cleanup\n- q_halt: done\n\n**Key transitions:**\n\n**Phase 1: Mark next 1 from first group**\n- δ(q₀, 1) = (q_copy, X, R) — mark first-group 1, start copy\n- δ(q₀, X) = (q₀, X, R) — skip marked\n- δ(q₀, #) = (q_clean, □, R) — first group done, cleanup\n\n**Phase 2: Copy second group (q_copy)**\n- δ(q_copy, 1) = (q_copy, 1, R)\n- δ(q_copy, #) = (q_copy, #, R)\n- δ(q_copy, Y) = (q_copy, Y, R) — skip marked in second group\n- δ(q_copy, □) = (q_mark, □, L) — reached end, go back to mark\n\nActually, let me redesign more carefully:\n\n**Better algorithm:**\n1. Mark one 1 from first group (change to X)\n2. For each 1 in second group (change to Y temporarily):\n   - Add one 1 to output area (after =)\n3. Unmark second group (Y → 1)\n4. Repeat until first group is all X's\n5. Clean up (remove X's and #)\n\n**Simplified trace for 1¹#1² = 1×2 = 2:**\n```\n11#11□□\nX1#11□□  (mark first 1)\nX1#Y1□1  (copy first 1 of second group)\nX1#YY□11 (copy second 1)\nX1#11□11 (unmark second group)\nXX#11□11 (mark second first-group 1)\nXX#YY□1111 (copy second group again)\nXX#11□1111 (unmark)\nClean up → 1111\n```\n\nOutput: 1⁴ = 1²×² ✓"
  },
  {
    "id": "cs203-t5-ex12",
    "subjectId": "cs203",
    "topicId": "cs203-topic-5",
    "type": "written",
    "title": "Enumerator Definition",
    "description": "Define an enumerator. Prove that a language is Turing-recognizable iff some enumerator enumerates it.",
    "difficulty": 5,
    "hints": [
      "Enumerator is TM with output tape",
      "Prints strings one by one",
      "Language = set of strings printed"
    ],
    "solution": "**Enumerators**\n\n**Definition:**\nAn enumerator E is a TM with:\n- No input tape\n- Output tape (write-only, or printer)\n- Work tape (read/write)\n\nE runs forever (or halts), printing strings separated by #.\nThe language enumerated by E is the set of strings printed.\n\n**Notation:** L(E) = {w | E eventually prints w}\n\n**Theorem:** L is Turing-recognizable ⟺ some enumerator enumerates L.\n\n**Proof (⟸): Enumerator → Recognizer**\n\nGiven enumerator E for L, construct recognizer M for L:\n\nM on input w:\n1. Run E\n2. Every time E prints a string s:\n   - Compare s with w\n   - If s = w: accept\n3. Continue forever (E runs forever)\n\n**Correctness:**\n- If w ∈ L: E eventually prints w, M accepts\n- If w ∉ L: E never prints w, M runs forever\nThis is exactly Turing-recognizable behavior.\n\n**Proof (⟹): Recognizer → Enumerator**\n\nGiven recognizer M for L, construct enumerator E for L:\n\nE's algorithm:\nLet s₁, s₂, s₃, ... be enumeration of all strings (lexicographic order)\n\nFor i = 1, 2, 3, ...:\n  For each j from 1 to i:\n    Run M on sⱼ for i steps\n    If M accepts sⱼ within i steps:\n      Print sⱼ (if not already printed)\n\n**Why this works:**\n- **Dovetailing:** Simulate multiple computations interleaved\n- If w ∈ L: M accepts w in some t steps\n  - When i = max(index of w, t), we run M on w for enough steps\n  - So w gets printed\n- If w ∉ L: M doesn't accept w, never printed\n- Every string gets tested eventually (and repeatedly)\n\n**Key technique: Dovetailing**\nCan't just run M on s₁ (might loop), then s₂, etc.\nInstead, interleave: run each for bounded time, increase bounds.\n\n**Corollary:** L is decidable ⟺ enumerator prints in lexicographic order\n(Can detect when we've passed where w should be)"
  },
  {
    "id": "cs203-t5-ex13",
    "subjectId": "cs203",
    "topicId": "cs203-topic-5",
    "type": "written",
    "title": "Closure Properties of Decidable Languages",
    "description": "Prove that decidable languages are closed under union, intersection, and complement.",
    "difficulty": 3,
    "hints": [
      "Given deciders for L₁ and L₂, build decider for combination",
      "Deciders always halt, so we can sequence them",
      "For complement, swap accept and reject"
    ],
    "solution": "**Closure Properties of Decidable Languages**\n\n**Theorem:** Decidable languages are closed under union, intersection, and complement.\n\n**Proof of Union:**\n\nLet M₁ decide L₁ and M₂ decide L₂.\n\nConstruct decider M for L₁ ∪ L₂:\n\nM on input w:\n1. Run M₁ on w\n2. If M₁ accepts, accept\n3. Run M₂ on w\n4. If M₂ accepts, accept\n5. Reject\n\n**Correctness:**\n- M halts: M₁ and M₂ both halt (deciders), so M halts\n- w ∈ L₁ ∪ L₂ ⟺ w ∈ L₁ or w ∈ L₂ ⟺ M₁ or M₂ accepts ⟺ M accepts ✓\n\n**Proof of Intersection:**\n\nConstruct decider M for L₁ ∩ L₂:\n\nM on input w:\n1. Run M₁ on w\n2. If M₁ rejects, reject\n3. Run M₂ on w\n4. If M₂ accepts, accept\n5. Reject\n\n**Correctness:**\n- M halts: both M₁, M₂ halt\n- w ∈ L₁ ∩ L₂ ⟺ w ∈ L₁ and w ∈ L₂ ⟺ both accept ⟺ M accepts ✓\n\n**Proof of Complement:**\n\nLet M decide L.\n\nConstruct decider M' for L̄:\n\nM' on input w:\n1. Run M on w\n2. If M accepts, reject\n3. If M rejects, accept\n\n**Correctness:**\n- M' halts: M halts (decider)\n- w ∈ L̄ ⟺ w ∉ L ⟺ M rejects w ⟺ M' accepts ✓\n\n**Additional closures (similar proofs):**\n- **Concatenation:** Run M₁ on all splits xy = w, accept if any (M₁ on x, M₂ on y) pair accepts\n- **Kleene star:** Similar, check all ways to split into L-words\n- **Reversal:** Run M on reversed input\n\n**Why recognizable languages aren't closed under complement:**\n- If M loops on some w ∉ L, we can't decide to accept w for L̄\n- No way to detect the loop\n- Would need to know when to \"give up\"\n\n**Key insight:** Deciders always halt, enabling sequential composition and complementation."
  },
  {
    "id": "cs203-t5-ex14",
    "subjectId": "cs203",
    "topicId": "cs203-topic-5",
    "type": "written",
    "title": "Turing Machine Variants",
    "description": "List and briefly describe five TM variants. State which ones are equivalent to the standard TM in computational power.",
    "difficulty": 1,
    "hints": [
      "Consider modifications to tape, head, states",
      "All \"reasonable\" variants are equivalent",
      "Some variants are more convenient but not more powerful"
    ],
    "solution": "**Turing Machine Variants**\n\n**1. Multi-tape TM**\n- Description: k tapes, k independent heads\n- Transitions: δ(q, a₁,...,aₖ) = (p, b₁,...,bₖ, D₁,...,Dₖ)\n- Power: **Equivalent to standard TM**\n- Simulation: Single tape encodes all k tapes (O(T²) overhead)\n- Advantage: More convenient for algorithms\n\n**2. Multi-head TM**\n- Description: Single tape, multiple heads\n- Each head can read/write independently\n- Power: **Equivalent to standard TM**\n- Simulation: Track head positions, simulate sequentially\n- Advantage: Random access patterns easier\n\n**3. Two-way Infinite Tape**\n- Description: Tape extends infinitely in both directions\n- Standard model: one-way infinite (left bound)\n- Power: **Equivalent to standard TM**\n- Simulation: Track two half-tapes, interleaved on one tape\n- Advantage: Symmetric, no special \"left end\"\n\n**4. Nondeterministic TM (NTM)**\n- Description: Multiple possible transitions, accept if any path accepts\n- Power: **Equivalent to standard TM**\n- Simulation: Search all paths (exponential time)\n- Advantage: Concise specification of search problems\n\n**5. Queue Automaton (instead of stack)**\n- Description: FIFO queue instead of tape\n- Power: **Equivalent to standard TM**\n- Simulation: TM can simulate queue; queue can simulate tape\n- Note: Two stacks = queue = TM power\n\n**6. Two-stack PDA**\n- Description: PDA with two stacks\n- Power: **Equivalent to standard TM**\n- Simulation: Stacks represent tape left/right of head\n- Key insight: Two stacks give random access\n\n**7. Random Access Machine (RAM)**\n- Description: Numbered registers, indirect addressing\n- Power: **Equivalent to standard TM** (if integers bounded)\n- More realistic model of actual computers\n- Polynomial relationship with TM\n\n**8. Multi-dimensional Tape**\n- Description: 2D, 3D, or kD tape\n- Power: **Equivalent to standard TM**\n- Simulation: Encode coordinates, simulate navigation\n\n**Non-equivalent variants (weaker):**\n- Single-tape one-way infinite TM with read-only input: Limited\n- TM that can only write 0s: Can't compute all functions\n- TM with bounded tape: Only regular languages\n\n**Thesis:** All \"reasonable\" models with unbounded memory and finite control are Turing-equivalent."
  },
  {
    "id": "cs203-t5-ex15",
    "subjectId": "cs203",
    "topicId": "cs203-topic-5",
    "type": "written",
    "title": "Tape Alphabet Reduction",
    "description": "Prove that any TM can be simulated by a TM using only tape alphabet {0, 1, □}.",
    "difficulty": 3,
    "hints": [
      "Encode larger alphabet in binary",
      "Read multiple cells to decode one symbol",
      "May need to shift tape contents"
    ],
    "solution": "**Reducing Tape Alphabet to {0, 1, □}**\n\n**Theorem:** Any TM M with tape alphabet Γ can be simulated by TM M' with tape alphabet {0, 1, □}.\n\n**Construction:**\n\nLet |Γ| = m. Choose k = ⌈log₂ m⌉.\nEncode each γ ∈ Γ as a k-bit binary string.\n\n**Encoding function:** enc: Γ → {0,1}ᵏ\n- enc(γ₁) = 00...0\n- enc(γ₂) = 00...1\n- etc.\n\n**M' simulates M:**\n\n**Tape representation:**\nM's tape: ... a b c ...\nM' tape: ... enc(a) enc(b) enc(c) ... = ... 010 110 001 ...\n\n**State representation:**\nM' states encode:\n- M's current state\n- Position within current k-block (1 to k)\n- Symbol being read (partially accumulated)\n\n**Simulating one step of M:**\n\n1. **Read:** Scan k cells of M' to read one symbol of M\n   - Accumulate in state memory\n   - Requires k sub-steps\n\n2. **Compute:** Determine M's transition\n   - δ_M(q, a) = (p, b, D)\n\n3. **Write:** Write enc(b) over current k cells\n   - Requires k sub-steps\n\n4. **Move:** Move k cells left or right\n   - If D = L: move k cells left\n   - If D = R: move k cells right\n\n**State blowup:**\n|Q'| = |Q| × k × |Γ| = O(|Q| × log m × m)\n\nStill finite!\n\n**Time overhead:** O(k) = O(log |Γ|) per step\n\n**Handling □:**\n- □ in M' represents multiple □s in encoded tape\n- Or use explicit encoding: enc(□) = 111...1 (or reserved code)\n\n**Key insight:**\nFinite control (states) can remember k bits of encoding.\nBinary is sufficient for any finite alphabet.\n\n**Extensions:**\n- Can reduce to {0, 1} only (use 11 as □ marker, double 1s in data)\n- Can reduce to unary {1, □} (less efficient but possible)"
  },
  {
    "id": "cs203-t5-ex16",
    "subjectId": "cs203",
    "topicId": "cs203-topic-5",
    "type": "written",
    "title": "Oracle Turing Machines",
    "description": "Define an oracle Turing machine. Explain how oracle TMs are used in complexity theory.",
    "difficulty": 5,
    "hints": [
      "Oracle provides \"free\" answers to queries about a language",
      "Measures relative complexity",
      "Used to study relationships between complexity classes"
    ],
    "solution": "**Oracle Turing Machines**\n\n**Definition:**\nAn oracle TM Mᴬ is a TM with access to an \"oracle\" for language A.\n\n**Mechanism:**\n- Special oracle tape\n- Special states: q_query, q_yes, q_no\n- Write query string w on oracle tape\n- Enter q_query\n- Instantly transition to q_yes if w ∈ A, q_no if w ∉ A\n- Oracle answers in one step (no cost)\n\n**Notation:**\n- Mᴬ: TM M with oracle A\n- L(Mᴬ): language decided/recognized by Mᴬ\n- Pᴬ: problems solvable in polynomial time with oracle A\n- NPᴬ: problems in NP with oracle A\n\n**Uses in Complexity Theory:**\n\n**1. Relative complexity:**\nEven if P ≠ NP, we can ask about Pᴬ vs NPᴬ for various A.\n\n**2. Relativization barrier:**\n**Theorem (Baker-Gill-Solovay):**\n- There exists oracle A where Pᴬ = NPᴬ\n- There exists oracle B where Pᴮ ≠ NPᴮ\n\n**Implication:** Any proof that P ≠ NP cannot relativize (must use non-relativizing techniques).\n\n**3. Complexity hierarchies:**\n- PH (polynomial hierarchy) defined using oracles\n- Σₖᴾ = NPᴺᴾᴺᴾ⋅⋅⋅ (k levels of alternating NP)\n\n**4. Reducibility:**\nTuring reducibility: A ≤_T B iff Aᴮ is decidable\nAllows more than one query to oracle.\n\n**5. Hardness for complexity classes:**\nA is NP-hard under Turing reductions iff P = NP implies A ∈ P.\n\n**Examples:**\n\n**Oracle for SAT:**\nWith SAT oracle, can solve any NP problem in polynomial time:\n- Given instance I of NP problem\n- Reduce to SAT formula φ (polynomial time)\n- Query oracle: \"Is φ satisfiable?\"\n- Use answer for I\n\n**Oracle for HALT (halting problem):**\nWith HALT oracle, can decide many undecidable problems:\n- \"Does M accept w?\" — query HALT, then simulate if halts\n- Gives much more power than standard TM\n\n**Arithmetic hierarchy:**\nLevels of oracle power using HALT repeatedly:\n- Σ₁⁰ = RE (recursively enumerable)\n- Π₁⁰ = co-RE\n- Σ₂⁰ = RE with oracle for Σ₁⁰\n- etc.\n\n**Significance:**\nOracle TMs help us understand the structure of complexity, even when absolute questions (like P vs NP) remain open."
  },
  {
    "id": "cs203-t6-ex1",
    "subjectId": "cs203",
    "topicId": "cs203-topic-6",
    "type": "written",
    "title": "The Halting Problem",
    "description": "State the halting problem and prove it is undecidable using diagonalization.",
    "difficulty": 5,
    "hints": [
      "Assume a halting decider H exists",
      "Build a machine D that uses H to contradict itself",
      "Consider what happens when D runs on its own encoding"
    ],
    "solution": "**The Halting Problem**\n\n**Definition:**\nHALT_TM = {⟨M, w⟩ | TM M halts on input w}\n\n**Theorem:** HALT_TM is undecidable.\n\n**Proof by diagonalization:**\n\nAssume for contradiction that HALT_TM is decidable.\nThen there exists a decider H where:\n- H(⟨M, w⟩) = accept if M halts on w\n- H(⟨M, w⟩) = reject if M doesn't halt on w\n\n**Construct machine D:**\n\nD on input ⟨M⟩:\n1. Run H on ⟨M, ⟨M⟩⟩\n2. If H accepts (M halts on ⟨M⟩):\n   Loop forever\n3. If H rejects (M doesn't halt on ⟨M⟩):\n   Halt (accept)\n\n**Consider D on input ⟨D⟩:**\n\n**Case 1:** Suppose D halts on ⟨D⟩\n- Then H(⟨D, ⟨D⟩⟩) accepts (since D halts on ⟨D⟩)\n- So D goes to step 2 and loops forever\n- Contradiction: D doesn't halt on ⟨D⟩\n\n**Case 2:** Suppose D doesn't halt on ⟨D⟩\n- Then H(⟨D, ⟨D⟩⟩) rejects (since D doesn't halt)\n- So D goes to step 3 and halts\n- Contradiction: D does halt on ⟨D⟩\n\n**Both cases lead to contradiction.**\n\nTherefore, our assumption that H exists is false.\nHALT_TM is undecidable. ∎\n\n**Key insight:** Self-reference creates the paradox\n- D \"asks\" about its own behavior\n- Then does the opposite\n- Like the liar's paradox: \"This statement is false\"\n\n**Corollary:** A_TM = {⟨M, w⟩ | M accepts w} is also undecidable\n(Similar proof, or reduce from HALT)"
  },
  {
    "id": "cs203-t6-ex2",
    "subjectId": "cs203",
    "topicId": "cs203-topic-6",
    "type": "written",
    "title": "Mapping Reductions",
    "description": "Define mapping reduction (≤_m) and prove that if A ≤_m B and B is decidable, then A is decidable.",
    "difficulty": 3,
    "hints": [
      "A ≤_m B means there is a computable function f where w ∈ A iff f(w) ∈ B",
      "f transforms A-instances to B-instances",
      "Use the decider for B to decide A"
    ],
    "solution": "**Mapping Reductions**\n\n**Definition:**\nA **mapping reduction** from A to B (written A ≤_m B) is a computable function f: Σ* → Σ* such that:\n\n∀w: w ∈ A ⟺ f(w) ∈ B\n\n**Intuition:**\n- f transforms any instance of A into an instance of B\n- The answer is preserved: YES ↔ YES, NO ↔ NO\n- f is computable (there's a TM computing it)\n\n**Theorem:** If A ≤_m B and B is decidable, then A is decidable.\n\n**Proof:**\n\nGiven:\n- f is the computable reduction from A to B\n- M_B is a decider for B\n\nConstruct decider M_A for A:\n\nM_A on input w:\n1. Compute f(w)\n2. Run M_B on f(w)\n3. Output what M_B outputs\n\n**Correctness:**\n- f is computable, so step 1 terminates\n- M_B is a decider, so step 2 terminates\n- M_A accepts w ⟺ M_B accepts f(w) ⟺ f(w) ∈ B ⟺ w ∈ A ✓\n\n**M_A decides A. ∎**\n\n**Contrapositive (more often used):**\nIf A ≤_m B and A is undecidable, then B is undecidable.\n\nThis is how we prove new undecidability results:\n1. Show known undecidable A reduces to B\n2. Conclude B is undecidable\n\n**Example:** Reducing HALT to A_TM\n\nDefine f(⟨M, w⟩) = ⟨M', w⟩ where M':\n- On input x: Run M on w; if M halts, accept\n\nThen:\n⟨M, w⟩ ∈ HALT ⟺ M halts on w ⟺ M' accepts w ⟺ ⟨M', w⟩ ∈ A_TM\n\nSo HALT ≤_m A_TM. Since HALT is undecidable, A_TM is undecidable.\n\n**Properties of ≤_m:**\n- Reflexive: A ≤_m A (use identity function)\n- Transitive: A ≤_m B and B ≤_m C implies A ≤_m C\n- Not symmetric: A ≤_m B does not imply B ≤_m A"
  },
  {
    "id": "cs203-t6-ex3",
    "subjectId": "cs203",
    "topicId": "cs203-topic-6",
    "type": "written",
    "title": "Rice's Theorem",
    "description": "State Rice's theorem and use it to prove that \"Does TM M accept any string?\" is undecidable.",
    "difficulty": 5,
    "hints": [
      "Rice's theorem is about semantic properties of languages",
      "Non-trivial means some TMs have the property, some don't",
      "The property \"L(M) ≠ ∅\" is semantic and non-trivial"
    ],
    "solution": "**Rice's Theorem**\n\n**Statement:**\nLet P be a property of Turing-recognizable languages (not TMs themselves).\nIf P is **non-trivial** (some TMs have it, some don't), then the problem:\n\n\"Does L(M) satisfy P?\"\n\nis undecidable.\n\n**Formal statement:**\nLet P be a set of Turing-recognizable languages (P ⊂ RE).\nIf P ≠ ∅ and P ≠ RE, then:\nL_P = {⟨M⟩ | L(M) ∈ P} is undecidable.\n\n**Application: \"Does M accept any string?\"**\n\n**The property:** P = {L | L ≠ ∅} (non-empty languages)\n\n**Is P non-trivial?**\n- Some TMs recognize non-empty languages: L = {a} ∈ P ✓\n- Some TMs recognize empty languages: L = ∅ ∉ P ✓\n- P is non-trivial ✓\n\n**Is P semantic?**\n- P depends only on L(M), not on M's structure\n- Two different TMs with L(M₁) = L(M₂) give same answer ✓\n\n**By Rice's theorem:**\nE_TM = {⟨M⟩ | L(M) = ∅} is undecidable.\nEquivalently, NE_TM = {⟨M⟩ | L(M) ≠ ∅} is undecidable. ∎\n\n**Proof of Rice's theorem (sketch):**\n\nReduce from A_TM.\n\nGiven ⟨M, w⟩, construct M' such that:\n- If M accepts w: L(M') ∈ P\n- If M doesn't accept w: L(M') ∉ P\n\nConstruction of M' depends on P:\n- Let M_P be a TM with L(M_P) ∈ P (exists since P non-empty)\n- Let M_∅ be a TM with L(M_∅) ∉ P (exists since P ≠ RE)\n\nM' on input x:\n1. Run M on w (may loop)\n2. If M accepts w, simulate M_P on x\n\nThen:\n- M accepts w ⟹ L(M') = L(M_P) ∈ P\n- M doesn't accept w ⟹ L(M') = ∅ or subset, arrange ∉ P\n\n**More examples using Rice's theorem:**\n- \"Is L(M) regular?\" — undecidable\n- \"Is L(M) = Σ*?\" — undecidable\n- \"Is L(M) finite?\" — undecidable\n- \"Does M accept string w?\" — NOT Rice (depends on specific w, not just property of L(M))"
  },
  {
    "id": "cs203-t6-ex4",
    "subjectId": "cs203",
    "topicId": "cs203-topic-6",
    "type": "written",
    "title": "Decidability of DFA Problems",
    "description": "Prove that the following problems are decidable for DFAs: (a) emptiness, (b) membership, (c) equivalence.",
    "difficulty": 3,
    "hints": [
      "DFAs have finite state structure",
      "Emptiness: is any accepting state reachable?",
      "Equivalence: use symmetric difference"
    ],
    "solution": "**Decidability of DFA Problems**\n\n**(a) DFA Emptiness: E_DFA = {⟨M⟩ | L(M) = ∅}**\n\n**Algorithm:**\n1. Mark the start state\n2. Repeat: mark any state reachable from a marked state\n3. Accept if no accepting state is marked\n4. Reject if some accepting state is marked\n\n**Correctness:**\n- L(M) = ∅ ⟺ no accepting state is reachable from start\n- BFS/DFS finds all reachable states in O(|Q| × |Σ|) time\n\n**Decidable in polynomial time. ✓**\n\n**(b) DFA Membership: A_DFA = {⟨M, w⟩ | M accepts w}**\n\n**Algorithm:**\n1. Simulate M on input w\n2. Process one symbol at a time: q := δ(q, aᵢ)\n3. Accept if final state is in F\n4. Reject otherwise\n\n**Correctness:**\n- DFA is deterministic, so simulation is straightforward\n- Exactly |w| transitions\n- Time: O(|w|)\n\n**Decidable in linear time. ✓**\n\n**(c) DFA Equivalence: EQ_DFA = {⟨M₁, M₂⟩ | L(M₁) = L(M₂)}**\n\n**Algorithm using symmetric difference:**\n\nL(M₁) = L(M₂) ⟺ L(M₁) △ L(M₂) = ∅\n\nwhere △ is symmetric difference:\nL₁ △ L₂ = (L₁ ∩ L̄₂) ∪ (L̄₁ ∩ L₂)\n\n**Steps:**\n1. Construct DFA for L̄₁ (complement of M₁)\n2. Construct DFA for L̄₂ (complement of M₂)\n3. Construct DFA for L₁ ∩ L̄₂ (product construction)\n4. Construct DFA for L̄₁ ∩ L₂ (product construction)\n5. Construct DFA for (L₁ ∩ L̄₂) ∪ (L̄₁ ∩ L₂)\n6. Test if this DFA accepts empty language\n\n**Correctness:**\n- Each step preserves regularity\n- Final DFA accepts L₁ △ L₂\n- L₁ = L₂ ⟺ symmetric difference is empty\n\n**Complexity:** O(|Q₁| × |Q₂|) states in product DFAs\n\n**Decidable in polynomial time. ✓**\n\n**Alternative for equivalence:**\nMinimize both DFAs, check if isomorphic.\nMinimization: O(n log n), isomorphism check: O(n)\n\n**Summary:**\n| Problem | Decidable? | Complexity |\n|---------|------------|------------|\n| E_DFA | Yes | O(n) |\n| A_DFA | Yes | O(|w|) |\n| EQ_DFA | Yes | O(n²) |"
  },
  {
    "id": "cs203-t6-ex5",
    "subjectId": "cs203",
    "topicId": "cs203-topic-6",
    "type": "written",
    "title": "Undecidability of TM Emptiness",
    "description": "Prove that E_TM = {⟨M⟩ | L(M) = ∅} is undecidable by reducing from A_TM.",
    "difficulty": 5,
    "hints": [
      "Given A_TM instance ⟨M, w⟩, construct M' where emptiness depends on whether M accepts w",
      "M' should accept some string iff M accepts w",
      "Use the reduction to show E_TM is undecidable"
    ],
    "solution": "**Proving E_TM is Undecidable**\n\n**Claim:** E_TM = {⟨M⟩ | L(M) = ∅} is undecidable.\n\n**Proof by reduction from A_TM:**\n\nWe show A_TM ≤_m Ē_TM (complement of E_TM).\nThen since A_TM is undecidable, Ē_TM is undecidable, hence E_TM is undecidable.\n\nActually, let's reduce A_TM to Ē_TM directly:\n\n**Reduction:**\nGiven input ⟨M, w⟩ (instance of A_TM), construct TM M':\n\nM' on input x:\n1. Ignore x\n2. Run M on w\n3. If M accepts, accept\n4. (If M rejects or loops, M' doesn't accept x)\n\n**Computing the reduction:**\nOutput ⟨M'⟩\n\n**Correctness:**\n\n**Case: M accepts w**\n- M' reaches step 3 and accepts (for any x)\n- L(M') = Σ* ≠ ∅\n- ⟨M'⟩ ∈ Ē_TM\n\n**Case: M does not accept w**\n- M either rejects or loops forever in step 2\n- M' never accepts any x\n- L(M') = ∅\n- ⟨M'⟩ ∉ Ē_TM (equivalently, ⟨M'⟩ ∈ E_TM)\n\n**Summary:**\n⟨M, w⟩ ∈ A_TM ⟺ L(M') ≠ ∅ ⟺ ⟨M'⟩ ∈ Ē_TM\n\n**Conclusion:**\nA_TM ≤_m Ē_TM\nSince A_TM is undecidable, Ē_TM is undecidable.\nTherefore E_TM is undecidable (complement of undecidable is undecidable for recognizable languages, and we can show E_TM is co-recognizable).\n\nActually, more directly:\nIf E_TM were decidable, then Ē_TM would be decidable.\nBut A_TM ≤_m Ē_TM and A_TM is undecidable.\nSo Ē_TM is undecidable, hence E_TM is undecidable. ∎\n\n**Note:** This also shows E_TM is not Turing-recognizable (it's co-RE but not RE)."
  },
  {
    "id": "cs203-t6-ex6",
    "subjectId": "cs203",
    "topicId": "cs203-topic-6",
    "type": "written",
    "title": "Complement and Recognizability",
    "description": "Prove: L is decidable if and only if both L and L̄ are Turing-recognizable.",
    "difficulty": 3,
    "hints": [
      "For →: a decider recognizes L, and its complement recognizes L̄",
      "For ←: run both recognizers in parallel (dovetailing)",
      "One must accept, giving a decision"
    ],
    "solution": "**Theorem:** L is decidable ⟺ both L and L̄ are Turing-recognizable.\n\n**Proof of (⟹):**\nAssume L is decidable via decider M.\n\n**L is recognizable:**\nM itself recognizes L (deciders are recognizers).\n\n**L̄ is recognizable:**\nConstruct M' from M by swapping accept and reject.\nM' recognizes L̄.\n\n**Proof of (⟸):**\nAssume:\n- M₁ recognizes L\n- M₂ recognizes L̄\n\n**Construct decider D for L:**\n\nD on input w:\n1. Run M₁ and M₂ in parallel on w\n   (Alternate: 1 step of M₁, 1 step of M₂, repeat)\n2. If M₁ accepts, accept\n3. If M₂ accepts, reject\n\n**Correctness:**\n\nFor any w, exactly one of the following holds:\n- w ∈ L (so M₁ will eventually accept)\n- w ∈ L̄ (so M₂ will eventually accept)\n\nSince one must eventually accept, D always halts.\n\n**D accepts w ⟺ M₁ accepts w ⟺ w ∈ L ✓**\n\n**D is a decider for L. ∎**\n\n**Corollary:**\nIf L is recognizable but not decidable, then L̄ is not recognizable.\n\n**Proof:**\nIf L̄ were recognizable, then by the theorem, L would be decidable.\nBut L is not decidable.\nSo L̄ is not recognizable. ∎\n\n**Example:**\n- A_TM is recognizable (simulate M on w, accept if accepts)\n- Ā_TM is NOT recognizable\n- Therefore A_TM is not decidable\n\n**The technique of running in parallel is called \"dovetailing.\"**\n\nIt ensures we don't get stuck waiting on one machine that loops forever."
  },
  {
    "id": "cs203-t6-ex7",
    "subjectId": "cs203",
    "topicId": "cs203-topic-6",
    "type": "written",
    "title": "Self-Reference and Recursion Theorem",
    "description": "State the Recursion theorem for TMs. Explain why it implies that self-printing programs exist.",
    "difficulty": 5,
    "hints": [
      "The recursion theorem lets TMs \"know\" their own description",
      "A TM can compute with ⟨M⟩ where M is itself",
      "This enables quines (self-printing programs)"
    ],
    "solution": "**The Recursion Theorem**\n\n**Statement:**\nFor any computable function t: Σ* → Σ*, there exists a TM R such that:\nR is equivalent to the TM t(⟨R⟩)\n\nIn other words: R can compute using its own description ⟨R⟩.\n\n**Alternative formulation:**\nFor any TM T, there exists a TM R such that:\nR(w) = T(⟨R⟩, w) for all w\n\nR behaves like T, but with access to its own code ⟨R⟩.\n\n**Proof sketch (Kleene's trick):**\n\nLet T be a TM that computes t.\n\nBuild R in two parts: R = A ∘ B\n\n**Part A:**\nOn input w:\n1. Obtain description ⟨B⟩ (hardcoded)\n2. Compute ⟨A ∘ B⟩ = ⟨R⟩\n3. Run T on ⟨R⟩ to get t(⟨R⟩)\n4. Simulate t(⟨R⟩) on original input\n\n**Part B:**\nContains the code of A, allowing A to reconstruct ⟨R⟩.\n\nThe construction is self-referential but well-defined.\n\n**Application: Quines (Self-Printing Programs)**\n\n**Goal:** Build TM Q that prints ⟨Q⟩.\n\n**Using recursion theorem:**\nLet T be a TM that, given ⟨M⟩ and w, outputs ⟨M⟩.\nT(⟨M⟩, w) = ⟨M⟩\n\nBy recursion theorem, there exists Q such that:\nQ(w) = T(⟨Q⟩, w) = ⟨Q⟩\n\n**Q prints its own description! ∎**\n\n**Direct construction of a quine:**\n\nQuine structure: Print-A Print-B\n\nPart A: Print \"Print-B\" then print Part-B\nPart B: Contains encoded representation of Part-A\n\nWhen run:\n1. A prints \"Print-B\" (literal text)\n2. A prints B's contents (which encode A)\n3. Result: \"Print-A Print-B\" = the whole program\n\n**Other applications of recursion theorem:**\n1. Proving undecidability (self-referential constructions)\n2. Computer viruses (self-copying programs)\n3. Fixed-point theorems in computability\n4. Proof that certain problems have no algorithm\n\n**Example undecidability proof using recursion theorem:**\n\nClaim: {⟨M⟩ | M accepts ⟨M⟩} is undecidable.\n\nProof: Suppose decider D exists.\nBy recursion theorem, build M that on any input:\n1. Obtains ⟨M⟩\n2. Runs D on ⟨M⟩\n3. Does opposite of D's answer\n\nThis M contradicts D. So D doesn't exist. ∎"
  },
  {
    "id": "cs203-t6-ex8",
    "subjectId": "cs203",
    "topicId": "cs203-topic-6",
    "type": "written",
    "title": "Post Correspondence Problem",
    "description": "Define the Post Correspondence Problem (PCP). Prove it is undecidable.",
    "difficulty": 5,
    "hints": [
      "PCP is about matching sequences of dominoes",
      "Reduce from a known undecidable problem",
      "Can encode TM computations as domino sequences"
    ],
    "solution": "**Post Correspondence Problem (PCP)**\n\n**Definition:**\nGiven a set of \"dominoes\" (pairs of strings):\n{[t₁/b₁], [t₂/b₂], ..., [tₖ/bₖ]}\n\nwhere tᵢ is the \"top\" string and bᵢ is the \"bottom\" string.\n\n**Question:** Is there a sequence i₁, i₂, ..., iₙ (repeats allowed) such that:\nt_{i₁}t_{i₂}...t_{iₙ} = b_{i₁}b_{i₂}...b_{iₙ}\n\n(Concatenation of tops equals concatenation of bottoms)\n\n**Example:**\nDominoes: {[a/ab], [b/ca], [ca/a], [abc/c]}\n\nMatch: [a/ab][b/ca][ca/a][a/ab][abc/c]\nTops: a·b·ca·a·abc = abcaaabc\nBottoms: ab·ca·a·ab·c = abcaaabc ✓\n\n**Theorem:** PCP is undecidable.\n\n**Proof (reduction from A_TM):**\n\nGiven TM M and input w, construct PCP instance that has a match iff M accepts w.\n\n**Key idea:** Dominoes encode computation history of M on w.\nA match corresponds to a valid accepting computation.\n\n**Construction outline:**\n\n**1. Starting domino:**\nTop: #\nBottom: #q₀w#\n\nForces the match to begin with initial configuration.\n\n**2. Transition dominoes:**\nFor each transition δ(q, a) = (r, b, R):\nTop: qa\nBottom: br\n\nThis allows the match to extend with valid transitions.\n\n**3. Copying dominoes:**\nFor each symbol a:\nTop: a\nBottom: a\n\nCopies tape symbols not at head position.\n\n**4. Configuration separator:**\nTop: #\nBottom: #\n\nSeparates configurations.\n\n**5. Accepting dominoes:**\nWhen accept state appears, special dominoes to \"finish\" the match.\n\n**Why it works:**\n- Match starts with #⟨initial config⟩#\n- Each step extends with valid transition\n- If M accepts, match can be completed\n- If M doesn't accept, no valid match exists\n\n**Technical details:**\nThe construction ensures that any match must encode a valid computation of M on w, ending in accept.\n\n**Conclusion:**\nA_TM ≤_m PCP\nSince A_TM is undecidable, PCP is undecidable. ∎\n\n**MPCP (Modified PCP):**\nMPCP requires starting with a specific first domino.\nOften reduce to MPCP first, then to PCP.\n\n**Applications:**\nPCP undecidability implies many string/grammar problems are undecidable:\n- CFG ambiguity\n- Grammar equivalence\n- Other pattern matching problems"
  },
  {
    "id": "cs203-t6-ex9",
    "subjectId": "cs203",
    "topicId": "cs203-topic-6",
    "type": "written",
    "title": "Decidability of CFG Problems",
    "description": "Determine the decidability status of these CFG problems: (a) membership, (b) emptiness, (c) finiteness. Provide algorithms or prove undecidability.",
    "difficulty": 3,
    "hints": [
      "CFG membership: CYK algorithm works",
      "Emptiness: check if start symbol is useful",
      "Finiteness: look for cycles in grammar graph"
    ],
    "solution": "**Decidability of CFG Problems**\n\n**(a) CFG Membership: A_CFG = {⟨G, w⟩ | w ∈ L(G)}**\n\n**Decidable!**\n\n**Algorithm (CYK):**\n1. Convert G to Chomsky Normal Form\n2. Apply CYK dynamic programming algorithm\n3. Accept if start symbol can derive w\n\n**CYK Algorithm:**\n- Build table T[i,j] = {A | A derives substring wᵢ...wⱼ}\n- Base: T[i,i] = {A | A → wᵢ}\n- Induction: A ∈ T[i,j] if A → BC and B ∈ T[i,k], C ∈ T[k+1,j]\n- Accept if S ∈ T[1,n]\n\n**Complexity:** O(n³ × |G|)\n\n**Decidable in polynomial time. ✓**\n\n**(b) CFG Emptiness: E_CFG = {⟨G⟩ | L(G) = ∅}**\n\n**Decidable!**\n\n**Algorithm:**\n1. Mark all terminals as \"generating\"\n2. Repeat: mark variable A as generating if A → α where all symbols in α are generating\n3. Accept (L(G) = ∅) if S is not marked generating\n\n**Correctness:**\n- A is generating ⟺ A derives some terminal string\n- L(G) ≠ ∅ ⟺ S is generating\n\n**Complexity:** O(|G|)\n\n**Decidable in linear time. ✓**\n\n**(c) CFG Finiteness: FINITE_CFG = {⟨G⟩ | L(G) is finite}**\n\n**Decidable!**\n\n**Algorithm:**\n1. Remove useless symbols and productions\n2. Build \"dependency graph\" of useful variables\n3. L(G) is infinite ⟺ graph has a cycle\n\n**Dependency graph:**\n- Nodes: useful variables\n- Edge A → B if production A → αBβ exists\n\n**Correctness:**\n- Cycle means unbounded recursion, generating infinite language\n- No cycle means bounded derivation depth, finite language\n\n**Complexity:** O(|G|) for graph construction and cycle detection\n\n**Decidable in linear time. ✓**\n\n**Summary:**\n| Problem | Decidable? | Complexity |\n|---------|------------|------------|\n| A_CFG | Yes | O(n³) |\n| E_CFG | Yes | O(|G|) |\n| FINITE_CFG | Yes | O(|G|) |\n\n**Contrast with TM problems:**\nAll three are undecidable for TMs!\n\n**Undecidable CFG problems (for reference):**\n- Ambiguity: Is G ambiguous?\n- Equivalence: L(G₁) = L(G₂)?\n- Universality: L(G) = Σ*?"
  },
  {
    "id": "cs203-t6-ex10",
    "subjectId": "cs203",
    "topicId": "cs203-topic-6",
    "type": "written",
    "title": "The Diagonalization Method",
    "description": "Explain the diagonalization technique and how it's used to prove undecidability. Compare it to Cantor's diagonal argument.",
    "difficulty": 3,
    "hints": [
      "Enumerate all TMs and all strings",
      "Build a \"table\" of acceptance",
      "Construct something that differs from every row"
    ],
    "solution": "**The Diagonalization Method**\n\n**Origin: Cantor's Diagonal Argument (1891)**\n\n**Cantor's proof that reals are uncountable:**\n1. Assume reals in [0,1] are countable: r₁, r₂, r₃, ...\n2. Write decimal expansions:\n   r₁ = 0.d₁₁d₁₂d₁₃...\n   r₂ = 0.d₂₁d₂₂d₂₃...\n   r₃ = 0.d₃₁d₃₂d₃₃...\n   ...\n3. Construct x = 0.x₁x₂x₃... where xᵢ ≠ dᵢᵢ\n4. x differs from each rᵢ in the iᵗʰ position\n5. x is not in the list — contradiction!\n\n**Turing's Application to Computability (1936)**\n\n**Setup:**\n- Enumerate all TMs: M₁, M₂, M₃, ...\n- Enumerate all strings: w₁, w₂, w₃, ...\n- Build \"acceptance table\":\n\n|     | w₁ | w₂ | w₃ | ... |\n|-----|----|----|----|----- |\n| M₁  | 0  | 1  | 0  | ... |\n| M₂  | 1  | 1  | 0  | ... |\n| M₃  | 0  | 0  | 1  | ... |\n| ... | ...| ...| ...| ... |\n\nEntry (i,j) = 1 if Mᵢ accepts wⱼ, else 0.\n\n**Diagonal language:**\nD = {wᵢ | Mᵢ does not accept wᵢ}\n  = {wᵢ | entry (i,i) = 0}\n\n**Claim:** D is not Turing-recognizable.\n\n**Proof:**\nSuppose TM Mₖ recognizes D.\nConsider wₖ:\n- If wₖ ∈ D: Then Mₖ accepts wₖ, so wₖ ∉ D (by definition of D)\n- If wₖ ∉ D: Then Mₖ doesn't accept wₖ, so wₖ ∈ D (by definition of D)\n\nContradiction! So no such Mₖ exists. ∎\n\n**Halting problem proof uses similar structure:**\nInstead of D, define D_halt = {⟨M⟩ | M doesn't halt on ⟨M⟩}\nAssuming HALT is decidable lets us build a TM that contradicts itself.\n\n**Key insight:**\nSelf-reference creates paradox:\n- M is asked about its own behavior on its own description\n- M then does the opposite\n- This is impossible\n\n**Diagonalization template:**\n1. Assume problem is decidable/recognizable\n2. Use assumption to build \"diagonal\" object\n3. Show diagonal differs from every possibility\n4. Contradiction!\n\n**Power and limitations:**\n- Very powerful technique for undecidability\n- But: relativization shows limits (can't separate P vs NP)"
  },
  {
    "id": "cs203-t6-ex11",
    "subjectId": "cs203",
    "topicId": "cs203-topic-6",
    "type": "written",
    "title": "Computation Histories",
    "description": "Define computation history of a TM. Explain how encoding computation histories enables undecidability proofs.",
    "difficulty": 3,
    "hints": [
      "History is sequence of configurations",
      "Can be encoded as a string",
      "Checking validity is local/decidable"
    ],
    "solution": "**Computation Histories**\n\n**Definition:**\nA **computation history** of TM M on input w is a sequence of configurations:\nC₁, C₂, C₃, ..., Cₖ\n\nwhere:\n- C₁ is the starting configuration (q₀, w, start position)\n- Each Cᵢ₊₁ follows from Cᵢ by M's transition function\n- Cₖ is a halting configuration (accept or reject)\n\n**Encoding:**\nEncode each configuration as a string (state, tape contents, head position).\nSeparate configurations with delimiter (e.g., #).\n\nH = #C₁#C₂#...#Cₖ#\n\n**Properties of computation histories:**\n\n**1. Length bounded by time:**\nIf M runs in t steps, history has t+1 configurations.\n\n**2. Local validity checking:**\nTo verify H is valid:\n- Check C₁ is correct starting configuration\n- For each consecutive pair (Cᵢ, Cᵢ₊₁): check transition is valid\n- Check Cₖ is halting\n\nEach local check is decidable! (Just examine finite symbols)\n\n**3. Acceptingness checkable:**\nH is accepting if final configuration is in accept state.\n\n**Why histories enable undecidability proofs:**\n\n**Key insight:** Many problems reduce to \"Does a valid accepting history exist?\"\n\n**Example: A_TM**\n⟨M, w⟩ ∈ A_TM ⟺ ∃ accepting computation history of M on w\n\n**Technique for reduction:**\n\nTo reduce A_TM to problem P:\n1. Given ⟨M, w⟩, construct an instance I_P of P\n2. Design I_P so: I_P is a YES-instance ⟺ accepting history exists\n3. Use local structure of histories in the construction\n\n**Applications:**\n\n**PCP undecidability:**\nConstruct dominoes that \"build\" computation histories.\nMatch exists ⟺ valid accepting history exists.\n\n**CFG problems:**\nEncode history-checking in grammar productions.\nE.g., ALL_CFG = {⟨G⟩ | L(G) = Σ*} is undecidable:\n- Encode histories as strings\n- Accepting histories ∉ L(G) iff some symbol wrong\n- L(G) = Σ* iff no accepting history exists iff M doesn't accept\n\n**Linear Bounded Automata:**\nLBA computation histories are polynomially bounded.\nMany PSPACE-complete problems involve LBA history existence.\n\n**Summary:**\nComputation histories provide:\n- Formal representation of computation\n- Decidable local validity\n- Existential quantification reduces to search/matching\n- Bridge between TM problems and other formalisms"
  },
  {
    "id": "cs203-t6-ex12",
    "subjectId": "cs203",
    "topicId": "cs203-topic-6",
    "type": "written",
    "title": "Busy Beaver Function",
    "description": "Define the Busy Beaver function BB(n). Explain why it is not computable.",
    "difficulty": 5,
    "hints": [
      "BB(n) = max steps among halting n-state TMs",
      "If BB were computable, we could solve halting problem",
      "BB grows faster than any computable function"
    ],
    "solution": "**The Busy Beaver Function**\n\n**Definition:**\nBB(n) = maximum number of 1s that an n-state TM can write on a blank tape before halting.\n\n**Variant (Σ):**\nΣ(n) = max 1s written by halting n-state TM\nS(n) = max steps taken by halting n-state TM\n\n**Known values:**\n- BB(1) = 1\n- BB(2) = 4\n- BB(3) = 6\n- BB(4) = 13\n- BB(5) ≥ 4098 (current lower bound)\n- BB(6) ≥ 10^18267 (!)\n\n**Theorem:** BB(n) is not computable.\n\n**Proof:**\n\nAssume BB is computable (there's a TM computing BB).\n\n**Build halting decider H:**\n\nH on input ⟨M⟩ (encoding of n-state TM M):\n1. Compute BB(n) — we assumed this is possible\n2. Simulate M for BB(n) steps\n3. If M halts within BB(n) steps: report \"halts\"\n4. Otherwise: report \"doesn't halt\"\n\n**Correctness:**\nIf M halts, it does so within BB(n) steps (by definition of BB).\nSo H correctly determines halting.\n\n**But the halting problem is undecidable!**\nContradiction.\n\nTherefore BB is not computable. ∎\n\n**Growth rate:**\nBB grows faster than any computable function!\n\n**Proof:**\nLet f be any computable function.\nThen there's a TM M_f computing f with some fixed number of states c.\n\nFor n > c: an n-state machine can:\n1. Compute f(n) (using c states for M_f)\n2. Write f(n) ones\n3. Halt\n\nSo BB(n) ≥ f(n) for large n.\nSince f was arbitrary computable function, BB eventually dominates it.\n\n**Implications:**\n\n1. **Non-computability:** Can't program BB calculator\n\n2. **Independence:** For large n, BB(n)'s value is independent of ZFC set theory\n   - Can't prove exact value within standard math\n\n3. **Complexity explosion:** Shows how quickly complexity grows with TM size\n\n4. **Gödel incompleteness connection:**\n   - BB values encode unprovable statements\n   - ZFC can't prove BB(n) for all n\n\n**Fun fact:**\nA 5-state TM can simulate Rule 110 (universal cellular automaton), so BB(5) involves universal computation.\n\n**Open problems:**\n- Exact value of BB(5) unknown\n- Many \"busy beaver candidates\" remain unresolved"
  },
  {
    "id": "cs203-t6-ex13",
    "subjectId": "cs203",
    "topicId": "cs203-topic-6",
    "type": "written",
    "title": "Recursively Enumerable Sets",
    "description": "Prove that a set is recursively enumerable (Turing-recognizable) iff it is the range of a partial computable function.",
    "difficulty": 5,
    "hints": [
      "RE sets can be enumerated by a TM",
      "The TM output gives the range",
      "Conversely, given range of f, can recognize membership"
    ],
    "solution": "**Recursively Enumerable Sets and Partial Computable Functions**\n\n**Theorem:** S ⊆ ℕ is recursively enumerable ⟺ S = range(f) for some partial computable function f.\n\n**Note:** A set of strings is RE iff it corresponds to an RE set of natural numbers under standard encoding.\n\n**Proof of (⟹): RE set → Range of partial computable function**\n\nLet M be a TM recognizing S.\n\nDefine partial function f: ℕ → ℕ by:\nf(n) = output of running M on the n-th string, if M accepts\n\n**How f is computed:**\n1. Decode n to get string wₙ\n2. Run M on wₙ\n3. If M accepts, output wₙ (as number)\n4. If M rejects or loops, f(n) is undefined\n\n**Range of f:**\nrange(f) = {f(n) | f(n) is defined}\n         = {wₙ | M accepts wₙ}\n         = S ✓\n\n**f is partial computable** (just simulate M).\n\n**Proof of (⟸): Range of partial computable → RE set**\n\nLet f be a partial computable function.\nS = range(f) = {f(n) | f(n) is defined}\n\n**Build TM M recognizing S:**\n\nM on input x:\n1. For i = 1, 2, 3, ...:\n   For j = 1 to i:\n     Simulate f(j) for i steps\n     If f(j) halts and f(j) = x: accept\n2. Continue forever\n\n**Correctness:**\n- If x ∈ S: x = f(n) for some n\n  - At stage i ≥ max(n, steps to compute f(n)), we find f(n) = x\n  - M accepts\n- If x ∉ S: x is never output by f\n  - M never accepts (runs forever)\n\n**M recognizes S. ✓**\n\n**Dovetailing is crucial:**\nWe can't just compute f(1), f(2), f(3), ... sequentially.\nSome f(n) may be undefined (infinite loop).\nDovetailing ensures we don't get stuck.\n\n**Alternative characterizations of RE:**\n1. Recognized by some TM\n2. Range of a partial computable function\n3. Domain of a partial computable function\n4. Enumerable by a TM (prints elements)\n5. Semi-decidable (can verify YES, not NO)\n\n**Relationship to decidability:**\n- S decidable ⟺ S and S̄ both RE\n- S decidable ⟺ S = range(total computable function) in increasing order"
  },
  {
    "id": "cs203-t6-ex14",
    "subjectId": "cs203",
    "topicId": "cs203-topic-6",
    "type": "written",
    "title": "Undecidability of CFG Ambiguity",
    "description": "Prove that determining whether a CFG is ambiguous is undecidable.",
    "difficulty": 5,
    "hints": [
      "Reduce from PCP",
      "Create a grammar where ambiguity corresponds to PCP match",
      "Two derivations when PCP has solution"
    ],
    "solution": "**Undecidability of CFG Ambiguity**\n\n**Problem:** AMBIG_CFG = {⟨G⟩ | G is ambiguous}\n\n**Theorem:** AMBIG_CFG is undecidable.\n\n**Proof by reduction from PCP:**\n\nGiven PCP instance P = {[t₁/b₁], ..., [tₖ/bₖ]}, construct CFG G such that:\nG is ambiguous ⟺ PCP has a solution\n\n**Construction of G:**\n\n**Idea:** Create two paths to generate the same string iff PCP matches.\n\n**Variables:** S, T, B\n**Terminals:** Σ = {t₁, ..., tₖ, b₁, ..., bₖ} ∪ {a₁, ..., aₖ} (indices as markers)\n\n**Productions:**\n\nS → T | B\n\nT → t₁Ta₁ | t₂Ta₂ | ... | tₖTaₖ | t₁a₁ | t₂a₂ | ... | tₖaₖ\n(T generates strings tᵢ₁tᵢ₂...tᵢₙaᵢₙ...aᵢ₂aᵢ₁ - tops with reversed indices)\n\nB → b₁Ba₁ | b₂Ba₂ | ... | bₖBaₖ | b₁a₁ | b₂a₂ | ... | bₖaₖ\n(B generates strings bᵢ₁bᵢ₂...bᵢₙaᵢₙ...aᵢ₂aᵢ₁ - bottoms with reversed indices)\n\n**Key insight:**\n\nIf PCP has solution i₁, i₂, ..., iₙ (tops = bottoms), then:\n- tᵢ₁tᵢ₂...tᵢₙ = bᵢ₁bᵢ₂...bᵢₙ\n\nThe string tᵢ₁tᵢ₂...tᵢₙaᵢₙ...aᵢ₁ can be derived:\n1. Via S → T → ... (using T productions)\n2. Via S → B → ... (using B productions)\n\n**Two different parse trees, same string → ambiguous!**\n\n**Conversely:**\nIf G is ambiguous, some string has two derivations.\nThe only way this happens is through S → T and S → B producing the same string.\nThis means tᵢ₁...tᵢₙ = bᵢ₁...bᵢₙ for some sequence.\nThis is a PCP solution!\n\n**Correctness:**\nG is ambiguous ⟺ ∃ string with two parse trees\n                ⟺ T and B can generate the same string\n                ⟺ PCP has a solution\n\n**Conclusion:**\nPCP ≤_m AMBIG_CFG\nSince PCP is undecidable, AMBIG_CFG is undecidable. ∎\n\n**Note:** The grammar G is unambiguous if we only consider indices — ambiguity comes from the relationship between tops and bottoms in PCP."
  },
  {
    "id": "cs203-t6-ex15",
    "subjectId": "cs203",
    "topicId": "cs203-topic-6",
    "type": "written",
    "title": "The Arithmetic Hierarchy",
    "description": "Define the arithmetic hierarchy (Σ₁, Π₁, Σ₂, ...). Place HALT and TOTAL in this hierarchy.",
    "difficulty": 5,
    "hints": [
      "Hierarchy based on alternating quantifiers",
      "Σₙ: starts with ∃, n-1 alternations",
      "Πₙ: starts with ∀, n-1 alternations"
    ],
    "solution": "**The Arithmetic Hierarchy**\n\n**Definition:**\nSets classified by complexity of defining formula with quantifiers over ℕ.\n\n**Σ₁ (Recursively Enumerable):**\nS ∈ Σ₁ ⟺ S = {x | ∃y R(x,y)} where R is computable (decidable)\n- One existential quantifier\n- Same as Turing-recognizable (RE)\n\n**Π₁ (co-RE):**\nS ∈ Π₁ ⟺ S = {x | ∀y R(x,y)} where R is computable\n- One universal quantifier\n- Complements of RE sets\n\n**Σ₂:**\nS ∈ Σ₂ ⟺ S = {x | ∃y ∀z R(x,y,z)} where R is computable\n- Exists-forall pattern\n\n**Π₂:**\nS ∈ Π₂ ⟺ S = {x | ∀y ∃z R(x,y,z)} where R is computable\n- Forall-exists pattern\n\n**General pattern:**\n- Σₙ: ∃∀∃∀... (starts ∃, n alternations)\n- Πₙ: ∀∃∀∃... (starts ∀, n alternations)\n\n**Relationships:**\n- Σₙ = co-Πₙ (complements)\n- Σₙ ∪ Πₙ ⊆ Σₙ₊₁ ∩ Πₙ₊₁\n- Decidable = Σ₁ ∩ Π₁ = Δ₁\n\n**Placing HALT:**\n\nHALT = {⟨M, w⟩ | M halts on w}\n\n⟨M, w⟩ ∈ HALT ⟺ ∃t (M halts within t steps on w)\n\nThe predicate \"M halts within t steps\" is decidable (just simulate).\n\n**HALT ∈ Σ₁** (RE) ✓\n\n**HALT ∉ Π₁** (since HALT is undecidable, and Σ₁ ∩ Π₁ = decidable)\n\n**Placing TOTAL:**\n\nTOTAL = {⟨M⟩ | M halts on all inputs}\n      = {⟨M⟩ | ∀w M halts on w}\n\n⟨M⟩ ∈ TOTAL ⟺ ∀w ∃t (M halts on w within t steps)\n\n**TOTAL ∈ Π₂** ✓\n\nIs TOTAL Π₂-complete? Yes!\n- Not in Σ₂ (or even Σ₁ = RE)\n- Complete for Π₂ under many-one reductions\n\n**Summary:**\n| Set | Definition | Level |\n|-----|------------|-------|\n| Decidable | ∃ and ∀ finite | Δ₁ = Σ₁ ∩ Π₁ |\n| HALT | ∃t halts | Σ₁ |\n| H̄ALT | ∀t ¬halts | Π₁ |\n| TOTAL | ∀w ∃t halts | Π₂ |\n| FIN = {M : L(M) finite} | ∃n ∀w (|w|>n → reject) | Σ₂ |\n\n**The hierarchy is strict:**\nΣₙ ⊊ Σₙ₊₁ and Πₙ ⊊ Πₙ₊₁ (proper inclusions)"
  },
  {
    "id": "cs203-t6-ex16",
    "subjectId": "cs203",
    "topicId": "cs203-topic-6",
    "type": "written",
    "title": "Gödel's Incompleteness and Undecidability",
    "description": "Explain the connection between Turing's undecidability results and Gödel's incompleteness theorems.",
    "difficulty": 5,
    "hints": [
      "Both involve self-reference and diagonalization",
      "Undecidability → incompleteness of formal systems",
      "Can encode TM behavior in arithmetic"
    ],
    "solution": "**Gödel's Incompleteness and Turing's Undecidability**\n\n**Historical context:**\n- Gödel (1931): Incompleteness theorems\n- Turing (1936): Undecidability of halting problem\n- Both address fundamental limits of formal systems\n\n**Gödel's First Incompleteness Theorem:**\nAny consistent formal system F capable of expressing basic arithmetic contains statements that are true but unprovable in F.\n\n**Turing's Undecidability:**\nThe halting problem is undecidable — no algorithm can determine if an arbitrary program halts.\n\n**Connection via encoding:**\n\n**Key insight:** TM computations can be encoded in arithmetic.\n\n**Encoding scheme:**\n- TM configurations as numbers\n- Transitions as arithmetic relations\n- \"M halts on w\" becomes an arithmetic statement\n\n**From undecidability to incompleteness:**\n\n**Theorem:** If formal system F is:\n1. Consistent (doesn't prove contradictions)\n2. Sound (only proves true statements)\n3. Can express \"TM M halts on w\"\n\nThen F is incomplete.\n\n**Proof:**\n\nSuppose F is complete and sound.\n\nBuild halting decider H:\n- H(⟨M, w⟩): Search for proof in F of \"M halts\" or \"M doesn't halt\"\n- Since F is complete, one exists\n- Since F is sound, it's true\n- Output accordingly\n\nBut halting is undecidable! Contradiction.\n\nSo F must be incomplete. ∎\n\n**Gödel sentence vs. Halting:**\n\n**Gödel's approach:** Construct G saying \"G is unprovable in F\"\n- If G is provable → F proves something false → inconsistent\n- If ¬G is provable → G is provable → contradiction\n- So G is true but unprovable\n\n**Turing's approach:** Direct via halting\n- If halting were decidable, inconsistent F could be detected\n- Since F consistent, halting undecidable\n- Since F complete would decide halting, F incomplete\n\n**Deeper connections:**\n\n1. **Both use self-reference:**\n   - Gödel: statement about its own provability\n   - Turing: TM examining its own behavior\n\n2. **Both use diagonalization:**\n   - Gödel: enumerate proofs, construct diagonal\n   - Turing: enumerate TMs, construct diagonal\n\n3. **Arithmetization:**\n   - Gödel numbering ≈ TM encoding\n   - Both reduce meta-questions to arithmetic\n\n**Church-Turing thesis connection:**\n- \"Provable\" means \"derivable by algorithm\"\n- Incompleteness = non-computability of truth\n- Formal systems are essentially TMs\n\n**Summary:**\n| Concept | Gödel | Turing |\n|---------|-------|--------|\n| Object | Formal proofs | TM computations |\n| Self-reference | \"This is unprovable\" | \"Does M halt on ⟨M⟩?\" |\n| Result | True but unprovable | Undecidable |\n| Method | Diagonalization | Diagonalization |"
  },
  {
    "id": "cs203-t7-ex1",
    "subjectId": "cs203",
    "topicId": "cs203-topic-7",
    "type": "written",
    "title": "Time Complexity Analysis",
    "description": "Determine the time complexity of a single-tape TM that checks if the input is a palindrome. Show why it requires O(n²) time.",
    "difficulty": 3,
    "hints": [
      "TM must compare first and last symbols",
      "Each comparison requires traversing the tape",
      "Count the total number of tape head movements"
    ],
    "solution": "**Palindrome Recognition on Single-Tape TM**\n\n**Algorithm:**\n1. Compare first and last symbols\n2. If match, mark both, repeat with remaining string\n3. If mismatch, reject\n4. Accept when all symbols matched\n\n**Detailed steps per iteration:**\n1. Read first unmarked symbol (remember it)\n2. Move right to find last unmarked symbol\n3. Compare (accept or reject if mismatch)\n4. Move left to find first position\n5. Mark both symbols\n6. Move to next unmarked position\n\n**Analysis:**\n\nFor input of length n:\n\n**Round 1:**\n- Move right: ~n steps to reach end\n- Move left: ~n steps to return\n- Total: ~2n steps\n\n**Round 2:**\n- Move right: ~(n-2) steps\n- Move left: ~(n-2) steps\n- Total: ~2(n-2) steps\n\n**Round k (for k ≤ n/2):**\n- ~2(n-2k) steps\n\n**Total time:**\nT(n) = 2n + 2(n-2) + 2(n-4) + ... + 2·2\n     = 2(n + (n-2) + (n-4) + ... + 2)\n     = 2 · (n/2 terms) · (n/2) approximately\n     = 2 · (n/2) · (n+2)/2\n     = **O(n²)**\n\n**More precisely:**\nSum = n + (n-2) + (n-4) + ... ≈ n·(n/2)/2 = n²/4\n\n**Why O(n²) is necessary for single-tape:**\n\n- Must compare position 1 with position n\n- Then position 2 with position n-1\n- Each comparison needs Ω(n) moves (far apart)\n- Ω(n/2) comparisons needed\n- Total: Ω(n²)\n\n**Improvement with 2 tapes:**\n- Copy input to tape 2 in reverse: O(n)\n- Compare tape 1 forward, tape 2 forward: O(n)\n- Total: O(n)\n\n**Single-tape palindrome is a classic O(n²) problem!**"
  },
  {
    "id": "cs203-t7-ex2",
    "subjectId": "cs203",
    "topicId": "cs203-topic-7",
    "type": "written",
    "title": "Polynomial Time Verification",
    "description": "Prove that the HAMILTONIAN-PATH problem is in NP by describing a polynomial-time verifier.",
    "difficulty": 3,
    "hints": [
      "What certificate would prove a Hamiltonian path exists?",
      "The path itself is a certificate",
      "Verification checks the path is valid"
    ],
    "solution": "**HAMILTONIAN-PATH is in NP**\n\n**Problem definition:**\nHAMPATH = {⟨G, s, t⟩ | G has a Hamiltonian path from s to t}\n\nA Hamiltonian path visits every vertex exactly once.\n\n**NP membership via verifier:**\n\nTo show HAMPATH ∈ NP, we provide a polynomial-time verifier V.\n\n**Certificate:** A sequence of vertices c = (v₁, v₂, ..., vₙ) claiming to be a Hamiltonian path.\n\n**Certificate size:** O(n) where n = |V| (polynomial in input size)\n\n**Verifier V on input ⟨G, s, t, c⟩:**\n\n1. **Check path starts at s:**\n   Verify v₁ = s\n   Time: O(1)\n\n2. **Check path ends at t:**\n   Verify vₙ = t\n   Time: O(1)\n\n3. **Check all vertices included:**\n   Verify {v₁, v₂, ..., vₙ} = V\n   - Check length is n\n   - Check no duplicates (sort and scan, or use hash set)\n   Time: O(n log n) or O(n) with hashing\n\n4. **Check edges exist:**\n   For i = 1 to n-1:\n     Verify (vᵢ, vᵢ₊₁) ∈ E\n   Time: O(n) checks, each O(degree) or O(1) with adjacency matrix\n   Total: O(n²) or O(n)\n\n5. **If all checks pass:** Accept\n   **Otherwise:** Reject\n\n**Total verification time:** O(n²) — polynomial! ✓\n\n**Correctness:**\n\n**Completeness:** If ⟨G, s, t⟩ ∈ HAMPATH:\n- A Hamiltonian path P = (s, ..., t) exists\n- Using P as certificate, V accepts\n\n**Soundness:** If V accepts with certificate c:\n- c is a path from s to t (checks 1, 2, 4)\n- c visits all vertices exactly once (check 3)\n- So c is a Hamiltonian path\n- Therefore ⟨G, s, t⟩ ∈ HAMPATH\n\n**Conclusion:** HAMPATH has a polynomial-time verifier, so HAMPATH ∈ NP. ∎"
  },
  {
    "id": "cs203-t7-ex3",
    "subjectId": "cs203",
    "topicId": "cs203-topic-7",
    "type": "written",
    "title": "Polynomial-Time Reduction",
    "description": "Show that 3-SAT ≤_p CLIQUE by describing the reduction and proving its correctness.",
    "difficulty": 5,
    "hints": [
      "For each clause, create vertices for each literal",
      "Connect vertices from different clauses if they're consistent",
      "A k-clique corresponds to a satisfying assignment"
    ],
    "solution": "**Reduction: 3-SAT ≤_p CLIQUE**\n\n**Given:** 3-SAT formula φ with m clauses: C₁ ∧ C₂ ∧ ... ∧ Cₘ\nEach clause Cᵢ = (ℓᵢ₁ ∨ ℓᵢ₂ ∨ ℓᵢ₃) has 3 literals.\n\n**Construct:** Graph G = (V, E) and number k\n\n**Construction:**\n\n**Vertices:**\nFor each clause Cᵢ and each literal ℓᵢⱼ in Cᵢ:\nCreate vertex vᵢⱼ = (i, ℓᵢⱼ)\n\nSo |V| = 3m (3 vertices per clause)\n\n**Edges:**\nConnect vᵢⱼ and vₖₗ with an edge if:\n1. i ≠ k (different clauses)\n2. ℓᵢⱼ and ℓₖₗ are consistent (not x and ¬x)\n\n**Set k = m** (number of clauses)\n\n**Claim:** φ is satisfiable ⟺ G has a clique of size m.\n\n**Proof (⟹): Satisfying assignment → m-clique**\n\nLet τ be a satisfying assignment for φ.\n\nFor each clause Cᵢ, at least one literal is true under τ.\nPick one true literal ℓᵢⱼᵢ from each clause.\n\n**Claim:** The vertices {v₁ⱼ₁, v₂ⱼ₂, ..., vₘⱼₘ} form an m-clique.\n\n**Check it's a clique:**\n- Has m vertices (one from each clause)\n- Any two vertices vᵢⱼᵢ and vₖⱼₖ (i ≠ k) are connected because:\n  - Different clauses (i ≠ k) ✓\n  - Literals are consistent: both true under τ, so not x and ¬x ✓\n\n**So G has an m-clique. ✓**\n\n**Proof (⟸): m-clique → satisfying assignment**\n\nLet S = {v₁, v₂, ..., vₘ} be an m-clique in G.\n\n**Observation 1:** S has exactly one vertex from each clause.\n- S has m vertices, m clauses\n- No two clique vertices from same clause (no edges within clause)\n- So exactly one from each\n\n**Observation 2:** Literals in S are consistent.\n- Any two clique vertices have an edge\n- Edges only connect consistent literals\n- So no xᵢ and ¬xᵢ both in S\n\n**Build assignment τ:**\n- If literal xᵢ appears in S: set xᵢ = TRUE\n- If literal ¬xᵢ appears in S: set xᵢ = FALSE\n- For unassigned variables: set arbitrarily\n\n**τ is consistent** (by Observation 2).\n\n**τ satisfies φ:**\n- Each clause has a vertex in S\n- That vertex's literal is true under τ\n- So each clause has a true literal ✓\n\n**φ is satisfiable. ✓**\n\n**Polynomial time:**\n- Create 3m vertices: O(m)\n- Create O(m²) edges: O(m²)\n- Total: polynomial\n\n**Conclusion:** 3-SAT ≤_p CLIQUE ∎\n\nSince 3-SAT is NP-complete, CLIQUE is NP-hard.\nSince CLIQUE ∈ NP, CLIQUE is NP-complete."
  },
  {
    "id": "cs203-t7-ex4",
    "subjectId": "cs203",
    "topicId": "cs203-topic-7",
    "type": "written",
    "title": "P vs NP Implications",
    "description": "Describe three significant implications if P = NP and three implications if P ≠ NP.",
    "difficulty": 3,
    "hints": [
      "Consider cryptography, optimization, AI",
      "Think about practical and theoretical implications",
      "What would change about problem-solving?"
    ],
    "solution": "**Implications of P = NP**\n\n**1. Cryptography collapses:**\n- RSA, AES, and most cryptosystems rely on hardness assumptions\n- Factoring large numbers becomes easy\n- Discrete logarithm becomes easy\n- All current encryption would be breakable\n- Would need completely new cryptographic paradigms\n\n**2. Optimization becomes tractable:**\n- Traveling Salesman: optimal routes found quickly\n- Scheduling problems solved efficiently\n- Resource allocation optimized perfectly\n- Supply chain, logistics, manufacturing revolutionized\n- Protein folding potentially solved\n\n**3. Creative/discovery tasks automated:**\n- Mathematical theorem proving becomes algorithmic\n- Finding proofs as easy as verifying them\n- Drug discovery accelerated\n- Scientific hypothesis generation automated\n- \"Creativity\" reduced to search\n\n**Additional implications if P = NP:**\n- Machine learning: optimal models found directly\n- Game playing: perfect strategies computed\n- Planning: optimal plans found efficiently\n\n---\n\n**Implications of P ≠ NP**\n\n**1. Cryptography remains secure:**\n- One-way functions exist (informally)\n- Current encryption schemes remain valid\n- Digital signatures, authentication preserved\n- E-commerce, banking systems stay secure\n- Privacy-preserving computation possible\n\n**2. Fundamental limits on computation:**\n- Some problems inherently hard\n- No silver bullet for NP-hard problems\n- Must use heuristics, approximations\n- Validates decades of algorithm research\n- Complexity theory has meaningful structure\n\n**3. Creativity/discovery not fully automatable:**\n- Finding proofs harder than checking them\n- Human insight remains valuable\n- AI has fundamental limits\n- Scientific discovery requires more than search\n- Mathematical creativity is \"special\"\n\n**Additional implications if P ≠ NP:**\n- Approximation algorithms remain necessary\n- Parameterized complexity is meaningful\n- Average-case vs worst-case distinction matters\n\n---\n\n**The current practical situation:**\n- We operate AS IF P ≠ NP\n- Security depends on this assumption\n- Heuristics and approximations are standard\n- No polynomial algorithms found despite decades of effort"
  },
  {
    "id": "cs203-t7-ex5",
    "subjectId": "cs203",
    "topicId": "cs203-topic-7",
    "type": "written",
    "title": "NP-Complete Problems",
    "description": "List five NP-complete problems from different domains and briefly explain what each problem asks.",
    "difficulty": 1,
    "hints": [
      "Think of problems from graphs, logic, numbers, scheduling",
      "Each should be a decision problem",
      "Consider why verification is easy"
    ],
    "solution": "**Five NP-Complete Problems**\n\n**1. SATISFIABILITY (SAT) — Logic**\n\n**Input:** Boolean formula φ in CNF\n**Question:** Is there an assignment to variables making φ true?\n\n**Example:** φ = (x ∨ y) ∧ (¬x ∨ z) ∧ (¬y ∨ ¬z)\n**Certificate:** Assignment (x=T, y=F, z=T)\n\nFirst problem proven NP-complete (Cook-Levin, 1971).\n\n---\n\n**2. VERTEX COVER — Graphs**\n\n**Input:** Graph G = (V, E), integer k\n**Question:** Is there a set S ⊆ V with |S| ≤ k such that every edge has at least one endpoint in S?\n\n**Example:** Find 3 vertices covering all edges\n**Certificate:** The k vertices\n\nApplications: network security, facility location.\n\n---\n\n**3. SUBSET SUM — Numbers**\n\n**Input:** Set of integers S = {s₁, ..., sₙ}, target t\n**Question:** Is there a subset T ⊆ S such that Σᵢ∈T sᵢ = t?\n\n**Example:** S = {3, 7, 1, 8, 4}, t = 11\n**Certificate:** Subset {3, 8} or {7, 4}\n\nRelated to knapsack, partition problems.\n\n---\n\n**4. HAMILTONIAN CYCLE — Graphs**\n\n**Input:** Graph G = (V, E)\n**Question:** Is there a cycle visiting every vertex exactly once?\n\n**Example:** Find tour through all cities returning to start\n**Certificate:** The cycle as sequence of vertices\n\nBasis for Traveling Salesman Problem.\n\n---\n\n**5. GRAPH COLORING — Graphs**\n\n**Input:** Graph G = (V, E), integer k\n**Question:** Can G be colored with k colors so no adjacent vertices share a color?\n\n**Example:** 3-coloring a map\n**Certificate:** Color assignment to each vertex\n\nApplications: scheduling, register allocation.\n\n---\n\n**Additional notable NP-complete problems:**\n- **3-SAT:** SAT with exactly 3 literals per clause\n- **CLIQUE:** Does G have a clique of size k?\n- **INDEPENDENT SET:** Does G have k pairwise non-adjacent vertices?\n- **SET COVER:** Cover universe with k sets?\n- **INTEGER PROGRAMMING:** Feasible integer solution exists?\n\n**Common features:**\n- Easy to verify (given solution)\n- Hard to find (no known polynomial algorithm)\n- All equivalent under polynomial reductions"
  },
  {
    "id": "cs203-t7-ex6",
    "subjectId": "cs203",
    "topicId": "cs203-topic-7",
    "type": "written",
    "title": "Space Complexity Classes",
    "description": "Define L, NL, PSPACE, and NPSPACE. State Savitch's theorem and explain its significance.",
    "difficulty": 3,
    "hints": [
      "L and NL use logarithmic space",
      "PSPACE uses polynomial space",
      "Savitch's theorem relates deterministic and nondeterministic space"
    ],
    "solution": "**Space Complexity Classes**\n\n**L (Deterministic Log Space):**\nL = SPACE(log n)\n\nLanguages decidable by a DTM using O(log n) work tape space (input tape is read-only).\n\n**Examples in L:**\n- PATH in undirected graphs (Reingold, 2004)\n- Palindrome checking (with read-only input)\n- Evaluating Boolean formulas\n\n**Key constraint:** Can only store O(log n) bits — just enough to hold a constant number of pointers into the input.\n\n---\n\n**NL (Nondeterministic Log Space):**\nNL = NSPACE(log n)\n\nLanguages decidable by an NTM using O(log n) space.\n\n**Examples in NL:**\n- PATH (directed s-t connectivity) — NL-complete\n- 2-SAT\n- Reachability in graphs\n\n---\n\n**PSPACE (Polynomial Space):**\nPSPACE = ⋃_k SPACE(n^k)\n\nLanguages decidable with polynomial space.\n\n**Examples in PSPACE:**\n- TQBF (True Quantified Boolean Formulas) — PSPACE-complete\n- Generalized games (chess, Go on n×n board)\n- Regular expression equivalence\n\n---\n\n**NPSPACE (Nondeterministic Polynomial Space):**\nNPSPACE = ⋃_k NSPACE(n^k)\n\nLanguages decidable by NTM with polynomial space.\n\n---\n\n**Savitch's Theorem:**\n\n**Theorem:** NSPACE(f(n)) ⊆ SPACE(f(n)²) for f(n) ≥ log n\n\n**In particular:** NPSPACE = PSPACE\n\nNondeterminism gives at most a square blowup in space!\n\n**Proof idea:**\nConvert NTM reachability to deterministic reachability via divide-and-conquer.\n\nTo check if configuration C₁ can reach C₂ in ≤ t steps:\n- Guess midpoint configuration Cₘ\n- Recursively check C₁ → Cₘ in ≤ t/2 steps\n- Recursively check Cₘ → C₂ in ≤ t/2 steps\n\n**Space:** Recursion depth O(log t), each level stores O(f(n)) for configuration.\nTotal: O(f(n) · log t) = O(f(n)²) since t ≤ 2^O(f(n))\n\n---\n\n**Significance of Savitch's Theorem:**\n\n1. **PSPACE = NPSPACE:**\n   Unlike time (where P vs NP is open), nondeterminism doesn't significantly help for space.\n\n2. **Space vs Time:**\n   Space is \"reusable\" — can explore nondeterministic paths sequentially, reusing space.\n\n3. **Complexity hierarchy:**\n   L ⊆ NL ⊆ P ⊆ NP ⊆ PSPACE = NPSPACE ⊆ EXPTIME\n\n4. **Contrasts with time:**\n   - NP ⊆ PSPACE (guess and verify uses space, not just time)\n   - If NP = PSPACE, likely P = NP (but not proven)"
  },
  {
    "id": "cs203-t7-ex7",
    "subjectId": "cs203",
    "topicId": "cs203-topic-7",
    "type": "written",
    "title": "Cook-Levin Theorem",
    "description": "State the Cook-Levin theorem and explain the key idea of the proof (without full details).",
    "difficulty": 5,
    "hints": [
      "SAT is the first NP-complete problem",
      "Any NP verifier can be encoded as a SAT instance",
      "Use Boolean variables to represent computation"
    ],
    "solution": "**The Cook-Levin Theorem**\n\n**Theorem (Cook 1971, Levin 1973):**\nSAT is NP-complete.\n\n**What this means:**\n1. SAT ∈ NP (satisfying assignments can be verified in polynomial time)\n2. Every language L ∈ NP is polynomial-time reducible to SAT\n\n**Significance:**\n- First NP-complete problem ever identified\n- All other NP-completeness proofs reduce from SAT (or its descendants)\n- Shows logic (SAT) captures computational hardness\n\n---\n\n**Proof Idea:**\n\n**Part 1: SAT ∈ NP**\nGiven formula φ and assignment τ:\n- Substitute τ into φ\n- Evaluate: O(|φ|) time\n- Accept if φ evaluates to TRUE\n\n**Part 2: Every NP language reduces to SAT**\n\nLet L ∈ NP with polynomial-time verifier V.\nV accepts (w, c) iff w ∈ L for some certificate c of length p(|w|).\n\n**Key insight:** V's computation can be encoded as a Boolean formula.\n\n**The encoding:**\n\n**Variables represent:**\n- Tape cell contents at each time step\n- Head position at each time step\n- State at each time step\n- Certificate bits\n\nFor input w of length n, V runs in time T = poly(n).\n\nCreate variables:\n- x_{i,j,t} = \"cell i contains symbol j at time t\"\n- h_{i,t} = \"head is at position i at time t\"\n- q_{s,t} = \"machine is in state s at time t\"\n- c_i = \"certificate bit i\"\n\n**Formula φ_w encodes:**\n\n1. **Initial configuration:**\n   - Input w on tape\n   - Head at start\n   - Initial state\n\n2. **Valid computation:**\n   - At most one symbol per cell\n   - Exactly one state, one head position per time\n   - Transitions follow V's transition function\n\n3. **Acceptance:**\n   - Final state is accepting\n\n**Construction is polynomial:**\n- O(T²) variables (time × space)\n- O(T²) clauses for transition constraints\n- All generated in polynomial time\n\n**Correctness:**\n- φ_w satisfiable ⟺ ∃ certificate c such that V(w,c) accepts ⟺ w ∈ L\n\n---\n\n**Key techniques in the proof:**\n\n1. **Tableau method:**\n   Represent computation as a T × T table.\n   Rows = time steps, columns = tape cells.\n\n2. **Local checking:**\n   Transitions affect only local cells.\n   Constraint for each \"window\" of adjacent cells.\n\n3. **CNF construction:**\n   Convert arbitrary Boolean constraints to CNF.\n   At most polynomial blowup.\n\n**Result:**\nFor any w, can construct φ_w in polynomial time.\nw ∈ L ⟺ φ_w ∈ SAT\n\nTherefore L ≤_p SAT for all L ∈ NP. ∎"
  },
  {
    "id": "cs203-t7-ex8",
    "subjectId": "cs203",
    "topicId": "cs203-topic-7",
    "type": "written",
    "title": "coNP and NP ∩ coNP",
    "description": "Define coNP and give examples. Discuss what it means for a problem to be in NP ∩ coNP.",
    "difficulty": 3,
    "hints": [
      "coNP = complements of NP languages",
      "Short proofs of NO instances",
      "NP ∩ coNP: short proofs for both YES and NO"
    ],
    "solution": "**coNP and NP ∩ coNP**\n\n**Definition of coNP:**\ncoNP = {L | L̄ ∈ NP}\n\nLanguages whose complements are in NP.\n\n**Equivalently:**\nL ∈ coNP iff for w ∉ L, there exists a polynomial-size proof that w ∉ L.\n\n---\n\n**Examples in coNP:**\n\n**1. TAUTOLOGY**\nInput: Boolean formula φ\nQuestion: Is φ true under ALL assignments?\n\n- In coNP: If φ is NOT a tautology, a falsifying assignment proves it\n- Certificate: Assignment making φ false\n- Complement of SAT\n\n**2. UNSAT (Unsatisfiability)**\nInput: Boolean formula φ\nQuestion: Is φ unsatisfiable?\n\n- In coNP: Complement of SAT (which is in NP)\n- Certificate for NO: satisfying assignment\n\n**3. PRIMES (before 2002)**\nInput: Integer n\nQuestion: Is n prime?\n\n- Was known to be in NP ∩ coNP\n- Primality certificate (Pratt certificate)\n- Now known to be in P (AKS algorithm)\n\n---\n\n**NP ∩ coNP:**\n\n**Meaning:**\nL ∈ NP ∩ coNP iff both L and L̄ have polynomial-time verifiers.\n\n- YES instances have short proofs\n- NO instances have short proofs\n\n**Why interesting:**\n- Seems \"almost decidable\"\n- If NP ≠ coNP, then P ≠ NP\n- Many natural problems are in NP ∩ coNP\n\n**Examples in NP ∩ coNP:**\n\n**1. Primality**\n- In NP: Pratt certificates\n- In coNP: Factor as proof of compositeness\n- Now known: in P\n\n**2. Factoring (decision version)**\n\"Does n have a factor in [a, b]?\"\n- In NP: Factor is certificate\n- In coNP: Complete factorization shows no factor in range\n- Not known to be in P (basis of RSA)\n\n**3. Linear Programming (feasibility)**\n- In P (Karmarkar's algorithm)\n- So also in NP ∩ coNP\n\n---\n\n**Key question: NP = coNP?**\n\nIf SAT ∈ coNP:\n- UNSAT would have short proofs\n- Could prove \"no satisfying assignment exists\" briefly\n- Seems unlikely (no known such proofs)\n\n**Belief:** NP ≠ coNP\n\n**Implications:**\n- If NP ≠ coNP: P ≠ NP (since P = coP)\n- NP-complete problems are not in coNP (unless NP = coNP)\n\n**Hierarchy:**\nP ⊆ NP ∩ coNP ⊆ NP ∪ coNP ⊆ PSPACE\n\n**Open:** Is (NP ∩ coNP) = P?"
  },
  {
    "id": "cs203-t7-ex9",
    "subjectId": "cs203",
    "topicId": "cs203-topic-7",
    "type": "written",
    "title": "Hierarchy Theorems",
    "description": "State the Time Hierarchy Theorem and explain its significance for complexity theory.",
    "difficulty": 5,
    "hints": [
      "More time allows solving strictly more problems",
      "Based on diagonalization",
      "Separates complexity classes"
    ],
    "solution": "**The Time Hierarchy Theorem**\n\n**Theorem (Hartmanis-Stearns, 1965):**\nIf f, g are time-constructible functions with f(n) = o(g(n)/log g(n)), then:\n\nTIME(f(n)) ⊊ TIME(g(n))\n\n**Simplified version:**\nFor any time-constructible f(n):\n\nTIME(f(n)) ⊊ TIME(f(n)²)\n\nMore time means strictly more computational power.\n\n---\n\n**Meaning:**\n- TIME(n) ⊊ TIME(n²) ⊊ TIME(n³) ⊊ ...\n- P ⊊ EXPTIME (can separate by large enough gap)\n- There is a strict hierarchy of time complexity\n\n---\n\n**Proof Idea (Diagonalization):**\n\nConstruct a language L that:\n1. Can be decided in O(g(n)) time\n2. Cannot be decided in O(f(n)) time\n\n**Construction of L:**\nL = {⟨M, w⟩ | M rejects ⟨M, w⟩ within f(|⟨M, w⟩|) steps}\n\n**L differs from every f(n)-time TM:**\n- Enumerate all TMs: M₁, M₂, ...\n- For each Mᵢ, the string ⟨Mᵢ, 0ᵏ⟩ for appropriate k\n- Mᵢ on ⟨Mᵢ, 0ᵏ⟩ has behavior X\n- L does opposite of X on this input\n\n**Key technical points:**\n- Need time to simulate Mᵢ → requires g(n) >> f(n)\n- log factor accounts for simulation overhead\n- Time-constructibility ensures we can count steps\n\n---\n\n**Corollary: P ⊊ EXPTIME**\n\nP = ∪_k TIME(nᵏ)\nEXPTIME = ∪_k TIME(2^(nᵏ))\n\n2^n grows faster than any polynomial.\nBy hierarchy theorem: TIME(nᵏ) ⊊ TIME(2^n) for all k.\n\nSo P ⊊ EXPTIME.\n\n**This is a proven separation!**\n\n---\n\n**Space Hierarchy Theorem:**\n\n**Theorem:** For space-constructible f:\nSPACE(f(n)) ⊊ SPACE(f(n) · log f(n))\n\nEven tighter than time hierarchy (log factor vs. square).\n\n**Corollary:** L ⊊ PSPACE\n\n---\n\n**Significance:**\n\n**1. Proves real separations exist:**\nUnlike P vs NP (open), we know P ≠ EXPTIME and L ≠ PSPACE.\n\n**2. Validates complexity measures:**\nTime and space genuinely measure difficulty — more resources enable solving more problems.\n\n**3. Methodology limitation:**\nDiagonalization proves P ≠ EXPTIME but cannot separate P from NP (relativization barrier).\n\n**4. Shows hierarchy is \"proper\":**\nNo collapse at any level (given sufficient gap).\n\n**Open questions:**\n- Does TIME(n) ⊊ TIME(n·log n)? (Unknown!)\n- P vs NP requires non-diagonalization techniques"
  },
  {
    "id": "cs203-t7-ex10",
    "subjectId": "cs203",
    "topicId": "cs203-topic-7",
    "type": "written",
    "title": "PSPACE-Completeness",
    "description": "Prove that TQBF (True Quantified Boolean Formulas) is PSPACE-complete.",
    "difficulty": 5,
    "hints": [
      "TQBF is QBF with answer TRUE",
      "Show TQBF ∈ PSPACE via recursive algorithm",
      "For hardness, encode PSPACE computation"
    ],
    "solution": "**TQBF is PSPACE-Complete**\n\n**Problem: TQBF (True Quantified Boolean Formulas)**\n\n**Input:** Quantified Boolean formula ψ = Q₁x₁ Q₂x₂ ... Qₙxₙ φ(x₁,...,xₙ)\nwhere Qᵢ ∈ {∀, ∃} and φ is quantifier-free.\n\n**Question:** Is ψ true?\n\n**Example:**\n∀x ∃y ((x ∨ y) ∧ (¬x ∨ ¬y))\nAnswer: TRUE (for any x, can find y making formula true)\n\n---\n\n**Theorem:** TQBF is PSPACE-complete.\n\n**Part 1: TQBF ∈ PSPACE**\n\n**Recursive algorithm:**\n\nTQBF(ψ):\n  If ψ is quantifier-free: evaluate and return\n  If ψ = ∃x φ: return TQBF(φ[x:=0]) OR TQBF(φ[x:=1])\n  If ψ = ∀x φ: return TQBF(φ[x:=0]) AND TQBF(φ[x:=1])\n\n**Space analysis:**\n- Recursion depth: n (number of variables)\n- Each level stores O(1) information\n- Formula size doesn't grow (substitution is immediate)\n- Total space: O(n) = O(|ψ|) = polynomial\n\n**TQBF ∈ PSPACE ✓**\n\n---\n\n**Part 2: TQBF is PSPACE-hard**\n\nShow: For every L ∈ PSPACE, L ≤_p TQBF.\n\n**Approach:** Encode PSPACE TM computation as QBF.\n\nLet M be a PSPACE machine for L, using space s(n) = poly(n).\n\n**Key insight:** Computation is reachability in configuration graph.\n- Configurations: O(2^s(n)) possible\n- M accepts w iff start config reaches accept config\n\n**Encoding configurations:**\nConfiguration C is a string of s(n) bits.\nCan encode as Boolean variables.\n\n**Encoding reachability:**\n\nLet R(C₁, C₂, t) = \"C₂ reachable from C₁ in ≤ t steps\"\n\n**Base case (t = 1):**\nR(C₁, C₂, 1) encodes: C₂ follows from C₁ in one transition (or C₁ = C₂)\nThis is polynomial-size formula.\n\n**Recursive case:**\nR(C₁, C₂, 2t) = ∃Cₘ [R(C₁, Cₘ, t) ∧ R(Cₘ, C₂, t)]\n\n**Problem:** Formula size doubles each level → exponential.\n\n**Savitch-style fix using ∀:**\nR(C₁, C₂, 2t) = ∃Cₘ ∀(C₃, C₄) ∈ {(C₁,Cₘ), (Cₘ,C₂)} : R(C₃, C₄, t)\n\n**Better notation:**\nR(C₁, C₂, 2t) = ∃Cₘ ∀D₁ ∀D₂ [ ((D₁,D₂)=(C₁,Cₘ) ∨ (D₁,D₂)=(Cₘ,C₂)) → R(D₁, D₂, t) ]\n\n**This adds only O(s(n)) variables per level.**\n\n**Final formula:**\nψ_w = R(C_start, C_accept, 2^s(n))\n\n**Size:** O(s(n)²) = polynomial\n**Levels of recursion:** O(s(n)) (halving each time)\n\n**Correctness:**\nψ_w is true ⟺ M accepts w ⟺ w ∈ L\n\n**Conclusion:**\nL ≤_p TQBF for all L ∈ PSPACE.\nTQBF is PSPACE-hard. ∎\n\n**TQBF is PSPACE-complete. ∎**"
  },
  {
    "id": "cs203-t7-ex11",
    "subjectId": "cs203",
    "topicId": "cs203-topic-7",
    "type": "written",
    "title": "NL-Completeness",
    "description": "Define NL-completeness and log-space reductions. Prove that PATH (directed s-t connectivity) is NL-complete.",
    "difficulty": 5,
    "hints": [
      "NL-complete under log-space reductions",
      "PATH in NL: guess path one vertex at a time",
      "Hardness: encode NTM configurations as vertices"
    ],
    "solution": "**NL-Completeness**\n\n**Definition:**\nA language L is **NL-complete** if:\n1. L ∈ NL\n2. For every A ∈ NL: A ≤_L L (log-space reduction)\n\n**Log-space reduction:**\nf is a log-space reduction if:\n- f is computable in O(log n) space\n- w ∈ A ⟺ f(w) ∈ B\n\n**Key properties:**\n- Log-space ⊆ polynomial-time (so log-space reducible implies poly-time reducible)\n- Composition of log-space reductions is log-space\n\n---\n\n**PATH (Directed s-t Connectivity)**\n\n**Input:** Directed graph G = (V, E), vertices s, t\n**Question:** Is there a directed path from s to t in G?\n\n**Theorem:** PATH is NL-complete.\n\n**Part 1: PATH ∈ NL**\n\n**NTM algorithm:**\n```\ncurrent := s\nfor i := 1 to |V|:\n    if current = t: accept\n    nondeterministically guess next vertex v\n    if (current, v) ∈ E:\n        current := v\n    else: reject\nreject\n```\n\n**Space analysis:**\n- Store current vertex: O(log |V|)\n- Store counter i: O(log |V|)\n- Total: O(log n)\n\n**Correctness:**\n- If path exists: some nondeterministic branch follows it\n- If no path: all branches reject\n\n**PATH ∈ NL ✓**\n\n---\n\n**Part 2: PATH is NL-hard**\n\n**Show:** For every L ∈ NL, L ≤_L PATH.\n\n**Construction:**\nGiven NL machine M for L and input w:\n\nBuild graph G_w where vertices represent configurations of M on w.\n\n**Vertices:**\n- Each configuration C = (state, head position, work tape contents)\n- Work tape uses O(log n) space\n- Number of configurations: poly(n) (still representable)\n\n**Edges:**\n(C₁, C₂) ∈ E iff M can transition from C₁ to C₂ (possibly nondeterministically)\n\n**Special vertices:**\n- s = initial configuration\n- t = accepting configuration (or add new t with edges from all accepting configs)\n\n**Claim:** w ∈ L ⟺ there is a path from s to t in G_w\n\n**Proof:**\n- w ∈ L ⟺ some computation path of M accepts\n- ⟺ some sequence of configurations leads to acceptance\n- ⟺ path exists in G_w from s to t\n\n**Log-space constructibility:**\n- Can output edges one at a time\n- For each pair of configurations, check if transition is valid\n- This requires only O(log n) space\n\n**Conclusion:**\nL ≤_L PATH for all L ∈ NL.\nPATH is NL-hard. ∎\n\n**PATH is NL-complete. ∎**\n\n---\n\n**Important corollary: NL = coNL**\n\n**Immerman-Szelepcsényi Theorem (1988):**\nNL = coNL\n\nNondeterministic log-space is closed under complement!\n\nThis means: We can decide if there is NO path from s to t in NL.\n\n**Proof idea:** Count reachable vertices at each distance, verify count, then check t is not reachable."
  },
  {
    "id": "cs203-t7-ex12",
    "subjectId": "cs203",
    "topicId": "cs203-topic-7",
    "type": "written",
    "title": "Approximation and NP-Hardness",
    "description": "Explain how NP-hardness affects our approach to optimization problems. Give an example of an approximation algorithm.",
    "difficulty": 3,
    "hints": [
      "Can't find optimal in polynomial time (unless P=NP)",
      "Approximation gives near-optimal with guarantees",
      "Vertex cover has a 2-approximation"
    ],
    "solution": "**Approximation Algorithms for NP-Hard Problems**\n\n**The Challenge:**\nNP-hard optimization problems likely have no polynomial-time algorithms for finding optimal solutions (unless P = NP).\n\n**Practical response:**\nInstead of optimal, find solutions that are provably close to optimal.\n\n**Approximation ratio:**\nFor minimization problem:\n- Algorithm A has ratio ρ if: A(I) ≤ ρ · OPT(I) for all instances I\n\nFor maximization:\n- A(I) ≥ OPT(I) / ρ\n\n**ρ = 1 means optimal; ρ = 2 means within factor 2.**\n\n---\n\n**Example: Vertex Cover 2-Approximation**\n\n**Problem:** Find minimum vertex cover (vertices covering all edges).\n\n**Greedy algorithm:**\n```\nC := empty set\nwhile edges remain:\n    pick any edge (u, v)\n    add both u and v to C\n    remove all edges incident to u or v\nreturn C\n```\n\n**Analysis:**\n\n**Claim:** This gives a 2-approximation.\n\n**Proof:**\nLet M = edges picked by algorithm.\nM is a matching (no two edges share a vertex).\n\n**Observation 1:** |C| = 2|M|\n\n**Observation 2:** Any vertex cover must include at least one endpoint of each edge in M (since M is matching).\n\nSo OPT ≥ |M|.\n\n**Therefore:** |C| = 2|M| ≤ 2·OPT\n\n**Approximation ratio = 2. ✓**\n\n---\n\n**Approximation Classes:**\n\n**APX:** Problems with constant-factor approximation\n- Vertex Cover ∈ APX (factor 2)\n- MAX-SAT ∈ APX (factor 2)\n\n**PTAS (Polynomial-Time Approximation Scheme):**\nFor any ε > 0, achieves (1+ε)-approximation in polynomial time (in n, not ε).\n\n- Euclidean TSP has PTAS\n- Knapsack has PTAS\n\n**FPTAS (Fully PTAS):**\nPolynomial in both n and 1/ε.\n- Knapsack has FPTAS\n\n---\n\n**Hardness of Approximation:**\n\nSome problems are hard to approximate within any constant!\n\n**Examples:**\n- **TSP (general):** No constant-factor approximation unless P = NP\n- **SET COVER:** No (1-ε)ln n approximation unless P = NP\n- **CLIQUE:** No n^(1-ε) approximation unless P = NP\n\n**PCP Theorem (1992):**\nProves many inapproximability results.\nConnects proof complexity to approximation.\n\n---\n\n**Summary:**\n\n| Problem | Best Known Approximation | Inapproximability |\n|---------|-------------------------|-------------------|\n| Vertex Cover | 2 | < 1.36 unless P=NP |\n| Metric TSP | 1.5 (Christofides) | < 1 + ε for small ε |\n| General TSP | None | Any constant |\n| MAX-3SAT | 7/8 | > 7/8 unless P=NP |\n\nNP-hardness forces us to accept approximate solutions, and complexity theory tells us how close we can get."
  },
  {
    "id": "cs203-t7-ex13",
    "subjectId": "cs203",
    "topicId": "cs203-topic-7",
    "type": "written",
    "title": "Polynomial Hierarchy",
    "description": "Define the Polynomial Hierarchy (PH). Explain what Σₖᴾ and Πₖᴾ represent.",
    "difficulty": 5,
    "hints": [
      "Built using alternating quantifiers/oracle access",
      "Σ₁ᴾ = NP, Π₁ᴾ = coNP",
      "Each level adds one more quantifier alternation"
    ],
    "solution": "**The Polynomial Hierarchy (PH)**\n\n**Motivation:**\nSome problems seem \"beyond\" NP but still tractable compared to PSPACE.\n\n**Definition via quantifiers:**\n\n**Σₖᴾ** = languages L where:\nw ∈ L ⟺ ∃y₁ ∀y₂ ∃y₃ ... Qₖyₖ R(w, y₁, ..., yₖ)\n\nwhere |yᵢ| ≤ poly(|w|), R is polynomial-time computable, and Qₖ = ∃ if k odd, ∀ if k even.\n\n**Πₖᴾ** = languages L where:\nw ∈ L ⟺ ∀y₁ ∃y₂ ∀y₃ ... Qₖyₖ R(w, y₁, ..., yₖ)\n\nwith Qₖ = ∀ if k odd, ∃ if k even.\n\n---\n\n**Base cases:**\n\n**Σ₀ᴾ = Π₀ᴾ = P**\n\n**Σ₁ᴾ = NP**\nw ∈ L ⟺ ∃y R(w, y) — one existential quantifier\n\n**Π₁ᴾ = coNP**\nw ∈ L ⟺ ∀y R(w, y) — one universal quantifier\n\n---\n\n**Higher levels:**\n\n**Σ₂ᴾ:**\nw ∈ L ⟺ ∃y ∀z R(w, y, z)\n\n**Example:** \"Does formula φ have a satisfying assignment that remains satisfying even if we flip any one variable?\"\n\n**Π₂ᴾ:**\nw ∈ L ⟺ ∀y ∃z R(w, y, z)\n\n**Example:** \"Is every clause of formula φ satisfiable individually?\"\n\n---\n\n**Definition via oracles:**\n\n**Σₖ₊₁ᴾ = NPᴾᵢₖ** (NP with Σₖ oracle)\n**Πₖ₊₁ᴾ = coNPᴾᵢₖ**\n\nThis gives same classes (provably equivalent definitions).\n\n---\n\n**The hierarchy:**\n\n```\n        PH\n       /   \\\n     Σ₃ᴾ   Π₃ᴾ\n    /   \\ /   \\\n  Σ₂ᴾ    Δ₃ᴾ   Π₂ᴾ\n /   \\  / \\  /   \\\nNP    Δ₂ᴾ   coNP\n  \\   /   \\   /\n    P\n```\n\nWhere Δₖᴾ = Σₖ₋₁ᴾ ∩ Πₖ₋₁ᴾ and PH = ⋃ₖ Σₖᴾ\n\n---\n\n**Key properties:**\n\n**1. Containments:**\nΣₖᴾ ⊆ Σₖ₊₁ᴾ ∩ Πₖ₊₁ᴾ\nΠₖᴾ ⊆ Σₖ₊₁ᴾ ∩ Πₖ₊₁ᴾ\n\n**2. Complements:**\nΣₖᴾ = co-Πₖᴾ\n\n**3. PH ⊆ PSPACE:**\nTQBF captures all of PH and is in PSPACE.\n\n**4. Collapse:**\nIf Σₖᴾ = Πₖᴾ for some k, then PH = Σₖᴾ (hierarchy collapses).\n\n---\n\n**Significance:**\n\n**1. Captures \"bounded alternation\":**\nNatural problems at each level.\n\n**2. P = NP implies PH = P:**\nStrongest evidence against P = NP.\n\n**3. Separations unknown:**\nWe don't know if Σₖᴾ ≠ Πₖᴾ for any k > 0.\n\n**4. Complete problems exist:**\nΣₖᴾ-complete and Πₖᴾ-complete problems known for all k.\n\n**Example Σ₂ᴾ-complete:**\n∃∀-SAT: ∃x ∀y φ(x, y) — does setting for x make formula true for all y?"
  },
  {
    "id": "cs203-t7-ex14",
    "subjectId": "cs203",
    "topicId": "cs203-topic-7",
    "type": "written",
    "title": "Complexity of Games",
    "description": "Explain why many two-player games are PSPACE-complete. Use generalized geography as an example.",
    "difficulty": 5,
    "hints": [
      "Games involve alternating moves (∀ and ∃)",
      "Asking \"can player 1 win?\" involves quantifier alternation",
      "PSPACE captures polynomial-space alternating computation"
    ],
    "solution": "**PSPACE-Completeness of Two-Player Games**\n\n**Why games are hard:**\n\nTwo-player games naturally involve alternating quantifiers:\n- \"Player 1 can win\" = ∃ move for P1 such that ∀ responses by P2, ∃ move for P1, ...\n\nThis structure matches TQBF (which is PSPACE-complete).\n\n---\n\n**Generalized Geography**\n\n**Game rules:**\n- Graph G with designated start vertex s\n- Players alternate choosing edges\n- Must follow an outgoing edge from current vertex\n- Cannot revisit vertices\n- Player who cannot move loses\n\n**Decision problem:**\nGG = {⟨G, s⟩ | Player 1 has winning strategy starting from s}\n\n**Theorem:** Generalized Geography is PSPACE-complete.\n\n---\n\n**Proof: GG ∈ PSPACE**\n\n**Recursive algorithm:**\n```\nwins(G, v, visited):\n    if no unvisited neighbors: return LOSE (current player loses)\n    for each unvisited neighbor u of v:\n        if wins(G, u, visited ∪ {v}) == LOSE:\n            return WIN (found winning move)\n    return LOSE (all moves lead to opponent winning)\n```\n\n**Space analysis:**\n- Recursion depth: at most |V| (each vertex visited once)\n- Each level stores: current vertex, visited set bitmap\n- Total: O(|V|) space = polynomial\n\n**GG ∈ PSPACE ✓**\n\n---\n\n**Proof: GG is PSPACE-hard**\n\n**Reduce from TQBF:**\n\nGiven QBF ψ = Q₁x₁ Q₂x₂ ... Qₙxₙ φ\n\nConstruct geography graph G:\n\n**Structure:**\n1. **Variable gadgets:** For each variable xᵢ\n   - Diamond shape: choice of TRUE or FALSE\n   - If ∃xᵢ: current player chooses\n   - If ∀xᵢ: opponent chooses\n\n2. **Clause gadgets:** For each clause in φ\n   - Can be reached if clause is satisfiable under choices\n\n3. **Connecting structure:**\n   - Chain through variables in order\n   - After variables, check if formula satisfied\n\n**Key insight:**\n- Player 1 controls ∃ variables\n- Player 2 controls ∀ variables\n- Player 1 wins iff ψ is TRUE\n\n**Polynomial construction:** O(|ψ|) vertices\n\n**GG is PSPACE-hard. ∎**\n\n---\n\n**Other PSPACE-complete games:**\n\n**1. Generalized Chess**\nPlaying optimal chess on n×n board.\n- Polynomial-length games (50-move rule generalized)\n- Alternating moves\n- PSPACE-complete (or EXPTIME for some variants)\n\n**2. Generalized Checkers**\nSimilar analysis, PSPACE-complete.\n\n**3. Go**\nGeneralized Go is EXPTIME-complete (longer games possible).\n\n**4. Hex**\nDetermining winner from position is PSPACE-complete.\n\n**5. Reversi/Othello**\nPSPACE-complete.\n\n---\n\n**The pattern:**\n- Polynomial-length games → PSPACE\n- Exponential-length games → EXPTIME\n- Alternating moves → quantifier alternation\n- Perfect information → deterministic evaluation\n\n**PSPACE captures \"polynomial-turn games.\"**"
  },
  {
    "id": "cs203-t7-ex15",
    "subjectId": "cs203",
    "topicId": "cs203-topic-7",
    "type": "written",
    "title": "TIME vs SPACE",
    "description": "Prove that TIME(f(n)) ⊆ SPACE(f(n)) and SPACE(f(n)) ⊆ TIME(2^O(f(n))). Discuss what these relationships imply.",
    "difficulty": 3,
    "hints": [
      "Time bounds space linearly",
      "Space bounds time exponentially (via configurations)",
      "These give P ⊆ PSPACE and L ⊆ P"
    ],
    "solution": "**Time-Space Relationships**\n\n**Theorem 1: TIME(f(n)) ⊆ SPACE(f(n))**\n\n**Proof:**\n\nLet M be a TM running in time f(n).\n\nIn f(n) steps, M can visit at most f(n) tape cells.\n(Head moves at most 1 cell per step)\n\nTherefore M uses at most f(n) space.\n\nSo L(M) ∈ SPACE(f(n)). ∎\n\n**Intuition:** You can't use more space than you have time to write.\n\n---\n\n**Theorem 2: SPACE(f(n)) ⊆ TIME(2^O(f(n)))** (for f(n) ≥ log n)\n\n**Proof:**\n\nLet M be a TM using space f(n).\n\n**Count configurations:**\n\nA configuration consists of:\n- State: |Q| choices\n- Head position: f(n) choices\n- Tape contents: |Γ|^f(n) choices\n\nTotal configurations: |Q| × f(n) × |Γ|^f(n) = 2^O(f(n))\n\n**Key observation:**\nIf M halts, it must halt within 2^O(f(n)) steps.\n\n**Why?**\n- If M repeats a configuration, it loops forever\n- So halting computation visits each configuration at most once\n- At most 2^O(f(n)) configurations exist\n- Therefore halts in at most 2^O(f(n)) steps\n\n**Running time:** 2^O(f(n))\n\nSo SPACE(f(n)) ⊆ TIME(2^O(f(n))). ∎\n\n---\n\n**Implications:**\n\n**1. P ⊆ PSPACE:**\n\nTIME(n^k) ⊆ SPACE(n^k) (by Theorem 1)\n\nSo P = ∪_k TIME(n^k) ⊆ ∪_k SPACE(n^k) = PSPACE ✓\n\n**2. L ⊆ P:**\n\nSPACE(log n) ⊆ TIME(2^O(log n)) = TIME(n^O(1)) = P (by Theorem 2)\n\nSo L ⊆ P ✓\n\n**3. PSPACE ⊆ EXPTIME:**\n\nSPACE(n^k) ⊆ TIME(2^O(n^k)) ⊆ TIME(2^n^(k+1)) (by Theorem 2)\n\nSo PSPACE ⊆ EXPTIME ✓\n\n---\n\n**The complexity landscape:**\n\nL ⊆ NL ⊆ P ⊆ NP ⊆ PSPACE = NPSPACE ⊆ EXPTIME\n\n**Known separations:**\n- L ⊊ PSPACE (by space hierarchy)\n- P ⊊ EXPTIME (by time hierarchy)\n\n**Unknown:**\n- L vs P\n- P vs NP\n- NP vs PSPACE\n\n---\n\n**Open question:**\n\nIs SPACE(n) ⊂ TIME(n²)?\n\nWe know: SPACE(n) ⊆ TIME(2^O(n))\nBut can we do better?\n\n**Conjecture:** Space is \"more powerful\" than time.\nIntuition: Space is reusable, time is not.\n\nThe exact relationship between linear space and polynomial time remains open!"
  },
  {
    "id": "cs203-t7-ex16",
    "subjectId": "cs203",
    "topicId": "cs203-topic-7",
    "type": "written",
    "title": "Barriers to P vs NP",
    "description": "Describe the three main barriers (relativization, natural proofs, algebrization) that prevent current techniques from resolving P vs NP.",
    "difficulty": 5,
    "hints": [
      "Each barrier shows a technique that works for some oracle but fails for others",
      "Relativization: diagonalization fails",
      "Natural proofs: combinatorial methods fail"
    ],
    "solution": "**Barriers to Resolving P vs NP**\n\n**Why is P vs NP so hard to settle?**\n\nThree major barriers show that standard proof techniques cannot work.\n\n---\n\n**1. Relativization Barrier (Baker-Gill-Solovay, 1975)**\n\n**Theorem:**\n- There exists an oracle A such that P^A = NP^A\n- There exists an oracle B such that P^B ≠ NP^B\n\n**Implication:**\nAny proof technique that \"relativizes\" (works the same with any oracle) cannot settle P vs NP.\n\n**What relativizes:**\n- Diagonalization\n- Simulation arguments\n- Most \"standard\" complexity proofs\n\n**The hierarchy theorems relativize**, which is why they don't separate P from NP.\n\n**To prove P ≠ NP, we need non-relativizing techniques.**\n\n---\n\n**2. Natural Proofs Barrier (Razborov-Rudich, 1997)**\n\n**Setup:**\nMany circuit lower bound proofs follow a pattern:\n1. Find a \"property\" P that random functions have\n2. Show functions in the target class don't have property P\n3. Conclude lower bound\n\n**Theorem:**\nIf one-way functions exist (a cryptographic assumption), then \"natural proofs\" cannot prove super-polynomial circuit lower bounds for NP.\n\n**Why:**\n- If P is natural, it can distinguish \"hard\" functions from random\n- But one-way functions are hard AND look random\n- So natural property P can't exist (contradicts one-way functions)\n\n**Implication:**\nIf we believe cryptography is possible, our standard combinatorial techniques for circuit lower bounds won't work for NP.\n\n**Many known lower bounds (for restricted circuits) use natural proofs.**\n\n---\n\n**3. Algebrization Barrier (Aaronson-Wigderson, 2008)**\n\n**Extends relativization:**\nConsiders oracles where we also have access to a \"low-degree extension\" of the oracle.\n\n**Theorem:**\nThere exist algebrizing oracles A, B such that:\n- P^A = NP^A (with low-degree extensions)\n- P^B ≠ NP^B (with low-degree extensions)\n\n**Implication:**\nAlgebraic techniques (arithmetization, used in IP=PSPACE) also cannot settle P vs NP.\n\n**Many \"sophisticated\" techniques algebrize:**\n- Interactive proofs\n- PCPs\n- Algebraic proof systems\n\n---\n\n**What does this mean?**\n\n**All three barriers say:**\n\"Standard techniques, even sophisticated ones, won't resolve P vs NP.\"\n\n**A proof of P ≠ NP must:**\n1. Not relativize (use oracle-specific properties)\n2. Not be natural (avoid distinguishers for hard functions)\n3. Not algebrize (go beyond arithmetic circuits)\n\n**Current research directions:**\n- **Geometric Complexity Theory:** Uses algebraic geometry\n- **Circuit complexity for explicit functions:** Beyond natural proofs\n- **Proof complexity:** Lower bounds on proof systems\n\n---\n\n**Summary:**\n\n| Barrier | Year | Blocks |\n|---------|------|--------|\n| Relativization | 1975 | Diagonalization |\n| Natural Proofs | 1997 | Combinatorial circuit lower bounds |\n| Algebrization | 2008 | Algebraic/interactive techniques |\n\n**The barriers don't say P = NP or P ≠ NP; they say our current tools are insufficient.**\n\nAny resolution requires genuinely new ideas!"
  }
]