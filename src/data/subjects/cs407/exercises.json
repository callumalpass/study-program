[
  {
    "id": "cs407-ex-1-1",
    "subjectId": "cs407",
    "topicId": "cs407-t1",
    "title": "Data Quality Checker",
    "description": "Write a function that analyzes a pandas DataFrame and returns a dictionary with data quality metrics: total rows, missing values per column, and completeness percentage.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef analyze_data_quality(df):\n    # Return dict with 'total_rows', 'missing_per_column' (dict), 'completeness_pct' (overall)\n    pass",
    "solution": "import pandas as pd\n\ndef analyze_data_quality(df):\n    total_rows = len(df)\n    missing_per_column = df.isnull().sum().to_dict()\n    total_cells = df.size\n    missing_cells = df.isnull().sum().sum()\n    completeness_pct = ((total_cells - missing_cells) / total_cells * 100) if total_cells > 0 else 100.0\n\n    return {\n        'total_rows': total_rows,\n        'missing_per_column': missing_per_column,\n        'completeness_pct': round(completeness_pct, 2)\n    }",
    "testCases": [
      {
        "input": "pd.DataFrame({\"a\": [1, 2, None], \"b\": [4, None, 6]})",
        "expectedOutput": "{'total_rows': 3, 'missing_per_column': {'a': 1, 'b': 1}, 'completeness_pct': 66.67}",
        "isHidden": false,
        "description": "DataFrame with some missing values"
      },
      {
        "input": "pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})",
        "expectedOutput": "{'total_rows': 3, 'missing_per_column': {'x': 0, 'y': 0}, 'completeness_pct': 100.0}",
        "isHidden": false,
        "description": "Complete DataFrame"
      }
    ],
    "hints": [
      "Use df.isnull() to detect missing values",
      "Use .sum() on boolean DataFrame to count True values",
      "Calculate completeness as (total_cells - missing_cells) / total_cells * 100"
    ]
  },
  {
    "id": "cs407-ex-1-2",
    "subjectId": "cs407",
    "topicId": "cs407-t1",
    "title": "API Data Fetcher",
    "description": "Write a function that makes a GET request to a JSON API endpoint and returns the parsed data. Handle errors gracefully.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import requests\n\ndef fetch_api_data(url):\n    # Return parsed JSON data or None if error\n    pass",
    "solution": "import requests\n\ndef fetch_api_data(url):\n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n        return response.json()\n    except requests.exceptions.RequestException:\n        return None",
    "testCases": [
      {
        "input": "\"https://api.example.com/data\"",
        "expectedOutput": "dict or None",
        "isHidden": false,
        "description": "Fetch from valid endpoint"
      }
    ],
    "hints": [
      "Use requests.get() to make HTTP requests",
      "Use response.json() to parse JSON",
      "Wrap in try-except to handle errors",
      "Use response.raise_for_status() to check for HTTP errors"
    ]
  },
  {
    "id": "cs407-ex-1-3",
    "subjectId": "cs407",
    "topicId": "cs407-t1",
    "title": "JSON Data Parser",
    "description": "Write a function that parses a JSON string and extracts specific nested fields. Handle malformed JSON gracefully.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import json\n\ndef extract_nested_field(json_string, field_path):\n    # field_path is a list like ['user', 'address', 'city']\n    # Return the value at that path or None if not found\n    pass",
    "solution": "import json\n\ndef extract_nested_field(json_string, field_path):\n    try:\n        data = json.loads(json_string)\n        for field in field_path:\n            data = data[field]\n        return data\n    except (json.JSONDecodeError, KeyError, TypeError):\n        return None",
    "testCases": [
      {
        "input": "'{\"user\": {\"address\": {\"city\": \"NYC\"}}}', [\"user\", \"address\", \"city\"]",
        "expectedOutput": "\"NYC\"",
        "isHidden": false,
        "description": "Valid nested JSON"
      },
      {
        "input": "'{\"user\": {\"name\": \"John\"}}', [\"user\", \"address\", \"city\"]",
        "expectedOutput": "None",
        "isHidden": false,
        "description": "Missing nested field"
      },
      {
        "input": "'{invalid json}', [\"user\"]",
        "expectedOutput": "None",
        "isHidden": false,
        "description": "Malformed JSON"
      }
    ],
    "hints": [
      "Use json.loads() to parse JSON string",
      "Iterate through field_path to access nested fields",
      "Catch json.JSONDecodeError, KeyError, and TypeError",
      "Return None for any error"
    ]
  },
  {
    "id": "cs407-ex-1-4",
    "subjectId": "cs407",
    "topicId": "cs407-t1",
    "title": "CSV to DataFrame Loader",
    "description": "Write a function that loads a CSV file into a pandas DataFrame and handles common issues (encoding, delimiters, missing headers).",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef load_csv_robust(filepath, delimiter=',', encoding='utf-8', has_header=True):\n    # Return DataFrame or None if error\n    pass",
    "solution": "import pandas as pd\n\ndef load_csv_robust(filepath, delimiter=',', encoding='utf-8', has_header=True):\n    try:\n        if has_header:\n            df = pd.read_csv(filepath, delimiter=delimiter, encoding=encoding)\n        else:\n            df = pd.read_csv(filepath, delimiter=delimiter, encoding=encoding, header=None)\n        return df\n    except Exception:\n        return None",
    "testCases": [
      {
        "input": "\"data.csv\", \",\", \"utf-8\", True",
        "expectedOutput": "DataFrame or None",
        "isHidden": false,
        "description": "Standard CSV with header"
      },
      {
        "input": "\"data.tsv\", \"\\t\", \"utf-8\", True",
        "expectedOutput": "DataFrame or None",
        "isHidden": false,
        "description": "Tab-separated file"
      }
    ],
    "hints": [
      "Use pd.read_csv() with appropriate parameters",
      "Set header=None when has_header is False",
      "Wrap in try-except to handle file/encoding errors",
      "Return None on any exception"
    ]
  },
  {
    "id": "cs407-ex-1-5",
    "subjectId": "cs407",
    "topicId": "cs407-t1",
    "title": "Web Scraper - Extract Links",
    "description": "Write a function that extracts all hyperlinks from an HTML string using BeautifulSoup.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "from bs4 import BeautifulSoup\n\ndef extract_links(html_string):\n    # Return list of URLs from all <a> tags\n    pass",
    "solution": "from bs4 import BeautifulSoup\n\ndef extract_links(html_string):\n    soup = BeautifulSoup(html_string, 'html.parser')\n    links = []\n    for a_tag in soup.find_all('a', href=True):\n        links.append(a_tag['href'])\n    return links",
    "testCases": [
      {
        "input": "'<html><a href=\"http://example.com\">Link1</a><a href=\"http://test.com\">Link2</a></html>'",
        "expectedOutput": "[\"http://example.com\", \"http://test.com\"]",
        "isHidden": false,
        "description": "HTML with two links"
      },
      {
        "input": "'<html><p>No links here</p></html>'",
        "expectedOutput": "[]",
        "isHidden": false,
        "description": "HTML with no links"
      }
    ],
    "hints": [
      "Create BeautifulSoup object with html.parser",
      "Use .find_all(\"a\", href=True) to find anchor tags with href",
      "Extract href attribute from each tag",
      "Return list of URLs"
    ]
  },
  {
    "id": "cs407-ex-1-6",
    "subjectId": "cs407",
    "topicId": "cs407-t1",
    "title": "REST API Pagination Handler",
    "description": "Write a function that fetches paginated API data by making multiple requests until all pages are retrieved.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import requests\n\ndef fetch_paginated_data(base_url, page_param='page', max_pages=10):\n    # Return list of all items from paginated API\n    # Stop when response is empty or max_pages reached\n    pass",
    "solution": "import requests\n\ndef fetch_paginated_data(base_url, page_param='page', max_pages=10):\n    all_items = []\n    page = 1\n\n    while page <= max_pages:\n        try:\n            response = requests.get(base_url, params={page_param: page}, timeout=10)\n            response.raise_for_status()\n            data = response.json()\n\n            if not data or (isinstance(data, list) and len(data) == 0):\n                break\n\n            if isinstance(data, list):\n                all_items.extend(data)\n            elif isinstance(data, dict) and 'items' in data:\n                all_items.extend(data['items'])\n                if not data['items']:\n                    break\n\n            page += 1\n        except requests.exceptions.RequestException:\n            break\n\n    return all_items",
    "testCases": [
      {
        "input": "\"https://api.example.com/data\", \"page\", 5",
        "expectedOutput": "list of items from all pages",
        "isHidden": false,
        "description": "Paginated API with multiple pages"
      }
    ],
    "hints": [
      "Use a loop to fetch pages sequentially",
      "Pass page number as query parameter",
      "Stop when response is empty or max_pages reached",
      "Accumulate items from all pages in a list",
      "Handle both list responses and dict with items key"
    ]
  },
  {
    "id": "cs407-ex-1-7",
    "subjectId": "cs407",
    "topicId": "cs407-t1",
    "title": "Data Type Validator",
    "description": "Write a function that validates if DataFrame columns match expected data types and returns a report of mismatches.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef validate_column_types(df, expected_types):\n    # expected_types is dict like {'col1': 'int64', 'col2': 'object'}\n    # Return dict with mismatched columns and their actual types\n    pass",
    "solution": "import pandas as pd\n\ndef validate_column_types(df, expected_types):\n    mismatches = {}\n\n    for col, expected_type in expected_types.items():\n        if col not in df.columns:\n            mismatches[col] = 'missing'\n        elif str(df[col].dtype) != expected_type:\n            mismatches[col] = str(df[col].dtype)\n\n    return mismatches",
    "testCases": [
      {
        "input": "pd.DataFrame({\"a\": [1, 2], \"b\": [\"x\", \"y\"]}), {\"a\": \"int64\", \"b\": \"object\"}",
        "expectedOutput": "{}",
        "isHidden": false,
        "description": "All types match"
      },
      {
        "input": "pd.DataFrame({\"a\": [1.0, 2.0], \"b\": [\"x\", \"y\"]}), {\"a\": \"int64\", \"b\": \"object\"}",
        "expectedOutput": "{\"a\": \"float64\"}",
        "isHidden": false,
        "description": "Type mismatch for column a"
      },
      {
        "input": "pd.DataFrame({\"a\": [1, 2]}), {\"a\": \"int64\", \"c\": \"object\"}",
        "expectedOutput": "{\"c\": \"missing\"}",
        "isHidden": false,
        "description": "Missing column c"
      }
    ],
    "hints": [
      "Iterate through expected_types dictionary",
      "Check if column exists in DataFrame",
      "Compare str(df[col].dtype) with expected type",
      "Store mismatches in result dictionary"
    ]
  },
  {
    "id": "cs407-ex-1-8",
    "subjectId": "cs407",
    "topicId": "cs407-t1",
    "title": "HTML Table Extractor",
    "description": "Write a function that extracts the first HTML table from a webpage and converts it to a pandas DataFrame.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import pandas as pd\nfrom bs4 import BeautifulSoup\n\ndef extract_table_to_dataframe(html_string):\n    # Return first table as DataFrame or None if no table found\n    pass",
    "solution": "import pandas as pd\nfrom bs4 import BeautifulSoup\n\ndef extract_table_to_dataframe(html_string):\n    soup = BeautifulSoup(html_string, 'html.parser')\n    table = soup.find('table')\n\n    if not table:\n        return None\n\n    rows = []\n    for tr in table.find_all('tr'):\n        cells = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]\n        if cells:\n            rows.append(cells)\n\n    if not rows:\n        return None\n\n    # First row as header\n    return pd.DataFrame(rows[1:], columns=rows[0]) if len(rows) > 1 else pd.DataFrame(rows)",
    "testCases": [
      {
        "input": "'<table><tr><th>Name</th><th>Age</th></tr><tr><td>John</td><td>30</td></tr></table>'",
        "expectedOutput": "DataFrame with columns Name, Age and one row",
        "isHidden": false,
        "description": "Valid HTML table"
      },
      {
        "input": "'<html><p>No table here</p></html>'",
        "expectedOutput": "None",
        "isHidden": false,
        "description": "HTML with no table"
      }
    ],
    "hints": [
      "Use BeautifulSoup to parse HTML",
      "Find first table element with soup.find(\"table\")",
      "Extract rows with find_all(\"tr\")",
      "Extract cells with find_all([\"td\", \"th\"])",
      "Use first row as column headers"
    ]
  },
  {
    "id": "cs407-ex-1-9",
    "subjectId": "cs407",
    "topicId": "cs407-t1",
    "title": "JSON Array Flattener",
    "description": "Write a function that flattens a nested JSON structure into a flat dictionary with dot-notation keys.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "def flatten_json(nested_dict, parent_key='', sep='.'):\n    # Return flattened dict like {'user.name': 'John', 'user.age': 30}\n    pass",
    "solution": "def flatten_json(nested_dict, parent_key='', sep='.'):\n    items = []\n\n    for key, value in nested_dict.items():\n        new_key = f\"{parent_key}{sep}{key}\" if parent_key else key\n\n        if isinstance(value, dict):\n            items.extend(flatten_json(value, new_key, sep=sep).items())\n        elif isinstance(value, list):\n            for i, item in enumerate(value):\n                if isinstance(item, dict):\n                    items.extend(flatten_json(item, f\"{new_key}[{i}]\", sep=sep).items())\n                else:\n                    items.append((f\"{new_key}[{i}]\", item))\n        else:\n            items.append((new_key, value))\n\n    return dict(items)",
    "testCases": [
      {
        "input": "{\"user\": {\"name\": \"John\", \"age\": 30}}",
        "expectedOutput": "{\"user.name\": \"John\", \"user.age\": 30}",
        "isHidden": false,
        "description": "Nested dictionary"
      },
      {
        "input": "{\"user\": {\"name\": \"John\", \"hobbies\": [\"reading\", \"coding\"]}}",
        "expectedOutput": "{\"user.name\": \"John\", \"user.hobbies[0]\": \"reading\", \"user.hobbies[1]\": \"coding\"}",
        "isHidden": false,
        "description": "Dictionary with array"
      }
    ],
    "hints": [
      "Use recursion to handle nested dictionaries",
      "Build new keys by concatenating with separator",
      "Handle lists by adding index notation [i]",
      "Accumulate flattened items in a list of tuples"
    ]
  },
  {
    "id": "cs407-ex-1-10",
    "subjectId": "cs407",
    "topicId": "cs407-t1",
    "title": "API Rate Limiter",
    "description": "Write a function that fetches data from multiple URLs while respecting a rate limit (requests per second).",
    "difficulty": 4,
    "language": "python",
    "starterCode": "import requests\nimport time\n\ndef fetch_with_rate_limit(urls, requests_per_second=2):\n    # Return list of responses (or None for failed requests)\n    pass",
    "solution": "import requests\nimport time\n\ndef fetch_with_rate_limit(urls, requests_per_second=2):\n    results = []\n    delay = 1.0 / requests_per_second\n\n    for url in urls:\n        try:\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n            results.append(response.json())\n        except requests.exceptions.RequestException:\n            results.append(None)\n\n        if url != urls[-1]:  # Don't delay after last request\n            time.sleep(delay)\n\n    return results",
    "testCases": [
      {
        "input": "[\"https://api.example.com/1\", \"https://api.example.com/2\"], 1",
        "expectedOutput": "list of 2 responses or None values",
        "isHidden": false,
        "description": "Two URLs with 1 request per second"
      }
    ],
    "hints": [
      "Calculate delay as 1.0 / requests_per_second",
      "Use time.sleep(delay) between requests",
      "Store each response in results list",
      "Append None for failed requests",
      "Do not delay after the last request"
    ]
  },
  {
    "id": "cs407-ex-1-11",
    "subjectId": "cs407",
    "topicId": "cs407-t1",
    "title": "CSV Encoding Detector",
    "description": "Write a function that detects the encoding of a CSV file by trying multiple common encodings.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef detect_csv_encoding(filepath):\n    # Try encodings: utf-8, latin-1, cp1252, iso-8859-1\n    # Return the first encoding that works or None\n    pass",
    "solution": "import pandas as pd\n\ndef detect_csv_encoding(filepath):\n    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n\n    for encoding in encodings:\n        try:\n            pd.read_csv(filepath, encoding=encoding, nrows=5)\n            return encoding\n        except (UnicodeDecodeError, Exception):\n            continue\n\n    return None",
    "testCases": [
      {
        "input": "\"data_utf8.csv\"",
        "expectedOutput": "\"utf-8\"",
        "isHidden": false,
        "description": "UTF-8 encoded file"
      },
      {
        "input": "\"data_latin1.csv\"",
        "expectedOutput": "\"latin-1\"",
        "isHidden": false,
        "description": "Latin-1 encoded file"
      }
    ],
    "hints": [
      "Define list of common encodings to try",
      "Use pd.read_csv() with nrows=5 to test quickly",
      "Catch UnicodeDecodeError and other exceptions",
      "Return first encoding that works",
      "Return None if all encodings fail"
    ]
  },
  {
    "id": "cs407-ex-1-12",
    "subjectId": "cs407",
    "topicId": "cs407-t1",
    "title": "Web Scraper - Extract Metadata",
    "description": "Write a function that extracts metadata (title, description, keywords) from HTML meta tags.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "from bs4 import BeautifulSoup\n\ndef extract_metadata(html_string):\n    # Return dict with 'title', 'description', 'keywords'\n    pass",
    "solution": "from bs4 import BeautifulSoup\n\ndef extract_metadata(html_string):\n    soup = BeautifulSoup(html_string, 'html.parser')\n\n    metadata = {\n        'title': None,\n        'description': None,\n        'keywords': None\n    }\n\n    # Extract title\n    title_tag = soup.find('title')\n    if title_tag:\n        metadata['title'] = title_tag.get_text(strip=True)\n\n    # Extract description\n    desc_tag = soup.find('meta', attrs={'name': 'description'})\n    if desc_tag and desc_tag.get('content'):\n        metadata['description'] = desc_tag['content']\n\n    # Extract keywords\n    keywords_tag = soup.find('meta', attrs={'name': 'keywords'})\n    if keywords_tag and keywords_tag.get('content'):\n        metadata['keywords'] = keywords_tag['content']\n\n    return metadata",
    "testCases": [
      {
        "input": "'<html><head><title>Test Page</title><meta name=\"description\" content=\"A test\"><meta name=\"keywords\" content=\"test,page\"></head></html>'",
        "expectedOutput": "{\"title\": \"Test Page\", \"description\": \"A test\", \"keywords\": \"test,page\"}",
        "isHidden": false,
        "description": "HTML with all metadata"
      },
      {
        "input": "'<html><head><title>Only Title</title></head></html>'",
        "expectedOutput": "{\"title\": \"Only Title\", \"description\": None, \"keywords\": None}",
        "isHidden": false,
        "description": "HTML with only title"
      }
    ],
    "hints": [
      "Use soup.find(\"title\") for title tag",
      "Use soup.find(\"meta\", attrs={\"name\": \"description\"}) for description",
      "Extract content attribute from meta tags",
      "Return None for missing metadata"
    ]
  },
  {
    "id": "cs407-ex-1-13",
    "subjectId": "cs407",
    "topicId": "cs407-t1",
    "title": "JSON Schema Validator",
    "description": "Write a function that validates if a JSON object has all required fields and correct types.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "def validate_json_schema(data, schema):\n    # schema is dict like {'name': str, 'age': int, 'email': str}\n    # Return (is_valid: bool, errors: list)\n    pass",
    "solution": "def validate_json_schema(data, schema):\n    errors = []\n\n    if not isinstance(data, dict):\n        return (False, ['Data must be a dictionary'])\n\n    for field, expected_type in schema.items():\n        if field not in data:\n            errors.append(f\"Missing required field: {field}\")\n        elif not isinstance(data[field], expected_type):\n            errors.append(f\"Field '{field}' must be {expected_type.__name__}, got {type(data[field]).__name__}\")\n\n    return (len(errors) == 0, errors)",
    "testCases": [
      {
        "input": "{\"name\": \"John\", \"age\": 30}, {\"name\": str, \"age\": int}",
        "expectedOutput": "(True, [])",
        "isHidden": false,
        "description": "Valid data matching schema"
      },
      {
        "input": "{\"name\": \"John\"}, {\"name\": str, \"age\": int}",
        "expectedOutput": "(False, [\"Missing required field: age\"])",
        "isHidden": false,
        "description": "Missing required field"
      },
      {
        "input": "{\"name\": \"John\", \"age\": \"30\"}, {\"name\": str, \"age\": int}",
        "expectedOutput": "(False, [\"Field 'age' must be int, got str\"])",
        "isHidden": false,
        "description": "Wrong type for field"
      }
    ],
    "hints": [
      "Check if data is a dictionary",
      "Iterate through schema fields",
      "Check if field exists in data",
      "Check if field type matches expected type using isinstance()",
      "Accumulate errors in a list"
    ]
  },
  {
    "id": "cs407-ex-1-14",
    "subjectId": "cs407",
    "topicId": "cs407-t1",
    "title": "API Response Cache",
    "description": "Write a function that caches API responses to avoid redundant requests. Use a dictionary to store responses.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import requests\n\nclass APICache:\n    def __init__(self):\n        self.cache = {}\n\n    def get(self, url):\n        # Return cached response or fetch if not cached\n        pass",
    "solution": "import requests\n\nclass APICache:\n    def __init__(self):\n        self.cache = {}\n\n    def get(self, url):\n        if url in self.cache:\n            return self.cache[url]\n\n        try:\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n            data = response.json()\n            self.cache[url] = data\n            return data\n        except requests.exceptions.RequestException:\n            return None",
    "testCases": [
      {
        "input": "cache = APICache(); cache.get(\"https://api.example.com/data\")",
        "expectedOutput": "dict or None (fetched)",
        "isHidden": false,
        "description": "First request - fetch"
      },
      {
        "input": "cache = APICache(); cache.get(\"https://api.example.com/data\"); cache.get(\"https://api.example.com/data\")",
        "expectedOutput": "dict or None (from cache)",
        "isHidden": false,
        "description": "Second request - cached"
      }
    ],
    "hints": [
      "Check if URL is in cache dictionary",
      "Return cached value if found",
      "Otherwise fetch from API using requests.get()",
      "Store response in cache before returning",
      "Return None on errors"
    ]
  },
  {
    "id": "cs407-ex-1-15",
    "subjectId": "cs407",
    "topicId": "cs407-t1",
    "title": "Multi-format Data Loader",
    "description": "Write a function that loads data from CSV, JSON, or Excel files based on file extension.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import pandas as pd\nimport json\n\ndef load_data_file(filepath):\n    # Return DataFrame for CSV/Excel, dict/list for JSON, or None on error\n    pass",
    "solution": "import pandas as pd\nimport json\n\ndef load_data_file(filepath):\n    try:\n        if filepath.endswith('.csv'):\n            return pd.read_csv(filepath)\n        elif filepath.endswith('.json'):\n            with open(filepath, 'r') as f:\n                return json.load(f)\n        elif filepath.endswith('.xlsx') or filepath.endswith('.xls'):\n            return pd.read_excel(filepath)\n        else:\n            return None\n    except Exception:\n        return None",
    "testCases": [
      {
        "input": "\"data.csv\"",
        "expectedOutput": "DataFrame",
        "isHidden": false,
        "description": "Load CSV file"
      },
      {
        "input": "\"data.json\"",
        "expectedOutput": "dict or list",
        "isHidden": false,
        "description": "Load JSON file"
      },
      {
        "input": "\"data.xlsx\"",
        "expectedOutput": "DataFrame",
        "isHidden": false,
        "description": "Load Excel file"
      }
    ],
    "hints": [
      "Check file extension with filepath.endswith()",
      "Use pd.read_csv() for CSV files",
      "Use json.load() for JSON files",
      "Use pd.read_excel() for Excel files",
      "Return None for unsupported formats or errors"
    ]
  },
  {
    "id": "cs407-ex-1-16",
    "subjectId": "cs407",
    "topicId": "cs407-t1",
    "title": "Duplicate Data Detector",
    "description": "Write a function that detects duplicate rows in a DataFrame and returns statistics about duplicates.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef detect_duplicates(df, subset=None):\n    # Return dict with 'total_duplicates', 'duplicate_percentage', 'duplicate_rows' (indices)\n    pass",
    "solution": "import pandas as pd\n\ndef detect_duplicates(df, subset=None):\n    duplicates = df.duplicated(subset=subset, keep=False)\n    num_duplicates = duplicates.sum()\n    total_rows = len(df)\n    duplicate_pct = (num_duplicates / total_rows * 100) if total_rows > 0 else 0.0\n    duplicate_indices = df[duplicates].index.tolist()\n\n    return {\n        'total_duplicates': int(num_duplicates),\n        'duplicate_percentage': round(duplicate_pct, 2),\n        'duplicate_rows': duplicate_indices\n    }",
    "testCases": [
      {
        "input": "pd.DataFrame({\"a\": [1, 2, 1, 3], \"b\": [4, 5, 4, 6]})",
        "expectedOutput": "{\"total_duplicates\": 2, \"duplicate_percentage\": 50.0, \"duplicate_rows\": [0, 2]}",
        "isHidden": false,
        "description": "DataFrame with duplicates"
      },
      {
        "input": "pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})",
        "expectedOutput": "{\"total_duplicates\": 0, \"duplicate_percentage\": 0.0, \"duplicate_rows\": []}",
        "isHidden": false,
        "description": "DataFrame without duplicates"
      }
    ],
    "hints": [
      "Use df.duplicated(keep=False) to mark all duplicates",
      "Count duplicates with .sum() on boolean series",
      "Calculate percentage as (duplicates / total_rows) * 100",
      "Get indices with df[duplicates].index.tolist()"
    ]
  },
  {
    "id": "cs407-ex-2-1",
    "subjectId": "cs407",
    "topicId": "cs407-t2",
    "title": "Missing Data Imputation",
    "description": "Write a function that imputes missing values in a pandas Series using the mean for numeric data or mode for categorical data.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef impute_missing(series):\n    # Return series with missing values filled\n    pass",
    "solution": "import pandas as pd\n\ndef impute_missing(series):\n    if pd.api.types.is_numeric_dtype(series):\n        return series.fillna(series.mean())\n    else:\n        return series.fillna(series.mode()[0] if not series.mode().empty else series.iloc[0])",
    "testCases": [
      {
        "input": "pd.Series([1, 2, None, 4])",
        "expectedOutput": "pd.Series([1.0, 2.0, 2.33, 4.0])",
        "isHidden": false,
        "description": "Numeric series with missing value"
      },
      {
        "input": "pd.Series([\"a\", \"b\", None, \"a\"])",
        "expectedOutput": "pd.Series([\"a\", \"b\", \"a\", \"a\"])",
        "isHidden": false,
        "description": "Categorical series with missing value"
      }
    ],
    "hints": [
      "Check if series is numeric using pd.api.types.is_numeric_dtype()",
      "Use .fillna() with .mean() for numeric data",
      "Use .fillna() with .mode()[0] for categorical data"
    ]
  },
  {
    "id": "cs407-ex-2-2",
    "subjectId": "cs407",
    "topicId": "cs407-t2",
    "title": "Outlier Detection",
    "description": "Write a function that detects outliers using the IQR method. Return a boolean mask where True indicates an outlier.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef detect_outliers_iqr(series):\n    # Return boolean Series where True = outlier\n    pass",
    "solution": "import pandas as pd\n\ndef detect_outliers_iqr(series):\n    Q1 = series.quantile(0.25)\n    Q3 = series.quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    return (series < lower_bound) | (series > upper_bound)",
    "testCases": [
      {
        "input": "pd.Series([1, 2, 3, 4, 5, 100])",
        "expectedOutput": "pd.Series([False, False, False, False, False, True])",
        "isHidden": false,
        "description": "Series with one outlier"
      }
    ],
    "hints": [
      "Calculate Q1 (25th percentile) and Q3 (75th percentile)",
      "IQR = Q3 - Q1",
      "Lower bound = Q1 - 1.5 * IQR, Upper bound = Q3 + 1.5 * IQR",
      "Return boolean mask for values outside bounds"
    ]
  },
  {
    "id": "cs407-ex-2-3",
    "subjectId": "cs407",
    "topicId": "cs407-t2",
    "title": "Z-Score Outlier Detection",
    "description": "Write a function that detects outliers using z-scores. Mark values with |z-score| > threshold as outliers.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\nimport numpy as np\n\ndef detect_outliers_zscore(series, threshold=3):\n    # Return boolean Series where True = outlier\n    pass",
    "solution": "import pandas as pd\nimport numpy as np\n\ndef detect_outliers_zscore(series, threshold=3):\n    mean = series.mean()\n    std = series.std()\n    z_scores = np.abs((series - mean) / std)\n    return z_scores > threshold",
    "testCases": [
      {
        "input": "pd.Series([1, 2, 3, 4, 5, 100]), 3",
        "expectedOutput": "pd.Series([False, False, False, False, False, True])",
        "isHidden": false,
        "description": "Series with one extreme outlier"
      },
      {
        "input": "pd.Series([10, 12, 13, 12, 11, 13]), 2",
        "expectedOutput": "pd.Series([False, False, False, False, False, False])",
        "isHidden": false,
        "description": "Series with no outliers"
      }
    ],
    "hints": [
      "Calculate mean and standard deviation",
      "Compute z-score as (value - mean) / std",
      "Use np.abs() to get absolute values",
      "Return boolean mask where |z-score| > threshold"
    ]
  },
  {
    "id": "cs407-ex-2-4",
    "subjectId": "cs407",
    "topicId": "cs407-t2",
    "title": "Remove Duplicate Rows",
    "description": "Write a function that removes duplicate rows from a DataFrame, keeping either the first or last occurrence.",
    "difficulty": 1,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef remove_duplicates(df, keep='first', subset=None):\n    # Return DataFrame with duplicates removed\n    pass",
    "solution": "import pandas as pd\n\ndef remove_duplicates(df, keep='first', subset=None):\n    return df.drop_duplicates(subset=subset, keep=keep)",
    "testCases": [
      {
        "input": "pd.DataFrame({\"a\": [1, 2, 1, 3], \"b\": [4, 5, 4, 6]}), \"first\"",
        "expectedOutput": "DataFrame with 3 rows (indices 0, 1, 3)",
        "isHidden": false,
        "description": "Remove duplicates keeping first"
      },
      {
        "input": "pd.DataFrame({\"a\": [1, 2, 1, 3], \"b\": [4, 5, 4, 6]}), \"last\"",
        "expectedOutput": "DataFrame with 3 rows (indices 1, 2, 3)",
        "isHidden": false,
        "description": "Remove duplicates keeping last"
      }
    ],
    "hints": [
      "Use df.drop_duplicates() method",
      "Pass keep parameter to control which duplicate to keep",
      "Use subset parameter to check specific columns"
    ]
  },
  {
    "id": "cs407-ex-2-5",
    "subjectId": "cs407",
    "topicId": "cs407-t2",
    "title": "Forward Fill Missing Values",
    "description": "Write a function that fills missing values using forward fill (propagate last valid value forward).",
    "difficulty": 1,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef forward_fill_missing(series):\n    # Return series with forward fill applied\n    pass",
    "solution": "import pandas as pd\n\ndef forward_fill_missing(series):\n    return series.fillna(method='ffill')",
    "testCases": [
      {
        "input": "pd.Series([1, None, None, 4, None, 6])",
        "expectedOutput": "pd.Series([1.0, 1.0, 1.0, 4.0, 4.0, 6.0])",
        "isHidden": false,
        "description": "Series with missing values"
      },
      {
        "input": "pd.Series([None, 2, None, 4])",
        "expectedOutput": "pd.Series([NaN, 2.0, 2.0, 4.0])",
        "isHidden": false,
        "description": "Series starting with NaN"
      }
    ],
    "hints": [
      "Use .fillna() with method=\"ffill\"",
      "Forward fill propagates last valid value forward",
      "First value stays NaN if it starts with missing"
    ]
  },
  {
    "id": "cs407-ex-2-6",
    "subjectId": "cs407",
    "topicId": "cs407-t2",
    "title": "Interpolate Missing Values",
    "description": "Write a function that interpolates missing numeric values using linear interpolation.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef interpolate_missing(series):\n    # Return series with linear interpolation applied\n    pass",
    "solution": "import pandas as pd\n\ndef interpolate_missing(series):\n    return series.interpolate(method='linear')",
    "testCases": [
      {
        "input": "pd.Series([1, None, None, 4])",
        "expectedOutput": "pd.Series([1.0, 2.0, 3.0, 4.0])",
        "isHidden": false,
        "description": "Linearly interpolate between 1 and 4"
      },
      {
        "input": "pd.Series([10, None, 30])",
        "expectedOutput": "pd.Series([10.0, 20.0, 30.0])",
        "isHidden": false,
        "description": "Interpolate middle value"
      }
    ],
    "hints": [
      "Use .interpolate() with method=\"linear\"",
      "Linear interpolation estimates values on a straight line",
      "Works best for numeric series with trend"
    ]
  },
  {
    "id": "cs407-ex-2-7",
    "subjectId": "cs407",
    "topicId": "cs407-t2",
    "title": "Cap Outliers",
    "description": "Write a function that caps outliers to specified lower and upper percentiles (winsorization).",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef cap_outliers(series, lower_percentile=5, upper_percentile=95):\n    # Return series with values capped to percentile bounds\n    pass",
    "solution": "import pandas as pd\n\ndef cap_outliers(series, lower_percentile=5, upper_percentile=95):\n    lower_bound = series.quantile(lower_percentile / 100)\n    upper_bound = series.quantile(upper_percentile / 100)\n    return series.clip(lower=lower_bound, upper=upper_bound)",
    "testCases": [
      {
        "input": "pd.Series([1, 2, 3, 4, 5, 100]), 10, 90",
        "expectedOutput": "pd.Series with 100 capped to 90th percentile value",
        "isHidden": false,
        "description": "Cap extreme values"
      }
    ],
    "hints": [
      "Calculate bounds using .quantile()",
      "Convert percentiles to fractions (divide by 100)",
      "Use .clip(lower=..., upper=...) to cap values",
      "Values outside bounds are set to nearest bound"
    ]
  },
  {
    "id": "cs407-ex-2-8",
    "subjectId": "cs407",
    "topicId": "cs407-t2",
    "title": "Standardize Column Names",
    "description": "Write a function that standardizes DataFrame column names (lowercase, replace spaces with underscores, remove special chars).",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\nimport re\n\ndef standardize_column_names(df):\n    # Return DataFrame with standardized column names\n    pass",
    "solution": "import pandas as pd\nimport re\n\ndef standardize_column_names(df):\n    df = df.copy()\n    df.columns = [re.sub(r'[^a-zA-Z0-9_]', '', col.lower().replace(' ', '_')) for col in df.columns]\n    return df",
    "testCases": [
      {
        "input": "pd.DataFrame({\"First Name\": [1], \"Last-Name\": [2], \"AGE!\": [3]})",
        "expectedOutput": "DataFrame with columns: first_name, last_name, age",
        "isHidden": false,
        "description": "Standardize various column name formats"
      }
    ],
    "hints": [
      "Convert to lowercase with .lower()",
      "Replace spaces with underscores using .replace()",
      "Remove special characters using re.sub()",
      "Apply transformation to df.columns"
    ]
  },
  {
    "id": "cs407-ex-2-9",
    "subjectId": "cs407",
    "topicId": "cs407-t2",
    "title": "Remove Constant Columns",
    "description": "Write a function that removes columns from a DataFrame where all values are the same.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef remove_constant_columns(df):\n    # Return DataFrame with constant columns removed\n    pass",
    "solution": "import pandas as pd\n\ndef remove_constant_columns(df):\n    return df.loc[:, df.nunique() > 1]",
    "testCases": [
      {
        "input": "pd.DataFrame({\"a\": [1, 2, 3], \"b\": [5, 5, 5], \"c\": [7, 8, 9]})",
        "expectedOutput": "DataFrame with only columns a and c",
        "isHidden": false,
        "description": "Remove column b with constant values"
      },
      {
        "input": "pd.DataFrame({\"x\": [1, 1, 1], \"y\": [2, 2, 2]})",
        "expectedOutput": "Empty DataFrame (all columns constant)",
        "isHidden": false,
        "description": "All columns are constant"
      }
    ],
    "hints": [
      "Use .nunique() to count unique values per column",
      "Filter columns where nunique() > 1",
      "Use .loc[:, condition] to select columns"
    ]
  },
  {
    "id": "cs407-ex-2-10",
    "subjectId": "cs407",
    "topicId": "cs407-t2",
    "title": "Handle Mixed Data Types",
    "description": "Write a function that converts a column with mixed types to a consistent type (numeric or string).",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef convert_to_consistent_type(series, target_type='numeric'):\n    # target_type can be 'numeric' or 'string'\n    # Return converted series, coerce errors to NaN for numeric\n    pass",
    "solution": "import pandas as pd\n\ndef convert_to_consistent_type(series, target_type='numeric'):\n    if target_type == 'numeric':\n        return pd.to_numeric(series, errors='coerce')\n    elif target_type == 'string':\n        return series.astype(str)\n    else:\n        return series",
    "testCases": [
      {
        "input": "pd.Series([1, \"2\", 3.5, \"invalid\"]), \"numeric\"",
        "expectedOutput": "pd.Series([1.0, 2.0, 3.5, NaN])",
        "isHidden": false,
        "description": "Convert mixed types to numeric"
      },
      {
        "input": "pd.Series([1, 2, 3]), \"string\"",
        "expectedOutput": "pd.Series([\"1\", \"2\", \"3\"])",
        "isHidden": false,
        "description": "Convert to strings"
      }
    ],
    "hints": [
      "Use pd.to_numeric() with errors=\"coerce\" for numeric conversion",
      "Use .astype(str) for string conversion",
      "Coerce converts invalid values to NaN"
    ]
  },
  {
    "id": "cs407-ex-2-11",
    "subjectId": "cs407",
    "topicId": "cs407-t2",
    "title": "Normalize Text Data",
    "description": "Write a function that normalizes text in a Series (lowercase, trim whitespace, remove extra spaces).",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef normalize_text(series):\n    # Return series with normalized text\n    pass",
    "solution": "import pandas as pd\n\ndef normalize_text(series):\n    return series.str.strip().str.lower().str.replace(r'\\s+', ' ', regex=True)",
    "testCases": [
      {
        "input": "pd.Series([\"  Hello World  \", \"DATA  Science\", \"Python   \"])",
        "expectedOutput": "pd.Series([\"hello world\", \"data science\", \"python\"])",
        "isHidden": false,
        "description": "Normalize various text formats"
      }
    ],
    "hints": [
      "Use .str.strip() to remove leading/trailing whitespace",
      "Use .str.lower() to convert to lowercase",
      "Use .str.replace() with regex to collapse multiple spaces",
      "Chain string methods together"
    ]
  },
  {
    "id": "cs407-ex-2-12",
    "subjectId": "cs407",
    "topicId": "cs407-t2",
    "title": "Drop High Missing Columns",
    "description": "Write a function that drops columns with missing value percentage above a threshold.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef drop_high_missing_columns(df, threshold=50):\n    # threshold is percentage (e.g., 50 means drop if >50% missing)\n    # Return DataFrame with high-missing columns removed\n    pass",
    "solution": "import pandas as pd\n\ndef drop_high_missing_columns(df, threshold=50):\n    missing_pct = (df.isnull().sum() / len(df)) * 100\n    cols_to_keep = missing_pct[missing_pct <= threshold].index\n    return df[cols_to_keep]",
    "testCases": [
      {
        "input": "pd.DataFrame({\"a\": [1, 2, None, None], \"b\": [1, None, 3, 4], \"c\": [1, 2, 3, 4]}), 50",
        "expectedOutput": "DataFrame with columns b and c only (column a has >50% missing)",
        "isHidden": false,
        "description": "Drop column with high missing rate"
      }
    ],
    "hints": [
      "Calculate missing percentage per column",
      "Divide missing count by total rows and multiply by 100",
      "Filter columns where percentage <= threshold",
      "Return DataFrame with selected columns"
    ]
  },
  {
    "id": "cs407-ex-2-13",
    "subjectId": "cs407",
    "topicId": "cs407-t2",
    "title": "Parse Date Strings",
    "description": "Write a function that parses date strings in various formats to datetime objects.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef parse_dates(series, date_format=None):\n    # Try to parse dates, return datetime series or original if fails\n    pass",
    "solution": "import pandas as pd\n\ndef parse_dates(series, date_format=None):\n    try:\n        if date_format:\n            return pd.to_datetime(series, format=date_format, errors='coerce')\n        else:\n            return pd.to_datetime(series, infer_datetime_format=True, errors='coerce')\n    except Exception:\n        return series",
    "testCases": [
      {
        "input": "pd.Series([\"2023-01-15\", \"2023-02-20\", \"2023-03-25\"])",
        "expectedOutput": "Series of datetime objects",
        "isHidden": false,
        "description": "Parse ISO format dates"
      },
      {
        "input": "pd.Series([\"01/15/2023\", \"02/20/2023\"]), \"%m/%d/%Y\"",
        "expectedOutput": "Series of datetime objects",
        "isHidden": false,
        "description": "Parse with specific format"
      }
    ],
    "hints": [
      "Use pd.to_datetime() to parse dates",
      "Pass format parameter if specific format provided",
      "Use infer_datetime_format=True for automatic detection",
      "Use errors=\"coerce\" to handle invalid dates"
    ]
  },
  {
    "id": "cs407-ex-2-14",
    "subjectId": "cs407",
    "topicId": "cs407-t2",
    "title": "Remove Outliers by IQR",
    "description": "Write a function that removes rows containing outliers detected by IQR method in any numeric column.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef remove_outliers_iqr(df):\n    # Return DataFrame with outlier rows removed\n    pass",
    "solution": "import pandas as pd\n\ndef remove_outliers_iqr(df):\n    df_numeric = df.select_dtypes(include=['number'])\n    outlier_mask = pd.Series([False] * len(df))\n\n    for col in df_numeric.columns:\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        outlier_mask |= (df[col] < lower_bound) | (df[col] > upper_bound)\n\n    return df[~outlier_mask]",
    "testCases": [
      {
        "input": "pd.DataFrame({\"a\": [1, 2, 3, 100], \"b\": [10, 20, 30, 40]})",
        "expectedOutput": "DataFrame with outlier row removed",
        "isHidden": false,
        "description": "Remove row with outlier in column a"
      }
    ],
    "hints": [
      "Select numeric columns only",
      "For each numeric column, calculate IQR bounds",
      "Build boolean mask marking outlier rows",
      "Use OR operator to combine masks across columns",
      "Filter DataFrame to keep non-outlier rows"
    ]
  },
  {
    "id": "cs407-ex-2-15",
    "subjectId": "cs407",
    "topicId": "cs407-t2",
    "title": "Fill Missing with Group Mean",
    "description": "Write a function that fills missing values in a column with the mean of its group.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef fill_missing_with_group_mean(df, target_col, group_col):\n    # Fill missing values in target_col with mean of its group_col\n    # Return DataFrame with filled values\n    pass",
    "solution": "import pandas as pd\n\ndef fill_missing_with_group_mean(df, target_col, group_col):\n    df = df.copy()\n    df[target_col] = df.groupby(group_col)[target_col].transform(lambda x: x.fillna(x.mean()))\n    return df",
    "testCases": [
      {
        "input": "pd.DataFrame({\"group\": [\"A\", \"A\", \"B\", \"B\"], \"value\": [10, None, 20, None]}), \"value\", \"group\"",
        "expectedOutput": "DataFrame with value column: [10, 10, 20, 20]",
        "isHidden": false,
        "description": "Fill missing values with group means"
      }
    ],
    "hints": [
      "Use .groupby() to group by group_col",
      "Use .transform() to apply function within groups",
      "Fill missing with group mean using fillna(x.mean())",
      "Make a copy to avoid modifying original"
    ]
  },
  {
    "id": "cs407-ex-2-16",
    "subjectId": "cs407",
    "topicId": "cs407-t2",
    "title": "Clean Phone Numbers",
    "description": "Write a function that cleans phone number strings by removing non-digit characters and formatting consistently.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\nimport re\n\ndef clean_phone_numbers(series):\n    # Remove non-digits, format as XXX-XXX-XXXX (10 digits) or return original if invalid\n    pass",
    "solution": "import pandas as pd\nimport re\n\ndef clean_phone_numbers(series):\n    def format_phone(phone):\n        if pd.isna(phone):\n            return phone\n        digits = re.sub(r'\\D', '', str(phone))\n        if len(digits) == 10:\n            return f\"{digits[:3]}-{digits[3:6]}-{digits[6:]}\"\n        return phone\n\n    return series.apply(format_phone)",
    "testCases": [
      {
        "input": "pd.Series([\"(555) 123-4567\", \"555.123.4567\", \"5551234567\", \"123\"])",
        "expectedOutput": "pd.Series([\"555-123-4567\", \"555-123-4567\", \"555-123-4567\", \"123\"])",
        "isHidden": false,
        "description": "Clean various phone formats"
      }
    ],
    "hints": [
      "Use re.sub(r\"\\D\", \"\", str(phone)) to extract digits",
      "Check if exactly 10 digits",
      "Format as XXX-XXX-XXXX using string slicing",
      "Return original value if not 10 digits",
      "Use .apply() to process each value"
    ]
  },
  {
    "id": "cs407-ex-3-1",
    "subjectId": "cs407",
    "topicId": "cs407-t3",
    "title": "Statistical Summary",
    "description": "Write a function that computes descriptive statistics (mean, median, std, min, max) for a numeric Series.",
    "difficulty": 1,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef compute_statistics(series):\n    # Return dict with 'mean', 'median', 'std', 'min', 'max'\n    pass",
    "solution": "import pandas as pd\n\ndef compute_statistics(series):\n    return {\n        'mean': series.mean(),\n        'median': series.median(),\n        'std': series.std(),\n        'min': series.min(),\n        'max': series.max()\n    }",
    "testCases": [
      {
        "input": "pd.Series([1, 2, 3, 4, 5])",
        "expectedOutput": "{'mean': 3.0, 'median': 3.0, 'std': 1.58, 'min': 1, 'max': 5}",
        "isHidden": false,
        "description": "Simple numeric series"
      }
    ],
    "hints": [
      "Use built-in pandas Series methods: .mean(), .median(), .std(), .min(), .max()",
      "Return results as a dictionary"
    ]
  },
  {
    "id": "cs407-ex-3-2",
    "subjectId": "cs407",
    "topicId": "cs407-t3",
    "title": "Correlation Matrix",
    "description": "Write a function that computes the correlation matrix for all numeric columns in a DataFrame.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef compute_correlation_matrix(df):\n    # Return correlation matrix for numeric columns\n    pass",
    "solution": "import pandas as pd\n\ndef compute_correlation_matrix(df):\n    return df.select_dtypes(include=['number']).corr()",
    "testCases": [
      {
        "input": "pd.DataFrame({\"a\": [1, 2, 3], \"b\": [2, 4, 6], \"c\": [\"x\", \"y\", \"z\"]})",
        "expectedOutput": "DataFrame with correlation between a and b",
        "isHidden": false,
        "description": "Mixed DataFrame"
      }
    ],
    "hints": [
      "Use .select_dtypes(include=[\"number\"]) to get numeric columns",
      "Use .corr() method to compute correlation matrix"
    ]
  },
  {
    "id": "cs407-ex-3-3",
    "subjectId": "cs407",
    "topicId": "cs407-t3",
    "title": "Distribution Analysis",
    "description": "Write a function that computes skewness and kurtosis for a numeric Series to analyze its distribution shape.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\nfrom scipy import stats\n\ndef analyze_distribution(series):\n    # Return dict with 'skewness' and 'kurtosis'\n    pass",
    "solution": "import pandas as pd\nfrom scipy import stats\n\ndef analyze_distribution(series):\n    return {\n        'skewness': series.skew(),\n        'kurtosis': series.kurtosis()\n    }",
    "testCases": [
      {
        "input": "pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])",
        "expectedOutput": "{\"skewness\": ~0.0, \"kurtosis\": ~-1.2}",
        "isHidden": false,
        "description": "Uniform distribution"
      },
      {
        "input": "pd.Series([1, 2, 2, 3, 3, 3, 4, 4, 5])",
        "expectedOutput": "dict with skewness and kurtosis values",
        "isHidden": false,
        "description": "Slightly skewed distribution"
      }
    ],
    "hints": [
      "Use .skew() method for skewness",
      "Use .kurtosis() method for kurtosis",
      "Skewness measures asymmetry",
      "Kurtosis measures tail heaviness"
    ]
  },
  {
    "id": "cs407-ex-3-4",
    "subjectId": "cs407",
    "topicId": "cs407-t3",
    "title": "Percentile Calculator",
    "description": "Write a function that computes specific percentiles (25th, 50th, 75th, 90th, 95th) for a numeric Series.",
    "difficulty": 1,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef compute_percentiles(series):\n    # Return dict with percentile values\n    pass",
    "solution": "import pandas as pd\n\ndef compute_percentiles(series):\n    return {\n        'p25': series.quantile(0.25),\n        'p50': series.quantile(0.50),\n        'p75': series.quantile(0.75),\n        'p90': series.quantile(0.90),\n        'p95': series.quantile(0.95)\n    }",
    "testCases": [
      {
        "input": "pd.Series(range(1, 101))",
        "expectedOutput": "{\"p25\": 25.75, \"p50\": 50.5, \"p75\": 75.25, \"p90\": 90.1, \"p95\": 95.05}",
        "isHidden": false,
        "description": "Series from 1 to 100"
      }
    ],
    "hints": [
      "Use .quantile() method",
      "Pass percentile as decimal (0.25 for 25th percentile)",
      "Build dictionary with results"
    ]
  },
  {
    "id": "cs407-ex-3-5",
    "subjectId": "cs407",
    "topicId": "cs407-t3",
    "title": "Value Frequency Counter",
    "description": "Write a function that counts value frequencies and returns the top N most common values with their counts.",
    "difficulty": 1,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef get_top_values(series, n=5):\n    # Return dict with top N values and their counts\n    pass",
    "solution": "import pandas as pd\n\ndef get_top_values(series, n=5):\n    value_counts = series.value_counts().head(n)\n    return value_counts.to_dict()",
    "testCases": [
      {
        "input": "pd.Series([\"a\", \"b\", \"a\", \"c\", \"a\", \"b\", \"d\"]), 2",
        "expectedOutput": "{\"a\": 3, \"b\": 2}",
        "isHidden": false,
        "description": "Get top 2 most frequent values"
      }
    ],
    "hints": [
      "Use .value_counts() to count occurrences",
      "Use .head(n) to get top N",
      "Use .to_dict() to convert to dictionary"
    ]
  },
  {
    "id": "cs407-ex-3-6",
    "subjectId": "cs407",
    "topicId": "cs407-t3",
    "title": "Two-Sample T-Test",
    "description": "Write a function that performs a two-sample t-test and returns the t-statistic and p-value.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "from scipy import stats\n\ndef perform_ttest(sample1, sample2):\n    # Return dict with 't_statistic' and 'p_value'\n    pass",
    "solution": "from scipy import stats\n\ndef perform_ttest(sample1, sample2):\n    t_stat, p_value = stats.ttest_ind(sample1, sample2)\n    return {\n        't_statistic': t_stat,\n        'p_value': p_value\n    }",
    "testCases": [
      {
        "input": "[1, 2, 3, 4, 5], [2, 3, 4, 5, 6]",
        "expectedOutput": "dict with t_statistic and p_value",
        "isHidden": false,
        "description": "Two samples with slight difference"
      }
    ],
    "hints": [
      "Use scipy.stats.ttest_ind() for independent two-sample t-test",
      "Function returns (t_statistic, p_value)",
      "Return both values in a dictionary"
    ]
  },
  {
    "id": "cs407-ex-3-7",
    "subjectId": "cs407",
    "topicId": "cs407-t3",
    "title": "Chi-Square Test",
    "description": "Write a function that performs a chi-square test of independence on a contingency table.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "from scipy import stats\nimport numpy as np\n\ndef chi_square_test(contingency_table):\n    # Return dict with 'chi2', 'p_value', 'dof', 'expected'\n    pass",
    "solution": "from scipy import stats\nimport numpy as np\n\ndef chi_square_test(contingency_table):\n    chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n    return {\n        'chi2': chi2,\n        'p_value': p_value,\n        'dof': dof,\n        'expected': expected\n    }",
    "testCases": [
      {
        "input": "[[10, 20, 30], [6, 9, 17]]",
        "expectedOutput": "dict with chi2, p_value, dof, and expected frequencies",
        "isHidden": false,
        "description": "Contingency table test"
      }
    ],
    "hints": [
      "Use scipy.stats.chi2_contingency()",
      "Returns (chi2, p_value, degrees_of_freedom, expected_frequencies)",
      "Package results in a dictionary"
    ]
  },
  {
    "id": "cs407-ex-3-8",
    "subjectId": "cs407",
    "topicId": "cs407-t3",
    "title": "Normality Test",
    "description": "Write a function that performs the Shapiro-Wilk test for normality and returns whether data is likely normal.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "from scipy import stats\n\ndef test_normality(data, alpha=0.05):\n    # Return dict with 'statistic', 'p_value', 'is_normal'\n    pass",
    "solution": "from scipy import stats\n\ndef test_normality(data, alpha=0.05):\n    statistic, p_value = stats.shapiro(data)\n    return {\n        'statistic': statistic,\n        'p_value': p_value,\n        'is_normal': p_value > alpha\n    }",
    "testCases": [
      {
        "input": "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 0.05",
        "expectedOutput": "dict with statistic, p_value, and is_normal boolean",
        "isHidden": false,
        "description": "Test normality of uniform data"
      }
    ],
    "hints": [
      "Use scipy.stats.shapiro() for Shapiro-Wilk test",
      "Returns (statistic, p_value)",
      "If p_value > alpha, fail to reject null (data is normal)",
      "Add is_normal boolean to result"
    ]
  },
  {
    "id": "cs407-ex-3-9",
    "subjectId": "cs407",
    "topicId": "cs407-t3",
    "title": "Confidence Interval",
    "description": "Write a function that computes a confidence interval for the mean of a sample.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import numpy as np\nfrom scipy import stats\n\ndef compute_confidence_interval(sample, confidence=0.95):\n    # Return (lower_bound, upper_bound)\n    pass",
    "solution": "import numpy as np\nfrom scipy import stats\n\ndef compute_confidence_interval(sample, confidence=0.95):\n    mean = np.mean(sample)\n    se = stats.sem(sample)\n    margin = se * stats.t.ppf((1 + confidence) / 2, len(sample) - 1)\n    return (mean - margin, mean + margin)",
    "testCases": [
      {
        "input": "[10, 12, 14, 16, 18], 0.95",
        "expectedOutput": "tuple with lower and upper bounds",
        "isHidden": false,
        "description": "95% confidence interval"
      }
    ],
    "hints": [
      "Calculate sample mean and standard error",
      "Use stats.t.ppf() to get t-critical value",
      "Margin of error = standard_error * t_critical",
      "CI = (mean - margin, mean + margin)"
    ]
  },
  {
    "id": "cs407-ex-3-10",
    "subjectId": "cs407",
    "topicId": "cs407-t3",
    "title": "Covariance Matrix",
    "description": "Write a function that computes the covariance matrix for numeric columns in a DataFrame.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef compute_covariance_matrix(df):\n    # Return covariance matrix for numeric columns\n    pass",
    "solution": "import pandas as pd\n\ndef compute_covariance_matrix(df):\n    return df.select_dtypes(include=['number']).cov()",
    "testCases": [
      {
        "input": "pd.DataFrame({\"a\": [1, 2, 3, 4], \"b\": [2, 4, 6, 8], \"c\": [\"x\", \"y\", \"z\", \"w\"]})",
        "expectedOutput": "Covariance matrix DataFrame",
        "isHidden": false,
        "description": "Compute covariance for numeric columns"
      }
    ],
    "hints": [
      "Select numeric columns with .select_dtypes()",
      "Use .cov() method to compute covariance matrix",
      "Covariance measures linear relationship strength"
    ]
  },
  {
    "id": "cs407-ex-3-11",
    "subjectId": "cs407",
    "topicId": "cs407-t3",
    "title": "ANOVA Test",
    "description": "Write a function that performs one-way ANOVA to test if means of multiple groups are equal.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "from scipy import stats\n\ndef perform_anova(*groups):\n    # Return dict with 'f_statistic' and 'p_value'\n    pass",
    "solution": "from scipy import stats\n\ndef perform_anova(*groups):\n    f_stat, p_value = stats.f_oneway(*groups)\n    return {\n        'f_statistic': f_stat,\n        'p_value': p_value\n    }",
    "testCases": [
      {
        "input": "[1, 2, 3], [4, 5, 6], [7, 8, 9]",
        "expectedOutput": "dict with f_statistic and p_value",
        "isHidden": false,
        "description": "ANOVA for three groups"
      }
    ],
    "hints": [
      "Use scipy.stats.f_oneway() for one-way ANOVA",
      "Pass groups as separate arguments using *groups",
      "Returns (f_statistic, p_value)",
      "Tests null hypothesis that all group means are equal"
    ]
  },
  {
    "id": "cs407-ex-3-12",
    "subjectId": "cs407",
    "topicId": "cs407-t3",
    "title": "Grouped Statistics",
    "description": "Write a function that computes mean, median, and std for each group in a DataFrame.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef compute_grouped_stats(df, group_col, value_col):\n    # Return DataFrame with mean, median, std for each group\n    pass",
    "solution": "import pandas as pd\n\ndef compute_grouped_stats(df, group_col, value_col):\n    return df.groupby(group_col)[value_col].agg(['mean', 'median', 'std'])",
    "testCases": [
      {
        "input": "pd.DataFrame({\"group\": [\"A\", \"A\", \"B\", \"B\"], \"value\": [10, 20, 30, 40]}), \"group\", \"value\"",
        "expectedOutput": "DataFrame with mean, median, std for groups A and B",
        "isHidden": false,
        "description": "Grouped statistics"
      }
    ],
    "hints": [
      "Use .groupby(group_col) to group data",
      "Select value_col after grouping",
      "Use .agg() with list of functions",
      "Returns DataFrame with group as index"
    ]
  },
  {
    "id": "cs407-ex-3-13",
    "subjectId": "cs407",
    "topicId": "cs407-t3",
    "title": "Outlier Impact Analysis",
    "description": "Write a function that compares mean/median before and after removing outliers to assess outlier impact.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef analyze_outlier_impact(series):\n    # Return dict with 'original_mean', 'original_median', 'clean_mean', 'clean_median'\n    pass",
    "solution": "import pandas as pd\n\ndef analyze_outlier_impact(series):\n    original_mean = series.mean()\n    original_median = series.median()\n\n    Q1 = series.quantile(0.25)\n    Q3 = series.quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n\n    clean_series = series[(series >= lower_bound) & (series <= upper_bound)]\n    clean_mean = clean_series.mean()\n    clean_median = clean_series.median()\n\n    return {\n        'original_mean': original_mean,\n        'original_median': original_median,\n        'clean_mean': clean_mean,\n        'clean_median': clean_median\n    }",
    "testCases": [
      {
        "input": "pd.Series([1, 2, 3, 4, 5, 100])",
        "expectedOutput": "dict showing mean changes significantly, median stays stable",
        "isHidden": false,
        "description": "Series with one extreme outlier"
      }
    ],
    "hints": [
      "Calculate original mean and median",
      "Use IQR method to identify outliers",
      "Filter series to remove outliers",
      "Calculate clean mean and median",
      "Compare to see outlier impact"
    ]
  },
  {
    "id": "cs407-ex-3-14",
    "subjectId": "cs407",
    "topicId": "cs407-t3",
    "title": "Pearson Correlation",
    "description": "Write a function that computes Pearson correlation coefficient and p-value between two Series.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "from scipy import stats\n\ndef compute_pearson_correlation(x, y):\n    # Return dict with 'correlation' and 'p_value'\n    pass",
    "solution": "from scipy import stats\n\ndef compute_pearson_correlation(x, y):\n    correlation, p_value = stats.pearsonr(x, y)\n    return {\n        'correlation': correlation,\n        'p_value': p_value\n    }",
    "testCases": [
      {
        "input": "[1, 2, 3, 4, 5], [2, 4, 6, 8, 10]",
        "expectedOutput": "{\"correlation\": 1.0, \"p_value\": very small}",
        "isHidden": false,
        "description": "Perfect positive correlation"
      },
      {
        "input": "[1, 2, 3, 4, 5], [5, 4, 3, 2, 1]",
        "expectedOutput": "{\"correlation\": -1.0, \"p_value\": very small}",
        "isHidden": false,
        "description": "Perfect negative correlation"
      }
    ],
    "hints": [
      "Use scipy.stats.pearsonr() for Pearson correlation",
      "Returns (correlation_coefficient, p_value)",
      "Correlation ranges from -1 to 1",
      "p_value tests if correlation is significantly different from 0"
    ]
  },
  {
    "id": "cs407-ex-3-15",
    "subjectId": "cs407",
    "topicId": "cs407-t3",
    "title": "Distribution Comparison",
    "description": "Write a function that compares two distributions using the Kolmogorov-Smirnov test.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "from scipy import stats\n\ndef compare_distributions(sample1, sample2):\n    # Return dict with 'statistic', 'p_value', 'are_different'\n    pass",
    "solution": "from scipy import stats\n\ndef compare_distributions(sample1, sample2, alpha=0.05):\n    statistic, p_value = stats.ks_2samp(sample1, sample2)\n    return {\n        'statistic': statistic,\n        'p_value': p_value,\n        'are_different': p_value < alpha\n    }",
    "testCases": [
      {
        "input": "[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]",
        "expectedOutput": "dict with are_different: False",
        "isHidden": false,
        "description": "Same distribution"
      },
      {
        "input": "[1, 2, 3, 4, 5], [10, 20, 30, 40, 50]",
        "expectedOutput": "dict with are_different: True",
        "isHidden": false,
        "description": "Different distributions"
      }
    ],
    "hints": [
      "Use scipy.stats.ks_2samp() for two-sample KS test",
      "Returns (statistic, p_value)",
      "If p_value < alpha, distributions are significantly different",
      "Add are_different boolean to result"
    ]
  },
  {
    "id": "cs407-ex-3-16",
    "subjectId": "cs407",
    "topicId": "cs407-t3",
    "title": "Variance Analysis",
    "description": "Write a function that computes variance, coefficient of variation, and variance ratio between two Series.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\nimport numpy as np\n\ndef analyze_variance(series1, series2):\n    # Return dict with variance info for both series and their ratio\n    pass",
    "solution": "import pandas as pd\nimport numpy as np\n\ndef analyze_variance(series1, series2):\n    var1 = series1.var()\n    var2 = series2.var()\n    cv1 = (series1.std() / series1.mean()) * 100 if series1.mean() != 0 else None\n    cv2 = (series2.std() / series2.mean()) * 100 if series2.mean() != 0 else None\n    variance_ratio = var1 / var2 if var2 != 0 else None\n\n    return {\n        'variance1': var1,\n        'variance2': var2,\n        'cv1': cv1,\n        'cv2': cv2,\n        'variance_ratio': variance_ratio\n    }",
    "testCases": [
      {
        "input": "pd.Series([1, 2, 3, 4, 5]), pd.Series([10, 20, 30, 40, 50])",
        "expectedOutput": "dict with variance metrics for both series",
        "isHidden": false,
        "description": "Compare variance of two series"
      }
    ],
    "hints": [
      "Use .var() for variance",
      "Coefficient of variation = (std / mean) * 100",
      "Variance ratio = var1 / var2",
      "Handle division by zero cases",
      "CV shows relative variability"
    ]
  },
  {
    "id": "cs407-ex-4-1",
    "subjectId": "cs407",
    "topicId": "cs407-t4",
    "title": "One-Hot Encoding",
    "description": "Write a function that performs one-hot encoding on a categorical column using pandas get_dummies.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef one_hot_encode(series):\n    # Return one-hot encoded DataFrame\n    pass",
    "solution": "import pandas as pd\n\ndef one_hot_encode(series):\n    return pd.get_dummies(series, prefix=series.name)",
    "testCases": [
      {
        "input": "pd.Series([\"red\", \"blue\", \"red\", \"green\"], name=\"color\")",
        "expectedOutput": "DataFrame with columns: color_blue, color_green, color_red",
        "isHidden": false,
        "description": "One-hot encode color column"
      }
    ],
    "hints": [
      "Use pd.get_dummies() for one-hot encoding",
      "Pass prefix parameter to name columns",
      "Each unique value becomes a binary column"
    ]
  },
  {
    "id": "cs407-ex-4-2",
    "subjectId": "cs407",
    "topicId": "cs407-t4",
    "title": "Label Encoding",
    "description": "Write a function that performs label encoding on a categorical Series, mapping each unique value to an integer.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef label_encode(series):\n    # Return encoded series and mapping dict\n    pass",
    "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef label_encode(series):\n    le = LabelEncoder()\n    encoded = le.fit_transform(series)\n    mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n    return pd.Series(encoded, index=series.index), mapping",
    "testCases": [
      {
        "input": "pd.Series([\"low\", \"medium\", \"high\", \"low\"])",
        "expectedOutput": "Encoded series and mapping dict",
        "isHidden": false,
        "description": "Label encode categorical data"
      }
    ],
    "hints": [
      "Use sklearn.preprocessing.LabelEncoder",
      "Call fit_transform() on the series",
      "Create mapping from le.classes_ and le.transform()",
      "Return both encoded series and mapping"
    ]
  },
  {
    "id": "cs407-ex-4-3",
    "subjectId": "cs407",
    "topicId": "cs407-t4",
    "title": "Min-Max Scaling",
    "description": "Write a function that applies min-max normalization to scale values to [0, 1] range.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef min_max_scale(series):\n    # Return scaled series\n    pass",
    "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef min_max_scale(series):\n    scaler = MinMaxScaler()\n    scaled = scaler.fit_transform(series.values.reshape(-1, 1))\n    return pd.Series(scaled.flatten(), index=series.index)",
    "testCases": [
      {
        "input": "pd.Series([10, 20, 30, 40, 50])",
        "expectedOutput": "pd.Series([0.0, 0.25, 0.5, 0.75, 1.0])",
        "isHidden": false,
        "description": "Scale to [0, 1] range"
      }
    ],
    "hints": [
      "Use sklearn.preprocessing.MinMaxScaler",
      "Reshape series to column vector with .reshape(-1, 1)",
      "Call fit_transform() on reshaped data",
      "Flatten result and return as Series"
    ]
  },
  {
    "id": "cs407-ex-4-4",
    "subjectId": "cs407",
    "topicId": "cs407-t4",
    "title": "Standardization (Z-Score)",
    "description": "Write a function that standardizes a Series to have mean=0 and std=1.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef standardize(series):\n    # Return standardized series\n    pass",
    "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef standardize(series):\n    scaler = StandardScaler()\n    scaled = scaler.fit_transform(series.values.reshape(-1, 1))\n    return pd.Series(scaled.flatten(), index=series.index)",
    "testCases": [
      {
        "input": "pd.Series([10, 20, 30, 40, 50])",
        "expectedOutput": "Series with mean ~0 and std ~1",
        "isHidden": false,
        "description": "Standardize numeric data"
      }
    ],
    "hints": [
      "Use sklearn.preprocessing.StandardScaler",
      "Reshape series to column vector",
      "Call fit_transform() to standardize",
      "Result has mean 0 and std 1"
    ]
  },
  {
    "id": "cs407-ex-4-5",
    "subjectId": "cs407",
    "topicId": "cs407-t4",
    "title": "Polynomial Features",
    "description": "Write a function that creates polynomial features up to specified degree.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import pandas as pd\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef create_polynomial_features(df, degree=2):\n    # Return DataFrame with polynomial features\n    pass",
    "solution": "import pandas as pd\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef create_polynomial_features(df, degree=2):\n    poly = PolynomialFeatures(degree=degree, include_bias=False)\n    poly_features = poly.fit_transform(df)\n    feature_names = poly.get_feature_names_out(df.columns)\n    return pd.DataFrame(poly_features, columns=feature_names, index=df.index)",
    "testCases": [
      {
        "input": "pd.DataFrame({\"x\": [1, 2, 3]}), 2",
        "expectedOutput": "DataFrame with x and x^2 columns",
        "isHidden": false,
        "description": "Create degree 2 polynomial features"
      }
    ],
    "hints": [
      "Use sklearn.preprocessing.PolynomialFeatures",
      "Set include_bias=False to exclude constant term",
      "Use fit_transform() to create features",
      "Get feature names with get_feature_names_out()",
      "Return as DataFrame with proper column names"
    ]
  },
  {
    "id": "cs407-ex-4-6",
    "subjectId": "cs407",
    "topicId": "cs407-t4",
    "title": "Binning Continuous Values",
    "description": "Write a function that bins continuous values into discrete categories using quantile-based binning.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef bin_values(series, n_bins=4):\n    # Return binned series with labels\n    pass",
    "solution": "import pandas as pd\n\ndef bin_values(series, n_bins=4):\n    labels = [f\"Q{i+1}\" for i in range(n_bins)]\n    return pd.qcut(series, q=n_bins, labels=labels, duplicates='drop')",
    "testCases": [
      {
        "input": "pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), 4",
        "expectedOutput": "Series with quartile labels Q1, Q2, Q3, Q4",
        "isHidden": false,
        "description": "Bin into 4 quartiles"
      }
    ],
    "hints": [
      "Use pd.qcut() for quantile-based binning",
      "Create labels for each bin",
      "Set duplicates=\"drop\" to handle duplicate bin edges",
      "Returns categorical series with bin labels"
    ]
  },
  {
    "id": "cs407-ex-4-7",
    "subjectId": "cs407",
    "topicId": "cs407-t4",
    "title": "Log Transformation",
    "description": "Write a function that applies log transformation to reduce skewness in data.",
    "difficulty": 1,
    "language": "python",
    "starterCode": "import pandas as pd\nimport numpy as np\n\ndef log_transform(series):\n    # Return log-transformed series (handle zeros)\n    pass",
    "solution": "import pandas as pd\nimport numpy as np\n\ndef log_transform(series):\n    return np.log1p(series)",
    "testCases": [
      {
        "input": "pd.Series([0, 1, 10, 100, 1000])",
        "expectedOutput": "Log-transformed series",
        "isHidden": false,
        "description": "Apply log transformation"
      }
    ],
    "hints": [
      "Use np.log1p() which computes log(1 + x)",
      "log1p handles zero values safely",
      "Useful for reducing right skewness",
      "Returns transformed series"
    ]
  },
  {
    "id": "cs407-ex-4-8",
    "subjectId": "cs407",
    "topicId": "cs407-t4",
    "title": "Feature Interaction",
    "description": "Write a function that creates interaction features by multiplying pairs of numeric columns.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\nfrom itertools import combinations\n\ndef create_interaction_features(df):\n    # Return DataFrame with original and interaction features\n    pass",
    "solution": "import pandas as pd\nfrom itertools import combinations\n\ndef create_interaction_features(df):\n    result = df.copy()\n    numeric_cols = df.select_dtypes(include=['number']).columns\n\n    for col1, col2 in combinations(numeric_cols, 2):\n        interaction_name = f\"{col1}_{col2}\"\n        result[interaction_name] = df[col1] * df[col2]\n\n    return result",
    "testCases": [
      {
        "input": "pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})",
        "expectedOutput": "DataFrame with a, b, and a_b (interaction) columns",
        "isHidden": false,
        "description": "Create interaction features"
      }
    ],
    "hints": [
      "Use itertools.combinations() to get column pairs",
      "Multiply values from each pair",
      "Name interaction column as col1_col2",
      "Add interactions to copy of original DataFrame"
    ]
  },
  {
    "id": "cs407-ex-4-9",
    "subjectId": "cs407",
    "topicId": "cs407-t4",
    "title": "Ordinal Encoding",
    "description": "Write a function that performs ordinal encoding with custom ordering for categorical values.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef ordinal_encode(series, ordering):\n    # ordering is a list like ['low', 'medium', 'high']\n    # Return encoded series with 0, 1, 2, ...\n    pass",
    "solution": "import pandas as pd\n\ndef ordinal_encode(series, ordering):\n    mapping = {val: idx for idx, val in enumerate(ordering)}\n    return series.map(mapping)",
    "testCases": [
      {
        "input": "pd.Series([\"low\", \"high\", \"medium\", \"low\"]), [\"low\", \"medium\", \"high\"]",
        "expectedOutput": "pd.Series([0, 2, 1, 0])",
        "isHidden": false,
        "description": "Ordinal encoding with custom order"
      }
    ],
    "hints": [
      "Create mapping dict from ordering list",
      "Use enumerate() to assign indices",
      "Use series.map() to apply mapping",
      "Preserves ordinal relationship in data"
    ]
  },
  {
    "id": "cs407-ex-4-10",
    "subjectId": "cs407",
    "topicId": "cs407-t4",
    "title": "Target Encoding",
    "description": "Write a function that performs target encoding (mean encoding) for a categorical feature.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef target_encode(categorical_series, target_series):\n    # Return series with categories replaced by mean target value\n    pass",
    "solution": "import pandas as pd\n\ndef target_encode(categorical_series, target_series):\n    target_means = target_series.groupby(categorical_series).mean()\n    return categorical_series.map(target_means)",
    "testCases": [
      {
        "input": "pd.Series([\"A\", \"B\", \"A\", \"B\"]), pd.Series([10, 20, 30, 40])",
        "expectedOutput": "pd.Series([20.0, 30.0, 20.0, 30.0])",
        "isHidden": false,
        "description": "Target encode based on mean target values"
      }
    ],
    "hints": [
      "Group target by categorical values",
      "Calculate mean target for each category",
      "Map categories to their mean target values",
      "Useful for high-cardinality categorical features"
    ]
  },
  {
    "id": "cs407-ex-4-11",
    "subjectId": "cs407",
    "topicId": "cs407-t4",
    "title": "Robust Scaling",
    "description": "Write a function that performs robust scaling using median and IQR (resistant to outliers).",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\nfrom sklearn.preprocessing import RobustScaler\n\ndef robust_scale(series):\n    # Return robust-scaled series\n    pass",
    "solution": "import pandas as pd\nfrom sklearn.preprocessing import RobustScaler\n\ndef robust_scale(series):\n    scaler = RobustScaler()\n    scaled = scaler.fit_transform(series.values.reshape(-1, 1))\n    return pd.Series(scaled.flatten(), index=series.index)",
    "testCases": [
      {
        "input": "pd.Series([1, 2, 3, 4, 100])",
        "expectedOutput": "Scaled series resistant to outlier (100)",
        "isHidden": false,
        "description": "Robust scaling with outlier"
      }
    ],
    "hints": [
      "Use sklearn.preprocessing.RobustScaler",
      "Uses median and IQR instead of mean and std",
      "Resistant to outliers",
      "Good for data with extreme values"
    ]
  },
  {
    "id": "cs407-ex-4-12",
    "subjectId": "cs407",
    "topicId": "cs407-t4",
    "title": "Frequency Encoding",
    "description": "Write a function that encodes categorical values by their frequency in the dataset.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef frequency_encode(series):\n    # Return series with values replaced by their frequencies\n    pass",
    "solution": "import pandas as pd\n\ndef frequency_encode(series):\n    frequency_map = series.value_counts(normalize=True).to_dict()\n    return series.map(frequency_map)",
    "testCases": [
      {
        "input": "pd.Series([\"a\", \"b\", \"a\", \"a\", \"b\"])",
        "expectedOutput": "pd.Series([0.6, 0.4, 0.6, 0.6, 0.4])",
        "isHidden": false,
        "description": "Encode by relative frequency"
      }
    ],
    "hints": [
      "Use value_counts(normalize=True) to get frequencies",
      "Convert to dictionary",
      "Map original values to their frequencies",
      "Useful for categorical features with many categories"
    ]
  },
  {
    "id": "cs407-ex-4-13",
    "subjectId": "cs407",
    "topicId": "cs407-t4",
    "title": "Date Feature Extraction",
    "description": "Write a function that extracts useful features from datetime column (year, month, day, dayofweek, quarter).",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef extract_date_features(datetime_series):\n    # Return DataFrame with extracted date features\n    pass",
    "solution": "import pandas as pd\n\ndef extract_date_features(datetime_series):\n    df = pd.DataFrame()\n    df['year'] = datetime_series.dt.year\n    df['month'] = datetime_series.dt.month\n    df['day'] = datetime_series.dt.day\n    df['dayofweek'] = datetime_series.dt.dayofweek\n    df['quarter'] = datetime_series.dt.quarter\n    return df",
    "testCases": [
      {
        "input": "pd.Series(pd.to_datetime([\"2023-01-15\", \"2023-06-20\", \"2023-12-25\"]))",
        "expectedOutput": "DataFrame with year, month, day, dayofweek, quarter columns",
        "isHidden": false,
        "description": "Extract date features"
      }
    ],
    "hints": [
      "Use .dt accessor for datetime operations",
      "Extract year, month, day with .dt.year, .dt.month, .dt.day",
      "Get day of week with .dt.dayofweek (0=Monday)",
      "Get quarter with .dt.quarter",
      "Return all features in DataFrame"
    ]
  },
  {
    "id": "cs407-ex-4-14",
    "subjectId": "cs407",
    "topicId": "cs407-t4",
    "title": "Box-Cox Transformation",
    "description": "Write a function that applies Box-Cox transformation to normalize data distribution.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import pandas as pd\nfrom scipy import stats\n\ndef box_cox_transform(series):\n    # Return transformed series and lambda parameter\n    pass",
    "solution": "import pandas as pd\nfrom scipy import stats\n\ndef box_cox_transform(series):\n    # Box-Cox requires positive values\n    if (series <= 0).any():\n        series = series - series.min() + 1\n\n    transformed, lambda_param = stats.boxcox(series)\n    return pd.Series(transformed, index=series.index), lambda_param",
    "testCases": [
      {
        "input": "pd.Series([1, 2, 3, 4, 5, 10, 20])",
        "expectedOutput": "Transformed series and lambda parameter",
        "isHidden": false,
        "description": "Box-Cox transformation"
      }
    ],
    "hints": [
      "Use scipy.stats.boxcox()",
      "Box-Cox requires all positive values",
      "Shift data if needed: series - series.min() + 1",
      "Returns (transformed_data, lambda)",
      "Automatically finds optimal lambda"
    ]
  },
  {
    "id": "cs407-ex-4-15",
    "subjectId": "cs407",
    "topicId": "cs407-t4",
    "title": "Feature Hashing",
    "description": "Write a function that applies feature hashing to reduce dimensionality of categorical features.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import pandas as pd\nfrom sklearn.feature_extraction import FeatureHasher\n\ndef hash_features(series, n_features=10):\n    # Return hashed feature matrix\n    pass",
    "solution": "import pandas as pd\nfrom sklearn.feature_extraction import FeatureHasher\n\ndef hash_features(series, n_features=10):\n    hasher = FeatureHasher(n_features=n_features, input_type='string')\n    # Convert to list of dicts format\n    data = [{str(val): 1} for val in series]\n    hashed = hasher.transform(data).toarray()\n    return pd.DataFrame(hashed, index=series.index)",
    "testCases": [
      {
        "input": "pd.Series([\"cat\", \"dog\", \"bird\", \"cat\"]), 5",
        "expectedOutput": "DataFrame with 5 hashed feature columns",
        "isHidden": false,
        "description": "Hash categorical features"
      }
    ],
    "hints": [
      "Use sklearn.feature_extraction.FeatureHasher",
      "Convert series to list of dicts format",
      "Call transform() to get sparse matrix",
      "Convert to array with .toarray()",
      "Useful for high-cardinality features"
    ]
  },
  {
    "id": "cs407-ex-4-16",
    "subjectId": "cs407",
    "topicId": "cs407-t4",
    "title": "Ratio Features",
    "description": "Write a function that creates ratio features by dividing numeric columns pairwise.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\nfrom itertools import combinations\n\ndef create_ratio_features(df):\n    # Return DataFrame with original and ratio features\n    pass",
    "solution": "import pandas as pd\nfrom itertools import combinations\n\ndef create_ratio_features(df):\n    result = df.copy()\n    numeric_cols = df.select_dtypes(include=['number']).columns\n\n    for col1, col2 in combinations(numeric_cols, 2):\n        ratio_name = f\"{col1}_div_{col2}\"\n        # Avoid division by zero\n        result[ratio_name] = df[col1] / df[col2].replace(0, 1e-10)\n\n    return result",
    "testCases": [
      {
        "input": "pd.DataFrame({\"a\": [10, 20, 30], \"b\": [2, 4, 5]})",
        "expectedOutput": "DataFrame with a, b, and a_div_b (ratio) columns",
        "isHidden": false,
        "description": "Create ratio features"
      }
    ],
    "hints": [
      "Use itertools.combinations() for column pairs",
      "Divide col1 by col2 for each pair",
      "Handle division by zero with replace()",
      "Name ratio column as col1_div_col2",
      "Add ratios to copy of original DataFrame"
    ]
  },
  {
    "id": "cs407-ex-5-1",
    "subjectId": "cs407",
    "topicId": "cs407-t5",
    "title": "Create a Simple Line Plot",
    "description": "Write a function that creates a line plot using matplotlib with given x and y data, title, and axis labels.",
    "difficulty": 1,
    "language": "python",
    "starterCode": "import matplotlib.pyplot as plt\n\ndef create_line_plot(x, y, title, xlabel, ylabel):\n    # Create a line plot with the given parameters\n    # Return the figure object\n    pass",
    "solution": "import matplotlib.pyplot as plt\n\ndef create_line_plot(x, y, title, xlabel, ylabel):\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    return fig",
    "testCases": [
      {
        "input": "x=[1, 2, 3, 4], y=[1, 4, 9, 16], title=\"Squares\", xlabel=\"Number\", ylabel=\"Square\"",
        "expectedOutput": "Figure with line plot",
        "isHidden": false,
        "description": "Basic line plot"
      },
      {
        "input": "x=[0, 1, 2], y=[0, 1, 0], title=\"Triangle Wave\", xlabel=\"Time\", ylabel=\"Amplitude\"",
        "expectedOutput": "Figure with triangle wave",
        "isHidden": false,
        "description": "Triangle wave plot"
      }
    ],
    "hints": [
      "Use plt.subplots() to create figure and axes",
      "Use ax.plot(x, y) to create the line plot",
      "Use ax.set_title(), ax.set_xlabel(), ax.set_ylabel()",
      "Return the figure object"
    ]
  },
  {
    "id": "cs407-ex-5-2",
    "subjectId": "cs407",
    "topicId": "cs407-t5",
    "title": "Create a Bar Chart",
    "description": "Write a function that creates a bar chart showing categorical data with custom colors.",
    "difficulty": 1,
    "language": "python",
    "starterCode": "import matplotlib.pyplot as plt\n\ndef create_bar_chart(categories, values, title, color='blue'):\n    # Create a bar chart with the given parameters\n    # Return the figure object\n    pass",
    "solution": "import matplotlib.pyplot as plt\n\ndef create_bar_chart(categories, values, title, color='blue'):\n    fig, ax = plt.subplots()\n    ax.bar(categories, values, color=color)\n    ax.set_title(title)\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Value')\n    return fig",
    "testCases": [
      {
        "input": "categories=[\"A\", \"B\", \"C\"], values=[10, 20, 15], title=\"Sales\", color=\"green\"",
        "expectedOutput": "Figure with green bar chart",
        "isHidden": false,
        "description": "Basic bar chart"
      },
      {
        "input": "categories=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"], values=[100, 120, 110, 130], title=\"Quarterly Revenue\"",
        "expectedOutput": "Figure with blue bar chart",
        "isHidden": false,
        "description": "Quarterly data"
      }
    ],
    "hints": [
      "Use plt.subplots() to create figure and axes",
      "Use ax.bar(categories, values, color=color)",
      "Set title and axis labels",
      "Return the figure object"
    ]
  },
  {
    "id": "cs407-ex-5-3",
    "subjectId": "cs407",
    "topicId": "cs407-t5",
    "title": "Create a Scatter Plot",
    "description": "Write a function that creates a scatter plot with different marker sizes based on a third variable.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import matplotlib.pyplot as plt\n\ndef create_scatter_plot(x, y, sizes, title):\n    # Create a scatter plot where marker size varies\n    # Return the figure object\n    pass",
    "solution": "import matplotlib.pyplot as plt\n\ndef create_scatter_plot(x, y, sizes, title):\n    fig, ax = plt.subplots()\n    ax.scatter(x, y, s=sizes, alpha=0.6)\n    ax.set_title(title)\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    return fig",
    "testCases": [
      {
        "input": "x=[1, 2, 3, 4], y=[2, 4, 6, 8], sizes=[50, 100, 150, 200], title=\"Variable Size Scatter\"",
        "expectedOutput": "Figure with scatter plot",
        "isHidden": false,
        "description": "Scatter with varying sizes"
      },
      {
        "input": "x=[1, 1, 2, 2], y=[1, 2, 1, 2], sizes=[100, 100, 100, 100], title=\"Grid Points\"",
        "expectedOutput": "Figure with grid scatter",
        "isHidden": false,
        "description": "Grid pattern"
      }
    ],
    "hints": [
      "Use ax.scatter(x, y, s=sizes) for scatter plot",
      "The s parameter controls marker size",
      "Add alpha=0.6 for transparency",
      "Set title and axis labels"
    ]
  },
  {
    "id": "cs407-ex-5-4",
    "subjectId": "cs407",
    "topicId": "cs407-t5",
    "title": "Create a Histogram",
    "description": "Write a function that creates a histogram with a specified number of bins.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import matplotlib.pyplot as plt\n\ndef create_histogram(data, bins, title):\n    # Create a histogram with the given number of bins\n    # Return the figure object\n    pass",
    "solution": "import matplotlib.pyplot as plt\n\ndef create_histogram(data, bins, title):\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=bins, edgecolor='black', alpha=0.7)\n    ax.set_title(title)\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    return fig",
    "testCases": [
      {
        "input": "data=[1, 2, 2, 3, 3, 3, 4, 4, 5], bins=5, title=\"Distribution\"",
        "expectedOutput": "Figure with histogram",
        "isHidden": false,
        "description": "Basic histogram"
      },
      {
        "input": "data=[10, 20, 20, 30, 30, 30], bins=3, title=\"Grouped Data\"",
        "expectedOutput": "Figure with 3-bin histogram",
        "isHidden": false,
        "description": "Fewer bins"
      }
    ],
    "hints": [
      "Use ax.hist(data, bins=bins)",
      "Add edgecolor=\"black\" for bin borders",
      "Set alpha=0.7 for transparency",
      "Label axes appropriately"
    ]
  },
  {
    "id": "cs407-ex-5-5",
    "subjectId": "cs407",
    "topicId": "cs407-t5",
    "title": "Create Subplots",
    "description": "Write a function that creates a 2x2 grid of subplots with different plot types.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import matplotlib.pyplot as plt\n\ndef create_subplot_grid(data):\n    # data is a dict with keys 'line', 'bar', 'scatter', 'hist'\n    # Create 2x2 subplots showing all four plot types\n    # Return the figure object\n    pass",
    "solution": "import matplotlib.pyplot as plt\n\ndef create_subplot_grid(data):\n    fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n\n    # Line plot\n    axes[0, 0].plot(data['line']['x'], data['line']['y'])\n    axes[0, 0].set_title('Line Plot')\n\n    # Bar plot\n    axes[0, 1].bar(data['bar']['x'], data['bar']['y'])\n    axes[0, 1].set_title('Bar Plot')\n\n    # Scatter plot\n    axes[1, 0].scatter(data['scatter']['x'], data['scatter']['y'])\n    axes[1, 0].set_title('Scatter Plot')\n\n    # Histogram\n    axes[1, 1].hist(data['hist']['values'], bins=10)\n    axes[1, 1].set_title('Histogram')\n\n    plt.tight_layout()\n    return fig",
    "testCases": [
      {
        "input": "data with line, bar, scatter, and hist keys",
        "expectedOutput": "Figure with 2x2 subplots",
        "isHidden": false,
        "description": "Multiple plot types"
      }
    ],
    "hints": [
      "Use plt.subplots(2, 2) to create a 2x2 grid",
      "Access subplots with axes[row, col]",
      "Use different plot methods for each subplot",
      "Use plt.tight_layout() to adjust spacing"
    ]
  },
  {
    "id": "cs407-ex-5-6",
    "subjectId": "cs407",
    "topicId": "cs407-t5",
    "title": "Seaborn Box Plot",
    "description": "Write a function that creates a box plot using seaborn to show distributions across categories.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef create_box_plot(df, x_col, y_col, title):\n    # Create a box plot showing y_col distribution for each x_col category\n    # Return the figure object\n    pass",
    "solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef create_box_plot(df, x_col, y_col, title):\n    fig, ax = plt.subplots(figsize=(8, 6))\n    sns.boxplot(data=df, x=x_col, y=y_col, ax=ax)\n    ax.set_title(title)\n    return fig",
    "testCases": [
      {
        "input": "df with category and value columns",
        "expectedOutput": "Figure with box plot",
        "isHidden": false,
        "description": "Box plot by category"
      },
      {
        "input": "df with group and score columns",
        "expectedOutput": "Figure with box plot",
        "isHidden": false,
        "description": "Score distribution by group"
      }
    ],
    "hints": [
      "Use sns.boxplot(data=df, x=x_col, y=y_col, ax=ax)",
      "Create figure and axes first",
      "Set the title using ax.set_title()",
      "Return the figure object"
    ]
  },
  {
    "id": "cs407-ex-5-7",
    "subjectId": "cs407",
    "topicId": "cs407-t5",
    "title": "Correlation Heatmap",
    "description": "Write a function that creates a correlation heatmap using seaborn.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef create_correlation_heatmap(df, title):\n    # Create a heatmap showing correlations between numeric columns\n    # Include annotations showing correlation values\n    # Return the figure object\n    pass",
    "solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef create_correlation_heatmap(df, title):\n    fig, ax = plt.subplots(figsize=(10, 8))\n    correlation_matrix = df.corr()\n    sns.heatmap(correlation_matrix, annot=True, fmt='.2f',\n                cmap='coolwarm', center=0, ax=ax)\n    ax.set_title(title)\n    return fig",
    "testCases": [
      {
        "input": "df with multiple numeric columns",
        "expectedOutput": "Figure with correlation heatmap",
        "isHidden": false,
        "description": "Correlation matrix visualization"
      },
      {
        "input": "df with 3 numeric features",
        "expectedOutput": "Figure with 3x3 heatmap",
        "isHidden": false,
        "description": "Small correlation matrix"
      }
    ],
    "hints": [
      "Use df.corr() to get correlation matrix",
      "Use sns.heatmap() with annot=True to show values",
      "Set fmt=\".2f\" to format decimals",
      "Use cmap=\"coolwarm\" and center=0 for better visualization"
    ]
  },
  {
    "id": "cs407-ex-5-8",
    "subjectId": "cs407",
    "topicId": "cs407-t5",
    "title": "Violin Plot with Split",
    "description": "Write a function that creates a split violin plot to compare two groups.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef create_split_violin(df, x_col, y_col, hue_col, title):\n    # Create a split violin plot\n    # Return the figure object\n    pass",
    "solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef create_split_violin(df, x_col, y_col, hue_col, title):\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.violinplot(data=df, x=x_col, y=y_col, hue=hue_col,\n                   split=True, ax=ax)\n    ax.set_title(title)\n    return fig",
    "testCases": [
      {
        "input": "df with category, value, and group columns",
        "expectedOutput": "Figure with split violin plot",
        "isHidden": false,
        "description": "Comparing two groups"
      }
    ],
    "hints": [
      "Use sns.violinplot() with split=True",
      "Specify x, y, and hue parameters",
      "hue_col determines which groups to split",
      "Set figure size for better visibility"
    ]
  },
  {
    "id": "cs407-ex-5-9",
    "subjectId": "cs407",
    "topicId": "cs407-t5",
    "title": "Pair Plot",
    "description": "Write a function that creates a pair plot to show relationships between all numeric variables.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import seaborn as sns\n\ndef create_pair_plot(df, hue_col=None):\n    # Create a pair plot showing relationships between all numeric columns\n    # Color by hue_col if provided\n    # Return the PairGrid object\n    pass",
    "solution": "import seaborn as sns\n\ndef create_pair_plot(df, hue_col=None):\n    if hue_col:\n        pairplot = sns.pairplot(df, hue=hue_col, diag_kind='kde')\n    else:\n        pairplot = sns.pairplot(df, diag_kind='kde')\n    return pairplot",
    "testCases": [
      {
        "input": "df with numeric columns",
        "expectedOutput": "PairGrid with scatter and KDE plots",
        "isHidden": false,
        "description": "Basic pair plot"
      },
      {
        "input": "df with numeric columns and category for hue",
        "expectedOutput": "Colored pair plot",
        "isHidden": false,
        "description": "Pair plot with grouping"
      }
    ],
    "hints": [
      "Use sns.pairplot(df)",
      "Add hue=hue_col if hue_col is provided",
      "Set diag_kind=\"kde\" for diagonal plots",
      "Return the pairplot object"
    ]
  },
  {
    "id": "cs407-ex-5-10",
    "subjectId": "cs407",
    "topicId": "cs407-t5",
    "title": "Time Series Plot",
    "description": "Write a function that creates a time series plot with a rolling average overlay.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import matplotlib.pyplot as plt\nimport pandas as pd\n\ndef create_time_series_plot(dates, values, window, title):\n    # Create a time series plot with original data and rolling average\n    # window is the rolling window size\n    # Return the figure object\n    pass",
    "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\n\ndef create_time_series_plot(dates, values, window, title):\n    fig, ax = plt.subplots(figsize=(12, 6))\n\n    # Convert to Series for rolling calculation\n    series = pd.Series(values, index=pd.to_datetime(dates))\n    rolling_avg = series.rolling(window=window).mean()\n\n    ax.plot(series.index, series.values, label='Original', alpha=0.6)\n    ax.plot(rolling_avg.index, rolling_avg.values,\n            label=f'{window}-period Moving Avg', linewidth=2)\n    ax.set_title(title)\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Value')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    return fig",
    "testCases": [
      {
        "input": "dates and values with window=3",
        "expectedOutput": "Figure with time series and moving average",
        "isHidden": false,
        "description": "Time series with 3-period average"
      },
      {
        "input": "dates and values with window=7",
        "expectedOutput": "Figure with 7-period moving average",
        "isHidden": false,
        "description": "Weekly moving average"
      }
    ],
    "hints": [
      "Convert to pandas Series with datetime index",
      "Use series.rolling(window=window).mean()",
      "Plot both original and rolling average",
      "Add legend and grid for clarity"
    ]
  },
  {
    "id": "cs407-ex-5-11",
    "subjectId": "cs407",
    "topicId": "cs407-t5",
    "title": "Interactive Plotly Scatter",
    "description": "Write a function that creates an interactive scatter plot using Plotly with hover information.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import plotly.graph_objects as go\n\ndef create_interactive_scatter(x, y, names, title):\n    # Create an interactive scatter plot with names shown on hover\n    # Return the figure object\n    pass",
    "solution": "import plotly.graph_objects as go\n\ndef create_interactive_scatter(x, y, names, title):\n    fig = go.Figure(data=go.Scatter(\n        x=x,\n        y=y,\n        mode='markers',\n        text=names,\n        marker=dict(size=10, color='blue', opacity=0.6)\n    ))\n\n    fig.update_layout(\n        title=title,\n        xaxis_title='X',\n        yaxis_title='Y',\n        hovermode='closest'\n    )\n\n    return fig",
    "testCases": [
      {
        "input": "x=[1, 2, 3], y=[2, 4, 6], names=[\"A\", \"B\", \"C\"], title=\"Interactive Plot\"",
        "expectedOutput": "Plotly figure with interactive scatter",
        "isHidden": false,
        "description": "Basic interactive scatter"
      }
    ],
    "hints": [
      "Use go.Figure() with go.Scatter()",
      "Set mode=\"markers\" for scatter plot",
      "Use text parameter for hover labels",
      "Update layout with title and axis labels"
    ]
  },
  {
    "id": "cs407-ex-5-12",
    "subjectId": "cs407",
    "topicId": "cs407-t5",
    "title": "Plotly Bar Chart with Animation",
    "description": "Write a function that creates an animated bar chart using Plotly showing data over time.",
    "difficulty": 4,
    "language": "python",
    "starterCode": "import plotly.express as px\n\ndef create_animated_bar_chart(df, x_col, y_col, animation_col, title):\n    # Create an animated bar chart where frames are determined by animation_col\n    # Return the figure object\n    pass",
    "solution": "import plotly.express as px\n\ndef create_animated_bar_chart(df, x_col, y_col, animation_col, title):\n    fig = px.bar(df,\n                 x=x_col,\n                 y=y_col,\n                 animation_frame=animation_col,\n                 title=title,\n                 range_y=[0, df[y_col].max() * 1.1])\n\n    fig.update_layout(\n        xaxis_title=x_col,\n        yaxis_title=y_col,\n        showlegend=False\n    )\n\n    return fig",
    "testCases": [
      {
        "input": "df with category, value, and year columns",
        "expectedOutput": "Animated bar chart by year",
        "isHidden": false,
        "description": "Animated over time"
      }
    ],
    "hints": [
      "Use px.bar() with animation_frame parameter",
      "Set range_y to keep y-axis stable across frames",
      "Update layout for better appearance",
      "animation_frame determines which column creates frames"
    ]
  },
  {
    "id": "cs407-ex-5-13",
    "subjectId": "cs407",
    "topicId": "cs407-t5",
    "title": "Plotly 3D Surface Plot",
    "description": "Write a function that creates a 3D surface plot using Plotly.",
    "difficulty": 4,
    "language": "python",
    "starterCode": "import plotly.graph_objects as go\nimport numpy as np\n\ndef create_3d_surface(x, y, z, title):\n    # Create a 3D surface plot\n    # z should be a 2D array\n    # Return the figure object\n    pass",
    "solution": "import plotly.graph_objects as go\nimport numpy as np\n\ndef create_3d_surface(x, y, z, title):\n    fig = go.Figure(data=[go.Surface(x=x, y=y, z=z)])\n\n    fig.update_layout(\n        title=title,\n        scene=dict(\n            xaxis_title='X',\n            yaxis_title='Y',\n            zaxis_title='Z'\n        ),\n        autosize=True\n    )\n\n    return fig",
    "testCases": [
      {
        "input": "x, y as 1D arrays, z as 2D array",
        "expectedOutput": "3D surface plot",
        "isHidden": false,
        "description": "Surface visualization"
      }
    ],
    "hints": [
      "Use go.Surface() with x, y, z parameters",
      "z must be a 2D array (matrix)",
      "Update layout with scene dict for 3D axis labels",
      "Set autosize=True for responsive sizing"
    ]
  },
  {
    "id": "cs407-ex-5-14",
    "subjectId": "cs407",
    "topicId": "cs407-t5",
    "title": "Customized Plot Style",
    "description": "Write a function that creates a professional-looking plot with custom styling including colors, fonts, and grid.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import matplotlib.pyplot as plt\n\ndef create_styled_plot(x, y, title):\n    # Create a line plot with professional styling:\n    # - Custom color scheme\n    # - Grid with specific alpha\n    # - Custom font sizes\n    # - Figure size 12x6\n    # Return the figure object\n    pass",
    "solution": "import matplotlib.pyplot as plt\n\ndef create_styled_plot(x, y, title):\n    plt.style.use('seaborn-v0_8-darkgrid')\n    fig, ax = plt.subplots(figsize=(12, 6))\n\n    ax.plot(x, y, color='#2E86AB', linewidth=2.5, marker='o',\n            markersize=6, markerfacecolor='#A23B72')\n\n    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n    ax.set_xlabel('X Axis', fontsize=12)\n    ax.set_ylabel('Y Axis', fontsize=12)\n\n    ax.grid(True, alpha=0.3, linestyle='--')\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n    plt.tight_layout()\n    return fig",
    "testCases": [
      {
        "input": "x=[1, 2, 3, 4, 5], y=[2, 4, 3, 5, 6], title=\"Professional Plot\"",
        "expectedOutput": "Styled figure with custom colors and formatting",
        "isHidden": false,
        "description": "Professional styling"
      }
    ],
    "hints": [
      "Use plt.style.use() to set a base style",
      "Customize colors with hex codes",
      "Set linewidth, marker properties",
      "Remove top and right spines for cleaner look",
      "Use fontsize and fontweight for title"
    ]
  },
  {
    "id": "cs407-ex-5-15",
    "subjectId": "cs407",
    "topicId": "cs407-t5",
    "title": "Dashboard Layout",
    "description": "Write a function that creates a dashboard layout with multiple plots showing different aspects of the same dataset.",
    "difficulty": 4,
    "language": "python",
    "starterCode": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef create_dashboard(df, numeric_col, category_col):\n    # Create a 3-subplot dashboard:\n    # - Top: distribution histogram\n    # - Bottom left: box plot by category\n    # - Bottom right: violin plot by category\n    # Return the figure object\n    pass",
    "solution": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef create_dashboard(df, numeric_col, category_col):\n    fig = plt.figure(figsize=(14, 10))\n    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n\n    # Top subplot - spans both columns\n    ax1 = fig.add_subplot(gs[0, :])\n    ax1.hist(df[numeric_col], bins=20, edgecolor='black', alpha=0.7)\n    ax1.set_title(f'Distribution of {numeric_col}', fontsize=14, fontweight='bold')\n    ax1.set_xlabel(numeric_col)\n    ax1.set_ylabel('Frequency')\n\n    # Bottom left - box plot\n    ax2 = fig.add_subplot(gs[1, 0])\n    sns.boxplot(data=df, x=category_col, y=numeric_col, ax=ax2)\n    ax2.set_title(f'Box Plot by {category_col}', fontsize=12)\n\n    # Bottom right - violin plot\n    ax3 = fig.add_subplot(gs[1, 1])\n    sns.violinplot(data=df, x=category_col, y=numeric_col, ax=ax3)\n    ax3.set_title(f'Violin Plot by {category_col}', fontsize=12)\n\n    return fig",
    "testCases": [
      {
        "input": "df with numeric and categorical columns",
        "expectedOutput": "Dashboard with 3 plots",
        "isHidden": false,
        "description": "Multi-plot dashboard"
      }
    ],
    "hints": [
      "Use GridSpec for flexible subplot layout",
      "Top plot spans both columns: gs[0, :]",
      "Bottom plots use gs[1, 0] and gs[1, 1]",
      "Set hspace and wspace for spacing",
      "Use different plot types for different insights"
    ]
  },
  {
    "id": "cs407-ex-5-16",
    "subjectId": "cs407",
    "topicId": "cs407-t5",
    "title": "Geographic Bubble Map",
    "description": "Write a function that creates a geographic scatter plot (bubble map) using Plotly where bubble size represents a value.",
    "difficulty": 4,
    "language": "python",
    "starterCode": "import plotly.express as px\n\ndef create_geo_bubble_map(df, lat_col, lon_col, size_col, color_col, title):\n    # Create a geographic bubble map\n    # Bubble size determined by size_col\n    # Bubble color determined by color_col\n    # Return the figure object\n    pass",
    "solution": "import plotly.express as px\n\ndef create_geo_bubble_map(df, lat_col, lon_col, size_col, color_col, title):\n    fig = px.scatter_geo(df,\n                         lat=lat_col,\n                         lon=lon_col,\n                         size=size_col,\n                         color=color_col,\n                         hover_data=[size_col, color_col],\n                         title=title,\n                         projection='natural earth')\n\n    fig.update_layout(\n        geo=dict(\n            showland=True,\n            landcolor='lightgray',\n            showcountries=True,\n            countrycolor='white'\n        )\n    )\n\n    return fig",
    "testCases": [
      {
        "input": "df with lat, lon, population, and gdp columns",
        "expectedOutput": "Geographic bubble map",
        "isHidden": false,
        "description": "World data visualization"
      }
    ],
    "hints": [
      "Use px.scatter_geo() for geographic plots",
      "Specify lat, lon, size, and color parameters",
      "Set projection (e.g., \"natural earth\")",
      "Update geo layout for map appearance",
      "Add hover_data for interactivity"
    ]
  },
  {
    "id": "cs407-ex-6-1",
    "subjectId": "cs407",
    "topicId": "cs407-t6",
    "title": "Create Spark DataFrame",
    "description": "Write a function that creates a PySpark DataFrame from a list of tuples with column names.",
    "difficulty": 1,
    "language": "python",
    "starterCode": "from pyspark.sql import SparkSession\n\ndef create_spark_dataframe(data, columns):\n    # data is a list of tuples\n    # columns is a list of column names\n    # Return a Spark DataFrame\n    pass",
    "solution": "from pyspark.sql import SparkSession\n\ndef create_spark_dataframe(data, columns):\n    spark = SparkSession.builder.appName(\"CreateDF\").getOrCreate()\n    df = spark.createDataFrame(data, columns)\n    return df",
    "testCases": [
      {
        "input": "data=[(1, \"Alice\", 25), (2, \"Bob\", 30)], columns=[\"id\", \"name\", \"age\"]",
        "expectedOutput": "Spark DataFrame with 2 rows",
        "isHidden": false,
        "description": "Basic DataFrame creation"
      },
      {
        "input": "data=[(100, \"Product A\"), (200, \"Product B\")], columns=[\"price\", \"name\"]",
        "expectedOutput": "Spark DataFrame with product data",
        "isHidden": false,
        "description": "Product DataFrame"
      }
    ],
    "hints": [
      "Get or create a SparkSession",
      "Use spark.createDataFrame(data, columns)",
      "data should be a list of tuples",
      "Return the DataFrame"
    ]
  },
  {
    "id": "cs407-ex-6-2",
    "subjectId": "cs407",
    "topicId": "cs407-t6",
    "title": "Filter Spark DataFrame",
    "description": "Write a function that filters a Spark DataFrame based on a condition.",
    "difficulty": 1,
    "language": "python",
    "starterCode": "def filter_dataframe(df, column, value):\n    # Filter df where column equals value\n    # Return filtered DataFrame\n    pass",
    "solution": "def filter_dataframe(df, column, value):\n    return df.filter(df[column] == value)",
    "testCases": [
      {
        "input": "df with age column, filter age > 25",
        "expectedOutput": "Filtered DataFrame",
        "isHidden": false,
        "description": "Filter by age"
      },
      {
        "input": "df with category column, filter category = \"A\"",
        "expectedOutput": "Filtered DataFrame with category A",
        "isHidden": false,
        "description": "Filter by category"
      }
    ],
    "hints": [
      "Use df.filter() method",
      "Access column with df[column]",
      "Use == for equality comparison",
      "Return the filtered DataFrame"
    ]
  },
  {
    "id": "cs407-ex-6-3",
    "subjectId": "cs407",
    "topicId": "cs407-t6",
    "title": "Select and Rename Columns",
    "description": "Write a function that selects specific columns and renames them.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "def select_and_rename(df, old_names, new_names):\n    # Select columns in old_names and rename to new_names\n    # Return transformed DataFrame\n    pass",
    "solution": "def select_and_rename(df, old_names, new_names):\n    selected = df.select(*old_names)\n    for old, new in zip(old_names, new_names):\n        selected = selected.withColumnRenamed(old, new)\n    return selected",
    "testCases": [
      {
        "input": "df with columns, old_names=[\"col1\", \"col2\"], new_names=[\"a\", \"b\"]",
        "expectedOutput": "DataFrame with renamed columns",
        "isHidden": false,
        "description": "Select and rename"
      }
    ],
    "hints": [
      "Use df.select(*old_names) to select columns",
      "Use withColumnRenamed(old, new) for renaming",
      "Loop through old and new names together",
      "Return the transformed DataFrame"
    ]
  },
  {
    "id": "cs407-ex-6-4",
    "subjectId": "cs407",
    "topicId": "cs407-t6",
    "title": "Add Computed Column",
    "description": "Write a function that adds a new column based on a computation from existing columns.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "from pyspark.sql import functions as F\n\ndef add_computed_column(df, col1, col2, new_col_name):\n    # Add a new column that is the sum of col1 and col2\n    # Return DataFrame with new column\n    pass",
    "solution": "from pyspark.sql import functions as F\n\ndef add_computed_column(df, col1, col2, new_col_name):\n    return df.withColumn(new_col_name, F.col(col1) + F.col(col2))",
    "testCases": [
      {
        "input": "df with columns a and b, add column c = a + b",
        "expectedOutput": "DataFrame with new computed column",
        "isHidden": false,
        "description": "Sum two columns"
      },
      {
        "input": "df with price and tax, add total = price + tax",
        "expectedOutput": "DataFrame with total column",
        "isHidden": false,
        "description": "Calculate total"
      }
    ],
    "hints": [
      "Use withColumn(new_col_name, expression)",
      "Use F.col() to reference columns",
      "Add columns with + operator",
      "Return the modified DataFrame"
    ]
  },
  {
    "id": "cs407-ex-6-5",
    "subjectId": "cs407",
    "topicId": "cs407-t6",
    "title": "Group By and Aggregate",
    "description": "Write a function that groups data by a column and computes aggregates.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "from pyspark.sql import functions as F\n\ndef group_and_aggregate(df, group_col, agg_col):\n    # Group by group_col and compute sum, mean, and count of agg_col\n    # Return aggregated DataFrame\n    pass",
    "solution": "from pyspark.sql import functions as F\n\ndef group_and_aggregate(df, group_col, agg_col):\n    return df.groupBy(group_col).agg(\n        F.sum(agg_col).alias(f'{agg_col}_sum'),\n        F.mean(agg_col).alias(f'{agg_col}_mean'),\n        F.count(agg_col).alias(f'{agg_col}_count')\n    )",
    "testCases": [
      {
        "input": "df with category and sales columns",
        "expectedOutput": "Aggregated DataFrame with sum, mean, count",
        "isHidden": false,
        "description": "Sales aggregation"
      },
      {
        "input": "df with department and employee_count",
        "expectedOutput": "Aggregated by department",
        "isHidden": false,
        "description": "Department statistics"
      }
    ],
    "hints": [
      "Use df.groupBy(group_col)",
      "Use .agg() with multiple aggregation functions",
      "F.sum(), F.mean(), F.count() for aggregations",
      "Use .alias() to name aggregated columns"
    ]
  },
  {
    "id": "cs407-ex-6-6",
    "subjectId": "cs407",
    "topicId": "cs407-t6",
    "title": "Join DataFrames",
    "description": "Write a function that performs an inner join between two Spark DataFrames.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "def join_dataframes(df1, df2, join_column):\n    # Perform inner join on join_column\n    # Return joined DataFrame\n    pass",
    "solution": "def join_dataframes(df1, df2, join_column):\n    return df1.join(df2, on=join_column, how='inner')",
    "testCases": [
      {
        "input": "df1 with id and name, df2 with id and age, join on id",
        "expectedOutput": "Joined DataFrame",
        "isHidden": false,
        "description": "Inner join on id"
      },
      {
        "input": "customers and orders DataFrames, join on customer_id",
        "expectedOutput": "Joined customer orders",
        "isHidden": false,
        "description": "Customer orders join"
      }
    ],
    "hints": [
      "Use df1.join(df2, on=join_column)",
      "Set how=\"inner\" for inner join",
      "Both DataFrames must have the join_column",
      "Return the joined DataFrame"
    ]
  },
  {
    "id": "cs407-ex-6-7",
    "subjectId": "cs407",
    "topicId": "cs407-t6",
    "title": "Handle Null Values",
    "description": "Write a function that removes rows with null values in specific columns.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "def remove_nulls(df, columns):\n    # Remove rows where any of the specified columns have null values\n    # Return cleaned DataFrame\n    pass",
    "solution": "def remove_nulls(df, columns):\n    return df.dropna(subset=columns)",
    "testCases": [
      {
        "input": "df with nulls, columns=[\"age\", \"name\"]",
        "expectedOutput": "DataFrame without nulls in age or name",
        "isHidden": false,
        "description": "Drop nulls in specific columns"
      },
      {
        "input": "df with nulls, columns=[\"price\"]",
        "expectedOutput": "DataFrame without nulls in price",
        "isHidden": false,
        "description": "Drop nulls in price"
      }
    ],
    "hints": [
      "Use df.dropna() to remove null values",
      "Specify subset=columns to target specific columns",
      "This removes entire rows with nulls",
      "Return the cleaned DataFrame"
    ]
  },
  {
    "id": "cs407-ex-6-8",
    "subjectId": "cs407",
    "topicId": "cs407-t6",
    "title": "Window Functions",
    "description": "Write a function that adds a ranking column using window functions.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "from pyspark.sql import Window\nfrom pyspark.sql import functions as F\n\ndef add_rank_column(df, partition_col, order_col, rank_col_name):\n    # Add ranking within each partition\n    # Rank by order_col (descending)\n    # Return DataFrame with rank column\n    pass",
    "solution": "from pyspark.sql import Window\nfrom pyspark.sql import functions as F\n\ndef add_rank_column(df, partition_col, order_col, rank_col_name):\n    window_spec = Window.partitionBy(partition_col).orderBy(F.col(order_col).desc())\n    return df.withColumn(rank_col_name, F.rank().over(window_spec))",
    "testCases": [
      {
        "input": "df with category and sales, rank sales within each category",
        "expectedOutput": "DataFrame with rank column",
        "isHidden": false,
        "description": "Rank sales by category"
      },
      {
        "input": "df with department and score, rank by score",
        "expectedOutput": "DataFrame with rankings",
        "isHidden": false,
        "description": "Rank employees by score"
      }
    ],
    "hints": [
      "Create Window.partitionBy(partition_col)",
      "Add .orderBy(F.col(order_col).desc())",
      "Use F.rank().over(window_spec)",
      "Add column with withColumn()"
    ]
  },
  {
    "id": "cs407-ex-6-9",
    "subjectId": "cs407",
    "topicId": "cs407-t6",
    "title": "Pivot Table",
    "description": "Write a function that creates a pivot table in Spark.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "from pyspark.sql import functions as F\n\ndef create_pivot_table(df, index_col, pivot_col, value_col):\n    # Create pivot table with index_col as rows,\n    # pivot_col values as columns, and sum of value_col as values\n    # Return pivoted DataFrame\n    pass",
    "solution": "from pyspark.sql import functions as F\n\ndef create_pivot_table(df, index_col, pivot_col, value_col):\n    return df.groupBy(index_col).pivot(pivot_col).sum(value_col)",
    "testCases": [
      {
        "input": "df with date, product, sales - pivot by product",
        "expectedOutput": "Pivoted DataFrame with products as columns",
        "isHidden": false,
        "description": "Sales pivot table"
      },
      {
        "input": "df with region, year, revenue - pivot by year",
        "expectedOutput": "Pivoted by year",
        "isHidden": false,
        "description": "Regional revenue by year"
      }
    ],
    "hints": [
      "Use df.groupBy(index_col)",
      "Chain .pivot(pivot_col)",
      "Apply aggregation like .sum(value_col)",
      "Return the pivoted result"
    ]
  },
  {
    "id": "cs407-ex-6-10",
    "subjectId": "cs407",
    "topicId": "cs407-t6",
    "title": "Read and Write Parquet",
    "description": "Write functions to read from and write to Parquet format.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "def write_parquet(df, path):\n    # Write DataFrame to parquet format at path\n    pass\n\ndef read_parquet(spark, path):\n    # Read parquet file from path\n    # Return DataFrame\n    pass",
    "solution": "def write_parquet(df, path):\n    df.write.parquet(path, mode='overwrite')\n\ndef read_parquet(spark, path):\n    return spark.read.parquet(path)",
    "testCases": [
      {
        "input": "df and path=\"/tmp/data.parquet\"",
        "expectedOutput": "DataFrame written and read successfully",
        "isHidden": false,
        "description": "Parquet I/O"
      }
    ],
    "hints": [
      "Use df.write.parquet(path) to write",
      "Use spark.read.parquet(path) to read",
      "Set mode=\"overwrite\" when writing",
      "Parquet is a columnar storage format"
    ]
  },
  {
    "id": "cs407-ex-6-11",
    "subjectId": "cs407",
    "topicId": "cs407-t6",
    "title": "SQL Query on DataFrame",
    "description": "Write a function that executes a SQL query on a Spark DataFrame.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "def execute_sql_query(df, table_name, query):\n    # Register df as a temporary table\n    # Execute SQL query\n    # Return result DataFrame\n    pass",
    "solution": "def execute_sql_query(df, table_name, query):\n    df.createOrReplaceTempView(table_name)\n    spark = df.sparkSession\n    return spark.sql(query)",
    "testCases": [
      {
        "input": "df, \"people\", \"SELECT * FROM people WHERE age > 25\"",
        "expectedOutput": "Filtered DataFrame",
        "isHidden": false,
        "description": "SQL SELECT with WHERE"
      },
      {
        "input": "df, \"sales\", \"SELECT category, SUM(amount) FROM sales GROUP BY category\"",
        "expectedOutput": "Aggregated DataFrame",
        "isHidden": false,
        "description": "SQL GROUP BY"
      }
    ],
    "hints": [
      "Use df.createOrReplaceTempView(table_name)",
      "Get SparkSession with df.sparkSession",
      "Execute query with spark.sql(query)",
      "Return the result DataFrame"
    ]
  },
  {
    "id": "cs407-ex-6-12",
    "subjectId": "cs407",
    "topicId": "cs407-t6",
    "title": "Union DataFrames",
    "description": "Write a function that combines multiple DataFrames vertically.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "def union_dataframes(dfs):\n    # dfs is a list of DataFrames\n    # Union all DataFrames\n    # Return combined DataFrame\n    pass",
    "solution": "def union_dataframes(dfs):\n    if not dfs:\n        return None\n    result = dfs[0]\n    for df in dfs[1:]:\n        result = result.union(df)\n    return result",
    "testCases": [
      {
        "input": "list of 3 DataFrames with same schema",
        "expectedOutput": "Combined DataFrame with all rows",
        "isHidden": false,
        "description": "Union multiple DataFrames"
      },
      {
        "input": "list of 2 DataFrames",
        "expectedOutput": "Combined DataFrame",
        "isHidden": false,
        "description": "Union two DataFrames"
      }
    ],
    "hints": [
      "Start with the first DataFrame",
      "Loop through remaining DataFrames",
      "Use result.union(df) to combine",
      "All DataFrames must have same schema"
    ]
  },
  {
    "id": "cs407-ex-6-13",
    "subjectId": "cs407",
    "topicId": "cs407-t6",
    "title": "Distinct Values",
    "description": "Write a function that returns distinct values from a column.",
    "difficulty": 1,
    "language": "python",
    "starterCode": "def get_distinct_values(df, column):\n    # Get distinct values from column\n    # Return list of distinct values\n    pass",
    "solution": "def get_distinct_values(df, column):\n    distinct_rows = df.select(column).distinct()\n    return [row[column] for row in distinct_rows.collect()]",
    "testCases": [
      {
        "input": "df with category column having [\"A\", \"B\", \"A\", \"C\"]",
        "expectedOutput": "[\"A\", \"B\", \"C\"]",
        "isHidden": false,
        "description": "Distinct categories"
      },
      {
        "input": "df with status column",
        "expectedOutput": "List of unique statuses",
        "isHidden": false,
        "description": "Unique statuses"
      }
    ],
    "hints": [
      "Use df.select(column) to select the column",
      "Use .distinct() to get unique values",
      "Use .collect() to get rows",
      "Extract values from rows and return as list"
    ]
  },
  {
    "id": "cs407-ex-6-14",
    "subjectId": "cs407",
    "topicId": "cs407-t6",
    "title": "Cache and Persist",
    "description": "Write a function that caches a DataFrame for better performance.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "from pyspark import StorageLevel\n\ndef cache_dataframe(df, storage_level='MEMORY_AND_DISK'):\n    # Cache the DataFrame with specified storage level\n    # Return cached DataFrame\n    pass",
    "solution": "from pyspark import StorageLevel\n\ndef cache_dataframe(df, storage_level='MEMORY_AND_DISK'):\n    if storage_level == 'MEMORY_ONLY':\n        return df.cache()\n    elif storage_level == 'MEMORY_AND_DISK':\n        return df.persist(StorageLevel.MEMORY_AND_DISK)\n    elif storage_level == 'DISK_ONLY':\n        return df.persist(StorageLevel.DISK_ONLY)\n    return df.cache()",
    "testCases": [
      {
        "input": "df, storage_level=\"MEMORY_ONLY\"",
        "expectedOutput": "Cached DataFrame in memory",
        "isHidden": false,
        "description": "Memory caching"
      },
      {
        "input": "df, storage_level=\"MEMORY_AND_DISK\"",
        "expectedOutput": "Persisted DataFrame",
        "isHidden": false,
        "description": "Memory and disk persistence"
      }
    ],
    "hints": [
      "Use df.cache() for memory-only caching",
      "Use df.persist(StorageLevel.X) for other levels",
      "Common levels: MEMORY_ONLY, MEMORY_AND_DISK, DISK_ONLY",
      "Caching improves performance for repeated operations"
    ]
  },
  {
    "id": "cs407-ex-6-15",
    "subjectId": "cs407",
    "topicId": "cs407-t6",
    "title": "Broadcast Join",
    "description": "Write a function that performs a broadcast join for joining a large DataFrame with a small one.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "from pyspark.sql.functions import broadcast\n\ndef broadcast_join(large_df, small_df, join_column):\n    # Perform broadcast join\n    # Broadcast the small DataFrame\n    # Return joined DataFrame\n    pass",
    "solution": "from pyspark.sql.functions import broadcast\n\ndef broadcast_join(large_df, small_df, join_column):\n    return large_df.join(broadcast(small_df), on=join_column, how='inner')",
    "testCases": [
      {
        "input": "large transaction df, small product lookup df",
        "expectedOutput": "Joined DataFrame with product info",
        "isHidden": false,
        "description": "Broadcast small lookup table"
      }
    ],
    "hints": [
      "Use broadcast(small_df) to mark for broadcasting",
      "Use .join() as normal",
      "Broadcasting avoids shuffling the large DataFrame",
      "Only broadcast small DataFrames that fit in memory"
    ]
  },
  {
    "id": "cs407-ex-6-16",
    "subjectId": "cs407",
    "topicId": "cs407-t6",
    "title": "ETL Pipeline",
    "description": "Write a function that implements a simple ETL pipeline: Extract from CSV, Transform data, Load to Parquet.",
    "difficulty": 4,
    "language": "python",
    "starterCode": "from pyspark.sql import functions as F\n\ndef etl_pipeline(spark, input_path, output_path, filter_col, filter_value):\n    # Extract: Read CSV from input_path\n    # Transform: Filter rows where filter_col > filter_value, add processed_date column\n    # Load: Write to Parquet at output_path\n    # Return the transformed DataFrame\n    pass",
    "solution": "from pyspark.sql import functions as F\n\ndef etl_pipeline(spark, input_path, output_path, filter_col, filter_value):\n    # Extract\n    df = spark.read.csv(input_path, header=True, inferSchema=True)\n\n    # Transform\n    filtered_df = df.filter(F.col(filter_col) > filter_value)\n    transformed_df = filtered_df.withColumn('processed_date', F.current_date())\n\n    # Load\n    transformed_df.write.parquet(output_path, mode='overwrite')\n\n    return transformed_df",
    "testCases": [
      {
        "input": "CSV with sales data, filter amount > 100",
        "expectedOutput": "Filtered and transformed data in Parquet",
        "isHidden": false,
        "description": "Sales ETL pipeline"
      },
      {
        "input": "CSV with user data, filter age > 18",
        "expectedOutput": "Adult users in Parquet",
        "isHidden": false,
        "description": "User filtering pipeline"
      }
    ],
    "hints": [
      "Extract: spark.read.csv() with header=True",
      "Transform: Use .filter() and .withColumn()",
      "Add current_date with F.current_date()",
      "Load: df.write.parquet() with mode=\"overwrite\"",
      "Return the transformed DataFrame"
    ]
  },
  {
    "id": "cs407-ex-7-1",
    "subjectId": "cs407",
    "topicId": "cs407-t7",
    "title": "Data Anonymization",
    "description": "Write a function that anonymizes personally identifiable information (PII) by replacing names with unique IDs.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef anonymize_names(df, name_column):\n    # Replace names with anonymous IDs like 'USER_001', 'USER_002', etc.\n    pass",
    "solution": "import pandas as pd\n\ndef anonymize_names(df, name_column):\n    unique_names = df[name_column].unique()\n    name_to_id = {name: f'USER_{str(i+1).zfill(3)}' for i, name in enumerate(unique_names)}\n    df[name_column] = df[name_column].map(name_to_id)\n    return df",
    "testCases": [
      {
        "input": "pd.DataFrame({\"name\": [\"Alice\", \"Bob\", \"Alice\"]}), \"name\"",
        "expectedOutput": "pd.DataFrame({\"name\": [\"USER_001\", \"USER_002\", \"USER_001\"]})",
        "isHidden": false,
        "description": "Anonymize names"
      }
    ],
    "hints": [
      "Get unique values from the column",
      "Create a mapping dictionary from names to IDs",
      "Use .map() to replace names with IDs",
      "Use str.zfill() to pad numbers with zeros"
    ]
  },
  {
    "id": "cs407-ex-7-2",
    "subjectId": "cs407",
    "topicId": "cs407-t7",
    "title": "Email Masking",
    "description": "Write a function that masks email addresses for privacy by showing only the domain.",
    "difficulty": 1,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef mask_emails(df, email_column):\n    # Replace emails with masked version: user@domain.com -> ****@domain.com\n    # Return modified DataFrame\n    pass",
    "solution": "import pandas as pd\n\ndef mask_emails(df, email_column):\n    df[email_column] = df[email_column].apply(\n        lambda email: '****@' + email.split('@')[1] if '@' in email else email\n    )\n    return df",
    "testCases": [
      {
        "input": "pd.DataFrame({\"email\": [\"alice@example.com\", \"bob@test.org\"]}), \"email\"",
        "expectedOutput": "DataFrame with masked emails",
        "isHidden": false,
        "description": "Mask email addresses"
      },
      {
        "input": "pd.DataFrame({\"email\": [\"user1@company.com\"]}), \"email\"",
        "expectedOutput": "DataFrame with ****@company.com",
        "isHidden": false,
        "description": "Single email masking"
      }
    ],
    "hints": [
      "Split email by \"@\" to get domain",
      "Replace username part with \"****\"",
      "Use .apply() with lambda function",
      "Handle cases without @ symbol"
    ]
  },
  {
    "id": "cs407-ex-7-3",
    "subjectId": "cs407",
    "topicId": "cs407-t7",
    "title": "K-Anonymity Check",
    "description": "Write a function that checks if a dataset satisfies k-anonymity for given quasi-identifiers.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef check_k_anonymity(df, quasi_identifiers, k):\n    # Check if every combination of quasi_identifiers appears at least k times\n    # Return True if k-anonymity is satisfied, False otherwise\n    pass",
    "solution": "import pandas as pd\n\ndef check_k_anonymity(df, quasi_identifiers, k):\n    group_sizes = df.groupby(quasi_identifiers).size()\n    min_group_size = group_sizes.min()\n    return min_group_size >= k",
    "testCases": [
      {
        "input": "df with zip codes and ages, k=2",
        "expectedOutput": "True or False based on group sizes",
        "isHidden": false,
        "description": "Check 2-anonymity"
      },
      {
        "input": "df with gender and city, k=3",
        "expectedOutput": "Boolean result",
        "isHidden": false,
        "description": "Check 3-anonymity"
      }
    ],
    "hints": [
      "Group by quasi_identifiers columns",
      "Use .size() to count group sizes",
      "Find minimum group size",
      "Return True if min size >= k"
    ]
  },
  {
    "id": "cs407-ex-7-4",
    "subjectId": "cs407",
    "topicId": "cs407-t7",
    "title": "Bias Detection in Groups",
    "description": "Write a function that detects demographic parity bias by comparing outcome rates across groups.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef detect_demographic_parity_bias(df, sensitive_attr, outcome_col, threshold=0.1):\n    # Calculate outcome rate for each group\n    # Check if difference exceeds threshold\n    # Return dict with rates and bias_detected flag\n    pass",
    "solution": "import pandas as pd\n\ndef detect_demographic_parity_bias(df, sensitive_attr, outcome_col, threshold=0.1):\n    group_rates = df.groupby(sensitive_attr)[outcome_col].mean()\n    max_rate = group_rates.max()\n    min_rate = group_rates.min()\n    difference = max_rate - min_rate\n\n    return {\n        'group_rates': group_rates.to_dict(),\n        'difference': round(difference, 3),\n        'bias_detected': difference > threshold\n    }",
    "testCases": [
      {
        "input": "df with gender and approved columns",
        "expectedOutput": "Dict with rates and bias detection",
        "isHidden": false,
        "description": "Gender bias detection"
      },
      {
        "input": "df with race and hired columns",
        "expectedOutput": "Bias analysis results",
        "isHidden": false,
        "description": "Hiring bias detection"
      }
    ],
    "hints": [
      "Group by sensitive attribute",
      "Calculate mean of outcome for each group",
      "Find difference between max and min rates",
      "Compare difference to threshold"
    ]
  },
  {
    "id": "cs407-ex-7-5",
    "subjectId": "cs407",
    "topicId": "cs407-t7",
    "title": "Equal Opportunity Metric",
    "description": "Write a function that calculates the equal opportunity difference for a binary classifier.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef calculate_equal_opportunity(df, sensitive_attr, y_true_col, y_pred_col, favorable_label=1):\n    # Calculate True Positive Rate for each group\n    # Return difference between TPRs (equal opportunity metric)\n    pass",
    "solution": "import pandas as pd\n\ndef calculate_equal_opportunity(df, sensitive_attr, y_true_col, y_pred_col, favorable_label=1):\n    def tpr(group):\n        positives = group[group[y_true_col] == favorable_label]\n        if len(positives) == 0:\n            return 0\n        return (positives[y_pred_col] == favorable_label).sum() / len(positives)\n\n    tpr_by_group = df.groupby(sensitive_attr).apply(tpr)\n    difference = tpr_by_group.max() - tpr_by_group.min()\n\n    return {\n        'tpr_by_group': tpr_by_group.to_dict(),\n        'equal_opportunity_difference': round(difference, 3)\n    }",
    "testCases": [
      {
        "input": "df with predictions and true labels by group",
        "expectedOutput": "TPR difference between groups",
        "isHidden": false,
        "description": "Equal opportunity calculation"
      }
    ],
    "hints": [
      "Filter for true positive cases (y_true == favorable_label)",
      "Calculate TPR = correct predictions / total positives",
      "Compute TPR for each group",
      "Return difference between max and min TPR"
    ]
  },
  {
    "id": "cs407-ex-7-6",
    "subjectId": "cs407",
    "topicId": "cs407-t7",
    "title": "Data Suppression",
    "description": "Write a function that suppresses rare values in a column to protect privacy.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef suppress_rare_values(df, column, min_count):\n    # Replace values that appear less than min_count times with \"SUPPRESSED\"\n    # Return modified DataFrame\n    pass",
    "solution": "import pandas as pd\n\ndef suppress_rare_values(df, column, min_count):\n    value_counts = df[column].value_counts()\n    rare_values = value_counts[value_counts < min_count].index\n    df[column] = df[column].apply(\n        lambda x: 'SUPPRESSED' if x in rare_values else x\n    )\n    return df",
    "testCases": [
      {
        "input": "df with occupation column, min_count=2",
        "expectedOutput": "DataFrame with rare occupations suppressed",
        "isHidden": false,
        "description": "Suppress rare occupations"
      },
      {
        "input": "df with city column, min_count=3",
        "expectedOutput": "Rare cities replaced with SUPPRESSED",
        "isHidden": false,
        "description": "Suppress rare cities"
      }
    ],
    "hints": [
      "Use value_counts() to count occurrences",
      "Filter for values with count < min_count",
      "Replace rare values with \"SUPPRESSED\"",
      "Use .apply() with conditional logic"
    ]
  },
  {
    "id": "cs407-ex-7-7",
    "subjectId": "cs407",
    "topicId": "cs407-t7",
    "title": "Age Binning for Privacy",
    "description": "Write a function that bins ages into ranges to reduce identifiability.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef bin_ages(df, age_column, bin_size=10):\n    # Convert ages to bins (e.g., 20-29, 30-39)\n    # Return modified DataFrame\n    pass",
    "solution": "import pandas as pd\n\ndef bin_ages(df, age_column, bin_size=10):\n    def age_to_bin(age):\n        lower = (age // bin_size) * bin_size\n        upper = lower + bin_size - 1\n        return f'{lower}-{upper}'\n\n    df[age_column] = df[age_column].apply(age_to_bin)\n    return df",
    "testCases": [
      {
        "input": "df with ages [25, 32, 45], bin_size=10",
        "expectedOutput": "DataFrame with age ranges [\"20-29\", \"30-39\", \"40-49\"]",
        "isHidden": false,
        "description": "Bin ages by decade"
      },
      {
        "input": "df with ages [18, 22, 35], bin_size=5",
        "expectedOutput": "Age ranges with 5-year bins",
        "isHidden": false,
        "description": "5-year age bins"
      }
    ],
    "hints": [
      "Calculate lower bound: (age // bin_size) * bin_size",
      "Calculate upper bound: lower + bin_size - 1",
      "Format as string: \"lower-upper\"",
      "Use .apply() to transform all ages"
    ]
  },
  {
    "id": "cs407-ex-7-8",
    "subjectId": "cs407",
    "topicId": "cs407-t7",
    "title": "Consent Tracking",
    "description": "Write a function that filters data based on user consent for specific purposes.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef filter_by_consent(df, purpose):\n    # df has columns: user_id, consent_purposes (list), data\n    # Filter to only include users who consented to 'purpose'\n    # Return filtered DataFrame\n    pass",
    "solution": "import pandas as pd\n\ndef filter_by_consent(df, purpose):\n    return df[df['consent_purposes'].apply(lambda x: purpose in x)]",
    "testCases": [
      {
        "input": "df with consent_purposes, filter for \"marketing\"",
        "expectedOutput": "DataFrame with only users who consented to marketing",
        "isHidden": false,
        "description": "Filter by marketing consent"
      },
      {
        "input": "df with consent_purposes, filter for \"analytics\"",
        "expectedOutput": "Users who consented to analytics",
        "isHidden": false,
        "description": "Filter by analytics consent"
      }
    ],
    "hints": [
      "consent_purposes is a list for each user",
      "Check if purpose is in the list",
      "Use .apply() with lambda to check membership",
      "Filter DataFrame based on result"
    ]
  },
  {
    "id": "cs407-ex-7-9",
    "subjectId": "cs407",
    "topicId": "cs407-t7",
    "title": "Differential Privacy Noise",
    "description": "Write a function that adds Laplacian noise to numeric data for differential privacy.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import pandas as pd\nimport numpy as np\n\ndef add_laplacian_noise(df, columns, epsilon):\n    # Add Laplacian noise with scale = sensitivity / epsilon\n    # Assume sensitivity = 1 for each column\n    # Return DataFrame with noisy data\n    pass",
    "solution": "import pandas as pd\nimport numpy as np\n\ndef add_laplacian_noise(df, columns, epsilon):\n    df_noisy = df.copy()\n    scale = 1.0 / epsilon\n\n    for col in columns:\n        noise = np.random.laplace(0, scale, size=len(df))\n        df_noisy[col] = df[col] + noise\n\n    return df_noisy",
    "testCases": [
      {
        "input": "df with salary and age columns, epsilon=0.1",
        "expectedOutput": "DataFrame with Laplacian noise added",
        "isHidden": false,
        "description": "Add noise for privacy"
      },
      {
        "input": "df with score column, epsilon=1.0",
        "expectedOutput": "Noisy scores with differential privacy",
        "isHidden": false,
        "description": "Differential privacy on scores"
      }
    ],
    "hints": [
      "Scale = sensitivity / epsilon",
      "Use np.random.laplace(0, scale, size)",
      "Add noise to each specified column",
      "Return a copy of the DataFrame with noise"
    ]
  },
  {
    "id": "cs407-ex-7-10",
    "subjectId": "cs407",
    "topicId": "cs407-t7",
    "title": "Fairness Through Unawareness",
    "description": "Write a function that removes sensitive attributes from a dataset.",
    "difficulty": 1,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef remove_sensitive_attributes(df, sensitive_columns):\n    # Remove specified sensitive columns\n    # Return DataFrame without sensitive attributes\n    pass",
    "solution": "import pandas as pd\n\ndef remove_sensitive_attributes(df, sensitive_columns):\n    return df.drop(columns=sensitive_columns)",
    "testCases": [
      {
        "input": "df with gender, race, age columns, remove [\"gender\", \"race\"]",
        "expectedOutput": "DataFrame without gender and race",
        "isHidden": false,
        "description": "Remove sensitive attributes"
      },
      {
        "input": "df with religion and ethnicity, remove both",
        "expectedOutput": "DataFrame with attributes removed",
        "isHidden": false,
        "description": "Remove multiple sensitive columns"
      }
    ],
    "hints": [
      "Use df.drop(columns=sensitive_columns)",
      "This implements fairness through unawareness",
      "Note: This does not guarantee fairness",
      "Correlated attributes may still encode bias"
    ]
  },
  {
    "id": "cs407-ex-7-11",
    "subjectId": "cs407",
    "topicId": "cs407-t7",
    "title": "Disparate Impact Ratio",
    "description": "Write a function that calculates the disparate impact ratio to detect bias.",
    "difficulty": 3,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef calculate_disparate_impact(df, sensitive_attr, outcome_col, privileged_value):\n    # Calculate selection rate for privileged and unprivileged groups\n    # Disparate Impact = unprivileged_rate / privileged_rate\n    # Return ratio (should be >= 0.8 for fairness)\n    pass",
    "solution": "import pandas as pd\n\ndef calculate_disparate_impact(df, sensitive_attr, outcome_col, privileged_value):\n    privileged = df[df[sensitive_attr] == privileged_value]\n    unprivileged = df[df[sensitive_attr] != privileged_value]\n\n    privileged_rate = privileged[outcome_col].mean()\n    unprivileged_rate = unprivileged[outcome_col].mean()\n\n    if privileged_rate == 0:\n        return float('inf')\n\n    di_ratio = unprivileged_rate / privileged_rate\n\n    return {\n        'privileged_rate': round(privileged_rate, 3),\n        'unprivileged_rate': round(unprivileged_rate, 3),\n        'disparate_impact_ratio': round(di_ratio, 3),\n        'fair': di_ratio >= 0.8\n    }",
    "testCases": [
      {
        "input": "df with gender and hired columns",
        "expectedOutput": "Disparate impact analysis",
        "isHidden": false,
        "description": "Hiring fairness check"
      },
      {
        "input": "df with race and approved columns",
        "expectedOutput": "Approval fairness analysis",
        "isHidden": false,
        "description": "Loan approval fairness"
      }
    ],
    "hints": [
      "Split data into privileged and unprivileged groups",
      "Calculate mean outcome for each group",
      "Divide unprivileged_rate by privileged_rate",
      "80% rule: ratio should be >= 0.8 for fairness"
    ]
  },
  {
    "id": "cs407-ex-7-12",
    "subjectId": "cs407",
    "topicId": "cs407-t7",
    "title": "Data Retention Policy",
    "description": "Write a function that removes data older than a specified retention period.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\nfrom datetime import datetime, timedelta\n\ndef apply_retention_policy(df, date_column, retention_days):\n    # Remove rows where date is older than retention_days from today\n    # Return filtered DataFrame\n    pass",
    "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\n\ndef apply_retention_policy(df, date_column, retention_days):\n    cutoff_date = datetime.now() - timedelta(days=retention_days)\n    df[date_column] = pd.to_datetime(df[date_column])\n    return df[df[date_column] >= cutoff_date]",
    "testCases": [
      {
        "input": "df with created_at column, retention_days=365",
        "expectedOutput": "DataFrame with only data from last year",
        "isHidden": false,
        "description": "One year retention"
      },
      {
        "input": "df with timestamp, retention_days=90",
        "expectedOutput": "Data from last 90 days",
        "isHidden": false,
        "description": "90-day retention"
      }
    ],
    "hints": [
      "Calculate cutoff date: now - retention_days",
      "Convert date column to datetime",
      "Filter for dates >= cutoff_date",
      "Use pd.to_datetime() for conversion"
    ]
  },
  {
    "id": "cs407-ex-7-13",
    "subjectId": "cs407",
    "topicId": "cs407-t7",
    "title": "Pseudonymization",
    "description": "Write a function that pseudonymizes user IDs using a hash function.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\nimport hashlib\n\ndef pseudonymize_ids(df, id_column, salt=''):\n    # Hash each ID with optional salt\n    # Return DataFrame with pseudonymized IDs\n    pass",
    "solution": "import pandas as pd\nimport hashlib\n\ndef pseudonymize_ids(df, id_column, salt=''):\n    def hash_id(user_id):\n        data = str(user_id) + salt\n        return hashlib.sha256(data.encode()).hexdigest()[:16]\n\n    df[id_column] = df[id_column].apply(hash_id)\n    return df",
    "testCases": [
      {
        "input": "df with user_id column",
        "expectedOutput": "DataFrame with hashed IDs",
        "isHidden": false,
        "description": "Pseudonymize user IDs"
      },
      {
        "input": "df with customer_id, salt=\"secret123\"",
        "expectedOutput": "Salted hash pseudonyms",
        "isHidden": false,
        "description": "Pseudonymize with salt"
      }
    ],
    "hints": [
      "Convert ID to string and concatenate with salt",
      "Use hashlib.sha256() to hash",
      "Encode string before hashing",
      "Use hexdigest()[:16] for shorter hash"
    ]
  },
  {
    "id": "cs407-ex-7-14",
    "subjectId": "cs407",
    "topicId": "cs407-t7",
    "title": "Calibration Fairness",
    "description": "Write a function that checks if a model is calibrated across different groups.",
    "difficulty": 4,
    "language": "python",
    "starterCode": "import pandas as pd\nimport numpy as np\n\ndef check_calibration_fairness(df, sensitive_attr, y_true_col, y_prob_col, n_bins=10):\n    # For each group, check if predicted probabilities match actual outcomes\n    # Return calibration error for each group\n    pass",
    "solution": "import pandas as pd\nimport numpy as np\n\ndef check_calibration_fairness(df, sensitive_attr, y_true_col, y_prob_col, n_bins=10):\n    results = {}\n\n    for group in df[sensitive_attr].unique():\n        group_df = df[df[sensitive_attr] == group]\n\n        # Create bins\n        bins = np.linspace(0, 1, n_bins + 1)\n        group_df['bin'] = pd.cut(group_df[y_prob_col], bins=bins, include_lowest=True)\n\n        # Calculate calibration error\n        calibration = group_df.groupby('bin').agg({\n            y_prob_col: 'mean',\n            y_true_col: 'mean'\n        })\n\n        calibration_error = np.abs(\n            calibration[y_prob_col] - calibration[y_true_col]\n        ).mean()\n\n        results[group] = round(calibration_error, 4)\n\n    return results",
    "testCases": [
      {
        "input": "df with predictions, true labels, and demographic group",
        "expectedOutput": "Calibration error for each group",
        "isHidden": false,
        "description": "Check model calibration fairness"
      }
    ],
    "hints": [
      "Split data by sensitive attribute groups",
      "Bin predicted probabilities (0 to 1)",
      "For each bin, compare avg prediction to avg outcome",
      "Calibration error = mean absolute difference"
    ]
  },
  {
    "id": "cs407-ex-7-15",
    "subjectId": "cs407",
    "topicId": "cs407-t7",
    "title": "Right to be Forgotten",
    "description": "Write a function that removes all data associated with specific user IDs.",
    "difficulty": 2,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef remove_user_data(df, user_id_column, users_to_remove):\n    # Remove all rows associated with users_to_remove\n    # Return DataFrame without the specified users\n    pass",
    "solution": "import pandas as pd\n\ndef remove_user_data(df, user_id_column, users_to_remove):\n    return df[~df[user_id_column].isin(users_to_remove)]",
    "testCases": [
      {
        "input": "df with user_id, remove [123, 456]",
        "expectedOutput": "DataFrame without users 123 and 456",
        "isHidden": false,
        "description": "Remove specific users"
      },
      {
        "input": "df with customer_id, remove single user",
        "expectedOutput": "DataFrame with user removed",
        "isHidden": false,
        "description": "GDPR data deletion"
      }
    ],
    "hints": [
      "Use .isin() to check if ID is in removal list",
      "Use ~ to negate (get rows NOT in list)",
      "Return filtered DataFrame",
      "This implements right to erasure (GDPR)"
    ]
  },
  {
    "id": "cs407-ex-7-16",
    "subjectId": "cs407",
    "topicId": "cs407-t7",
    "title": "Fairness Report",
    "description": "Write a function that generates a comprehensive fairness report for a model across multiple metrics.",
    "difficulty": 4,
    "language": "python",
    "starterCode": "import pandas as pd\n\ndef generate_fairness_report(df, sensitive_attr, y_true_col, y_pred_col):\n    # Calculate multiple fairness metrics:\n    # - Demographic parity difference\n    # - Equal opportunity difference\n    # - Disparate impact ratio\n    # Return comprehensive report dictionary\n    pass",
    "solution": "import pandas as pd\n\ndef generate_fairness_report(df, sensitive_attr, y_true_col, y_pred_col):\n    # Demographic Parity\n    selection_rates = df.groupby(sensitive_attr)[y_pred_col].mean()\n    dp_diff = selection_rates.max() - selection_rates.min()\n\n    # Equal Opportunity (TPR difference)\n    def tpr(group):\n        positives = group[group[y_true_col] == 1]\n        if len(positives) == 0:\n            return 0\n        return (positives[y_pred_col] == 1).sum() / len(positives)\n\n    tpr_by_group = df.groupby(sensitive_attr).apply(tpr)\n    eo_diff = tpr_by_group.max() - tpr_by_group.min()\n\n    # Disparate Impact\n    max_rate = selection_rates.max()\n    min_rate = selection_rates.min()\n    di_ratio = min_rate / max_rate if max_rate > 0 else 0\n\n    return {\n        'selection_rates': selection_rates.to_dict(),\n        'demographic_parity_difference': round(dp_diff, 3),\n        'equal_opportunity_difference': round(eo_diff, 3),\n        'disparate_impact_ratio': round(di_ratio, 3),\n        'passes_80_percent_rule': di_ratio >= 0.8,\n        'summary': 'Fair' if (dp_diff < 0.1 and eo_diff < 0.1 and di_ratio >= 0.8) else 'Potentially Biased'\n    }",
    "testCases": [
      {
        "input": "df with sensitive attribute, predictions, and true labels",
        "expectedOutput": "Comprehensive fairness report",
        "isHidden": false,
        "description": "Full fairness assessment"
      }
    ],
    "hints": [
      "Calculate selection rate for each group",
      "Calculate TPR for equal opportunity",
      "Compute disparate impact ratio",
      "Check 80% rule and threshold violations",
      "Combine all metrics into report dictionary"
    ]
  }
]