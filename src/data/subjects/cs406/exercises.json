[
  {
    "id": "cs406-t1-ex01",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Simple Reflex Agent for Thermostat",
    "difficulty": 1,
    "description": "Implement a simple reflex agent that controls a thermostat.\n\nYour implementation should:\n- Read temperature from a sensor\n- Turn on heating if temperature < 18°C\n- Turn on cooling if temperature > 24°C\n- Turn off both otherwise\n- Log all state changes",
    "starterCode": "class ThermostatAgent:\n    def __init__(self, target_low=18, target_high=24):\n        self.target_low = target_low\n        self.target_high = target_high\n        self.heating_on = False\n        self.cooling_on = False\n\n    def perceive(self, temperature: float) -> dict:\n        # Return perception\n        pass\n\n    def decide_action(self, percept: dict) -> str:\n        # Decide what action to take\n        pass\n\n    def execute(self, action: str):\n        # Execute the action\n        pass\n\n# Example usage:\n# agent = ThermostatAgent()\n# agent.perceive(15.0)  # Cold\n# agent.perceive(22.0)  # Just right\n# agent.perceive(26.0)  # Hot",
    "solution": "class ThermostatAgent:\n    def __init__(self, target_low=18, target_high=24):\n        self.target_low = target_low\n        self.target_high = target_high\n        self.heating_on = False\n        self.cooling_on = False\n        self.log = []\n\n    def perceive(self, temperature: float) -> dict:\n        return {\n            'temperature': temperature,\n            'too_cold': temperature < self.target_low,\n            'too_hot': temperature > self.target_high,\n            'comfortable': self.target_low <= temperature <= self.target_high\n        }\n\n    def decide_action(self, percept: dict) -> str:\n        if percept['too_cold']:\n            return 'HEAT_ON'\n        elif percept['too_hot']:\n            return 'COOL_ON'\n        else:\n            return 'OFF'\n\n    def execute(self, action: str):\n        if action == 'HEAT_ON':\n            if not self.heating_on:\n                self.heating_on = True\n                self.cooling_on = False\n                self.log.append('Heating ON')\n        elif action == 'COOL_ON':\n            if not self.cooling_on:\n                self.heating_on = False\n                self.cooling_on = True\n                self.log.append('Cooling ON')\n        else:  # OFF\n            if self.heating_on or self.cooling_on:\n                self.heating_on = False\n                self.cooling_on = False\n                self.log.append('System OFF')\n\n    def run(self, temperature: float):\n        percept = self.perceive(temperature)\n        action = self.decide_action(percept)\n        self.execute(action)\n        return action\n\n# Test\nagent = ThermostatAgent()\nprint(agent.run(15.0))  # HEAT_ON\nprint(agent.run(22.0))  # OFF\nprint(agent.run(26.0))  # COOL_ON\nprint('Log:', agent.log)",
    "testCases": [
      {
        "input": "agent.run(15.0)",
        "isHidden": false,
        "description": "Test cold temperature (below 18°C)"
      },
      {
        "input": "agent.run(22.0)",
        "isHidden": false,
        "description": "Test comfortable temperature (18-24°C)"
      },
      {
        "input": "agent.run(26.0)",
        "isHidden": false,
        "description": "Test hot temperature (above 24°C)"
      }
    ],
    "hints": [
      "Start by implementing the perceive() method to create a dictionary with temperature information",
      "The decide_action() method should check percept values and return appropriate action strings",
      "Use boolean flags (heating_on, cooling_on) to track current state and avoid duplicate log entries"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex02",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Model-Based Agent with State",
    "difficulty": 2,
    "description": "Implement a model-based vacuum cleaner agent that maintains state.\n\nThe agent should:\n- Track which locations it has visited\n- Remember which locations are clean\n- Move to dirty locations efficiently\n- Handle a 2D grid environment",
    "starterCode": "class VacuumAgent:\n    def __init__(self, grid_size=(3, 3)):\n        self.grid_size = grid_size\n        self.position = (0, 0)\n        # TODO: Initialize internal state\n\n    def update_state(self, percept):\n        # Update internal model of world\n        pass\n\n    def choose_action(self):\n        # Choose action based on state\n        pass\n\n# Grid: 0=clean, 1=dirty\n# Actions: 'SUCK', 'UP', 'DOWN', 'LEFT', 'RIGHT'",
    "solution": "class VacuumAgent:\n    def __init__(self, grid_size=(3, 3)):\n        self.grid_size = grid_size\n        self.position = (0, 0)\n        self.world_state = {}  # (x,y) -> 'clean'/'dirty'/'unknown'\n        self.visited = set()\n\n        # Initialize all as unknown\n        for i in range(grid_size[0]):\n            for j in range(grid_size[1]):\n                self.world_state[(i, j)] = 'unknown'\n\n    def update_state(self, percept):\n        # percept = {'location': (x,y), 'dirty': bool}\n        loc = percept['location']\n        self.position = loc\n        self.visited.add(loc)\n\n        if percept['dirty']:\n            self.world_state[loc] = 'dirty'\n        else:\n            self.world_state[loc] = 'clean'\n\n    def choose_action(self):\n        x, y = self.position\n\n        # If current location dirty, suck\n        if self.world_state[self.position] == 'dirty':\n            self.world_state[self.position] = 'clean'\n            return 'SUCK'\n\n        # Find nearest dirty or unknown location\n        target = self.find_target()\n\n        if target is None:\n            return 'DONE'  # All clean\n\n        # Move toward target\n        tx, ty = target\n        if tx < x and x > 0:\n            return 'LEFT'\n        elif tx > x and x < self.grid_size[0] - 1:\n            return 'RIGHT'\n        elif ty < y and y > 0:\n            return 'DOWN'\n        elif ty > y and y < self.grid_size[1] - 1:\n            return 'UP'\n\n        return 'DONE'\n\n    def find_target(self):\n        # Find nearest dirty or unknown location\n        for dx in range(max(self.grid_size)):\n            for dy in range(max(self.grid_size)):\n                # Check all locations at Manhattan distance dx+dy\n                for loc in self.get_locations_at_distance(dx + dy):\n                    if self.world_state.get(loc, 'unknown') in ['dirty', 'unknown']:\n                        return loc\n        return None\n\n    def get_locations_at_distance(self, dist):\n        x, y = self.position\n        locs = []\n        for i in range(self.grid_size[0]):\n            for j in range(self.grid_size[1]):\n                if abs(i - x) + abs(j - y) == dist:\n                    locs.append((i, j))\n        return locs\n\n# Test\nagent = VacuumAgent((3, 3))\nagent.update_state({'location': (0, 0), 'dirty': True})\nprint(agent.choose_action())  # SUCK\nagent.update_state({'location': (0, 0), 'dirty': False})\nprint(agent.choose_action())  # Move to explore",
    "testCases": [
      {
        "input": "agent.choose_action() when location is dirty",
        "isHidden": false,
        "description": "Test agent sucks when location is dirty"
      },
      {
        "input": "agent.choose_action() after cleaning",
        "isHidden": false,
        "description": "Test agent moves to explore after cleaning"
      },
      {
        "input": "agent.world_state tracking",
        "isHidden": false,
        "description": "Test internal state is maintained correctly"
      }
    ],
    "hints": [
      "Use a dictionary to store the world state, mapping (x,y) positions to clean/dirty/unknown",
      "The find_target() method should search for the nearest dirty or unknown location using Manhattan distance",
      "Remember to update the world_state after sucking to mark the location as clean"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex03",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Goal-Based Agent for Route Planning",
    "difficulty": 2,
    "description": "Implement a goal-based agent that plans routes to reach a destination.\n\nYour implementation should:\n- Maintain a world model (map)\n- Define goals (target locations)\n- Plan sequences of actions to reach goals\n- Adapt to changing environments",
    "starterCode": "class RouteAgent:\n    def __init__(self, world_map, start, goal):\n        self.world_map = world_map\n        self.position = start\n        self.goal = goal\n        self.plan = []\n\n    def make_plan(self):\n        # Create plan to reach goal\n        pass\n\n    def execute_plan(self):\n        # Execute the plan step by step\n        pass\n\n# World map: graph with nodes and edges\n# world_map = {'A': ['B', 'C'], 'B': ['D'], ...}",
    "solution": "from collections import deque\n\nclass RouteAgent:\n    def __init__(self, world_map, start, goal):\n        self.world_map = world_map\n        self.position = start\n        self.goal = goal\n        self.plan = []\n\n    def make_plan(self):\n        \"\"\"Use BFS to find path from current position to goal.\"\"\"\n        if self.position == self.goal:\n            return []\n\n        queue = deque([(self.position, [self.position])])\n        visited = set()\n\n        while queue:\n            node, path = queue.popleft()\n\n            if node in visited:\n                continue\n\n            visited.add(node)\n\n            if node == self.goal:\n                self.plan = path[1:]  # Exclude starting position\n                return self.plan\n\n            for neighbor in self.world_map.get(node, []):\n                if neighbor not in visited:\n                    queue.append((neighbor, path + [neighbor]))\n\n        return None  # No path found\n\n    def execute_plan(self):\n        \"\"\"Execute plan step by step.\"\"\"\n        if not self.plan:\n            return \"Already at goal or no plan\"\n\n        steps = []\n        for next_pos in self.plan:\n            steps.append(f\"Move from {self.position} to {next_pos}\")\n            self.position = next_pos\n\n        return steps\n\n    def replan_if_needed(self, blocked_nodes):\n        \"\"\"Replan if current plan goes through blocked nodes.\"\"\"\n        if any(node in blocked_nodes for node in self.plan):\n            # Remove blocked nodes from map temporarily\n            temp_map = {k: [v for v in vals if v not in blocked_nodes]\n                       for k, vals in self.world_map.items()}\n\n            old_map = self.world_map\n            self.world_map = temp_map\n\n            result = self.make_plan()\n\n            self.world_map = old_map\n            return result is not None\n\n        return True\n\n# Test\nworld_map = {\n    'A': ['B', 'C'],\n    'B': ['D', 'E'],\n    'C': ['F'],\n    'D': ['G'],\n    'E': ['G'],\n    'F': ['G'],\n    'G': []\n}\n\nagent = RouteAgent(world_map, 'A', 'G')\nprint(\"Initial position:\", agent.position)\nprint(\"Goal:\", agent.goal)\n\nplan = agent.make_plan()\nprint(\"Plan:\", plan)\n\nsteps = agent.execute_plan()\nprint(\"Execution:\")\nfor step in steps:\n    print(f\"  {step}\")\n\nprint(\"Final position:\", agent.position)",
    "testCases": [
      {
        "input": "agent.make_plan()",
        "isHidden": false,
        "description": "Test agent creates valid plan to goal"
      },
      {
        "input": "agent.execute_plan()",
        "isHidden": false,
        "description": "Test plan execution reaches goal"
      },
      {
        "input": "agent.replan_if_needed(blocked_nodes)",
        "isHidden": false,
        "description": "Test agent replans when obstacles appear"
      }
    ],
    "hints": [
      "Use BFS to find the shortest path from current position to goal",
      "Store the plan as a sequence of positions to visit",
      "Execute the plan by moving through each position in order"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex04",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Utility-Based Agent for Resource Allocation",
    "difficulty": 3,
    "description": "Implement a utility-based agent that allocates resources to maximize utility.\n\nYour implementation should:\n- Define utility function for different outcomes\n- Evaluate expected utility of actions\n- Choose action that maximizes expected utility\n- Handle uncertain outcomes",
    "starterCode": "class ResourceAgent:\n    def __init__(self, resources):\n        self.resources = resources\n\n    def utility(self, allocation):\n        # Calculate utility of resource allocation\n        pass\n\n    def choose_action(self, possible_actions):\n        # Choose action with maximum expected utility\n        pass\n\n# Example: Allocate budget between projects\n# Each project has cost and uncertain return",
    "solution": "import random\n\nclass ResourceAgent:\n    def __init__(self, resources):\n        self.resources = resources\n\n    def utility(self, allocation):\n        \"\"\"\n        Calculate utility of resource allocation.\n        allocation: dict mapping project -> amount\n\n        Utility increases with allocation but with diminishing returns.\n        \"\"\"\n        total_utility = 0\n\n        for project, amount in allocation.items():\n            # Diminishing returns: sqrt(amount) * project_value\n            project_value = {\n                'A': 100,\n                'B': 150,\n                'C': 80\n            }.get(project, 50)\n\n            total_utility += (amount ** 0.5) * project_value\n\n        return total_utility\n\n    def expected_utility(self, action, outcomes, probabilities):\n        \"\"\"\n        Calculate expected utility of action.\n        action: resource allocation\n        outcomes: list of possible results\n        probabilities: probability of each outcome\n        \"\"\"\n        expected = 0\n        for outcome, prob in zip(outcomes, probabilities):\n            # Apply outcome multiplier to allocation\n            adjusted = {k: v * outcome for k, v in action.items()}\n            expected += prob * self.utility(adjusted)\n\n        return expected\n\n    def choose_action(self, possible_actions, outcomes=None, probabilities=None):\n        \"\"\"Choose action with maximum expected utility.\"\"\"\n        if outcomes is None:\n            # Deterministic case\n            best_action = None\n            best_utility = float('-inf')\n\n            for action in possible_actions:\n                # Check if allocation is valid (within resources)\n                if sum(action.values()) <= self.resources:\n                    util = self.utility(action)\n                    if util > best_utility:\n                        best_utility = util\n                        best_action = action\n\n            return best_action, best_utility\n        else:\n            # Stochastic case\n            best_action = None\n            best_expected = float('-inf')\n\n            for action in possible_actions:\n                if sum(action.values()) <= self.resources:\n                    expected = self.expected_utility(action, outcomes, probabilities)\n                    if expected > best_expected:\n                        best_expected = expected\n                        best_action = action\n\n            return best_action, best_expected\n\n# Test\nagent = ResourceAgent(resources=100)\n\n# Possible allocations\npossible_actions = [\n    {'A': 50, 'B': 50, 'C': 0},\n    {'A': 60, 'B': 20, 'C': 20},\n    {'A': 30, 'B': 30, 'C': 40},\n    {'A': 0, 'B': 100, 'C': 0},\n    {'A': 40, 'B': 40, 'C': 20}\n]\n\nprint(\"Resource Allocation Agent\")\nprint(f\"Total resources: {agent.resources}\")\nprint()\n\n# Deterministic case\nprint(\"Deterministic utility:\")\nbest_action, best_util = agent.choose_action(possible_actions)\nprint(f\"Best allocation: {best_action}\")\nprint(f\"Utility: {best_util:.2f}\")\nprint()\n\n# Stochastic case (uncertain outcomes)\nprint(\"Stochastic case (uncertain project success):\")\noutcomes = [0.5, 1.0, 1.5]  # Project could underperform, meet, or exceed expectations\nprobabilities = [0.2, 0.5, 0.3]\n\nbest_action, best_expected = agent.choose_action(\n    possible_actions,\n    outcomes,\n    probabilities\n)\nprint(f\"Best allocation: {best_action}\")\nprint(f\"Expected utility: {best_expected:.2f}\")",
    "testCases": [
      {
        "input": "agent.utility(allocation)",
        "isHidden": false,
        "description": "Test utility calculation for allocation"
      },
      {
        "input": "agent.choose_action(possible_actions)",
        "isHidden": false,
        "description": "Test agent chooses action maximizing utility"
      },
      {
        "input": "agent.expected_utility(action, outcomes, probabilities)",
        "isHidden": false,
        "description": "Test expected utility with uncertainty"
      }
    ],
    "hints": [
      "Utility function should capture preferences (e.g., diminishing returns)",
      "Expected utility = sum of (probability × utility) over all possible outcomes",
      "Choose the action with highest expected utility among valid options"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex05",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Learning Agent with Experience",
    "difficulty": 3,
    "description": "Implement a learning agent that improves from experience.\n\nYour implementation should:\n- Maintain performance history\n- Learn from successes and failures\n- Update internal model based on experience\n- Improve decision-making over time",
    "starterCode": "class LearningAgent:\n    def __init__(self):\n        self.experience = []\n        self.model = {}\n\n    def act(self, state):\n        # Choose action based on current model\n        pass\n\n    def learn(self, state, action, reward):\n        # Update model based on experience\n        pass\n\n    def performance(self):\n        # Return average reward\n        pass",
    "solution": "import random\n\nclass LearningAgent:\n    def __init__(self, actions, learning_rate=0.1, epsilon=0.1):\n        self.actions = actions\n        self.experience = []\n\n        # Q-values: (state, action) -> expected reward\n        self.q_values = {}\n\n        self.learning_rate = learning_rate\n        self.epsilon = epsilon  # Exploration rate\n\n    def get_q_value(self, state, action):\n        \"\"\"Get Q-value for state-action pair.\"\"\"\n        return self.q_values.get((state, action), 0.0)\n\n    def act(self, state):\n        \"\"\"Choose action using epsilon-greedy policy.\"\"\"\n        # Exploration: random action\n        if random.random() < self.epsilon:\n            return random.choice(self.actions)\n\n        # Exploitation: best known action\n        best_action = None\n        best_value = float('-inf')\n\n        for action in self.actions:\n            q_val = self.get_q_value(state, action)\n            if q_val > best_value:\n                best_value = q_val\n                best_action = action\n\n        # If all Q-values are 0, choose randomly\n        if best_action is None:\n            best_action = random.choice(self.actions)\n\n        return best_action\n\n    def learn(self, state, action, reward, next_state=None):\n        \"\"\"Update Q-value based on experience.\"\"\"\n        self.experience.append((state, action, reward))\n\n        # Current Q-value\n        current_q = self.get_q_value(state, action)\n\n        # Simple update: moving average toward observed reward\n        if next_state is None:\n            # Terminal state\n            target = reward\n        else:\n            # Q-learning update: reward + best future Q-value\n            best_next_q = max(\n                (self.get_q_value(next_state, a) for a in self.actions),\n                default=0.0\n            )\n            target = reward + best_next_q\n\n        # Update Q-value\n        new_q = current_q + self.learning_rate * (target - current_q)\n        self.q_values[(state, action)] = new_q\n\n    def performance(self):\n        \"\"\"Return average reward over all experience.\"\"\"\n        if not self.experience:\n            return 0.0\n\n        total_reward = sum(reward for _, _, reward in self.experience)\n        return total_reward / len(self.experience)\n\n    def get_policy(self):\n        \"\"\"Extract learned policy.\"\"\"\n        policy = {}\n        states = set(state for (state, _), _ in self.q_values.items())\n\n        for state in states:\n            best_action = None\n            best_value = float('-inf')\n\n            for action in self.actions:\n                q_val = self.get_q_value(state, action)\n                if q_val > best_value:\n                    best_value = q_val\n                    best_action = action\n\n            policy[state] = best_action\n\n        return policy\n\n# Test with simple grid world\nactions = ['up', 'down', 'left', 'right']\nagent = LearningAgent(actions, learning_rate=0.1, epsilon=0.2)\n\nprint(\"Learning Agent Simulation\")\nprint(\"=\"*50)\n\n# Simulate episodes\n# State: (x, y), Goal: (5, 5), Reward: -1 per step, +10 at goal\n\ndef get_next_state(state, action):\n    \"\"\"Get next state given action.\"\"\"\n    x, y = state\n    if action == 'up':\n        return (x, min(y + 1, 5))\n    elif action == 'down':\n        return (x, max(y - 1, 0))\n    elif action == 'left':\n        return (max(x - 1, 0), y)\n    elif action == 'right':\n        return (min(x + 1, 5), y)\n    return state\n\ndef get_reward(state):\n    \"\"\"Get reward for reaching state.\"\"\"\n    if state == (5, 5):\n        return 10.0\n    return -1.0\n\n# Train for multiple episodes\nnum_episodes = 100\nfor episode in range(num_episodes):\n    state = (0, 0)\n    steps = 0\n    max_steps = 50\n\n    while state != (5, 5) and steps < max_steps:\n        action = agent.act(state)\n        next_state = get_next_state(state, action)\n        reward = get_reward(next_state)\n\n        agent.learn(state, action, reward, next_state)\n\n        state = next_state\n        steps += 1\n\n    if (episode + 1) % 20 == 0:\n        print(f\"Episode {episode + 1}: Avg reward = {agent.performance():.2f}\")\n\nprint()\nprint(f\"Final average reward: {agent.performance():.2f}\")\nprint(f\"Total experiences: {len(agent.experience)}\")\n\n# Show learned policy for some states\nprint(\"\\nLearned policy (sample states):\")\npolicy = agent.get_policy()\nfor state in [(0, 0), (2, 2), (4, 4), (5, 4)]:\n    if state in policy:\n        print(f\"  State {state}: {policy[state]}\")",
    "testCases": [
      {
        "input": "agent.act(state)",
        "isHidden": false,
        "description": "Test agent chooses actions (exploration vs exploitation)"
      },
      {
        "input": "agent.learn(state, action, reward)",
        "isHidden": false,
        "description": "Test learning updates Q-values"
      },
      {
        "input": "agent.performance()",
        "isHidden": false,
        "description": "Test performance improves over time"
      }
    ],
    "hints": [
      "Use Q-learning to learn value of state-action pairs",
      "Epsilon-greedy policy balances exploration (random) and exploitation (best known)",
      "Update Q-values using: Q(s,a) ← Q(s,a) + α[r + max Q(s',a') - Q(s,a)]"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex06",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Multi-Agent System - Collaborative Agents",
    "difficulty": 3,
    "description": "Implement multiple agents that collaborate to achieve a shared goal.\n\nYour implementation should:\n- Coordinate multiple agents\n- Share information between agents\n- Divide tasks among agents\n- Achieve goals faster through collaboration",
    "starterCode": "class CollaborativeAgent:\n    def __init__(self, agent_id, shared_knowledge):\n        self.id = agent_id\n        self.shared_knowledge = shared_knowledge\n\n    def perceive(self, environment):\n        pass\n\n    def communicate(self, message):\n        pass\n\n    def act(self):\n        pass",
    "solution": "class SharedKnowledge:\n    \"\"\"Shared knowledge base for collaborative agents.\"\"\"\n    def __init__(self):\n        self.discovered_items = set()\n        self.agent_positions = {}\n        self.assigned_tasks = {}\n\n    def add_discovery(self, item, location):\n        \"\"\"Add newly discovered item.\"\"\"\n        self.discovered_items.add((item, location))\n\n    def update_position(self, agent_id, position):\n        \"\"\"Update agent position.\"\"\"\n        self.agent_positions[agent_id] = position\n\n    def assign_task(self, agent_id, task):\n        \"\"\"Assign task to agent.\"\"\"\n        self.assigned_tasks[agent_id] = task\n\n    def get_unassigned_items(self):\n        \"\"\"Get items not yet assigned to any agent.\"\"\"\n        assigned = set(task for task in self.assigned_tasks.values() if task)\n        return [item for item in self.discovered_items if item not in assigned]\n\nclass CollaborativeAgent:\n    def __init__(self, agent_id, position, shared_knowledge):\n        self.id = agent_id\n        self.position = position\n        self.shared_knowledge = shared_knowledge\n        self.holding = None\n        self.target = None\n\n    def perceive(self, environment):\n        \"\"\"Perceive environment and share discoveries.\"\"\"\n        # Check current location for items\n        if self.position in environment:\n            item = environment[self.position]\n            self.shared_knowledge.add_discovery(item, self.position)\n\n        # Update own position\n        self.shared_knowledge.update_position(self.id, self.position)\n\n    def communicate(self, message):\n        \"\"\"Process message from another agent.\"\"\"\n        if message['type'] == 'request_help':\n            return self.offer_help(message)\n        elif message['type'] == 'task_complete':\n            return self.acknowledge_completion(message)\n\n    def select_task(self):\n        \"\"\"Select next task based on shared knowledge.\"\"\"\n        # Check if already have a task\n        if self.target:\n            return self.target\n\n        # Get unassigned items\n        unassigned = self.shared_knowledge.get_unassigned_items()\n\n        if not unassigned:\n            return None\n\n        # Choose nearest unassigned item\n        best_item = None\n        best_distance = float('inf')\n\n        for item, location in unassigned:\n            distance = abs(location[0] - self.position[0]) + abs(location[1] - self.position[1])\n            if distance < best_distance:\n                best_distance = distance\n                best_item = (item, location)\n\n        if best_item:\n            self.shared_knowledge.assign_task(self.id, best_item)\n            self.target = best_item\n\n        return self.target\n\n    def act(self):\n        \"\"\"Execute action toward current goal.\"\"\"\n        if not self.target:\n            self.select_task()\n\n        if not self.target:\n            return f\"Agent {self.id}: No tasks available\"\n\n        item, location = self.target\n\n        # Move toward target\n        if self.position != location:\n            # Simple movement (Manhattan)\n            x, y = self.position\n            tx, ty = location\n\n            if x < tx:\n                self.position = (x + 1, y)\n            elif x > tx:\n                self.position = (x - 1, y)\n            elif y < ty:\n                self.position = (x, y + 1)\n            elif y > ty:\n                self.position = (x, y - 1)\n\n            self.shared_knowledge.update_position(self.id, self.position)\n            return f\"Agent {self.id}: Moving to {location}\"\n        else:\n            # Reached target, pick up item\n            self.holding = item\n            self.target = None\n            self.shared_knowledge.assign_task(self.id, None)\n            return f\"Agent {self.id}: Collected {item}\"\n\n    def offer_help(self, message):\n        \"\"\"Offer to help another agent.\"\"\"\n        return {\n            'agent_id': self.id,\n            'available': self.target is None,\n            'position': self.position\n        }\n\n    def acknowledge_completion(self, message):\n        \"\"\"Acknowledge task completion.\"\"\"\n        return f\"Agent {self.id}: Acknowledged completion by Agent {message['agent_id']}\"\n\n# Test multi-agent collaboration\nprint(\"Multi-Agent Collaboration Simulation\")\nprint(\"=\"*50)\n\n# Environment: items at locations\nenvironment = {\n    (2, 3): 'ItemA',\n    (5, 1): 'ItemB',\n    (1, 5): 'ItemC',\n    (7, 7): 'ItemD',\n    (3, 8): 'ItemE'\n}\n\n# Shared knowledge\nshared = SharedKnowledge()\n\n# Create agents at different positions\nagents = [\n    CollaborativeAgent('Agent1', (0, 0), shared),\n    CollaborativeAgent('Agent2', (5, 5), shared),\n    CollaborativeAgent('Agent3', (8, 0), shared)\n]\n\nprint(f\"Environment has {len(environment)} items to collect\")\nprint(f\"Number of agents: {len(agents)}\")\nprint()\n\n# Simulate\ncollected = 0\nmax_steps = 50\n\nfor step in range(max_steps):\n    print(f\"Step {step + 1}:\")\n\n    # All agents perceive\n    for agent in agents:\n        agent.perceive(environment)\n\n    # All agents act\n    for agent in agents:\n        action = agent.act()\n        print(f\"  {action}\")\n\n        if \"Collected\" in action:\n            collected += 1\n\n    # Check if all items collected\n    if collected >= len(environment):\n        print(f\"\\nAll items collected in {step + 1} steps!\")\n        break\n\n    print()\n\nprint(f\"\\nFinal state:\")\nprint(f\"Items collected: {collected}/{len(environment)}\")\nfor agent in agents:\n    print(f\"  {agent.id} at {agent.position}, holding: {agent.holding}\")",
    "testCases": [
      {
        "input": "agent.perceive(environment)",
        "isHidden": false,
        "description": "Test agent perceives and shares information"
      },
      {
        "input": "agent.select_task()",
        "isHidden": false,
        "description": "Test agents coordinate to avoid duplicate work"
      },
      {
        "input": "agent.act()",
        "isHidden": false,
        "description": "Test agents work together to complete all tasks"
      }
    ],
    "hints": [
      "Use shared knowledge structure for communication between agents",
      "Agents should claim tasks to avoid duplication",
      "Each agent selects nearest unassigned task for efficiency"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex07",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Reactive Agent with Subsumption Architecture",
    "difficulty": 2,
    "description": "Implement a reactive agent using subsumption architecture with layered behaviors.\n\nYour implementation should:\n- Define multiple behavior layers\n- Implement priority-based arbitration\n- Higher layers can override lower layers\n- React quickly to environment changes",
    "starterCode": "class Behavior:\n    def __init__(self, name, priority):\n        self.name = name\n        self.priority = priority\n\n    def is_active(self, percepts):\n        # Check if behavior should activate\n        pass\n\n    def action(self, percepts):\n        # Return action to execute\n        pass\n\nclass SubsumptionAgent:\n    def __init__(self):\n        self.behaviors = []\n\n    def add_behavior(self, behavior):\n        pass\n\n    def select_action(self, percepts):\n        pass",
    "solution": "class Behavior:\n    \"\"\"Base class for behaviors in subsumption architecture.\"\"\"\n    def __init__(self, name, priority):\n        self.name = name\n        self.priority = priority\n\n    def is_active(self, percepts):\n        \"\"\"Check if behavior conditions are met.\"\"\"\n        raise NotImplementedError\n\n    def action(self, percepts):\n        \"\"\"Return action to execute.\"\"\"\n        raise NotImplementedError\n\nclass AvoidObstacle(Behavior):\n    \"\"\"High priority: avoid obstacles.\"\"\"\n    def __init__(self):\n        super().__init__(\"AvoidObstacle\", priority=3)\n\n    def is_active(self, percepts):\n        return percepts.get('obstacle_ahead', False)\n\n    def action(self, percepts):\n        # Turn away from obstacle\n        if percepts.get('obstacle_left', False):\n            return 'turn_right'\n        else:\n            return 'turn_left'\n\nclass MoveToGoal(Behavior):\n    \"\"\"Medium priority: move toward goal.\"\"\"\n    def __init__(self):\n        super().__init__(\"MoveToGoal\", priority=2)\n\n    def is_active(self, percepts):\n        return percepts.get('goal_detected', False)\n\n    def action(self, percepts):\n        goal_dir = percepts.get('goal_direction')\n        if goal_dir == 'left':\n            return 'turn_left'\n        elif goal_dir == 'right':\n            return 'turn_right'\n        else:\n            return 'move_forward'\n\nclass Wander(Behavior):\n    \"\"\"Low priority: random exploration.\"\"\"\n    def __init__(self):\n        super().__init__(\"Wander\", priority=1)\n\n    def is_active(self, percepts):\n        return True  # Always active as fallback\n\n    def action(self, percepts):\n        import random\n        return random.choice(['move_forward', 'turn_left', 'turn_right'])\n\nclass SubsumptionAgent:\n    \"\"\"Agent using subsumption architecture.\"\"\"\n    def __init__(self):\n        self.behaviors = []\n\n    def add_behavior(self, behavior):\n        \"\"\"Add behavior to agent.\"\"\"\n        self.behaviors.append(behavior)\n        # Sort by priority (highest first)\n        self.behaviors.sort(key=lambda b: b.priority, reverse=True)\n\n    def select_action(self, percepts):\n        \"\"\"Select action from highest priority active behavior.\"\"\"\n        for behavior in self.behaviors:\n            if behavior.is_active(percepts):\n                action = behavior.action(percepts)\n                return action, behavior.name\n\n        return 'stop', 'None'\n\n# Test subsumption agent\nprint(\"Subsumption Architecture Agent\")\nprint(\"=\"*50)\n\n# Create agent\nagent = SubsumptionAgent()\n\n# Add behaviors (order doesn't matter, sorted by priority)\nagent.add_behavior(Wander())\nagent.add_behavior(MoveToGoal())\nagent.add_behavior(AvoidObstacle())\n\nprint(\"Behaviors (by priority):\")\nfor b in agent.behaviors:\n    print(f\"  {b.name} (priority {b.priority})\")\nprint()\n\n# Test scenarios\nscenarios = [\n    {\n        'name': 'Obstacle ahead',\n        'percepts': {'obstacle_ahead': True, 'obstacle_left': False, 'goal_detected': True, 'goal_direction': 'forward'}\n    },\n    {\n        'name': 'Goal visible, no obstacles',\n        'percepts': {'obstacle_ahead': False, 'goal_detected': True, 'goal_direction': 'left'}\n    },\n    {\n        'name': 'No goal, no obstacles',\n        'percepts': {'obstacle_ahead': False, 'goal_detected': False}\n    },\n    {\n        'name': 'Goal right, obstacle ahead',\n        'percepts': {'obstacle_ahead': True, 'obstacle_left': True, 'goal_detected': True, 'goal_direction': 'right'}\n    }\n]\n\nfor scenario in scenarios:\n    action, behavior = agent.select_action(scenario['percepts'])\n    print(f\"Scenario: {scenario['name']}\")\n    print(f\"  Active behavior: {behavior}\")\n    print(f\"  Action: {action}\")\n    print()\n\n# Simulation\nprint(\"=\"*50)\nprint(\"Simulation\")\nprint(\"=\"*50)\n\nposition = (0, 0)\ngoal = (5, 5)\nobstacles = [(2, 2), (3, 2), (4, 3)]\n\nfor step in range(15):\n    # Create percepts based on position\n    percepts = {}\n\n    # Check obstacle ahead (simplified)\n    x, y = position\n    obstacle_positions = [\n        (x+1, y), (x, y+1), (x-1, y), (x, y-1)\n    ]\n    percepts['obstacle_ahead'] = any(pos in obstacles for pos in obstacle_positions)\n    percepts['obstacle_left'] = (x-1, y) in obstacles\n\n    # Check goal\n    percepts['goal_detected'] = True\n    gx, gy = goal\n    if gx > x:\n        percepts['goal_direction'] = 'right'\n    elif gx < x:\n        percepts['goal_direction'] = 'left'\n    elif gy > y:\n        percepts['goal_direction'] = 'forward'\n    else:\n        percepts['goal_direction'] = 'forward'\n\n    # Select action\n    action, behavior = agent.select_action(percepts)\n\n    print(f\"Step {step+1}: Position {position}, Behavior: {behavior}, Action: {action}\")\n\n    # Update position (simplified)\n    if action == 'move_forward' and not percepts['obstacle_ahead']:\n        if position[1] < goal[1]:\n            position = (position[0], position[1] + 1)\n        elif position[0] < goal[0]:\n            position = (position[0] + 1, position[1])\n    elif action == 'turn_right':\n        if position[0] < goal[0]:\n            position = (position[0] + 1, position[1])\n    elif action == 'turn_left':\n        if position[1] < goal[1]:\n            position = (position[0], position[1] + 1)\n\n    if position == goal:\n        print(f\"\\nGoal reached at step {step+1}!\")\n        break",
    "testCases": [
      {
        "input": "agent.select_action(percepts)",
        "isHidden": false,
        "description": "Test agent selects highest priority active behavior"
      },
      {
        "input": "behavior.is_active(percepts)",
        "isHidden": false,
        "description": "Test behavior activation conditions"
      },
      {
        "input": "agent with obstacle",
        "isHidden": false,
        "description": "Test avoid behavior overrides move-to-goal"
      }
    ],
    "hints": [
      "Subsumption: higher priority behaviors suppress lower ones",
      "Check behaviors in priority order, use first active one",
      "Avoid behavior should have highest priority for safety"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex08",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Agent Environment Interaction Loop",
    "difficulty": 2,
    "description": "Implement a complete agent-environment interaction simulation.\n\nYour implementation should:\n- Define environment with states and transitions\n- Implement agent with perception and action\n- Run interaction loop\n- Track performance metrics",
    "starterCode": "class Environment:\n    def __init__(self, initial_state):\n        self.state = initial_state\n\n    def get_percepts(self):\n        pass\n\n    def apply_action(self, action):\n        pass\n\nclass Agent:\n    def perceive(self, percepts):\n        pass\n\n    def decide(self):\n        pass\n\ndef run_simulation(env, agent, steps):\n    pass",
    "solution": "import random\n\nclass GridEnvironment:\n    \"\"\"Simple grid world environment.\"\"\"\n    def __init__(self, size, goal, obstacles):\n        self.size = size\n        self.goal = goal\n        self.obstacles = obstacles\n        self.agent_pos = (0, 0)\n        self.steps = 0\n        self.done = False\n\n    def get_percepts(self):\n        \"\"\"Return percepts for agent.\"\"\"\n        return {\n            'position': self.agent_pos,\n            'goal': self.goal,\n            'obstacles': self.obstacles,\n            'at_goal': self.agent_pos == self.goal,\n            'steps': self.steps\n        }\n\n    def apply_action(self, action):\n        \"\"\"Apply action and return reward.\"\"\"\n        if self.done:\n            return 0\n\n        x, y = self.agent_pos\n\n        # Apply movement\n        if action == 'up':\n            new_pos = (x, min(y + 1, self.size - 1))\n        elif action == 'down':\n            new_pos = (x, max(y - 1, 0))\n        elif action == 'left':\n            new_pos = (max(x - 1, 0), y)\n        elif action == 'right':\n            new_pos = (min(x + 1, self.size - 1), y)\n        else:\n            new_pos = self.agent_pos\n\n        # Check if valid (not obstacle)\n        if new_pos not in self.obstacles:\n            self.agent_pos = new_pos\n\n        self.steps += 1\n\n        # Calculate reward\n        if self.agent_pos == self.goal:\n            self.done = True\n            return 10  # Goal reached\n        elif self.agent_pos in self.obstacles:\n            return -5  # Hit obstacle\n        else:\n            return -1  # Step cost\n\nclass SimpleAgent:\n    \"\"\"Simple goal-directed agent.\"\"\"\n    def __init__(self):\n        self.percepts = None\n\n    def perceive(self, percepts):\n        \"\"\"Store percepts.\"\"\"\n        self.percepts = percepts\n\n    def decide(self):\n        \"\"\"Decide action based on percepts.\"\"\"\n        if self.percepts is None:\n            return 'stop'\n\n        if self.percepts['at_goal']:\n            return 'stop'\n\n        # Simple strategy: move toward goal\n        pos = self.percepts['position']\n        goal = self.percepts['goal']\n        obstacles = self.percepts['obstacles']\n\n        x, y = pos\n        gx, gy = goal\n\n        # Try to move toward goal\n        if gx > x:\n            if (x + 1, y) not in obstacles:\n                return 'right'\n        elif gx < x:\n            if (x - 1, y) not in obstacles:\n                return 'left'\n\n        if gy > y:\n            if (x, y + 1) not in obstacles:\n                return 'up'\n        elif gy < y:\n            if (x, y - 1) not in obstacles:\n                return 'down'\n\n        # If blocked, try random move\n        return random.choice(['up', 'down', 'left', 'right'])\n\ndef run_simulation(env, agent, max_steps=100):\n    \"\"\"Run agent-environment interaction loop.\"\"\"\n    total_reward = 0\n    history = []\n\n    for step in range(max_steps):\n        # Get percepts\n        percepts = env.get_percepts()\n\n        # Agent perceives\n        agent.perceive(percepts)\n\n        # Agent decides\n        action = agent.decide()\n\n        # Apply action to environment\n        reward = env.apply_action(action)\n        total_reward += reward\n\n        # Record history\n        history.append({\n            'step': step,\n            'position': percepts['position'],\n            'action': action,\n            'reward': reward\n        })\n\n        # Check if done\n        if env.done:\n            print(f\"Goal reached in {step + 1} steps!\")\n            break\n\n    return {\n        'total_reward': total_reward,\n        'steps': env.steps,\n        'success': env.done,\n        'history': history\n    }\n\n# Test simulation\nprint(\"Agent-Environment Simulation\")\nprint(\"=\"*50)\n\n# Create environment\nenv = GridEnvironment(\n    size=6,\n    goal=(5, 5),\n    obstacles=[(2, 2), (2, 3), (3, 2)]\n)\n\n# Create agent\nagent = SimpleAgent()\n\nprint(f\"Environment size: {env.size}x{env.size}\")\nprint(f\"Goal: {env.goal}\")\nprint(f\"Obstacles: {env.obstacles}\")\nprint(f\"Starting position: {env.agent_pos}\")\nprint()\n\n# Run simulation\nresults = run_simulation(env, agent, max_steps=100)\n\nprint()\nprint(\"Results:\")\nprint(f\"  Success: {results['success']}\")\nprint(f\"  Total steps: {results['steps']}\")\nprint(f\"  Total reward: {results['total_reward']}\")\nprint()\n\nprint(\"Path taken:\")\nfor i, entry in enumerate(results['history'][:20]):  # Show first 20 steps\n    print(f\"  Step {entry['step'] + 1}: {entry['position']} -> {entry['action']} (reward: {entry['reward']})\")\n\nif len(results['history']) > 20:\n    print(f\"  ... ({len(results['history']) - 20} more steps)\")",
    "testCases": [
      {
        "input": "env.get_percepts()",
        "isHidden": false,
        "description": "Test environment provides percepts"
      },
      {
        "input": "env.apply_action(action)",
        "isHidden": false,
        "description": "Test environment updates based on action"
      },
      {
        "input": "run_simulation(env, agent, steps)",
        "isHidden": false,
        "description": "Test complete simulation runs correctly"
      }
    ],
    "hints": [
      "Perception-action loop: percepts -> decision -> action -> update environment",
      "Environment maintains state and returns percepts to agent",
      "Agent chooses actions based on percepts, environment applies them"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex09",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Performance Measure for Agent Evaluation",
    "difficulty": 2,
    "description": "Implement performance measures to evaluate agent effectiveness.\n\nYour implementation should:\n- Define multiple performance metrics\n- Track agent performance over time\n- Compare different agent strategies\n- Visualize performance data",
    "starterCode": "class PerformanceMeasure:\n    def __init__(self):\n        pass\n\n    def record(self, state, action, reward):\n        pass\n\n    def evaluate(self):\n        pass\n\n    def compare_agents(self, agent_results):\n        pass",
    "solution": "class PerformanceMeasure:\n    \"\"\"Track and evaluate agent performance.\"\"\"\n    def __init__(self, metrics=None):\n        if metrics is None:\n            metrics = ['total_reward', 'steps', 'success_rate']\n\n        self.metrics = metrics\n        self.history = []\n        self.episodes = []\n        self.current_episode = {\n            'rewards': [],\n            'actions': [],\n            'steps': 0,\n            'success': False\n        }\n\n    def record(self, state, action, reward):\n        \"\"\"Record single step.\"\"\"\n        self.history.append({\n            'state': state,\n            'action': action,\n            'reward': reward\n        })\n\n        self.current_episode['rewards'].append(reward)\n        self.current_episode['actions'].append(action)\n        self.current_episode['steps'] += 1\n\n    def end_episode(self, success=False):\n        \"\"\"Mark episode as complete.\"\"\"\n        self.current_episode['success'] = success\n        self.current_episode['total_reward'] = sum(self.current_episode['rewards'])\n        self.episodes.append(self.current_episode.copy())\n\n        # Reset for next episode\n        self.current_episode = {\n            'rewards': [],\n            'actions': [],\n            'steps': 0,\n            'success': False\n        }\n\n    def evaluate(self):\n        \"\"\"Calculate performance metrics.\"\"\"\n        if not self.episodes:\n            return {}\n\n        metrics = {}\n\n        # Total reward\n        metrics['total_reward'] = sum(ep['total_reward'] for ep in self.episodes)\n        metrics['avg_reward_per_episode'] = metrics['total_reward'] / len(self.episodes)\n\n        # Steps\n        metrics['total_steps'] = sum(ep['steps'] for ep in self.episodes)\n        metrics['avg_steps_per_episode'] = metrics['total_steps'] / len(self.episodes)\n\n        # Success rate\n        successes = sum(1 for ep in self.episodes if ep['success'])\n        metrics['success_rate'] = successes / len(self.episodes)\n        metrics['num_episodes'] = len(self.episodes)\n        metrics['num_successes'] = successes\n\n        # Efficiency (reward per step)\n        metrics['efficiency'] = metrics['total_reward'] / max(metrics['total_steps'], 1)\n\n        return metrics\n\n    def compare_agents(self, agent_results):\n        \"\"\"Compare multiple agents.\"\"\"\n        comparison = {}\n\n        for agent_name, results in agent_results.items():\n            comparison[agent_name] = results.evaluate()\n\n        return comparison\n\n    def print_summary(self):\n        \"\"\"Print performance summary.\"\"\"\n        metrics = self.evaluate()\n\n        print(\"Performance Summary\")\n        print(\"=\"*50)\n        print(f\"Episodes: {metrics.get('num_episodes', 0)}\")\n        print(f\"Success rate: {metrics.get('success_rate', 0):.1%}\")\n        print(f\"Avg reward per episode: {metrics.get('avg_reward_per_episode', 0):.2f}\")\n        print(f\"Avg steps per episode: {metrics.get('avg_steps_per_episode', 0):.1f}\")\n        print(f\"Efficiency (reward/step): {metrics.get('efficiency', 0):.3f}\")\n\n    def plot_learning_curve(self):\n        \"\"\"Plot learning curve (rewards over episodes).\"\"\"\n        if not self.episodes:\n            print(\"No episodes to plot\")\n            return\n\n        episode_numbers = list(range(1, len(self.episodes) + 1))\n        rewards = [ep['total_reward'] for ep in self.episodes]\n\n        # Simple text-based plot\n        print(\"\\nLearning Curve (Total Reward per Episode)\")\n        print(\"=\"*50)\n\n        max_reward = max(rewards) if rewards else 1\n        min_reward = min(rewards) if rewards else 0\n\n        for i, (ep_num, reward) in enumerate(zip(episode_numbers[:20], rewards[:20])):\n            # Normalize to 0-40 characters\n            if max_reward != min_reward:\n                bar_len = int(40 * (reward - min_reward) / (max_reward - min_reward))\n            else:\n                bar_len = 20\n\n            bar = '#' * bar_len\n            print(f\"Ep {ep_num:3d}: {bar} ({reward:.1f})\")\n\n        if len(episode_numbers) > 20:\n            print(f\"... ({len(episode_numbers) - 20} more episodes)\")\n\n# Test performance measurement\nprint(\"Performance Measurement Example\")\nprint(\"=\"*50)\n\n# Simulate two agents with different strategies\npm_agent1 = PerformanceMeasure()\npm_agent2 = PerformanceMeasure()\n\n# Agent 1: Conservative (slower but steady)\nprint(\"Agent 1: Conservative strategy\")\nfor episode in range(10):\n    for step in range(15):\n        reward = -1  # Step cost\n        pm_agent1.record(None, 'move', reward)\n\n    # Reaches goal\n    pm_agent1.record(None, 'reach_goal', 10)\n    pm_agent1.end_episode(success=True)\n\nprint()\npm_agent1.print_summary()\n\n# Agent 2: Aggressive (faster but riskier)\nprint(\"\\nAgent 2: Aggressive strategy\")\nfor episode in range(10):\n    # Sometimes fails (hits obstacle)\n    if episode % 3 == 0:\n        for step in range(5):\n            pm_agent2.record(None, 'move', -1)\n        pm_agent2.record(None, 'hit_obstacle', -5)\n        pm_agent2.end_episode(success=False)\n    else:\n        # Reaches goal quickly\n        for step in range(8):\n            pm_agent2.record(None, 'move', -1)\n        pm_agent2.record(None, 'reach_goal', 10)\n        pm_agent2.end_episode(success=True)\n\nprint()\npm_agent2.print_summary()\n\n# Compare agents\nprint(\"\\n\" + \"=\"*50)\nprint(\"Agent Comparison\")\nprint(\"=\"*50)\n\ncomparison = pm_agent1.compare_agents({\n    'Conservative': pm_agent1,\n    'Aggressive': pm_agent2\n})\n\nfor agent_name, metrics in comparison.items():\n    print(f\"\\n{agent_name}:\")\n    print(f\"  Success rate: {metrics['success_rate']:.1%}\")\n    print(f\"  Avg reward: {metrics['avg_reward_per_episode']:.2f}\")\n    print(f\"  Avg steps: {metrics['avg_steps_per_episode']:.1f}\")\n    print(f\"  Efficiency: {metrics['efficiency']:.3f}\")\n\n# Show learning curve\npm_agent1.plot_learning_curve()",
    "testCases": [
      {
        "input": "pm.record(state, action, reward)",
        "isHidden": false,
        "description": "Test recording agent steps"
      },
      {
        "input": "pm.evaluate()",
        "isHidden": false,
        "description": "Test computing performance metrics"
      },
      {
        "input": "pm.compare_agents(agent_results)",
        "isHidden": false,
        "description": "Test comparing multiple agents"
      }
    ],
    "hints": [
      "Track multiple metrics: total reward, success rate, efficiency",
      "Record episodes separately to measure learning over time",
      "Compare agents using normalized metrics like reward per step"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex10",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Rational Agent Decision Making",
    "difficulty": 2,
    "description": "Implement a rational agent that selects actions to maximize expected utility.\n\nYour implementation should:\n- Evaluate utility of possible outcomes\n- Calculate expected utility for each action\n- Select action with highest expected utility\n- Handle probabilistic outcomes",
    "starterCode": "class RationalAgent:\n    def __init__(self, utility_function):\n        self.utility = utility_function\n\n    def expected_utility(self, action, outcomes):\n        # Calculate expected utility of action\n        pass\n\n    def choose_action(self, possible_actions):\n        # Choose action maximizing expected utility\n        pass",
    "solution": "class RationalAgent:\n    \"\"\"Agent that maximizes expected utility.\"\"\"\n    def __init__(self, utility_function):\n        self.utility = utility_function\n        self.history = []\n\n    def expected_utility(self, action, outcomes, probabilities):\n        \"\"\"\n        Calculate expected utility of action.\n\n        action: action identifier\n        outcomes: list of possible outcome states\n        probabilities: probability of each outcome\n        \"\"\"\n        expected = 0.0\n\n        for outcome, prob in zip(outcomes, probabilities):\n            utility_val = self.utility(outcome)\n            expected += prob * utility_val\n\n        return expected\n\n    def choose_action(self, possible_actions, outcome_model):\n        \"\"\"\n        Choose action maximizing expected utility.\n\n        possible_actions: list of available actions\n        outcome_model: function(action) -> (outcomes, probabilities)\n        \"\"\"\n        best_action = None\n        best_expected = float('-inf')\n\n        for action in possible_actions:\n            outcomes, probabilities = outcome_model(action)\n            expected = self.expected_utility(action, outcomes, probabilities)\n\n            if expected > best_expected:\n                best_expected = expected\n                best_action = action\n\n        self.history.append({\n            'action': best_action,\n            'expected_utility': best_expected\n        })\n\n        return best_action, best_expected\n\n    def performance_summary(self):\n        \"\"\"Summary of decision history.\"\"\"\n        if not self.history:\n            return \"No decisions made\"\n\n        total_expected = sum(h['expected_utility'] for h in self.history)\n        avg_expected = total_expected / len(self.history)\n\n        return {\n            'decisions': len(self.history),\n            'avg_expected_utility': avg_expected,\n            'total_expected_utility': total_expected\n        }\n\n# Example: Investment decision agent\n\ndef investment_utility(outcome):\n    \"\"\"Utility function for money (with risk aversion).\"\"\"\n    # Square root for diminishing marginal utility\n    money = outcome['money']\n    return money ** 0.5\n\n# Create agent\nagent = RationalAgent(investment_utility)\n\nprint(\"Rational Agent - Investment Decisions\")\nprint(\"=\"*50)\n\n# Define outcome models for different actions\ndef outcome_model(action):\n    \"\"\"Model outcomes and probabilities for each action.\"\"\"\n    if action == 'safe_investment':\n        # Safe: guaranteed 5% return\n        outcomes = [\n            {'money': 105}\n        ]\n        probabilities = [1.0]\n\n    elif action == 'moderate_risk':\n        # Moderate: 60% chance of 15% return, 40% chance of 0% return\n        outcomes = [\n            {'money': 115},\n            {'money': 100}\n        ]\n        probabilities = [0.6, 0.4]\n\n    elif action == 'high_risk':\n        # High risk: 40% chance of 50% return, 60% chance of -20% loss\n        outcomes = [\n            {'money': 150},\n            {'money': 80}\n        ]\n        probabilities = [0.4, 0.6]\n\n    else:\n        outcomes = [{'money': 100}]\n        probabilities = [1.0]\n\n    return outcomes, probabilities\n\n# Available actions\nactions = ['safe_investment', 'moderate_risk', 'high_risk']\n\nprint(\"Initial capital: $100\")\nprint(\"\\nEvaluating investment options:\")\nprint()\n\n# Evaluate each action\nfor action in actions:\n    outcomes, probs = outcome_model(action)\n    expected = agent.expected_utility(action, outcomes, probs)\n\n    print(f\"{action}:\")\n    print(f\"  Possible outcomes:\")\n    for outcome, prob in zip(outcomes, probs):\n        print(f\"    ${outcome['money']} (prob: {prob:.1%})\")\n    print(f\"  Expected utility: {expected:.2f}\")\n    print()\n\n# Agent chooses best action\nbest_action, best_utility = agent.choose_action(actions, outcome_model)\n\nprint(\"=\"*50)\nprint(f\"Agent chooses: {best_action}\")\nprint(f\"Expected utility: {best_utility:.2f}\")\nprint()\n\n# Multiple decisions\nprint(\"=\"*50)\nprint(\"Multiple Decision Scenario\")\nprint(\"=\"*50)\n\ndecisions = []\nfor i in range(5):\n    action, utility = agent.choose_action(actions, outcome_model)\n    decisions.append(action)\n    print(f\"Decision {i+1}: {action} (EU: {utility:.2f})\")\n\nsummary = agent.performance_summary()\nprint(f\"\\nSummary:\")\nprint(f\"  Total decisions: {summary['decisions']}\")\nprint(f\"  Avg expected utility: {summary['avg_expected_utility']:.2f}\")\n\n# Test with different utility function (risk-seeking)\nprint(\"\\n\" + \"=\"*50)\nprint(\"Risk-Seeking Agent (logarithmic utility)\")\nprint(\"=\"*50)\n\ndef risk_seeking_utility(outcome):\n    \"\"\"Risk-seeking utility function.\"\"\"\n    import math\n    money = outcome['money']\n    # Exponential for risk-seeking behavior\n    return money ** 1.5\n\nagent2 = RationalAgent(risk_seeking_utility)\n\nfor action in actions:\n    outcomes, probs = outcome_model(action)\n    expected = agent2.expected_utility(action, outcomes, probs)\n    print(f\"{action}: EU = {expected:.2f}\")\n\nbest_action2, best_utility2 = agent2.choose_action(actions, outcome_model)\nprint(f\"\\nRisk-seeking agent chooses: {best_action2}\")",
    "testCases": [
      {
        "input": "agent.expected_utility(action, outcomes, probabilities)",
        "isHidden": false,
        "description": "Test expected utility calculation"
      },
      {
        "input": "agent.choose_action(possible_actions, outcome_model)",
        "isHidden": false,
        "description": "Test agent selects action with highest expected utility"
      },
      {
        "input": "different utility functions",
        "isHidden": false,
        "description": "Test different utility functions lead to different choices"
      }
    ],
    "hints": [
      "Expected utility = sum of (probability × utility) over all outcomes",
      "Compare expected utility of all actions, choose highest",
      "Utility function encodes agent preferences (risk-averse vs risk-seeking)"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex11",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Agent State Representation",
    "difficulty": 2,
    "description": "Implement different state representations for agents.\n\nYour implementation should:\n- Atomic state representation\n- Factored state representation (feature-based)\n- Structured state representation (objects and relations)\n- Convert between representations",
    "starterCode": "class AtomicState:\n    pass\n\nclass FactoredState:\n    pass\n\nclass StructuredState:\n    pass\n\ndef convert_atomic_to_factored(atomic_state):\n    pass\n\ndef convert_factored_to_structured(factored_state):\n    pass",
    "solution": "class AtomicState:\n    \"\"\"Atomic state: single indivisible identifier.\"\"\"\n    def __init__(self, state_id):\n        self.state_id = state_id\n\n    def __repr__(self):\n        return f\"AtomicState({self.state_id})\"\n\n    def __eq__(self, other):\n        return isinstance(other, AtomicState) and self.state_id == other.state_id\n\n    def __hash__(self):\n        return hash(self.state_id)\n\nclass FactoredState:\n    \"\"\"Factored state: set of variable-value pairs.\"\"\"\n    def __init__(self, variables):\n        self.variables = variables  # dict: variable -> value\n\n    def __repr__(self):\n        vars_str = \", \".join(f\"{k}={v}\" for k, v in self.variables.items())\n        return f\"FactoredState({vars_str})\"\n\n    def __eq__(self, other):\n        return isinstance(other, FactoredState) and self.variables == other.variables\n\n    def __hash__(self):\n        return hash(tuple(sorted(self.variables.items())))\n\n    def get(self, variable, default=None):\n        return self.variables.get(variable, default)\n\n    def set(self, variable, value):\n        self.variables[variable] = value\n\nclass StructuredState:\n    \"\"\"Structured state: objects with properties and relations.\"\"\"\n    def __init__(self):\n        self.objects = {}  # object_id -> properties dict\n        self.relations = []  # list of (relation_name, obj1, obj2)\n\n    def add_object(self, obj_id, properties):\n        self.objects[obj_id] = properties\n\n    def add_relation(self, relation_name, obj1, obj2):\n        self.relations.append((relation_name, obj1, obj2))\n\n    def get_object_property(self, obj_id, property_name):\n        return self.objects.get(obj_id, {}).get(property_name)\n\n    def query_relations(self, relation_name):\n        return [(o1, o2) for rel, o1, o2 in self.relations if rel == relation_name]\n\n    def __repr__(self):\n        objs_str = f\"{len(self.objects)} objects\"\n        rels_str = f\"{len(self.relations)} relations\"\n        return f\"StructuredState({objs_str}, {rels_str})\"\n\ndef convert_atomic_to_factored(atomic_state, state_mapping):\n    \"\"\"\n    Convert atomic state to factored representation.\n    state_mapping: dict mapping state_id -> variables dict\n    \"\"\"\n    variables = state_mapping.get(atomic_state.state_id, {})\n    return FactoredState(variables.copy())\n\ndef convert_factored_to_atomic(factored_state, reverse_mapping):\n    \"\"\"\n    Convert factored state to atomic representation.\n    reverse_mapping: dict mapping variables tuple -> state_id\n    \"\"\"\n    key = tuple(sorted(factored_state.variables.items()))\n    state_id = reverse_mapping.get(key, f\"state_{hash(key)}\")\n    return AtomicState(state_id)\n\ndef convert_factored_to_structured(factored_state):\n    \"\"\"Convert factored state to structured representation.\"\"\"\n    structured = StructuredState()\n\n    # Extract objects from variables\n    # Assume variables like \"robot_x\", \"robot_y\", \"box_x\", \"box_y\"\n\n    objects = {}\n    for var, val in factored_state.variables.items():\n        parts = var.split('_')\n        if len(parts) >= 2:\n            obj_name = parts[0]\n            property_name = '_'.join(parts[1:])\n\n            if obj_name not in objects:\n                objects[obj_name] = {}\n            objects[obj_name][property_name] = val\n        else:\n            # Global property\n            if 'global' not in objects:\n                objects['global'] = {}\n            objects['global'][var] = val\n\n    # Add objects to structured state\n    for obj_name, properties in objects.items():\n        structured.add_object(obj_name, properties)\n\n    # Infer relations based on properties\n    # Example: if robot and box have same position, they're \"at\" same location\n    obj_names = list(objects.keys())\n    for i, obj1 in enumerate(obj_names):\n        for obj2 in obj_names[i+1:]:\n            if obj1 == 'global' or obj2 == 'global':\n                continue\n\n            # Check if same location\n            if objects[obj1].get('x') == objects[obj2].get('x') and                objects[obj1].get('y') == objects[obj2].get('y'):\n                structured.add_relation('same_location', obj1, obj2)\n\n    return structured\n\n# Test different representations\nprint(\"State Representation Examples\")\nprint(\"=\"*50)\n\n# Atomic state\nprint(\"\\n1. Atomic State:\")\natomic = AtomicState(\"state_42\")\nprint(f\"   {atomic}\")\n\n# Factored state\nprint(\"\\n2. Factored State:\")\nfactored = FactoredState({\n    'robot_x': 3,\n    'robot_y': 2,\n    'box_x': 3,\n    'box_y': 2,\n    'goal_x': 5,\n    'goal_y': 5,\n    'holding': True\n})\nprint(f\"   {factored}\")\nprint(f\"   Robot position: ({factored.get('robot_x')}, {factored.get('robot_y')})\")\nprint(f\"   Holding: {factored.get('holding')}\")\n\n# Structured state\nprint(\"\\n3. Structured State:\")\nstructured = StructuredState()\nstructured.add_object('robot', {'x': 3, 'y': 2, 'holding': True})\nstructured.add_object('box', {'x': 3, 'y': 2, 'weight': 10})\nstructured.add_object('goal', {'x': 5, 'y': 5})\nstructured.add_relation('at', 'robot', 'box')\nstructured.add_relation('holding', 'robot', 'box')\n\nprint(f\"   {structured}\")\nprint(f\"   Objects:\")\nfor obj_id, props in structured.objects.items():\n    print(f\"     {obj_id}: {props}\")\nprint(f\"   Relations:\")\nfor rel in structured.relations:\n    print(f\"     {rel[0]}({rel[1]}, {rel[2]})\")\n\n# Conversions\nprint(\"\\n\" + \"=\"*50)\nprint(\"State Conversions\")\nprint(\"=\"*50)\n\n# Atomic to Factored\nprint(\"\\nAtomic -> Factored:\")\nstate_mapping = {\n    \"state_42\": {'x': 5, 'y': 3, 'energy': 100}\n}\natomic = AtomicState(\"state_42\")\nfactored_from_atomic = convert_atomic_to_factored(atomic, state_mapping)\nprint(f\"   {atomic} -> {factored_from_atomic}\")\n\n# Factored to Structured\nprint(\"\\nFactored -> Structured:\")\nfactored = FactoredState({\n    'robot_x': 2,\n    'robot_y': 3,\n    'box_x': 2,\n    'box_y': 3,\n})\nstructured_from_factored = convert_factored_to_structured(factored)\nprint(f\"   {factored}\")\nprint(f\"   -> {structured_from_factored}\")\nprint(f\"   Objects: {list(structured_from_factored.objects.keys())}\")\nprint(f\"   Relations: {structured_from_factored.relations}\")\n\n# Compare representations\nprint(\"\\n\" + \"=\"*50)\nprint(\"Representation Comparison\")\nprint(\"=\"*50)\nprint(\"\\nAtomic State:\")\nprint(\"  + Simplest representation\")\nprint(\"  + Easy to implement\")\nprint(\"  - No structure for reasoning\")\nprint(\"  - Exponential number of states\")\n\nprint(\"\\nFactored State:\")\nprint(\"  + Compact representation\")\nprint(\"  + Easy to query properties\")\nprint(\"  + Good for feature-based learning\")\nprint(\"  - No explicit object relations\")\n\nprint(\"\\nStructured State:\")\nprint(\"  + Explicit objects and relations\")\nprint(\"  + Supports complex reasoning\")\nprint(\"  + Natural for many domains\")\nprint(\"  - More complex to implement\")",
    "testCases": [
      {
        "input": "factored_state.get(variable)",
        "isHidden": false,
        "description": "Test factored state variable access"
      },
      {
        "input": "structured_state.query_relations(name)",
        "isHidden": false,
        "description": "Test structured state relation queries"
      },
      {
        "input": "convert_factored_to_structured(factored)",
        "isHidden": false,
        "description": "Test conversion between representations"
      }
    ],
    "hints": [
      "Atomic: single identifier, simplest but least informative",
      "Factored: variable-value pairs, good for feature-based reasoning",
      "Structured: objects and relations, most expressive"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex12",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "PEAS Analysis for Agent Design",
    "difficulty": 1,
    "description": "Perform PEAS analysis (Performance, Environment, Actuators, Sensors) for different agent types.\n\nYour implementation should:\n- Define PEAS for various agent types\n- Validate PEAS specifications\n- Compare different agent designs\n- Generate agent specifications from PEAS",
    "starterCode": "class PEASSpecification:\n    def __init__(self, performance, environment, actuators, sensors):\n        pass\n\n    def validate(self):\n        # Check if specification is complete\n        pass\n\n    def compare(self, other_spec):\n        # Compare two PEAS specifications\n        pass",
    "solution": "class PEASSpecification:\n    \"\"\"PEAS specification for agent design.\"\"\"\n    def __init__(self, agent_type, performance, environment, actuators, sensors):\n        self.agent_type = agent_type\n        self.performance = performance  # list of performance measures\n        self.environment = environment  # dict describing environment\n        self.actuators = actuators  # list of actuators\n        self.sensors = sensors  # list of sensors\n\n    def validate(self):\n        \"\"\"Check if specification is complete and consistent.\"\"\"\n        errors = []\n\n        if not self.performance:\n            errors.append(\"No performance measures defined\")\n\n        if not self.environment:\n            errors.append(\"Environment not specified\")\n\n        if not self.actuators:\n            errors.append(\"No actuators defined\")\n\n        if not self.sensors:\n            errors.append(\"No sensors defined\")\n\n        # Check environment properties\n        required_props = ['observable', 'deterministic', 'episodic', 'static', 'discrete', 'agents']\n        for prop in required_props:\n            if prop not in self.environment:\n                errors.append(f\"Environment property '{prop}' not specified\")\n\n        return len(errors) == 0, errors\n\n    def compare(self, other_spec):\n        \"\"\"Compare two PEAS specifications.\"\"\"\n        comparison = {\n            'agent_types': (self.agent_type, other_spec.agent_type),\n            'differences': []\n        }\n\n        # Compare performance measures\n        perf_diff = set(self.performance) ^ set(other_spec.performance)\n        if perf_diff:\n            comparison['differences'].append(f\"Performance measures differ: {perf_diff}\")\n\n        # Compare actuators\n        act_diff = set(self.actuators) ^ set(other_spec.actuators)\n        if act_diff:\n            comparison['differences'].append(f\"Actuators differ: {act_diff}\")\n\n        # Compare sensors\n        sens_diff = set(self.sensors) ^ set(other_spec.sensors)\n        if sens_diff:\n            comparison['differences'].append(f\"Sensors differ: {sens_diff}\")\n\n        # Compare environment\n        for key in self.environment:\n            if key in other_spec.environment:\n                if self.environment[key] != other_spec.environment[key]:\n                    comparison['differences'].append(\n                        f\"Environment {key}: {self.environment[key]} vs {other_spec.environment[key]}\"\n                    )\n\n        return comparison\n\n    def __repr__(self):\n        return f\"PEAS({self.agent_type})\"\n\n    def print_specification(self):\n        \"\"\"Print detailed PEAS specification.\"\"\"\n        print(f\"\\nPEAS Specification: {self.agent_type}\")\n        print(\"=\"*60)\n\n        print(\"\\nPerformance Measures:\")\n        for pm in self.performance:\n            print(f\"  - {pm}\")\n\n        print(\"\\nEnvironment:\")\n        for key, val in self.environment.items():\n            print(f\"  - {key}: {val}\")\n\n        print(\"\\nActuators:\")\n        for act in self.actuators:\n            print(f\"  - {act}\")\n\n        print(\"\\nSensors:\")\n        for sens in self.sensors:\n            print(f\"  - {sens}\")\n\n# Example PEAS specifications\n\n# 1. Autonomous Vacuum Cleaner\nvacuum_peas = PEASSpecification(\n    agent_type=\"Autonomous Vacuum Cleaner\",\n    performance=[\n        \"Amount of dirt cleaned\",\n        \"Energy efficiency\",\n        \"Coverage area\",\n        \"Time to complete cleaning\"\n    ],\n    environment={\n        'observable': 'partially',  # Can sense immediate surroundings\n        'deterministic': 'stochastic',  # Dirt appears unpredictably\n        'episodic': False,  # Continuous cleaning\n        'static': False,  # Dirt can appear while cleaning\n        'discrete': True,  # Grid-based world\n        'agents': 'single',  # Usually one vacuum\n        'properties': ['Floor type', 'Obstacles', 'Dirt distribution']\n    },\n    actuators=[\n        \"Wheels (forward, backward, turn)\",\n        \"Vacuum motor\",\n        \"Brush\",\n        \"Charging contact\"\n    ],\n    sensors=[\n        \"Dirt detector\",\n        \"Bump sensor\",\n        \"Cliff detector\",\n        \"Battery level sensor\",\n        \"Camera (optional)\"\n    ]\n)\n\n# 2. Self-Driving Car\ncar_peas = PEASSpecification(\n    agent_type=\"Self-Driving Car\",\n    performance=[\n        \"Safety (no accidents)\",\n        \"Destination reached\",\n        \"Travel time\",\n        \"Fuel efficiency\",\n        \"Passenger comfort\",\n        \"Traffic law compliance\"\n    ],\n    environment={\n        'observable': 'partially',  # Limited by sensors\n        'deterministic': 'stochastic',  # Other drivers unpredictable\n        'episodic': False,  # Continuous driving\n        'static': False,  # Dynamic traffic\n        'discrete': False,  # Continuous space and time\n        'agents': 'multi',  # Many other vehicles\n        'properties': ['Roads', 'Traffic', 'Weather', 'Pedestrians']\n    },\n    actuators=[\n        \"Steering\",\n        \"Accelerator\",\n        \"Brake\",\n        \"Turn signals\",\n        \"Horn\"\n    ],\n    sensors=[\n        \"Cameras\",\n        \"Lidar\",\n        \"Radar\",\n        \"GPS\",\n        \"IMU (Inertial Measurement Unit)\",\n        \"Wheel encoders\",\n        \"Ultrasonic sensors\"\n    ]\n)\n\n# 3. Chess-Playing Agent\nchess_peas = PEASSpecification(\n    agent_type=\"Chess Player\",\n    performance=[\n        \"Win/loss/draw outcome\",\n        \"Move quality\",\n        \"Time per move\",\n        \"Rating improvement\"\n    ],\n    environment={\n        'observable': 'fully',  # Complete board state visible\n        'deterministic': 'deterministic',  # Rules are deterministic\n        'episodic': True,  # Each game is independent\n        'static': False,  # Opponent makes moves\n        'discrete': True,  # Discrete board and moves\n        'agents': 'two',  # Two players\n        'properties': ['Chess board', '8x8 grid', 'Standard rules']\n    },\n    actuators=[\n        \"Move pieces\"\n    ],\n    sensors=[\n        \"Board state sensor\"\n    ]\n)\n\n# 4. Medical Diagnosis Agent\nmedical_peas = PEASSpecification(\n    agent_type=\"Medical Diagnosis System\",\n    performance=[\n        \"Diagnostic accuracy\",\n        \"False positive/negative rates\",\n        \"Time to diagnosis\",\n        \"Patient outcome improvement\"\n    ],\n    environment={\n        'observable': 'partially',  # Limited by tests available\n        'deterministic': 'stochastic',  # Disease progression uncertain\n        'episodic': True,  # Each patient case separate\n        'static': False,  # Patient condition changes\n        'discrete': False,  # Continuous measurements\n        'agents': 'multi',  # Doctors, other systems\n        'properties': ['Patient symptoms', 'Test results', 'Medical history']\n    },\n    actuators=[\n        \"Request tests\",\n        \"Propose diagnosis\",\n        \"Suggest treatment\",\n        \"Alert doctor\"\n    ],\n    sensors=[\n        \"Patient symptoms input\",\n        \"Lab test results\",\n        \"Imaging data\",\n        \"Vital signs\",\n        \"Medical history database\"\n    ]\n)\n\n# Display specifications\nprint(\"PEAS Specifications for Different Agent Types\")\nprint(\"=\"*60)\n\nfor peas in [vacuum_peas, car_peas, chess_peas, medical_peas]:\n    peas.print_specification()\n\n    is_valid, errors = peas.validate()\n    if is_valid:\n        print(\"\\n✓ Specification is valid and complete\")\n    else:\n        print(\"\\n✗ Specification issues:\")\n        for error in errors:\n            print(f\"  - {error}\")\n\n# Compare specifications\nprint(\"\\n\" + \"=\"*60)\nprint(\"Comparing Specifications\")\nprint(\"=\"*60)\n\ncomparison = vacuum_peas.compare(car_peas)\nprint(f\"\\nComparing: {comparison['agent_types'][0]} vs {comparison['agent_types'][1]}\")\nif comparison['differences']:\n    print(\"\\nKey differences:\")\n    for diff in comparison['differences']:\n        print(f\"  - {diff}\")\nelse:\n    print(\"\\nNo significant differences found\")\n\n# Environment type analysis\nprint(\"\\n\" + \"=\"*60)\nprint(\"Environment Type Analysis\")\nprint(\"=\"*60)\n\nagents = [vacuum_peas, car_peas, chess_peas, medical_peas]\n\nprint(\"\\n{:<30} {:<15} {:<15} {:<10} {:<10}\".format(\n    \"Agent\", \"Observable\", \"Deterministic\", \"Episodic\", \"Agents\"\n))\nprint(\"-\" * 80)\n\nfor peas in agents:\n    env = peas.environment\n    print(\"{:<30} {:<15} {:<15} {:<10} {:<10}\".format(\n        peas.agent_type[:28],\n        env['observable'],\n        env['deterministic'],\n        str(env['episodic']),\n        env['agents']\n    ))",
    "testCases": [
      {
        "input": "peas.validate()",
        "isHidden": false,
        "description": "Test PEAS specification validation"
      },
      {
        "input": "peas.compare(other_peas)",
        "isHidden": false,
        "description": "Test comparing two PEAS specifications"
      },
      {
        "input": "peas.print_specification()",
        "isHidden": false,
        "description": "Test printing complete PEAS details"
      }
    ],
    "hints": [
      "PEAS: Performance measure, Environment, Actuators, Sensors",
      "Performance: how agent success is measured",
      "Environment: properties like observable, deterministic, episodic",
      "Actuators: how agent affects environment",
      "Sensors: how agent perceives environment"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex13",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Agent Communication Language (ACL)",
    "difficulty": 3,
    "description": "Implement basic agent communication using ACL primitives.\n\nYour implementation should:\n- Define message types (inform, request, query)\n- Implement message passing between agents\n- Handle speech acts and performatives\n- Coordinate multi-agent tasks through communication",
    "starterCode": "class Message:\n    def __init__(self, sender, receiver, performative, content):\n        pass\n\nclass CommunicatingAgent:\n    def __init__(self, agent_id):\n        self.id = agent_id\n        self.inbox = []\n\n    def send(self, message):\n        pass\n\n    def receive(self):\n        pass\n\n    def handle_message(self, message):\n        pass",
    "solution": "from collections import deque\nimport time\n\nclass Message:\n    \"\"\"ACL message with performative and content.\"\"\"\n    def __init__(self, sender, receiver, performative, content, reply_to=None):\n        self.sender = sender\n        self.receiver = receiver\n        self.performative = performative  # inform, request, query, agree, refuse, etc.\n        self.content = content\n        self.reply_to = reply_to  # message being replied to\n        self.timestamp = time.time()\n\n    def __repr__(self):\n        return f\"Message({self.sender}->{self.receiver}: {self.performative}({self.content}))\"\n\nclass MessageBroker:\n    \"\"\"Central message broker for agent communication.\"\"\"\n    def __init__(self):\n        self.agents = {}  # agent_id -> agent\n        self.message_log = []\n\n    def register_agent(self, agent):\n        self.agents[agent.id] = agent\n\n    def send_message(self, message):\n        \"\"\"Route message to receiver.\"\"\"\n        self.message_log.append(message)\n\n        if message.receiver in self.agents:\n            self.agents[message.receiver].receive(message)\n            return True\n        elif message.receiver == 'broadcast':\n            # Broadcast to all except sender\n            for agent_id, agent in self.agents.items():\n                if agent_id != message.sender:\n                    agent.receive(message)\n            return True\n        else:\n            print(f\"Warning: Agent {message.receiver} not found\")\n            return False\n\nclass CommunicatingAgent:\n    \"\"\"Agent capable of ACL communication.\"\"\"\n    def __init__(self, agent_id, broker):\n        self.id = agent_id\n        self.broker = broker\n        self.inbox = deque()\n        self.knowledge_base = {}  # stores received information\n        self.pending_requests = {}  # request_id -> original message\n\n    def send(self, receiver, performative, content, reply_to=None):\n        \"\"\"Send message to another agent.\"\"\"\n        message = Message(self.id, receiver, performative, content, reply_to)\n        self.broker.send_message(message)\n        return message\n\n    def receive(self, message):\n        \"\"\"Receive message (called by broker).\"\"\"\n        self.inbox.append(message)\n\n    def process_messages(self):\n        \"\"\"Process all messages in inbox.\"\"\"\n        responses = []\n\n        while self.inbox:\n            message = self.inbox.popleft()\n            response = self.handle_message(message)\n            if response:\n                responses.append(response)\n\n        return responses\n\n    def handle_message(self, message):\n        \"\"\"Handle incoming message based on performative.\"\"\"\n        print(f\"[{self.id}] Received: {message.performative} from {message.sender}: {message.content}\")\n\n        if message.performative == 'inform':\n            return self.handle_inform(message)\n        elif message.performative == 'request':\n            return self.handle_request(message)\n        elif message.performative == 'query':\n            return self.handle_query(message)\n        elif message.performative == 'agree':\n            return self.handle_agree(message)\n        elif message.performative == 'refuse':\n            return self.handle_refuse(message)\n        elif message.performative == 'confirm':\n            return self.handle_confirm(message)\n        else:\n            print(f\"[{self.id}] Unknown performative: {message.performative}\")\n            return None\n\n    def handle_inform(self, message):\n        \"\"\"Handle inform message (receiving information).\"\"\"\n        # Store information in knowledge base\n        if isinstance(message.content, dict):\n            self.knowledge_base.update(message.content)\n        else:\n            self.knowledge_base[f\"info_from_{message.sender}\"] = message.content\n\n        # Acknowledge receipt\n        return self.send(message.sender, 'confirm', f\"Received: {message.content}\", message)\n\n    def handle_request(self, message):\n        \"\"\"Handle request message.\"\"\"\n        # Check if we can fulfill request\n        request = message.content\n\n        if self.can_fulfill(request):\n            # Agree to request\n            response = self.send(message.sender, 'agree', f\"Will fulfill: {request}\", message)\n            # Actually fulfill it\n            result = self.fulfill_request(request)\n            self.send(message.sender, 'inform', result, message)\n            return response\n        else:\n            # Refuse request\n            return self.send(message.sender, 'refuse', f\"Cannot fulfill: {request}\", message)\n\n    def handle_query(self, message):\n        \"\"\"Handle query message.\"\"\"\n        query = message.content\n\n        # Look up in knowledge base\n        result = self.knowledge_base.get(query, \"Unknown\")\n\n        return self.send(message.sender, 'inform', {query: result}, message)\n\n    def handle_agree(self, message):\n        \"\"\"Handle agreement to our request.\"\"\"\n        print(f\"[{self.id}] {message.sender} agreed to: {message.content}\")\n        return None\n\n    def handle_refuse(self, message):\n        \"\"\"Handle refusal of our request.\"\"\"\n        print(f\"[{self.id}] {message.sender} refused: {message.content}\")\n        return None\n\n    def handle_confirm(self, message):\n        \"\"\"Handle confirmation message.\"\"\"\n        print(f\"[{self.id}] Confirmed by {message.sender}: {message.content}\")\n        return None\n\n    def can_fulfill(self, request):\n        \"\"\"Check if agent can fulfill request.\"\"\"\n        # Simple implementation: check if we have relevant knowledge\n        if isinstance(request, dict):\n            task = request.get('task', '')\n            return task in ['compute', 'search', 'inform']\n        return True\n\n    def fulfill_request(self, request):\n        \"\"\"Fulfill a request and return result.\"\"\"\n        if isinstance(request, dict):\n            task = request.get('task')\n            if task == 'compute':\n                return {'result': 'computation_done'}\n            elif task == 'search':\n                return {'result': 'search_complete'}\n        return {'result': 'task_completed'}\n\n# Test agent communication\nprint(\"Agent Communication Language (ACL) Example\")\nprint(\"=\"*60)\n\n# Create broker and agents\nbroker = MessageBroker()\n\nagent1 = CommunicatingAgent('Agent1', broker)\nagent2 = CommunicatingAgent('Agent2', broker)\nagent3 = CommunicatingAgent('Agent3', broker)\n\nbroker.register_agent(agent1)\nbroker.register_agent(agent2)\nbroker.register_agent(agent3)\n\nprint(\"\\nScenario 1: Information sharing\")\nprint(\"-\"*60)\n\n# Agent1 informs Agent2 of some information\nagent1.send('Agent2', 'inform', {'temperature': 25, 'humidity': 60})\nagent2.process_messages()\n\n# Agent2 queries Agent3\nagent2.send('Agent3', 'inform', {'location': 'lab', 'status': 'active'})\nagent3.process_messages()\n\nprint(\"\\nScenario 2: Request handling\")\nprint(\"-\"*60)\n\n# Agent1 requests Agent2 to perform a task\nagent1.send('Agent2', 'request', {'task': 'compute', 'data': [1, 2, 3]})\nagent2.process_messages()\nagent1.process_messages()  # Process agree/refuse and result\n\nprint(\"\\nScenario 3: Query\")\nprint(\"-\"*60)\n\n# Agent1 queries Agent3\nagent1.send('Agent3', 'query', 'location')\nagent3.process_messages()\nagent1.process_messages()  # Process response\n\nprint(\"\\nScenario 4: Broadcast\")\nprint(\"-\"*60)\n\n# Agent1 broadcasts to all\nagent1.send('broadcast', 'inform', {'announcement': 'System update at 10pm'})\nagent2.process_messages()\nagent3.process_messages()\n\nprint(\"\\nKnowledge Bases:\")\nprint(\"-\"*60)\nprint(f\"Agent1: {agent1.knowledge_base}\")\nprint(f\"Agent2: {agent2.knowledge_base}\")\nprint(f\"Agent3: {agent3.knowledge_base}\")\n\nprint(f\"\\nTotal messages sent: {len(broker.message_log)}\")",
    "testCases": [
      {
        "input": "agent.send(receiver, performative, content)",
        "isHidden": false,
        "description": "Test sending ACL messages"
      },
      {
        "input": "agent.handle_message(message)",
        "isHidden": false,
        "description": "Test handling different performatives"
      },
      {
        "input": "broker.send_message(message)",
        "isHidden": false,
        "description": "Test message routing through broker"
      }
    ],
    "hints": [
      "ACL performatives: inform (share info), request (ask to do something), query (ask question)",
      "Use message broker pattern for routing messages between agents",
      "Agents should respond appropriately based on performative type"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex14",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Belief-Desire-Intention (BDI) Agent",
    "difficulty": 4,
    "description": "Implement a BDI agent architecture with beliefs, desires, and intentions.\n\nYour implementation should:\n- Maintain belief base (world model)\n- Define desires (goals)\n- Form intentions (committed plans)\n- Update beliefs from percepts\n- Revise intentions based on beliefs",
    "starterCode": "class BDIAgent:\n    def __init__(self):\n        self.beliefs = {}\n        self.desires = []\n        self.intentions = []\n\n    def update_beliefs(self, percepts):\n        pass\n\n    def generate_options(self):\n        pass\n\n    def filter_intentions(self):\n        pass\n\n    def execute_intention(self):\n        pass",
    "solution": "from collections import deque\n\nclass Belief:\n    \"\"\"Represents agent's belief about the world.\"\"\"\n    def __init__(self, predicate, value, confidence=1.0):\n        self.predicate = predicate\n        self.value = value\n        self.confidence = confidence  # 0.0 to 1.0\n        self.timestamp = 0\n\n    def __repr__(self):\n        return f\"Belief({self.predicate}={self.value}, conf={self.confidence:.2f})\"\n\nclass Desire:\n    \"\"\"Represents agent's goal/desire.\"\"\"\n    def __init__(self, goal, priority=1.0):\n        self.goal = goal\n        self.priority = priority\n\n    def __repr__(self):\n        return f\"Desire({self.goal}, priority={self.priority:.1f})\"\n\nclass Intention:\n    \"\"\"Represents agent's intention (committed plan).\"\"\"\n    def __init__(self, goal, plan):\n        self.goal = goal\n        self.plan = deque(plan)  # sequence of actions\n        self.completed = False\n\n    def next_action(self):\n        \"\"\"Get next action in plan.\"\"\"\n        if self.plan:\n            return self.plan[0]\n        return None\n\n    def execute_step(self):\n        \"\"\"Execute next step of plan.\"\"\"\n        if self.plan:\n            return self.plan.popleft()\n        self.completed = True\n        return None\n\n    def is_complete(self):\n        \"\"\"Check if intention is complete.\"\"\"\n        return len(self.plan) == 0\n\n    def __repr__(self):\n        return f\"Intention({self.goal}, {len(self.plan)} steps remaining)\"\n\nclass BDIAgent:\n    \"\"\"Belief-Desire-Intention agent architecture.\"\"\"\n    def __init__(self, name=\"BDI-Agent\"):\n        self.name = name\n        self.beliefs = {}  # predicate -> Belief\n        self.desires = []  # list of Desire\n        self.intentions = []  # list of Intention\n        self.time = 0\n\n    def update_beliefs(self, percepts):\n        \"\"\"Update beliefs based on percepts (sense-think-act cycle).\"\"\"\n        print(f\"[{self.name}] Updating beliefs from percepts...\")\n\n        for predicate, value in percepts.items():\n            if predicate in self.beliefs:\n                # Update existing belief\n                old_value = self.beliefs[predicate].value\n                self.beliefs[predicate].value = value\n                self.beliefs[predicate].timestamp = self.time\n                print(f\"  Updated: {predicate} = {value} (was {old_value})\")\n            else:\n                # New belief\n                self.beliefs[predicate] = Belief(predicate, value)\n                self.beliefs[predicate].timestamp = self.time\n                print(f\"  New: {predicate} = {value}\")\n\n    def add_desire(self, goal, priority=1.0):\n        \"\"\"Add new desire/goal.\"\"\"\n        desire = Desire(goal, priority)\n        self.desires.append(desire)\n        print(f\"[{self.name}] New desire: {desire}\")\n\n    def generate_options(self):\n        \"\"\"Generate possible options (plans) for desires.\"\"\"\n        options = []\n\n        for desire in self.desires:\n            # Check if desire is achievable given current beliefs\n            if self.is_achievable(desire):\n                plan = self.plan_for_desire(desire)\n                if plan:\n                    options.append((desire, plan))\n\n        return options\n\n    def is_achievable(self, desire):\n        \"\"\"Check if desire is achievable given current beliefs.\"\"\"\n        goal = desire.goal\n\n        # Simple check: can we reach the goal?\n        if 'blocked' in self.beliefs and self.beliefs['blocked'].value:\n            return False  # Can't achieve anything if blocked\n\n        return True\n\n    def plan_for_desire(self, desire):\n        \"\"\"Generate plan to achieve desire.\"\"\"\n        goal = desire.goal\n\n        # Simple planning based on goal type\n        if goal == 'reach_location':\n            target = self.beliefs.get('target_location')\n            if target:\n                return ['move_toward_target', 'move_toward_target', 'arrive']\n\n        elif goal == 'collect_resource':\n            if self.beliefs.get('resource_location'):\n                return ['move_to_resource', 'pickup_resource', 'return']\n\n        elif goal == 'avoid_danger':\n            if self.beliefs.get('danger_present', Belief('danger_present', False)).value:\n                return ['move_away', 'find_safe_location', 'wait']\n\n        return []\n\n    def filter_intentions(self, options):\n        \"\"\"Filter options to select intentions (deliberation).\"\"\"\n        print(f\"[{self.name}] Filtering {len(options)} options...\")\n\n        # Remove completed intentions\n        self.intentions = [i for i in self.intentions if not i.is_complete()]\n\n        # If no current intentions, select from options\n        if not self.intentions and options:\n            # Select highest priority desire\n            options.sort(key=lambda x: x[0].priority, reverse=True)\n            desire, plan = options[0]\n\n            intention = Intention(desire.goal, plan)\n            self.intentions.append(intention)\n            print(f\"  Selected intention: {intention}\")\n\n        # Revise intentions based on changed beliefs\n        self.revise_intentions()\n\n    def revise_intentions(self):\n        \"\"\"Revise intentions based on new beliefs.\"\"\"\n        # Check if intentions are still valid\n        to_remove = []\n\n        for intention in self.intentions:\n            # Check if goal is already achieved\n            if self.is_goal_achieved(intention.goal):\n                print(f\"  Goal achieved: {intention.goal}\")\n                to_remove.append(intention)\n\n            # Check if intention is impossible now\n            elif not self.is_intention_possible(intention):\n                print(f\"  Intention no longer possible: {intention.goal}\")\n                to_remove.append(intention)\n\n        for intention in to_remove:\n            self.intentions.remove(intention)\n\n    def is_goal_achieved(self, goal):\n        \"\"\"Check if goal is achieved.\"\"\"\n        if goal == 'reach_location':\n            return self.beliefs.get('at_target', Belief('at_target', False)).value\n        elif goal == 'collect_resource':\n            return self.beliefs.get('has_resource', Belief('has_resource', False)).value\n        return False\n\n    def is_intention_possible(self, intention):\n        \"\"\"Check if intention is still possible given beliefs.\"\"\"\n        if 'blocked' in self.beliefs and self.beliefs['blocked'].value:\n            return False\n        return True\n\n    def execute_intention(self):\n        \"\"\"Execute current intention.\"\"\"\n        if not self.intentions:\n            print(f\"[{self.name}] No intentions to execute\")\n            return None\n\n        intention = self.intentions[0]\n        action = intention.execute_step()\n\n        if action:\n            print(f\"[{self.name}] Executing: {action}\")\n            return action\n        else:\n            print(f\"[{self.name}] Intention complete: {intention.goal}\")\n            self.intentions.remove(intention)\n            return None\n\n    def run_cycle(self, percepts):\n        \"\"\"Run one BDI cycle.\"\"\"\n        print(f\"\\n{'='*60}\")\n        print(f\"[{self.name}] BDI Cycle {self.time}\")\n        print(f\"{'='*60}\")\n\n        # 1. Update beliefs from percepts\n        self.update_beliefs(percepts)\n\n        # 2. Generate options from desires\n        options = self.generate_options()\n\n        # 3. Filter/select intentions\n        self.filter_intentions(options)\n\n        # 4. Execute intention\n        action = self.execute_intention()\n\n        self.time += 1\n\n        return action\n\n    def print_state(self):\n        \"\"\"Print current BDI state.\"\"\"\n        print(f\"\\n[{self.name}] Current State:\")\n        print(f\"  Beliefs:\")\n        for belief in self.beliefs.values():\n            print(f\"    {belief}\")\n        print(f\"  Desires:\")\n        for desire in self.desires:\n            print(f\"    {desire}\")\n        print(f\"  Intentions:\")\n        for intention in self.intentions:\n            print(f\"    {intention}\")\n\n# Test BDI agent\nprint(\"Belief-Desire-Intention (BDI) Agent\")\nprint(\"=\"*60)\n\n# Create agent\nagent = BDIAgent(\"Explorer\")\n\n# Add desires\nagent.add_desire('reach_location', priority=2.0)\nagent.add_desire('collect_resource', priority=1.5)\nagent.add_desire('avoid_danger', priority=3.0)\n\n# Simulation\nscenarios = [\n    {\n        'step': 1,\n        'percepts': {\n            'target_location': (10, 10),\n            'current_location': (0, 0),\n            'blocked': False,\n            'at_target': False\n        }\n    },\n    {\n        'step': 2,\n        'percepts': {\n            'target_location': (10, 10),\n            'current_location': (5, 5),\n            'blocked': False,\n            'at_target': False,\n            'resource_location': (5, 6)\n        }\n    },\n    {\n        'step': 3,\n        'percepts': {\n            'target_location': (10, 10),\n            'current_location': (8, 8),\n            'blocked': False,\n            'at_target': False,\n            'danger_present': True\n        }\n    },\n    {\n        'step': 4,\n        'percepts': {\n            'target_location': (10, 10),\n            'current_location': (10, 10),\n            'blocked': False,\n            'at_target': True,\n            'danger_present': False\n        }\n    }\n]\n\nfor scenario in scenarios:\n    action = agent.run_cycle(scenario['percepts'])\n    agent.print_state()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"BDI Architecture Summary\")\nprint(\"=\"*60)\nprint(\"\\nBeliefs: Agent's model of the world\")\nprint(\"  - Updated from percepts\")\nprint(\"  - Can be uncertain (confidence)\")\nprint(\"\\nDesires: Agent's goals\")\nprint(\"  - Have priorities\")\nprint(\"  - May conflict\")\nprint(\"\\nIntentions: Committed plans\")\nprint(\"  - Selected from options\")\nprint(\"  - Executed step-by-step\")\nprint(\"  - Revised when beliefs change\")",
    "testCases": [
      {
        "input": "agent.update_beliefs(percepts)",
        "isHidden": false,
        "description": "Test belief update from percepts"
      },
      {
        "input": "agent.generate_options()",
        "isHidden": false,
        "description": "Test generating options from desires"
      },
      {
        "input": "agent.filter_intentions(options)",
        "isHidden": false,
        "description": "Test selecting intentions from options"
      },
      {
        "input": "agent.execute_intention()",
        "isHidden": false,
        "description": "Test executing intention steps"
      }
    ],
    "hints": [
      "Beliefs: agent's knowledge about world state",
      "Desires: goals agent wants to achieve",
      "Intentions: plans agent commits to executing",
      "BDI cycle: update beliefs -> generate options -> filter -> execute"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex15",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Environment Types and Agent Design",
    "difficulty": 2,
    "description": "Implement agents optimized for different environment types.\n\nYour implementation should:\n- Handle fully vs partially observable environments\n- Adapt to deterministic vs stochastic environments\n- Work in episodic vs sequential environments\n- Function in single-agent vs multi-agent environments",
    "starterCode": "class EnvironmentAdapter:\n    def __init__(self, env_type):\n        self.env_type = env_type\n\n    def design_agent(self):\n        # Design agent appropriate for environment type\n        pass\n\n    def select_architecture(self):\n        # Select agent architecture based on environment\n        pass",
    "solution": "class Environment:\n    \"\"\"Base environment class.\"\"\"\n    def __init__(self, properties):\n        self.properties = properties\n        # Properties: observable, deterministic, episodic, static, discrete, agents\n\n    def __repr__(self):\n        props_str = \", \".join(f\"{k}={v}\" for k, v in self.properties.items())\n        return f\"Environment({props_str})\"\n\nclass Agent:\n    \"\"\"Base agent class.\"\"\"\n    def __init__(self, name, architecture):\n        self.name = name\n        self.architecture = architecture\n\n    def __repr__(self):\n        return f\"Agent({self.name}, {self.architecture})\"\n\nclass EnvironmentAdapter:\n    \"\"\"Adapt agent design to environment properties.\"\"\"\n    def __init__(self):\n        pass\n\n    def analyze_environment(self, env):\n        \"\"\"Analyze environment properties.\"\"\"\n        props = env.properties\n\n        analysis = {\n            'observability': 'full' if props.get('observable') == 'fully' else 'partial',\n            'determinism': 'deterministic' if props.get('deterministic') == 'deterministic' else 'stochastic',\n            'episodic': props.get('episodic', False),\n            'static': props.get('static', False),\n            'discrete': props.get('discrete', True),\n            'agents': props.get('agents', 'single')\n        }\n\n        return analysis\n\n    def design_agent(self, env):\n        \"\"\"Design agent appropriate for environment.\"\"\"\n        analysis = self.analyze_environment(env)\n\n        print(f\"\\nDesigning agent for environment:\")\n        print(f\"  Observability: {analysis['observability']}\")\n        print(f\"  Determinism: {analysis['determinism']}\")\n        print(f\"  Episodic: {analysis['episodic']}\")\n        print(f\"  Static: {analysis['static']}\")\n        print(f\"  Agents: {analysis['agents']}\")\n        print()\n\n        # Select architecture\n        architecture = self.select_architecture(analysis)\n\n        # Select required capabilities\n        capabilities = self.select_capabilities(analysis)\n\n        # Design considerations\n        considerations = self.design_considerations(analysis)\n\n        return {\n            'architecture': architecture,\n            'capabilities': capabilities,\n            'considerations': considerations\n        }\n\n    def select_architecture(self, analysis):\n        \"\"\"Select agent architecture based on environment.\"\"\"\n        if analysis['observability'] == 'partial' and analysis['determinism'] == 'stochastic':\n            # Complex environment: need sophisticated reasoning\n            if analysis['episodic']:\n                return \"Utility-based agent\"\n            else:\n                return \"Learning agent with probabilistic reasoning\"\n\n        elif analysis['observability'] == 'full' and analysis['determinism'] == 'deterministic':\n            # Simple environment: can use simpler architecture\n            if analysis['episodic']:\n                return \"Simple reflex agent\"\n            else:\n                return \"Model-based agent\"\n\n        elif analysis['observability'] == 'partial':\n            # Need to maintain state\n            return \"Model-based agent\"\n\n        elif analysis['determinism'] == 'stochastic':\n            # Need to handle uncertainty\n            return \"Utility-based agent\"\n\n        else:\n            return \"Goal-based agent\"\n\n    def select_capabilities(self, analysis):\n        \"\"\"Select required capabilities.\"\"\"\n        capabilities = []\n\n        if analysis['observability'] == 'partial':\n            capabilities.append(\"State estimation\")\n            capabilities.append(\"Sensor fusion\")\n\n        if analysis['determinism'] == 'stochastic':\n            capabilities.append(\"Probabilistic reasoning\")\n            capabilities.append(\"Risk assessment\")\n\n        if not analysis['episodic']:\n            capabilities.append(\"Planning\")\n            capabilities.append(\"Memory\")\n\n        if not analysis['static']:\n            capabilities.append(\"Continuous monitoring\")\n            capabilities.append(\"Replanning\")\n\n        if analysis['agents'] == 'multi':\n            capabilities.append(\"Communication\")\n            capabilities.append(\"Coordination\")\n            capabilities.append(\"Game-theoretic reasoning\")\n\n        return capabilities\n\n    def design_considerations(self, analysis):\n        \"\"\"Design considerations based on environment.\"\"\"\n        considerations = []\n\n        # Observability\n        if analysis['observability'] == 'partial':\n            considerations.append(\"Maintain belief state over possible worlds\")\n            considerations.append(\"Active sensing strategy may be beneficial\")\n        else:\n            considerations.append(\"Can use simple state representation\")\n\n        # Determinism\n        if analysis['determinism'] == 'stochastic':\n            considerations.append(\"Use probabilistic action models\")\n            considerations.append(\"Consider expected utility, not just goal achievement\")\n        else:\n            considerations.append(\"Can use deterministic planning algorithms\")\n\n        # Episodic vs Sequential\n        if analysis['episodic']:\n            considerations.append(\"No need to plan ahead\")\n            considerations.append(\"Each episode independent\")\n        else:\n            considerations.append(\"Actions affect future states\")\n            considerations.append(\"Need lookahead and planning\")\n\n        # Static vs Dynamic\n        if not analysis['static']:\n            considerations.append(\"Environment changes while agent deliberates\")\n            considerations.append(\"Fast decision-making important\")\n            considerations.append(\"May need reactive components\")\n        else:\n            considerations.append(\"Can take time to deliberate\")\n\n        # Single vs Multi-agent\n        if analysis['agents'] == 'multi':\n            considerations.append(\"Model other agents' behavior\")\n            considerations.append(\"Potential for cooperation or competition\")\n        else:\n            considerations.append(\"Environment predictable (except randomness)\")\n\n        return considerations\n\n# Test environment adapter\nprint(\"Environment-Based Agent Design\")\nprint(\"=\"*60)\n\nadapter = EnvironmentAdapter()\n\n# Test different environment types\n\n# 1. Chess\nprint(\"\\n\" + \"=\"*60)\nprint(\"Environment 1: Chess\")\nprint(\"=\"*60)\n\nchess_env = Environment({\n    'observable': 'fully',\n    'deterministic': 'deterministic',\n    'episodic': True,\n    'static': False,  # Opponent moves\n    'discrete': True,\n    'agents': 'two'\n})\n\ndesign = adapter.design_agent(chess_env)\nprint(f\"Recommended architecture: {design['architecture']}\")\nprint(f\"\\nRequired capabilities:\")\nfor cap in design['capabilities']:\n    print(f\"  - {cap}\")\nprint(f\"\\nDesign considerations:\")\nfor con in design['considerations']:\n    print(f\"  - {con}\")\n\n# 2. Self-driving car\nprint(\"\\n\" + \"=\"*60)\nprint(\"Environment 2: Self-Driving Car\")\nprint(\"=\"*60)\n\ncar_env = Environment({\n    'observable': 'partially',\n    'deterministic': 'stochastic',\n    'episodic': False,\n    'static': False,\n    'discrete': False,\n    'agents': 'multi'\n})\n\ndesign = adapter.design_agent(car_env)\nprint(f\"Recommended architecture: {design['architecture']}\")\nprint(f\"\\nRequired capabilities:\")\nfor cap in design['capabilities']:\n    print(f\"  - {cap}\")\nprint(f\"\\nDesign considerations:\")\nfor con in design['considerations']:\n    print(f\"  - {con}\")\n\n# 3. Robotic vacuum\nprint(\"\\n\" + \"=\"*60)\nprint(\"Environment 3: Robotic Vacuum\")\nprint(\"=\"*60)\n\nvacuum_env = Environment({\n    'observable': 'partially',\n    'deterministic': 'stochastic',\n    'episodic': False,\n    'static': False,  # Dirt appears\n    'discrete': True,\n    'agents': 'single'\n})\n\ndesign = adapter.design_agent(vacuum_env)\nprint(f\"Recommended architecture: {design['architecture']}\")\nprint(f\"\\nRequired capabilities:\")\nfor cap in design['capabilities']:\n    print(f\"  - {cap}\")\nprint(f\"\\nDesign considerations:\")\nfor con in design['considerations']:\n    print(f\"  - {con}\")\n\n# 4. Medical diagnosis\nprint(\"\\n\" + \"=\"*60)\nprint(\"Environment 4: Medical Diagnosis\")\nprint(\"=\"*60)\n\nmedical_env = Environment({\n    'observable': 'partially',\n    'deterministic': 'stochastic',\n    'episodic': True,  # Each patient separate\n    'static': False,  # Patient condition changes\n    'discrete': False,\n    'agents': 'multi'  # Doctors, other systems\n})\n\ndesign = adapter.design_agent(medical_env)\nprint(f\"Recommended architecture: {design['architecture']}\")\nprint(f\"\\nRequired capabilities:\")\nfor cap in design['capabilities']:\n    print(f\"  - {cap}\")\nprint(f\"\\nDesign considerations:\")\nfor con in design['considerations']:\n    print(f\"  - {con}\")\n\n# Summary table\nprint(\"\\n\" + \"=\"*60)\nprint(\"Architecture Selection Summary\")\nprint(\"=\"*60)\n\nenvironments = [\n    (\"Chess\", chess_env),\n    (\"Self-Driving Car\", car_env),\n    (\"Robotic Vacuum\", vacuum_env),\n    (\"Medical Diagnosis\", medical_env)\n]\n\nprint(\"\\n{:<20} {:<15} {:<15} {:<25}\".format(\n    \"Environment\", \"Observable\", \"Deterministic\", \"Architecture\"\n))\nprint(\"-\"*80)\n\nfor name, env in environments:\n    analysis = adapter.analyze_environment(env)\n    architecture = adapter.select_architecture(analysis)\n    print(\"{:<20} {:<15} {:<15} {:<25}\".format(\n        name[:18],\n        analysis['observability'],\n        analysis['determinism'],\n        architecture[:23]\n    ))",
    "testCases": [
      {
        "input": "adapter.analyze_environment(env)",
        "isHidden": false,
        "description": "Test environment analysis"
      },
      {
        "input": "adapter.select_architecture(analysis)",
        "isHidden": false,
        "description": "Test architecture selection"
      },
      {
        "input": "adapter.design_agent(env)",
        "isHidden": false,
        "description": "Test complete agent design"
      }
    ],
    "hints": [
      "Partially observable: need state estimation and memory",
      "Stochastic: need probabilistic reasoning and utility",
      "Sequential (not episodic): need planning and lookahead",
      "Multi-agent: need coordination and communication"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex16",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Agent Rationality and Bounded Rationality",
    "difficulty": 3,
    "description": "Implement agents with different rationality assumptions.\n\nYour implementation should:\n- Implement perfectly rational agent (unlimited computation)\n- Implement boundedly rational agent (limited time/resources)\n- Compare performance under resource constraints\n- Demonstrate satisficing vs optimizing behavior",
    "starterCode": "class RationalAgent:\n    def decide(self, state, actions):\n        # Perfect rationality: find optimal action\n        pass\n\nclass BoundedRationalAgent:\n    def decide(self, state, actions, time_limit):\n        # Bounded rationality: find good enough action\n        pass\n\ndef compare_rationality(rational, bounded):\n    pass",
    "solution": "import time\nimport random\n\nclass RationalAgent:\n    \"\"\"Perfectly rational agent (unlimited computation).\"\"\"\n    def __init__(self, utility_function):\n        self.utility = utility_function\n        self.decisions = []\n\n    def decide(self, state, actions):\n        \"\"\"\n        Find optimal action by evaluating all possibilities.\n        This may take a long time for complex problems.\n        \"\"\"\n        start_time = time.time()\n\n        best_action = None\n        best_utility = float('-inf')\n\n        # Evaluate all actions exhaustively\n        for action in actions:\n            # Simulate outcome\n            outcome = self.simulate_outcome(state, action)\n            utility = self.utility(outcome)\n\n            if utility > best_utility:\n                best_utility = utility\n                best_action = action\n\n        elapsed_time = time.time() - start_time\n\n        self.decisions.append({\n            'action': best_action,\n            'utility': best_utility,\n            'time': elapsed_time,\n            'actions_evaluated': len(actions)\n        })\n\n        return best_action, best_utility\n\n    def simulate_outcome(self, state, action):\n        \"\"\"Simulate outcome of action.\"\"\"\n        # Simple simulation\n        return {\n            'state': state,\n            'action': action,\n            'reward': random.gauss(action.get('expected_reward', 0), 1.0)\n        }\n\nclass BoundedRationalAgent:\n    \"\"\"Boundedly rational agent (limited computation).\"\"\"\n    def __init__(self, utility_function, satisficing_threshold=0.8):\n        self.utility = utility_function\n        self.satisficing_threshold = satisficing_threshold\n        self.decisions = []\n\n    def decide(self, state, actions, time_limit=None, max_evaluations=None):\n        \"\"\"\n        Find good enough action within resource constraints.\n        Uses satisficing: stop when action exceeds threshold.\n        \"\"\"\n        start_time = time.time()\n\n        best_action = None\n        best_utility = float('-inf')\n        evaluations = 0\n\n        # Shuffle actions for randomized search\n        actions_to_try = list(actions)\n        random.shuffle(actions_to_try)\n\n        for action in actions_to_try:\n            # Check resource constraints\n            if time_limit and (time.time() - start_time) > time_limit:\n                break\n\n            if max_evaluations and evaluations >= max_evaluations:\n                break\n\n            # Evaluate action\n            outcome = self.simulate_outcome(state, action)\n            utility = self.utility(outcome)\n            evaluations += 1\n\n            if utility > best_utility:\n                best_utility = utility\n                best_action = action\n\n            # Satisficing: stop if good enough\n            if utility >= self.satisficing_threshold * self.max_possible_utility():\n                break\n\n        elapsed_time = time.time() - start_time\n\n        self.decisions.append({\n            'action': best_action,\n            'utility': best_utility,\n            'time': elapsed_time,\n            'actions_evaluated': evaluations\n        })\n\n        return best_action, best_utility\n\n    def simulate_outcome(self, state, action):\n        \"\"\"Simulate outcome of action.\"\"\"\n        return {\n            'state': state,\n            'action': action,\n            'reward': random.gauss(action.get('expected_reward', 0), 1.0)\n        }\n\n    def max_possible_utility(self):\n        \"\"\"Estimate maximum possible utility.\"\"\"\n        return 1.0  # Normalize to [0, 1]\n\nclass AdaptiveAgent:\n    \"\"\"Agent that adapts rationality to situation.\"\"\"\n    def __init__(self, utility_function):\n        self.utility = utility_function\n        self.decisions = []\n\n    def decide(self, state, actions, urgency=0.5):\n        \"\"\"\n        Adapt decision strategy based on urgency.\n        Low urgency: more deliberation (rational)\n        High urgency: quick decision (bounded)\n        \"\"\"\n        start_time = time.time()\n\n        # Adapt evaluation strategy based on urgency\n        if urgency < 0.3:\n            # Low urgency: evaluate many actions\n            max_evals = len(actions)\n        elif urgency < 0.7:\n            # Medium urgency: evaluate subset\n            max_evals = len(actions) // 2\n        else:\n            # High urgency: quick heuristic\n            max_evals = 3\n\n        best_action = None\n        best_utility = float('-inf')\n\n        # Prioritize actions by heuristic\n        actions_sorted = sorted(\n            actions,\n            key=lambda a: a.get('expected_reward', 0),\n            reverse=True\n        )\n\n        for i, action in enumerate(actions_sorted[:max_evals]):\n            outcome = self.simulate_outcome(state, action)\n            utility = self.utility(outcome)\n\n            if utility > best_utility:\n                best_utility = utility\n                best_action = action\n\n        elapsed_time = time.time() - start_time\n\n        self.decisions.append({\n            'action': best_action,\n            'utility': best_utility,\n            'time': elapsed_time,\n            'actions_evaluated': min(max_evals, len(actions)),\n            'urgency': urgency\n        })\n\n        return best_action, best_utility\n\n    def simulate_outcome(self, state, action):\n        \"\"\"Simulate outcome of action.\"\"\"\n        return {\n            'state': state,\n            'action': action,\n            'reward': random.gauss(action.get('expected_reward', 0), 1.0)\n        }\n\ndef utility_function(outcome):\n    \"\"\"Simple utility function based on reward.\"\"\"\n    reward = outcome.get('reward', 0)\n    # Normalize to [0, 1]\n    return (reward + 10) / 20.0\n\ndef compare_rationality(rational, bounded, adaptive, num_trials=10):\n    \"\"\"Compare different rationality approaches.\"\"\"\n\n    print(\"\\nComparing Rationality Approaches\")\n    print(\"=\"*60)\n\n    for trial in range(num_trials):\n        # Generate decision problem\n        state = {'trial': trial}\n        num_actions = 20 + trial * 5  # Increasing complexity\n\n        actions = [\n            {'id': i, 'expected_reward': random.uniform(-5, 5)}\n            for i in range(num_actions)\n        ]\n\n        # Rational agent (no constraints)\n        rational.decide(state, actions)\n\n        # Bounded rational agent (limited evaluations)\n        bounded.decide(state, actions, max_evaluations=10)\n\n        # Adaptive agent (medium urgency)\n        adaptive.decide(state, actions, urgency=0.5)\n\n    # Print results\n    print(f\"\\nResults after {num_trials} trials:\")\n    print(\"-\"*60)\n\n    agents = [\n        ('Rational', rational),\n        ('Bounded Rational', bounded),\n        ('Adaptive', adaptive)\n    ]\n\n    for name, agent in agents:\n        decisions = agent.decisions\n        avg_utility = sum(d['utility'] for d in decisions) / len(decisions)\n        avg_time = sum(d['time'] for d in decisions) / len(decisions)\n        avg_evals = sum(d['actions_evaluated'] for d in decisions) / len(decisions)\n\n        print(f\"\\n{name}:\")\n        print(f\"  Avg utility: {avg_utility:.4f}\")\n        print(f\"  Avg time: {avg_time*1000:.2f} ms\")\n        print(f\"  Avg evaluations: {avg_evals:.1f}\")\n\n    return agents\n\n# Test different rationality levels\nprint(\"Agent Rationality Comparison\")\nprint(\"=\"*60)\n\n# Create agents\nrational_agent = RationalAgent(utility_function)\nbounded_agent = BoundedRationalAgent(utility_function, satisficing_threshold=0.7)\nadaptive_agent = AdaptiveAgent(utility_function)\n\n# Compare performance\nagents = compare_rationality(rational_agent, bounded_agent, adaptive_agent, num_trials=10)\n\n# Detailed analysis\nprint(\"\\n\" + \"=\"*60)\nprint(\"Rationality Trade-offs\")\nprint(\"=\"*60)\n\nprint(\"\\nPerfect Rationality:\")\nprint(\"  + Always finds optimal action\")\nprint(\"  - Computationally expensive\")\nprint(\"  - May be too slow for real-time decisions\")\n\nprint(\"\\nBounded Rationality:\")\nprint(\"  + Fast decision making\")\nprint(\"  + Satisficing: finds 'good enough' solutions\")\nprint(\"  - May miss optimal actions\")\nprint(\"  + More realistic model of human/agent behavior\")\n\nprint(\"\\nAdaptive Rationality:\")\nprint(\"  + Adapts to situation demands\")\nprint(\"  + Balances speed and optimality\")\nprint(\"  + Useful in varying time pressure\")\n\n# Visualize satisficing\nprint(\"\\n\" + \"=\"*60)\nprint(\"Satisficing Behavior\")\nprint(\"=\"*60)\n\nstate = {}\nactions = [\n    {'id': 1, 'expected_reward': 3.0},\n    {'id': 2, 'expected_reward': 5.0},\n    {'id': 3, 'expected_reward': 8.0},  # Optimal\n    {'id': 4, 'expected_reward': 7.5},\n    {'id': 5, 'expected_reward': 4.0},\n]\n\nprint(\"\\nAvailable actions:\")\nfor action in actions:\n    print(f\"  Action {action['id']}: expected reward = {action['expected_reward']}\")\n\n# Bounded agent with high threshold\nbounded_high = BoundedRationalAgent(utility_function, satisficing_threshold=0.9)\naction, utility = bounded_high.decide(state, actions, max_evaluations=5)\nprint(f\"\\nBounded agent (high threshold): chose action {action['id'] if action else 'None'}\")\n\n# Bounded agent with low threshold\nbounded_low = BoundedRationalAgent(utility_function, satisficing_threshold=0.6)\naction, utility = bounded_low.decide(state, actions, max_evaluations=5)\nprint(f\"Bounded agent (low threshold): chose action {action['id'] if action else 'None'}\")\n\n# Rational agent\naction, utility = rational_agent.decide(state, actions)\nprint(f\"Rational agent: chose action {action['id'] if action else 'None'}\")",
    "testCases": [
      {
        "input": "rational_agent.decide(state, actions)",
        "isHidden": false,
        "description": "Test rational agent finds optimal action"
      },
      {
        "input": "bounded_agent.decide(state, actions, time_limit)",
        "isHidden": false,
        "description": "Test bounded agent with resource constraints"
      },
      {
        "input": "compare_rationality(rational, bounded)",
        "isHidden": false,
        "description": "Test performance comparison"
      }
    ],
    "hints": [
      "Perfect rationality: evaluate all options, choose best (may be slow)",
      "Bounded rationality: evaluate limited options, satisfice (faster)",
      "Satisficing: stop when solution is \"good enough\" vs optimal",
      "Trade-off: decision quality vs computational resources"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t2-ex01",
    "subjectId": "cs406",
    "topicId": "cs406-topic-2",
    "title": "Breadth-First Search Implementation",
    "difficulty": 2,
    "description": "Implement BFS for finding the shortest path in a graph.\n\nYour implementation should:\n- Use a queue for frontier management\n- Track explored nodes\n- Return the path from start to goal\n- Handle graphs with cycles",
    "starterCode": "from collections import deque\n\ndef bfs(graph, start, goal):\n    \"\"\"\n    graph: dict where graph[node] = list of neighbors\n    start: starting node\n    goal: goal node\n    Returns: path from start to goal, or None if no path exists\n    \"\"\"\n    # TODO: Implement BFS\n    pass\n\n# Example:\n# graph = {\n#     'A': ['B', 'C'],\n#     'B': ['A', 'D', 'E'],\n#     'C': ['A', 'F'],\n#     'D': ['B'],\n#     'E': ['B', 'F'],\n#     'F': ['C', 'E']\n# }\n# print(bfs(graph, 'A', 'F'))  # Should return path like ['A', 'C', 'F']",
    "solution": "from collections import deque\n\ndef bfs(graph, start, goal):\n    \"\"\"\n    graph: dict where graph[node] = list of neighbors\n    start: starting node\n    goal: goal node\n    Returns: path from start to goal, or None if no path exists\n    \"\"\"\n    if start == goal:\n        return [start]\n\n    # Queue stores (node, path to that node)\n    frontier = deque([(start, [start])])\n    explored = set()\n\n    while frontier:\n        node, path = frontier.popleft()\n\n        if node in explored:\n            continue\n\n        explored.add(node)\n\n        for neighbor in graph.get(node, []):\n            if neighbor in explored:\n                continue\n\n            new_path = path + [neighbor]\n\n            if neighbor == goal:\n                return new_path\n\n            frontier.append((neighbor, new_path))\n\n    return None  # No path found\n\n# Test\ngraph = {\n    'A': ['B', 'C'],\n    'B': ['A', 'D', 'E'],\n    'C': ['A', 'F'],\n    'D': ['B'],\n    'E': ['B', 'F'],\n    'F': ['C', 'E']\n}\nprint(bfs(graph, 'A', 'F'))  # ['A', 'C', 'F']\nprint(bfs(graph, 'A', 'D'))  # ['A', 'B', 'D']",
    "testCases": [
      {
        "input": "bfs(graph, 'A', 'F')",
        "isHidden": false,
        "description": "Test BFS finds shortest path from A to F"
      },
      {
        "input": "bfs(graph, 'A', 'D')",
        "isHidden": false,
        "description": "Test BFS finds shortest path from A to D"
      },
      {
        "input": "bfs(graph, 'A', 'A')",
        "isHidden": false,
        "description": "Test BFS with start == goal"
      }
    ],
    "hints": [
      "Use a deque (double-ended queue) for efficient frontier management",
      "Track the path to each node, not just the node itself, in the frontier",
      "Use a set to keep track of explored nodes to avoid revisiting them"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t2-ex02",
    "subjectId": "cs406",
    "topicId": "cs406-topic-2",
    "title": "A* Search with Manhattan Distance",
    "difficulty": 3,
    "description": "Implement A* search for pathfinding on a 2D grid with obstacles.\n\nYour implementation should:\n- Use Manhattan distance as heuristic\n- Handle obstacles (blocked cells)\n- Return the optimal path\n- Use a priority queue for the frontier",
    "starterCode": "import heapq\n\ndef manhattan_distance(pos1, pos2):\n    # Calculate Manhattan distance\n    pass\n\ndef astar(grid, start, goal):\n    \"\"\"\n    grid: 2D list where 0=free, 1=obstacle\n    start: (x, y) starting position\n    goal: (x, y) goal position\n    Returns: list of (x, y) positions in path, or None\n    \"\"\"\n    # TODO: Implement A*\n    pass\n\n# Example:\n# grid = [\n#     [0, 0, 0, 0, 0],\n#     [0, 1, 1, 1, 0],\n#     [0, 0, 0, 0, 0],\n#     [0, 1, 1, 1, 0],\n#     [0, 0, 0, 0, 0]\n# ]\n# print(astar(grid, (0, 0), (4, 4)))",
    "solution": "import heapq\n\ndef manhattan_distance(pos1, pos2):\n    return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])\n\ndef astar(grid, start, goal):\n    \"\"\"\n    grid: 2D list where 0=free, 1=obstacle\n    start: (x, y) starting position\n    goal: (x, y) goal position\n    Returns: list of (x, y) positions in path, or None\n    \"\"\"\n    rows, cols = len(grid), len(grid[0])\n\n    # Priority queue: (f_score, g_score, position, path)\n    frontier = [(0 + manhattan_distance(start, goal), 0, start, [start])]\n    explored = set()\n\n    while frontier:\n        f_score, g_score, pos, path = heapq.heappop(frontier)\n\n        if pos == goal:\n            return path\n\n        if pos in explored:\n            continue\n\n        explored.add(pos)\n\n        # Explore neighbors (up, down, left, right)\n        x, y = pos\n        for dx, dy in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n            nx, ny = x + dx, y + dy\n\n            # Check bounds\n            if not (0 <= nx < cols and 0 <= ny < rows):\n                continue\n\n            # Check obstacle\n            if grid[ny][nx] == 1:\n                continue\n\n            if (nx, ny) in explored:\n                continue\n\n            new_g = g_score + 1\n            new_h = manhattan_distance((nx, ny), goal)\n            new_f = new_g + new_h\n            new_path = path + [(nx, ny)]\n\n            heapq.heappush(frontier, (new_f, new_g, (nx, ny), new_path))\n\n    return None  # No path found\n\n# Test\ngrid = [\n    [0, 0, 0, 0, 0],\n    [0, 1, 1, 1, 0],\n    [0, 0, 0, 0, 0],\n    [0, 1, 1, 1, 0],\n    [0, 0, 0, 0, 0]\n]\npath = astar(grid, (0, 0), (4, 4))\nprint(path)\nprint(f\"Path length: {len(path) if path else 'No path'}\")",
    "testCases": [
      {
        "input": "astar(grid, (0, 0), (4, 4))",
        "isHidden": false,
        "description": "Test A* finds path in grid with obstacles"
      },
      {
        "input": "manhattan_distance((0, 0), (3, 4))",
        "isHidden": false,
        "description": "Test Manhattan distance calculation"
      },
      {
        "input": "astar(grid, (0, 0), (1, 1))",
        "isHidden": false,
        "description": "Test A* finds short path"
      }
    ],
    "hints": [
      "Manhattan distance is |x1-x2| + |y1-y2|, a common heuristic for grid-based pathfinding",
      "Use a priority queue (heapq) to always expand the node with lowest f = g + h",
      "Remember to check for obstacles and bounds before adding neighbors to the frontier"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t2-ex03",
    "subjectId": "cs406",
    "topicId": "cs406-topic-2",
    "title": "Hill Climbing for 8-Queens",
    "difficulty": 3,
    "description": "Implement hill climbing to solve the 8-queens problem.\n\nYour implementation should:\n- Start with a random configuration\n- Use conflicts as the cost function\n- Implement steepest-ascent hill climbing\n- Detect local minima\n- Support random restarts",
    "starterCode": "import random\n\ndef count_conflicts(board):\n    # Count number of pairs of queens attacking each other\n    pass\n\ndef get_neighbors(board):\n    # Generate all neighbor states (move one queen in its column)\n    pass\n\ndef hill_climbing(n=8, max_restarts=100):\n    # Implement hill climbing with random restarts\n    pass",
    "solution": "import random\n\ndef count_conflicts(board):\n    \"\"\"Count number of pairs of queens attacking each other.\"\"\"\n    n = len(board)\n    conflicts = 0\n\n    for i in range(n):\n        for j in range(i + 1, n):\n            # Same row\n            if board[i] == board[j]:\n                conflicts += 1\n            # Same diagonal\n            if abs(board[i] - board[j]) == abs(i - j):\n                conflicts += 1\n\n    return conflicts\n\ndef get_neighbors(board):\n    \"\"\"Generate all neighbor states by moving one queen in its column.\"\"\"\n    n = len(board)\n    neighbors = []\n\n    for col in range(n):\n        for row in range(n):\n            if row != board[col]:\n                neighbor = board[:]\n                neighbor[col] = row\n                neighbors.append(neighbor)\n\n    return neighbors\n\ndef hill_climbing(n=8, max_restarts=100):\n    \"\"\"Hill climbing with random restarts for n-queens.\"\"\"\n\n    for restart in range(max_restarts):\n        # Random initial state: each queen in random row of its column\n        current = [random.randint(0, n - 1) for _ in range(n)]\n        current_cost = count_conflicts(current)\n\n        while True:\n            if current_cost == 0:\n                return current, restart  # Solution found\n\n            # Find best neighbor\n            neighbors = get_neighbors(current)\n            best_neighbor = None\n            best_cost = current_cost\n\n            for neighbor in neighbors:\n                cost = count_conflicts(neighbor)\n                if cost < best_cost:\n                    best_neighbor = neighbor\n                    best_cost = cost\n\n            # If no improvement, we're stuck (local minimum)\n            if best_neighbor is None:\n                break  # Restart\n\n            current = best_neighbor\n            current_cost = best_cost\n\n    return None, max_restarts  # Failed to find solution\n\n# Test\nsolution, restarts = hill_climbing(8, max_restarts=100)\nif solution:\n    print(f\"Solution found after {restarts} restarts:\")\n    print(solution)\n    print(f\"Conflicts: {count_conflicts(solution)}\")\nelse:\n    print(\"No solution found\")\n\n# Visualize\ndef print_board(board):\n    n = len(board)\n    for row in range(n):\n        line = \"\"\n        for col in range(n):\n            if board[col] == row:\n                line += \"Q \"\n            else:\n                line += \". \"\n        print(line)\n\nif solution:\n    print(\"\\nBoard visualization:\")\n    print_board(solution)",
    "testCases": [
      {
        "input": "hill_climbing(8, max_restarts=100)",
        "isHidden": false,
        "description": "Test hill climbing solves 8-queens"
      },
      {
        "input": "count_conflicts([0,1,2,3,4,5,6,7])",
        "isHidden": false,
        "description": "Test conflict counting for diagonal board"
      },
      {
        "input": "len(get_neighbors([0,0,0,0,0,0,0,0]))",
        "isHidden": false,
        "description": "Test neighbor generation produces correct number of states"
      }
    ],
    "hints": [
      "Count conflicts by checking all pairs of queens for row and diagonal attacks",
      "Steepest-ascent means always picking the best neighbor, not just the first improvement",
      "When stuck in a local minimum (no improving neighbors), restart with a new random state"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t3-ex01",
    "subjectId": "cs406",
    "topicId": "cs406-topic-3",
    "title": "Minimax for Tic-Tac-Toe",
    "difficulty": 2,
    "description": "Implement the Minimax algorithm for Tic-Tac-Toe.\n\nYour implementation should:\n- Evaluate terminal states (+1 for X win, -1 for O win, 0 for draw)\n- Recursively evaluate all possible moves\n- MAX player maximizes, MIN player minimizes\n- Return the best move and its value",
    "starterCode": "def check_winner(board):\n    # Return 'X', 'O', 'draw', or None (game ongoing)\n    pass\n\ndef get_moves(board):\n    # Return list of available positions\n    pass\n\ndef minimax(board, is_max_player):\n    # Implement minimax\n    # Returns: (best_value, best_move)\n    pass\n\n# Board: 3x3 list, with 'X', 'O', or None",
    "solution": "def check_winner(board):\n    \"\"\"Check if there's a winner. Returns 'X', 'O', 'draw', or None.\"\"\"\n    # Check rows\n    for row in board:\n        if row[0] == row[1] == row[2] and row[0] is not None:\n            return row[0]\n\n    # Check columns\n    for col in range(3):\n        if board[0][col] == board[1][col] == board[2][col] and board[0][col] is not None:\n            return board[0][col]\n\n    # Check diagonals\n    if board[0][0] == board[1][1] == board[2][2] and board[0][0] is not None:\n        return board[0][0]\n    if board[0][2] == board[1][1] == board[2][0] and board[0][2] is not None:\n        return board[0][2]\n\n    # Check if board is full (draw)\n    if all(board[i][j] is not None for i in range(3) for j in range(3)):\n        return 'draw'\n\n    return None  # Game ongoing\n\ndef get_moves(board):\n    \"\"\"Return list of available (row, col) positions.\"\"\"\n    moves = []\n    for i in range(3):\n        for j in range(3):\n            if board[i][j] is None:\n                moves.append((i, j))\n    return moves\n\ndef minimax(board, is_max_player):\n    \"\"\"\n    Minimax algorithm for Tic-Tac-Toe.\n    Returns: (best_value, best_move)\n    \"\"\"\n    winner = check_winner(board)\n\n    # Terminal states\n    if winner == 'X':\n        return (1, None)\n    elif winner == 'O':\n        return (-1, None)\n    elif winner == 'draw':\n        return (0, None)\n\n    moves = get_moves(board)\n    player = 'X' if is_max_player else 'O'\n\n    if is_max_player:\n        best_value = float('-inf')\n        best_move = None\n\n        for move in moves:\n            i, j = move\n            board[i][j] = player\n\n            value, _ = minimax(board, False)\n\n            board[i][j] = None  # Undo move\n\n            if value > best_value:\n                best_value = value\n                best_move = move\n\n        return (best_value, best_move)\n    else:\n        best_value = float('inf')\n        best_move = None\n\n        for move in moves:\n            i, j = move\n            board[i][j] = player\n\n            value, _ = minimax(board, True)\n\n            board[i][j] = None  # Undo move\n\n            if value < best_value:\n                best_value = value\n                best_move = move\n\n        return (best_value, best_move)\n\n# Test\nboard = [\n    ['X', 'O', 'X'],\n    ['O', 'X', None],\n    [None, None, 'O']\n]\n\nvalue, move = minimax(board, True)  # X's turn (MAX)\nprint(f\"Best move for X: {move}, value: {value}\")\n\n# Empty board test\nempty_board = [[None]*3 for _ in range(3)]\nvalue, move = minimax(empty_board, True)\nprint(f\"Best first move: {move}, value: {value}\")",
    "testCases": [
      {
        "input": "minimax(board, True)",
        "isHidden": false,
        "description": "Test minimax finds best move for X"
      },
      {
        "input": "check_winner(board)",
        "isHidden": false,
        "description": "Test winner detection"
      },
      {
        "input": "minimax(empty_board, True)",
        "isHidden": false,
        "description": "Test minimax on empty board"
      }
    ],
    "hints": [
      "Base case: return immediately if the game is over (win/loss/draw)",
      "MAX player (X) wants to maximize the value, MIN player (O) wants to minimize",
      "Remember to undo moves after exploring each branch (set cell back to None)"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t3-ex02",
    "subjectId": "cs406",
    "topicId": "cs406-topic-3",
    "title": "Alpha-Beta Pruning",
    "difficulty": 3,
    "description": "Extend Minimax with alpha-beta pruning for efficiency.\n\nYour implementation should:\n- Track alpha (best MAX can guarantee) and beta (best MIN can guarantee)\n- Prune branches when alpha >= beta\n- Count nodes pruned to demonstrate efficiency\n- Return same result as regular minimax",
    "starterCode": "def alpha_beta(board, depth, alpha, beta, is_max_player, nodes_visited):\n    # Implement alpha-beta pruning\n    # Return: (value, move, nodes_visited)\n    pass",
    "solution": "def check_winner(board):\n    \"\"\"Check if there's a winner. Returns 'X', 'O', 'draw', or None.\"\"\"\n    for row in board:\n        if row[0] == row[1] == row[2] and row[0] is not None:\n            return row[0]\n\n    for col in range(3):\n        if board[0][col] == board[1][col] == board[2][col] and board[0][col] is not None:\n            return board[0][col]\n\n    if board[0][0] == board[1][1] == board[2][2] and board[0][0] is not None:\n        return board[0][0]\n    if board[0][2] == board[1][1] == board[2][0] and board[0][2] is not None:\n        return board[0][2]\n\n    if all(board[i][j] is not None for i in range(3) for j in range(3)):\n        return 'draw'\n\n    return None\n\ndef get_moves(board):\n    moves = []\n    for i in range(3):\n        for j in range(3):\n            if board[i][j] is None:\n                moves.append((i, j))\n    return moves\n\ndef alpha_beta(board, depth, alpha, beta, is_max_player, nodes_visited={'count': 0}):\n    \"\"\"\n    Alpha-beta pruning for Tic-Tac-Toe.\n    Returns: (value, move, nodes_visited)\n    \"\"\"\n    nodes_visited['count'] += 1\n\n    winner = check_winner(board)\n\n    # Terminal states\n    if winner == 'X':\n        return (1, None)\n    elif winner == 'O':\n        return (-1, None)\n    elif winner == 'draw':\n        return (0, None)\n\n    moves = get_moves(board)\n    player = 'X' if is_max_player else 'O'\n\n    if is_max_player:\n        best_value = float('-inf')\n        best_move = None\n\n        for move in moves:\n            i, j = move\n            board[i][j] = player\n\n            value, _ = alpha_beta(board, depth + 1, alpha, beta, False, nodes_visited)\n\n            board[i][j] = None\n\n            if value > best_value:\n                best_value = value\n                best_move = move\n\n            alpha = max(alpha, value)\n\n            # Beta cutoff\n            if beta <= alpha:\n                break  # Prune remaining branches\n\n        return (best_value, best_move)\n    else:\n        best_value = float('inf')\n        best_move = None\n\n        for move in moves:\n            i, j = move\n            board[i][j] = player\n\n            value, _ = alpha_beta(board, depth + 1, alpha, beta, True, nodes_visited)\n\n            board[i][j] = None\n\n            if value < best_value:\n                best_value = value\n                best_move = move\n\n            beta = min(beta, value)\n\n            # Alpha cutoff\n            if beta <= alpha:\n                break  # Prune remaining branches\n\n        return (best_value, best_move)\n\n# Test and compare with regular minimax\nboard = [\n    ['X', 'O', 'X'],\n    ['O', None, None],\n    [None, None, 'O']\n]\n\n# With alpha-beta\nnodes_ab = {'count': 0}\nvalue_ab, move_ab = alpha_beta(board, 0, float('-inf'), float('inf'), True, nodes_ab)\nprint(f\"Alpha-Beta: value={value_ab}, move={move_ab}, nodes visited={nodes_ab['count']}\")\n\n# Compare with regular minimax\nfrom topic3_minimax import minimax\nnodes_mm = {'count': 0}\n\ndef minimax_with_count(board, is_max_player, nodes):\n    nodes['count'] += 1\n    winner = check_winner(board)\n    if winner == 'X':\n        return (1, None)\n    elif winner == 'O':\n        return (-1, None)\n    elif winner == 'draw':\n        return (0, None)\n\n    moves = get_moves(board)\n    player = 'X' if is_max_player else 'O'\n\n    if is_max_player:\n        best_value = float('-inf')\n        best_move = None\n        for move in moves:\n            i, j = move\n            board[i][j] = player\n            value, _ = minimax_with_count(board, False, nodes)\n            board[i][j] = None\n            if value > best_value:\n                best_value = value\n                best_move = move\n        return (best_value, best_move)\n    else:\n        best_value = float('inf')\n        best_move = None\n        for move in moves:\n            i, j = move\n            board[i][j] = player\n            value, _ = minimax_with_count(board, True, nodes)\n            board[i][j] = None\n            if value < best_value:\n                best_value = value\n                best_move = move\n        return (best_value, best_move)\n\nvalue_mm, move_mm = minimax_with_count(board, True, nodes_mm)\nprint(f\"Regular Minimax: value={value_mm}, move={move_mm}, nodes visited={nodes_mm['count']}\")\nprint(f\"Pruning efficiency: {100 * (1 - nodes_ab['count'] / nodes_mm['count']):.1f}% fewer nodes\")",
    "testCases": [
      {
        "input": "alpha_beta(board, 0, -inf, +inf, True, nodes)",
        "isHidden": false,
        "description": "Test alpha-beta returns same result as minimax"
      },
      {
        "input": "nodes_ab['count'] < nodes_mm['count']",
        "isHidden": false,
        "description": "Test alpha-beta visits fewer nodes"
      },
      {
        "input": "alpha_beta with pruning",
        "isHidden": false,
        "description": "Test pruning occurs when alpha >= beta"
      }
    ],
    "hints": [
      "Alpha is the best value MAX can guarantee, beta is the best value MIN can guarantee",
      "Prune (break) when beta <= alpha because the opponent won't allow this branch",
      "Always update alpha in MAX nodes and beta in MIN nodes after evaluating children"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t3-ex03",
    "subjectId": "cs406",
    "topicId": "cs406-topic-3",
    "title": "Monte Carlo Tree Search (MCTS)",
    "difficulty": 4,
    "description": "Implement basic MCTS with UCB1 for game playing.\n\nYour implementation should:\n- Implement all four MCTS phases: selection, expansion, simulation, backpropagation\n- Use UCB1 formula for selection\n- Perform random playouts for simulation\n- Update win statistics during backpropagation",
    "starterCode": "import math\nimport random\n\nclass MCTSNode:\n    def __init__(self, state, parent=None):\n        self.state = state\n        self.parent = parent\n        self.children = []\n        self.visits = 0\n        self.wins = 0\n\n    def ucb1(self, c=1.41):\n        # Calculate UCB1 value\n        pass\n\ndef mcts_search(root_state, iterations=1000):\n    # Implement MCTS\n    pass",
    "solution": "import math\nimport random\n\nclass MCTSNode:\n    def __init__(self, state, parent=None, move=None):\n        self.state = state  # Game state\n        self.parent = parent\n        self.move = move  # Move that led to this state\n        self.children = []\n        self.visits = 0\n        self.wins = 0\n        self.untried_moves = self.get_legal_moves()\n\n    def get_legal_moves(self):\n        # Get available moves for current state\n        moves = []\n        for i in range(3):\n            for j in range(3):\n                if self.state[i][j] is None:\n                    moves.append((i, j))\n        return moves\n\n    def ucb1(self, c=1.41):\n        \"\"\"Calculate UCB1 value for this node.\"\"\"\n        if self.visits == 0:\n            return float('inf')\n\n        exploitation = self.wins / self.visits\n        exploration = c * math.sqrt(math.log(self.parent.visits) / self.visits)\n\n        return exploitation + exploration\n\n    def select_child(self):\n        \"\"\"Select child with highest UCB1 value.\"\"\"\n        return max(self.children, key=lambda child: child.ucb1())\n\n    def expand(self):\n        \"\"\"Expand by adding a child for an untried move.\"\"\"\n        move = self.untried_moves.pop()\n        i, j = move\n\n        # Create new state\n        new_state = [row[:] for row in self.state]\n        player = 'X' if self.is_x_turn() else 'O'\n        new_state[i][j] = player\n\n        child = MCTSNode(new_state, parent=self, move=move)\n        self.children.append(child)\n        return child\n\n    def is_x_turn(self):\n        \"\"\"Determine whose turn it is based on piece count.\"\"\"\n        x_count = sum(row.count('X') for row in self.state)\n        o_count = sum(row.count('O') for row in self.state)\n        return x_count == o_count\n\n    def is_terminal(self):\n        \"\"\"Check if state is terminal.\"\"\"\n        return check_winner(self.state) is not None\n\n    def simulate(self):\n        \"\"\"Random playout from this state.\"\"\"\n        state = [row[:] for row in self.state]\n        x_turn = self.is_x_turn()\n\n        while True:\n            winner = check_winner(state)\n            if winner is not None:\n                if winner == 'X':\n                    return 1\n                elif winner == 'O':\n                    return 0\n                else:  # draw\n                    return 0.5\n\n            # Make random move\n            moves = []\n            for i in range(3):\n                for j in range(3):\n                    if state[i][j] is None:\n                        moves.append((i, j))\n\n            i, j = random.choice(moves)\n            state[i][j] = 'X' if x_turn else 'O'\n            x_turn = not x_turn\n\n    def backpropagate(self, result):\n        \"\"\"Backpropagate result up the tree.\"\"\"\n        self.visits += 1\n        self.wins += result\n\n        if self.parent:\n            # Flip result for opponent\n            self.parent.backpropagate(1 - result)\n\ndef check_winner(board):\n    \"\"\"Check winner (same as before).\"\"\"\n    for row in board:\n        if row[0] == row[1] == row[2] and row[0] is not None:\n            return row[0]\n    for col in range(3):\n        if board[0][col] == board[1][col] == board[2][col] and board[0][col] is not None:\n            return board[0][col]\n    if board[0][0] == board[1][1] == board[2][2] and board[0][0] is not None:\n        return board[0][0]\n    if board[0][2] == board[1][1] == board[2][0] and board[0][2] is not None:\n        return board[0][2]\n    if all(board[i][j] is not None for i in range(3) for j in range(3)):\n        return 'draw'\n    return None\n\ndef mcts_search(root_state, iterations=1000):\n    \"\"\"\n    MCTS main search function.\n    Returns best move from root state.\n    \"\"\"\n    root = MCTSNode(root_state)\n\n    for _ in range(iterations):\n        node = root\n\n        # 1. Selection\n        while node.untried_moves == [] and node.children != []:\n            node = node.select_child()\n\n        # 2. Expansion\n        if node.untried_moves != []:\n            node = node.expand()\n\n        # 3. Simulation\n        result = node.simulate()\n\n        # 4. Backpropagation\n        node.backpropagate(result)\n\n    # Return most visited child (most robust)\n    best_child = max(root.children, key=lambda c: c.visits)\n    return best_child.move\n\n# Test\nboard = [\n    ['X', 'O', 'X'],\n    ['O', None, None],\n    [None, None, 'O']\n]\n\nmove = mcts_search(board, iterations=1000)\nprint(f\"MCTS recommends move: {move}\")\n\n# Compare with minimax for validation\nfrom topic3_minimax import minimax\nvalue, minimax_move = minimax(board, True)\nprint(f\"Minimax recommends: {minimax_move}\")",
    "testCases": [
      {
        "input": "mcts_search(board, iterations=1000)",
        "isHidden": false,
        "description": "Test MCTS finds good move"
      },
      {
        "input": "node.ucb1()",
        "isHidden": false,
        "description": "Test UCB1 calculation balances exploration/exploitation"
      },
      {
        "input": "node.simulate()",
        "isHidden": false,
        "description": "Test random playout returns win/loss/draw result"
      }
    ],
    "hints": [
      "UCB1 formula: exploitation + c * sqrt(ln(parent_visits) / node_visits)",
      "Selection phase: traverse tree using UCB1 until reaching a node with unexplored children",
      "Simulation phase: perform random playout from selected node to terminal state",
      "Backpropagation: update visit counts and win statistics for all nodes in path"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t4-ex01",
    "subjectId": "cs406",
    "topicId": "cs406-topic-4",
    "title": "Backtracking Search for CSP",
    "difficulty": 2,
    "description": "Implement backtracking search for constraint satisfaction problems.\n\nYour implementation should:\n- Assign variables one at a time\n- Check constraints after each assignment\n- Backtrack when no legal values remain\n- Solve the N-Queens problem as a test case",
    "starterCode": "def is_consistent(assignment, var, value, constraints):\n    # Check if assigning value to var is consistent\n    pass\n\ndef backtracking_search(variables, domains, constraints):\n    # Implement backtracking search\n    # Returns: complete assignment or None\n    pass\n\n# Example: 4-Queens\n# variables = [0, 1, 2, 3]  # columns\n# domains = {i: [0, 1, 2, 3] for i in range(4)}  # rows\n# constraints = lambda v1, val1, v2, val2: ... # no attacks",
    "solution": "def is_consistent(assignment, var, value, constraints):\n    \"\"\"Check if assigning value to var is consistent with current assignment.\"\"\"\n    for assigned_var, assigned_value in assignment.items():\n        if not constraints(var, value, assigned_var, assigned_value):\n            return False\n    return True\n\ndef backtracking_search(variables, domains, constraints, assignment=None):\n    \"\"\"\n    Backtracking search for CSP.\n    Returns: complete assignment or None if no solution\n    \"\"\"\n    if assignment is None:\n        assignment = {}\n\n    # Check if assignment is complete\n    if len(assignment) == len(variables):\n        return assignment\n\n    # Select unassigned variable\n    unassigned = [v for v in variables if v not in assignment]\n    var = unassigned[0]\n\n    # Try each value in domain\n    for value in domains[var]:\n        if is_consistent(assignment, var, value, constraints):\n            assignment[var] = value\n\n            result = backtracking_search(variables, domains, constraints, assignment)\n\n            if result is not None:\n                return result\n\n            # Backtrack\n            del assignment[var]\n\n    return None  # No solution found\n\n# N-Queens constraints\ndef queens_constraint(col1, row1, col2, row2):\n    \"\"\"Two queens don't attack each other.\"\"\"\n    # Not same row\n    if row1 == row2:\n        return False\n    # Not same diagonal\n    if abs(row1 - row2) == abs(col1 - col2):\n        return False\n    return True\n\n# Solve 4-Queens\nn = 4\nvariables = list(range(n))  # columns\ndomains = {i: list(range(n)) for i in range(n)}  # rows\n\nsolution = backtracking_search(variables, domains, queens_constraint)\n\nif solution:\n    print(\"4-Queens solution:\", solution)\n    # Visualize\n    for row in range(n):\n        line = \"\"\n        for col in range(n):\n            if solution[col] == row:\n                line += \"Q \"\n            else:\n                line += \". \"\n        print(line)\nelse:\n    print(\"No solution found\")\n\n# Test with 8-Queens\nn = 8\nvariables = list(range(n))\ndomains = {i: list(range(n)) for i in range(n)}\n\nsolution = backtracking_search(variables, domains, queens_constraint)\nprint(f\"\\n8-Queens solution found: {solution is not None}\")",
    "testCases": [
      {
        "input": "backtracking_search(variables, domains, queens_constraint)",
        "isHidden": false,
        "description": "Test backtracking solves 4-Queens"
      },
      {
        "input": "is_consistent(assignment, var, value, constraints)",
        "isHidden": false,
        "description": "Test consistency checking"
      },
      {
        "input": "backtracking with 8-Queens",
        "isHidden": false,
        "description": "Test backtracking finds solution for 8-Queens"
      }
    ],
    "hints": [
      "Check consistency by verifying the new assignment doesn't violate constraints with existing assignments",
      "Select any unassigned variable and try values from its domain in order",
      "When a value fails, backtrack by removing it from the assignment and trying the next value"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t4-ex02",
    "subjectId": "cs406",
    "topicId": "cs406-topic-4",
    "title": "AC-3 Constraint Propagation",
    "difficulty": 3,
    "description": "Implement the AC-3 algorithm for arc consistency.\n\nYour implementation should:\n- Maintain a queue of arcs to check\n- Remove inconsistent values from domains\n- Add affected arcs back to queue when domains change\n- Return False if any domain becomes empty",
    "starterCode": "def revise(xi, xj, domains, constraints):\n    # Remove values from domain of xi that are inconsistent with xj\n    # Return True if domain was revised\n    pass\n\ndef ac3(variables, domains, constraints):\n    # Implement AC-3 algorithm\n    # Returns: True if arc consistent, False if inconsistent\n    pass",
    "solution": "from collections import deque\n\ndef revise(xi, xj, domains, constraints):\n    \"\"\"\n    Remove values from domain of xi that are inconsistent with xj.\n    Returns True if domain was revised.\n    \"\"\"\n    revised = False\n\n    # For each value in xi's domain\n    for x in list(domains[xi]):\n        # Check if there exists a value in xj's domain that satisfies constraint\n        satisfies = False\n\n        for y in domains[xj]:\n            if constraints(xi, x, xj, y):\n                satisfies = True\n                break\n\n        # If no value in xj's domain satisfies, remove x from xi's domain\n        if not satisfies:\n            domains[xi].remove(x)\n            revised = True\n\n    return revised\n\ndef ac3(variables, domains, constraints):\n    \"\"\"\n    AC-3 algorithm for arc consistency.\n    Returns: (consistent, domains) where consistent is True if arc consistent\n    \"\"\"\n    # Copy domains to avoid modifying original\n    domains = {var: domain[:] for var, domain in domains.items()}\n\n    # Initialize queue with all arcs\n    queue = deque()\n    for xi in variables:\n        for xj in variables:\n            if xi != xj:\n                queue.append((xi, xj))\n\n    while queue:\n        xi, xj = queue.popleft()\n\n        if revise(xi, xj, domains, constraints):\n            # Domain was revised\n\n            # If domain is empty, no solution exists\n            if len(domains[xi]) == 0:\n                return False, domains\n\n            # Add all arcs (xk, xi) where xk != xi and xk != xj\n            for xk in variables:\n                if xk != xi and xk != xj:\n                    queue.append((xk, xi))\n\n    return True, domains\n\n# Test with N-Queens\ndef queens_constraint(col1, row1, col2, row2):\n    \"\"\"Two queens don't attack each other.\"\"\"\n    if row1 == row2:\n        return False\n    if abs(row1 - row2) == abs(col1 - col2):\n        return False\n    return True\n\n# 4-Queens\nn = 4\nvariables = list(range(n))\ndomains = {i: list(range(n)) for i in range(n)}\n\nprint(\"Initial domains:\", domains)\n\nconsistent, new_domains = ac3(variables, domains, queens_constraint)\n\nprint(f\"Arc consistent: {consistent}\")\nprint(\"Reduced domains:\", new_domains)\n\n# Now use backtracking with reduced domains\nfrom topic4_backtracking import backtracking_search\n\nif consistent:\n    solution = backtracking_search(variables, new_domains, queens_constraint)\n    print(\"Solution:\", solution)\n\n    if solution:\n        for row in range(n):\n            line = \"\"\n            for col in range(n):\n                if solution[col] == row:\n                    line += \"Q \"\n                else:\n                    line += \". \"\n            print(line)\n\n# Test with graph coloring\nprint(\"\\n--- Graph Coloring Test ---\")\n\n# Graph: nodes and edges\nnodes = ['A', 'B', 'C', 'D']\nedges = [('A', 'B'), ('A', 'C'), ('B', 'C'), ('B', 'D'), ('C', 'D')]\ncolors = ['red', 'green', 'blue']\n\ndomains_coloring = {node: colors[:] for node in nodes}\n\ndef coloring_constraint(n1, c1, n2, c2):\n    \"\"\"Adjacent nodes must have different colors.\"\"\"\n    # Check if nodes are adjacent\n    if (n1, n2) in edges or (n2, n1) in edges:\n        return c1 != c2\n    return True\n\nconsistent, new_domains = ac3(nodes, domains_coloring, coloring_constraint)\nprint(f\"Coloring arc consistent: {consistent}\")\nprint(\"Reduced domains:\", new_domains)",
    "testCases": [
      {
        "input": "ac3(variables, domains, queens_constraint)",
        "isHidden": false,
        "description": "Test AC-3 reduces domains for N-Queens"
      },
      {
        "input": "revise(xi, xj, domains, constraints)",
        "isHidden": false,
        "description": "Test revise removes inconsistent values"
      },
      {
        "input": "ac3 with graph coloring",
        "isHidden": false,
        "description": "Test AC-3 with different constraint type"
      }
    ],
    "hints": [
      "Revise removes values from xi's domain that have no supporting value in xj's domain",
      "When a domain is revised, add all arcs (xk, xi) back to the queue to propagate changes",
      "Return False immediately if any domain becomes empty (no solution exists)"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t4-ex03",
    "subjectId": "cs406",
    "topicId": "cs406-topic-4",
    "title": "CSP with MRV and Forward Checking",
    "difficulty": 3,
    "description": "Enhance backtracking with Minimum Remaining Values heuristic and forward checking.\n\nYour implementation should:\n- Use MRV to select variables (most constrained first)\n- Use forward checking to prune domains after each assignment\n- Track inference made during search\n- Restore domains when backtracking",
    "starterCode": "def select_unassigned_variable_mrv(variables, assignment, domains):\n    # Select variable with minimum remaining values\n    pass\n\ndef forward_check(var, value, assignment, domains, constraints):\n    # Check consistency and prune domains\n    # Return: (consistent, inferences)\n    pass\n\ndef backtracking_mrv_fc(variables, domains, constraints):\n    # Backtracking with MRV and forward checking\n    pass",
    "solution": "def select_unassigned_variable_mrv(variables, assignment, domains):\n    \"\"\"Select unassigned variable with minimum remaining values (MRV).\"\"\"\n    unassigned = [v for v in variables if v not in assignment]\n\n    if not unassigned:\n        return None\n\n    # Select variable with smallest domain\n    return min(unassigned, key=lambda var: len(domains[var]))\n\ndef forward_check(var, value, assignment, domains, constraints):\n    \"\"\"\n    Perform forward checking after assigning var = value.\n    Returns: (consistent, inferences) where inferences = {var: [removed_values]}\n    \"\"\"\n    inferences = {}\n\n    for other_var in domains:\n        if other_var == var or other_var in assignment:\n            continue\n\n        removed = []\n\n        for other_value in list(domains[other_var]):\n            if not constraints(var, value, other_var, other_value):\n                removed.append(other_value)\n                domains[other_var].remove(other_value)\n\n        if removed:\n            inferences[other_var] = removed\n\n        # If domain becomes empty, inconsistent\n        if len(domains[other_var]) == 0:\n            return False, inferences\n\n    return True, inferences\n\ndef restore_domains(domains, inferences):\n    \"\"\"Restore domains by adding back inferred removals.\"\"\"\n    for var, removed_values in inferences.items():\n        domains[var].extend(removed_values)\n\ndef backtracking_mrv_fc(variables, domains, constraints, assignment=None, stats=None):\n    \"\"\"\n    Backtracking with MRV variable selection and forward checking.\n    Returns: complete assignment or None\n    \"\"\"\n    if stats is None:\n        stats = {'nodes': 0, 'backtracks': 0}\n\n    if assignment is None:\n        assignment = {}\n\n    stats['nodes'] += 1\n\n    # Check if assignment is complete\n    if len(assignment) == len(variables):\n        return assignment\n\n    # Select variable using MRV\n    var = select_unassigned_variable_mrv(variables, assignment, domains)\n\n    if var is None:\n        return assignment\n\n    # Try each value in domain (could add LCV here)\n    for value in list(domains[var]):\n        # Check consistency\n        consistent = True\n        for assigned_var, assigned_value in assignment.items():\n            if not constraints(var, value, assigned_var, assigned_value):\n                consistent = False\n                break\n\n        if consistent:\n            assignment[var] = value\n\n            # Save domain state\n            old_domain = domains[var]\n            domains[var] = [value]\n\n            # Forward checking\n            fc_consistent, inferences = forward_check(var, value, assignment, domains, constraints)\n\n            if fc_consistent:\n                result = backtracking_mrv_fc(variables, domains, constraints, assignment, stats)\n\n                if result is not None:\n                    return result\n\n            # Backtrack\n            stats['backtracks'] += 1\n            del assignment[var]\n            domains[var] = old_domain\n            restore_domains(domains, inferences)\n\n    return None\n\n# Test with 8-Queens\ndef queens_constraint(col1, row1, col2, row2):\n    if row1 == row2:\n        return False\n    if abs(row1 - row2) == abs(col1 - col2):\n        return False\n    return True\n\nn = 8\nvariables = list(range(n))\ndomains = {i: list(range(n)) for i in range(n)}\n\nstats = {'nodes': 0, 'backtracks': 0}\nsolution = backtracking_mrv_fc(variables, domains, queens_constraint, stats=stats)\n\nprint(f\"8-Queens solution found: {solution is not None}\")\nprint(f\"Nodes explored: {stats['nodes']}\")\nprint(f\"Backtracks: {stats['backtracks']}\")\n\nif solution:\n    print(\"\\nSolution:\")\n    for row in range(n):\n        line = \"\"\n        for col in range(n):\n            if solution[col] == row:\n                line += \"Q \"\n            else:\n                line += \". \"\n        print(line)\n\n# Compare with basic backtracking\nfrom topic4_backtracking import backtracking_search\n\ndomains_basic = {i: list(range(n)) for i in range(n)}\nstats_basic = {'nodes': 0}\n\ndef backtracking_with_stats(variables, domains, constraints, assignment=None, stats=None):\n    if assignment is None:\n        assignment = {}\n    stats['nodes'] += 1\n    if len(assignment) == len(variables):\n        return assignment\n    unassigned = [v for v in variables if v not in assignment]\n    var = unassigned[0]\n    for value in domains[var]:\n        consistent = True\n        for av, aval in assignment.items():\n            if not constraints(var, value, av, aval):\n                consistent = False\n                break\n        if consistent:\n            assignment[var] = value\n            result = backtracking_with_stats(variables, domains, constraints, assignment, stats)\n            if result is not None:\n                return result\n            del assignment[var]\n    return None\n\nsolution_basic = backtracking_with_stats(variables, domains_basic, queens_constraint, stats=stats_basic)\nprint(f\"\\nBasic backtracking nodes: {stats_basic['nodes']}\")\nprint(f\"MRV+FC improvement: {100 * (1 - stats['nodes'] / stats_basic['nodes']):.1f}% fewer nodes\")",
    "testCases": [
      {
        "input": "backtracking_mrv_fc(variables, domains, queens_constraint)",
        "isHidden": false,
        "description": "Test MRV+FC solves 8-Queens"
      },
      {
        "input": "select_unassigned_variable_mrv(variables, assignment, domains)",
        "isHidden": false,
        "description": "Test MRV selects most constrained variable"
      },
      {
        "input": "forward_check(var, value, assignment, domains, constraints)",
        "isHidden": false,
        "description": "Test forward checking prunes domains"
      }
    ],
    "hints": [
      "MRV heuristic: select the variable with the smallest remaining domain (most constrained)",
      "Forward checking: after assigning a variable, remove inconsistent values from neighbors' domains",
      "Remember to restore domains when backtracking by adding back removed values"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t5-ex01",
    "subjectId": "cs406",
    "topicId": "cs406-topic-5",
    "title": "STRIPS Planning Representation",
    "difficulty": 2,
    "description": "Implement a STRIPS-style planning problem representation.\n\nYour implementation should:\n- Define states as sets of propositions\n- Define actions with preconditions and effects\n- Check if an action is applicable in a state\n- Apply actions to generate successor states",
    "starterCode": "class Action:\n    def __init__(self, name, preconditions, add_effects, delete_effects):\n        pass\n\n    def is_applicable(self, state):\n        # Check if action can be applied in state\n        pass\n\n    def apply(self, state):\n        # Apply action to state, return new state\n        pass\n\nclass PlanningProblem:\n    def __init__(self, initial_state, goal, actions):\n        pass\n\n# Example: Blocks World\n# Blocks: A, B, C\n# States: sets like {'on(A,B)', 'on(B,table)', 'clear(A)', 'clear(C)'}",
    "solution": "class Action:\n    def __init__(self, name, preconditions, add_effects, delete_effects):\n        self.name = name\n        self.preconditions = set(preconditions)\n        self.add_effects = set(add_effects)\n        self.delete_effects = set(delete_effects)\n\n    def is_applicable(self, state):\n        \"\"\"Check if all preconditions are satisfied in state.\"\"\"\n        return self.preconditions.issubset(state)\n\n    def apply(self, state):\n        \"\"\"Apply action to state, return new state.\"\"\"\n        if not self.is_applicable(state):\n            return None\n\n        new_state = state.copy()\n        new_state -= self.delete_effects\n        new_state |= self.add_effects\n\n        return new_state\n\n    def __repr__(self):\n        return f\"Action({self.name})\"\n\nclass PlanningProblem:\n    def __init__(self, initial_state, goal, actions):\n        self.initial_state = initial_state\n        self.goal = goal\n        self.actions = actions\n\n    def is_goal(self, state):\n        \"\"\"Check if state satisfies goal.\"\"\"\n        return self.goal.issubset(state)\n\n    def get_applicable_actions(self, state):\n        \"\"\"Return list of actions applicable in state.\"\"\"\n        return [action for action in self.actions if action.is_applicable(state)]\n\n# Blocks World Example\n# Actions: move(X, Y) - move block X onto block Y\n\ndef create_blocks_world():\n    \"\"\"Create a simple blocks world problem.\"\"\"\n\n    initial_state = {\n        'on(A,table)', 'on(B,table)', 'on(C,A)',\n        'clear(C)', 'clear(B)', 'handempty'\n    }\n\n    goal = {\n        'on(A,B)', 'on(B,C)'\n    }\n\n    actions = []\n\n    # Action: pickup(X) - pick up block X from table\n    for block in ['A', 'B', 'C']:\n        actions.append(Action(\n            name=f'pickup({block})',\n            preconditions=[f'on({block},table)', f'clear({block})', 'handempty'],\n            add_effects=[f'holding({block})'],\n            delete_effects=[f'on({block},table)', f'clear({block})', 'handempty']\n        ))\n\n    # Action: putdown(X) - put down block X on table\n    for block in ['A', 'B', 'C']:\n        actions.append(Action(\n            name=f'putdown({block})',\n            preconditions=[f'holding({block})'],\n            add_effects=[f'on({block},table)', f'clear({block})', 'handempty'],\n            delete_effects=[f'holding({block})']\n        ))\n\n    # Action: stack(X, Y) - stack block X on block Y\n    for x in ['A', 'B', 'C']:\n        for y in ['A', 'B', 'C']:\n            if x != y:\n                actions.append(Action(\n                    name=f'stack({x},{y})',\n                    preconditions=[f'holding({x})', f'clear({y})'],\n                    add_effects=[f'on({x},{y})', f'clear({x})', 'handempty'],\n                    delete_effects=[f'holding({x})', f'clear({y})']\n                ))\n\n    # Action: unstack(X, Y) - unstack block X from block Y\n    for x in ['A', 'B', 'C']:\n        for y in ['A', 'B', 'C']:\n            if x != y:\n                actions.append(Action(\n                    name=f'unstack({x},{y})',\n                    preconditions=[f'on({x},{y})', f'clear({x})', 'handempty'],\n                    add_effects=[f'holding({x})', f'clear({y})'],\n                    delete_effects=[f'on({x},{y})', f'clear({x})', 'handempty']\n                ))\n\n    return PlanningProblem(initial_state, goal, actions)\n\n# Test\nproblem = create_blocks_world()\nprint(\"Initial state:\", problem.initial_state)\nprint(\"Goal:\", problem.goal)\nprint(f\"Is goal satisfied? {problem.is_goal(problem.initial_state)}\")\n\n# Test action application\nstate = problem.initial_state\nprint(f\"\\nApplicable actions in initial state:\")\nfor action in problem.get_applicable_actions(state):\n    print(f\"  {action.name}\")\n\n# Apply an action\nunstack_c_a = [a for a in problem.actions if a.name == 'unstack(C,A)'][0]\nnew_state = unstack_c_a.apply(state)\nprint(f\"\\nAfter {unstack_c_a.name}:\")\nprint(new_state)",
    "testCases": [
      {
        "input": "action.is_applicable(state)",
        "isHidden": false,
        "description": "Test action applicability checking"
      },
      {
        "input": "action.apply(state)",
        "isHidden": false,
        "description": "Test action application produces correct successor state"
      },
      {
        "input": "problem.get_applicable_actions(state)",
        "isHidden": false,
        "description": "Test finding all applicable actions in a state"
      }
    ],
    "hints": [
      "States are sets of propositions (strings like \"on(A,B)\", \"clear(C)\")",
      "An action is applicable if all its preconditions are in the current state",
      "Apply an action by removing delete effects and adding add effects to the state"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t5-ex02",
    "subjectId": "cs406",
    "topicId": "cs406-topic-5",
    "title": "Forward State-Space Planning",
    "difficulty": 3,
    "description": "Implement forward state-space search for planning.\n\nYour implementation should:\n- Search forward from initial state\n- Use A* with a planning heuristic\n- Generate successor states by applying actions\n- Return the plan (sequence of actions)",
    "starterCode": "import heapq\n\ndef h_goal_count(state, goal):\n    # Heuristic: number of unsatisfied goal propositions\n    pass\n\ndef forward_search(problem):\n    # Implement forward A* search for planning\n    # Return: list of actions (plan) or None\n    pass",
    "solution": "import heapq\n\ndef h_goal_count(state, goal):\n    \"\"\"Heuristic: number of unsatisfied goal propositions.\"\"\"\n    return len(goal - state)\n\ndef forward_search(problem, heuristic=h_goal_count):\n    \"\"\"\n    Forward state-space A* search for planning.\n    Returns: list of actions (plan) or None if no plan found\n    \"\"\"\n    initial = problem.initial_state\n\n    # Priority queue: (f, g, state, plan)\n    frontier = [(heuristic(initial, problem.goal), 0, initial, [])]\n    explored = set()\n\n    nodes_expanded = 0\n\n    while frontier:\n        f, g, state, plan = heapq.heappop(frontier)\n\n        # Convert state to frozenset for hashing\n        state_key = frozenset(state)\n\n        if state_key in explored:\n            continue\n\n        explored.add(state_key)\n        nodes_expanded += 1\n\n        # Goal check\n        if problem.is_goal(state):\n            print(f\"Plan found! Nodes expanded: {nodes_expanded}\")\n            return plan\n\n        # Expand state\n        for action in problem.get_applicable_actions(state):\n            new_state = action.apply(state)\n\n            if new_state is None:\n                continue\n\n            new_state_key = frozenset(new_state)\n\n            if new_state_key in explored:\n                continue\n\n            new_g = g + 1  # Unit cost per action\n            new_h = heuristic(new_state, problem.goal)\n            new_f = new_g + new_h\n            new_plan = plan + [action]\n\n            heapq.heappush(frontier, (new_f, new_g, new_state, new_plan))\n\n    print(f\"No plan found. Nodes expanded: {nodes_expanded}\")\n    return None\n\n# Test with Blocks World\nfrom topic5_strips import create_blocks_world\n\nproblem = create_blocks_world()\nprint(\"Initial state:\", problem.initial_state)\nprint(\"Goal:\", problem.goal)\n\nplan = forward_search(problem)\n\nif plan:\n    print(f\"\\nPlan ({len(plan)} steps):\")\n    state = problem.initial_state\n    for i, action in enumerate(plan):\n        print(f\"{i+1}. {action.name}\")\n        state = action.apply(state)\n    print(\"\\nFinal state:\", state)\n    print(f\"Goal satisfied: {problem.is_goal(state)}\")\nelse:\n    print(\"No plan found!\")\n\n# Test with more complex delete-relaxation heuristic\ndef h_delete_relaxation(state, goal):\n    \"\"\"\n    Better heuristic: solve delete-relaxed problem.\n    (Simplified version: just count goal propositions)\n    \"\"\"\n    # In full implementation, this would solve the problem\n    # ignoring delete effects to get admissible estimate\n    return len(goal - state)\n\nprint(\"\\n--- Testing with delete-relaxation heuristic ---\")\nplan2 = forward_search(problem, heuristic=h_delete_relaxation)\n\nif plan2:\n    print(f\"Plan length: {len(plan2)}\")",
    "testCases": [
      {
        "input": "forward_search(problem)",
        "isHidden": false,
        "description": "Test forward search finds valid plan"
      },
      {
        "input": "h_goal_count(state, goal)",
        "isHidden": false,
        "description": "Test heuristic counts unsatisfied goals"
      },
      {
        "input": "plan execution reaches goal",
        "isHidden": false,
        "description": "Test executing plan satisfies goal"
      }
    ],
    "hints": [
      "Use A* search with states instead of paths - the g-value is the number of actions taken",
      "The goal-count heuristic counts propositions in the goal that are not in the current state",
      "Use frozenset to make states hashable for the explored set"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t5-ex03",
    "subjectId": "cs406",
    "topicId": "cs406-topic-5",
    "title": "GraphPlan Algorithm",
    "difficulty": 4,
    "description": "Implement a simplified GraphPlan algorithm.\n\nYour implementation should:\n- Build planning graph with alternating proposition/action layers\n- Identify mutex relations between actions\n- Extract solution from the graph\n- Handle multiple levels until fixed point",
    "starterCode": "class PlanningGraph:\n    def __init__(self, problem):\n        self.problem = problem\n        self.prop_layers = []\n        self.action_layers = []\n\n    def expand(self):\n        # Add one proposition and action layer\n        pass\n\n    def extract_solution(self):\n        # Try to extract plan from graph\n        pass\n\ndef graphplan(problem):\n    # Implement GraphPlan\n    pass",
    "solution": "class PlanningGraph:\n    def __init__(self, problem):\n        self.problem = problem\n        self.prop_layers = [problem.initial_state.copy()]\n        self.action_layers = []\n\n    def expand(self):\n        \"\"\"Add one proposition layer and one action layer.\"\"\"\n        if not self.prop_layers:\n            return False\n\n        prev_props = self.prop_layers[-1]\n\n        # Action layer: applicable actions from previous prop layer\n        applicable = []\n        for action in self.problem.actions:\n            if action.is_applicable(prev_props):\n                applicable.append(action)\n\n        # Add no-op actions for each proposition\n        for prop in prev_props:\n            from topic5_strips import Action\n            noop = Action(\n                name=f'no-op({prop})',\n                preconditions=[prop],\n                add_effects=[prop],\n                delete_effects=[]\n            )\n            applicable.append(noop)\n\n        self.action_layers.append(applicable)\n\n        # Proposition layer: effects of all actions in action layer\n        new_props = set()\n        for action in applicable:\n            new_props |= action.add_effects\n\n        self.prop_layers.append(new_props)\n\n        return True\n\n    def all_goals_reachable(self):\n        \"\"\"Check if all goal propositions are in latest layer.\"\"\"\n        if not self.prop_layers:\n            return False\n\n        return self.problem.goal.issubset(self.prop_layers[-1])\n\n    def extract_solution(self, level=None):\n        \"\"\"\n        Try to extract plan from graph at given level.\n        Simplified version: just check if goals are reachable.\n        \"\"\"\n        if level is None:\n            level = len(self.prop_layers) - 1\n\n        if level < 0:\n            return None\n\n        # Check if goals are in this layer\n        if not self.problem.goal.issubset(self.prop_layers[level]):\n            return None\n\n        # For simplicity, find actions that achieve goals\n        # (Full GraphPlan would do backward search with mutex checking)\n\n        plan = []\n        current_goals = self.problem.goal.copy()\n        current_level = level\n\n        while current_level > 0 and current_goals:\n            # Find actions in previous layer that achieve current goals\n            actions = self.action_layers[current_level - 1]\n\n            # Greedy selection (not optimal)\n            selected = []\n            achieved = set()\n\n            for action in actions:\n                if action.name.startswith('no-op'):\n                    continue\n\n                # Check if action achieves any goal\n                if action.add_effects & current_goals:\n                    selected.append(action)\n                    achieved |= action.add_effects\n\n            plan = selected + plan\n            current_goals = set()\n\n            # New goals = preconditions of selected actions\n            for action in selected:\n                current_goals |= action.preconditions\n\n            current_level -= 1\n\n        return plan if current_level == 0 else None\n\n    def has_reached_fixed_point(self):\n        \"\"\"Check if two consecutive prop layers are identical.\"\"\"\n        if len(self.prop_layers) < 2:\n            return False\n\n        return self.prop_layers[-1] == self.prop_layers[-2]\n\ndef graphplan(problem, max_levels=10):\n    \"\"\"\n    GraphPlan algorithm (simplified).\n    Returns: plan (list of actions) or None\n    \"\"\"\n    graph = PlanningGraph(problem)\n\n    for level in range(max_levels):\n        print(f\"Level {level}: {len(graph.prop_layers[-1])} propositions\")\n\n        # Check if all goals are reachable\n        if graph.all_goals_reachable():\n            print(f\"Goals reachable at level {level}\")\n\n            # Try to extract solution\n            plan = graph.extract_solution()\n\n            if plan:\n                return plan\n\n        # Expand graph\n        graph.expand()\n\n        # Check for fixed point\n        if graph.has_reached_fixed_point():\n            print(\"Fixed point reached, no solution exists\")\n            return None\n\n    print(\"Max levels reached\")\n    return None\n\n# Test\nfrom topic5_strips import create_blocks_world\n\nproblem = create_blocks_world()\nprint(\"Initial state:\", problem.initial_state)\nprint(\"Goal:\", problem.goal)\n\nplan = graphplan(problem)\n\nif plan:\n    print(f\"\\nGraphPlan found plan ({len(plan)} steps):\")\n    state = problem.initial_state\n    for i, action in enumerate(plan):\n        print(f\"{i+1}. {action.name}\")\n        state = action.apply(state)\n\n    print(\"\\nFinal state:\", state)\n    print(f\"Goal satisfied: {problem.is_goal(state)}\")\nelse:\n    print(\"\\nNo plan found\")\n\n# Compare with forward search\nfrom topic5_forward import forward_search\nprint(\"\\n--- Comparing with Forward Search ---\")\nplan_forward = forward_search(problem)\nif plan_forward:\n    print(f\"Forward search plan length: {len(plan_forward)}\")\nif plan:\n    print(f\"GraphPlan plan length: {len(plan)}\")",
    "testCases": [
      {
        "input": "graphplan(problem)",
        "isHidden": false,
        "description": "Test GraphPlan finds valid plan"
      },
      {
        "input": "graph.expand()",
        "isHidden": false,
        "description": "Test graph expansion adds proposition and action layers"
      },
      {
        "input": "graph.all_goals_reachable()",
        "isHidden": false,
        "description": "Test goal reachability detection"
      }
    ],
    "hints": [
      "Build alternating layers: propositions, then actions, then propositions, etc.",
      "Include no-op actions for each proposition to maintain persistence",
      "Extract solution by working backwards from the goal layer, selecting actions that achieve goals",
      "Reach fixed point when two consecutive proposition layers are identical (no new propositions)"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t6-ex01",
    "subjectId": "cs406",
    "topicId": "cs406-topic-6",
    "title": "Propositional Logic Evaluation",
    "difficulty": 2,
    "description": "Implement a propositional logic evaluator.\n\nYour implementation should:\n- Parse and evaluate propositional formulas\n- Support operators: AND, OR, NOT, IMPLIES\n- Evaluate formulas given truth assignments\n- Check if formulas are tautologies",
    "starterCode": "class PropFormula:\n    pass\n\nclass Atom(PropFormula):\n    def __init__(self, name):\n        pass\n\nclass Not(PropFormula):\n    def __init__(self, formula):\n        pass\n\nclass And(PropFormula):\n    def __init__(self, left, right):\n        pass\n\ndef evaluate(formula, assignment):\n    # Evaluate formula under given assignment\n    pass\n\ndef is_tautology(formula, variables):\n    # Check if formula is true under all assignments\n    pass",
    "solution": "from itertools import product\n\nclass PropFormula:\n    \"\"\"Base class for propositional formulas.\"\"\"\n    pass\n\nclass Atom(PropFormula):\n    def __init__(self, name):\n        self.name = name\n\n    def evaluate(self, assignment):\n        return assignment.get(self.name, False)\n\n    def __repr__(self):\n        return self.name\n\nclass Not(PropFormula):\n    def __init__(self, formula):\n        self.formula = formula\n\n    def evaluate(self, assignment):\n        return not self.formula.evaluate(assignment)\n\n    def __repr__(self):\n        return f\"¬{self.formula}\"\n\nclass And(PropFormula):\n    def __init__(self, left, right):\n        self.left = left\n        self.right = right\n\n    def evaluate(self, assignment):\n        return self.left.evaluate(assignment) and self.right.evaluate(assignment)\n\n    def __repr__(self):\n        return f\"({self.left} ∧ {self.right})\"\n\nclass Or(PropFormula):\n    def __init__(self, left, right):\n        self.left = left\n        self.right = right\n\n    def evaluate(self, assignment):\n        return self.left.evaluate(assignment) or self.right.evaluate(assignment)\n\n    def __repr__(self):\n        return f\"({self.left} ∨ {self.right})\"\n\nclass Implies(PropFormula):\n    def __init__(self, left, right):\n        self.left = left\n        self.right = right\n\n    def evaluate(self, assignment):\n        return (not self.left.evaluate(assignment)) or self.right.evaluate(assignment)\n\n    def __repr__(self):\n        return f\"({self.left} → {self.right})\"\n\ndef evaluate(formula, assignment):\n    \"\"\"Evaluate formula under given truth assignment.\"\"\"\n    return formula.evaluate(assignment)\n\ndef is_tautology(formula, variables):\n    \"\"\"Check if formula is true under all possible truth assignments.\"\"\"\n    # Generate all possible assignments\n    for values in product([False, True], repeat=len(variables)):\n        assignment = dict(zip(variables, values))\n\n        if not formula.evaluate(assignment):\n            return False, assignment  # Counterexample\n\n    return True, None\n\ndef truth_table(formula, variables):\n    \"\"\"Generate complete truth table for formula.\"\"\"\n    print(\"Truth table for:\", formula)\n    print()\n\n    # Header\n    header = \" | \".join(variables) + \" | Result\"\n    print(header)\n    print(\"-\" * len(header))\n\n    # Rows\n    for values in product([False, True], repeat=len(variables)):\n        assignment = dict(zip(variables, values))\n        result = formula.evaluate(assignment)\n\n        row = \" | \".join(\"T\" if assignment[v] else \"F\" for v in variables)\n        row += \" | \" + (\"T\" if result else \"F\")\n        print(row)\n\n# Test formulas\nP = Atom('P')\nQ = Atom('Q')\nR = Atom('R')\n\n# P ∧ Q\nformula1 = And(P, Q)\nprint(\"Formula 1:\", formula1)\nprint(\"Evaluation (P=T, Q=T):\", evaluate(formula1, {'P': True, 'Q': True}))\nprint(\"Evaluation (P=T, Q=F):\", evaluate(formula1, {'P': True, 'Q': False}))\nprint()\n\n# P → Q (equivalent to ¬P ∨ Q)\nformula2 = Implies(P, Q)\nprint(\"Formula 2:\", formula2)\ntruth_table(formula2, ['P', 'Q'])\nprint()\n\n# Tautology: P ∨ ¬P\nformula3 = Or(P, Not(P))\nprint(\"Formula 3:\", formula3)\nis_taut, counterex = is_tautology(formula3, ['P'])\nprint(f\"Is tautology: {is_taut}\")\nprint()\n\n# Not a tautology: P ∧ Q\nis_taut, counterex = is_tautology(formula1, ['P', 'Q'])\nprint(f\"Formula 1 is tautology: {is_taut}\")\nif counterex:\n    print(f\"Counterexample: {counterex}\")\nprint()\n\n# Modus ponens: ((P → Q) ∧ P) → Q\nformula4 = Implies(And(Implies(P, Q), P), Q)\nprint(\"Formula 4 (Modus Ponens):\", formula4)\nis_taut, counterex = is_tautology(formula4, ['P', 'Q'])\nprint(f\"Is tautology: {is_taut}\")\ntruth_table(formula4, ['P', 'Q'])",
    "testCases": [
      {
        "input": "evaluate(And(P, Q), {\"P\": True, \"Q\": True})",
        "isHidden": false,
        "description": "Test AND evaluation"
      },
      {
        "input": "evaluate(Implies(P, Q), {\"P\": True, \"Q\": False})",
        "isHidden": false,
        "description": "Test IMPLIES evaluation"
      },
      {
        "input": "is_tautology(Or(P, Not(P)), [\"P\"])",
        "isHidden": false,
        "description": "Test tautology detection (law of excluded middle)"
      }
    ],
    "hints": [
      "Implement evaluate() method for each formula class (Atom, Not, And, Or, Implies)",
      "Implies is equivalent to (NOT left) OR right",
      "For tautology checking, generate all possible truth assignments and verify formula is true for all"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t6-ex02",
    "subjectId": "cs406",
    "topicId": "cs406-topic-6",
    "title": "Resolution Theorem Prover",
    "difficulty": 3,
    "description": "Implement a resolution-based theorem prover for propositional logic.\n\nYour implementation should:\n- Convert formulas to CNF (Conjunctive Normal Form)\n- Implement resolution rule\n- Detect when empty clause is derived\n- Prove theorems by refutation",
    "starterCode": "class Clause:\n    def __init__(self, literals):\n        # Clause is a set of literals\n        pass\n\ndef to_cnf(formula):\n    # Convert formula to CNF\n    # Return: set of clauses\n    pass\n\ndef resolve(clause1, clause2):\n    # Apply resolution rule\n    # Return: set of resolvent clauses\n    pass\n\ndef resolution_prove(kb, query):\n    # Prove query from knowledge base using resolution\n    pass",
    "solution": "class Literal:\n    def __init__(self, atom, negated=False):\n        self.atom = atom\n        self.negated = negated\n\n    def negate(self):\n        return Literal(self.atom, not self.negated)\n\n    def __eq__(self, other):\n        return self.atom == other.atom and self.negated == other.negated\n\n    def __hash__(self):\n        return hash((self.atom, self.negated))\n\n    def __repr__(self):\n        return f\"¬{self.atom}\" if self.negated else self.atom\n\nclass Clause:\n    def __init__(self, literals):\n        self.literals = frozenset(literals)\n\n    def is_empty(self):\n        return len(self.literals) == 0\n\n    def __eq__(self, other):\n        return self.literals == other.literals\n\n    def __hash__(self):\n        return hash(self.literals)\n\n    def __repr__(self):\n        if self.is_empty():\n            return \"□\"  # Empty clause\n        return \"{\" + \" ∨ \".join(str(lit) for lit in self.literals) + \"}\"\n\ndef resolve(clause1, clause2):\n    \"\"\"\n    Apply resolution rule to two clauses.\n    Returns: set of resolvent clauses (may be empty)\n    \"\"\"\n    resolvents = set()\n\n    # Try to resolve on each pair of complementary literals\n    for lit1 in clause1.literals:\n        for lit2 in clause2.literals:\n            # Check if complementary\n            if lit1.atom == lit2.atom and lit1.negated != lit2.negated:\n                # Resolve: remove complementary pair, union the rest\n                new_literals = (clause1.literals - {lit1}) | (clause2.literals - {lit2})\n                resolvent = Clause(new_literals)\n                resolvents.add(resolvent)\n\n    return resolvents\n\ndef resolution_prove(clauses, query_clause):\n    \"\"\"\n    Prove query using resolution (by refutation).\n    Negate query and add to KB, derive empty clause.\n\n    clauses: set of Clause objects (KB in CNF)\n    query_clause: Clause to prove\n\n    Returns: (proved, steps) where steps shows derivation\n    \"\"\"\n    # Negate query and add to clauses\n    # (Query is a clause, negating means we try to prove its negation leads to contradiction)\n    clauses = set(clauses)\n    clauses.add(query_clause)\n\n    new_clauses = set()\n    steps = []\n\n    iteration = 0\n    max_iterations = 100\n\n    while iteration < max_iterations:\n        iteration += 1\n\n        # Try resolving all pairs of clauses\n        clause_list = list(clauses)\n\n        for i in range(len(clause_list)):\n            for j in range(i + 1, len(clause_list)):\n                resolvents = resolve(clause_list[i], clause_list[j])\n\n                for resolvent in resolvents:\n                    # Check if empty clause derived\n                    if resolvent.is_empty():\n                        steps.append({\n                            'iteration': iteration,\n                            'clause1': clause_list[i],\n                            'clause2': clause_list[j],\n                            'resolvent': resolvent\n                        })\n                        return True, steps\n\n                    new_clauses.add(resolvent)\n                    steps.append({\n                        'iteration': iteration,\n                        'clause1': clause_list[i],\n                        'clause2': clause_list[j],\n                        'resolvent': resolvent\n                    })\n\n        # Check if no new clauses derived\n        if new_clauses.issubset(clauses):\n            return False, steps  # Cannot prove\n\n        clauses |= new_clauses\n\n    return False, steps  # Max iterations reached\n\n# Test\n# KB: {P → Q, Q → R, P}\n# Prove: R\n\n# Convert to CNF:\n# P → Q  ≡  ¬P ∨ Q\n# Q → R  ≡  ¬Q ∨ R\n# P\n\nP = 'P'\nQ = 'Q'\nR = 'R'\n\nkb = {\n    Clause([Literal(P, negated=True), Literal(Q)]),  # ¬P ∨ Q\n    Clause([Literal(Q, negated=True), Literal(R)]),  # ¬Q ∨ R\n    Clause([Literal(P)])  # P\n}\n\n# Query: R\n# To prove R, we negate it and try to derive empty clause\n# ¬R\nquery = Clause([Literal(R, negated=True)])\n\nprint(\"Knowledge Base:\")\nfor clause in kb:\n    print(f\"  {clause}\")\nprint(f\"\\nQuery (negated): {query}\")\nprint()\n\nproved, steps = resolution_prove(kb, query)\n\nprint(f\"Proved: {proved}\")\nprint(f\"\\nDerivation steps:\")\nfor step in steps[-10:]:  # Show last 10 steps\n    print(f\"  {step['clause1']} + {step['clause2']} → {step['resolvent']}\")\n\n# Another example: Modus Ponens\nprint(\"\\n\" + \"=\"*50)\nprint(\"Example 2: Modus Ponens\")\nprint(\"=\"*50)\n\n# KB: {P → Q, P}\n# Prove: Q\n\nA = 'A'\nB = 'B'\n\nkb2 = {\n    Clause([Literal(A, negated=True), Literal(B)]),  # ¬A ∨ B (A → B)\n    Clause([Literal(A)])  # A\n}\n\nquery2 = Clause([Literal(B, negated=True)])  # Prove B (negate it)\n\nprint(\"Knowledge Base:\")\nfor clause in kb2:\n    print(f\"  {clause}\")\nprint(f\"\\nQuery (negated): {query2}\")\n\nproved2, steps2 = resolution_prove(kb2, query2)\n\nprint(f\"\\nProved: {proved2}\")\nprint(f\"Derivation steps:\")\nfor step in steps2:\n    print(f\"  {step['clause1']} + {step['clause2']} → {step['resolvent']}\")",
    "testCases": [
      {
        "input": "resolve(clause1, clause2)",
        "isHidden": false,
        "description": "Test resolution produces correct resolvent"
      },
      {
        "input": "resolution_prove(kb, query)",
        "isHidden": false,
        "description": "Test resolution proves valid theorem"
      },
      {
        "input": "empty clause derivation",
        "isHidden": false,
        "description": "Test proof by refutation derives empty clause"
      }
    ],
    "hints": [
      "Resolution rule: if one clause contains literal L and another contains ¬L, create new clause with all literals except L and ¬L",
      "To prove a query, negate it and add to KB, then try to derive the empty clause (contradiction)",
      "Keep track of which clauses have been resolved to avoid infinite loops"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t6-ex03",
    "subjectId": "cs406",
    "topicId": "cs406-topic-6",
    "title": "First-Order Logic Unification",
    "difficulty": 4,
    "description": "Implement unification for first-order logic terms.\n\nYour implementation should:\n- Represent FOL terms (constants, variables, functions)\n- Implement occur check\n- Find most general unifier (MGU) for two terms\n- Handle complex nested terms",
    "starterCode": "class Term:\n    pass\n\nclass Constant(Term):\n    pass\n\nclass Variable(Term):\n    pass\n\nclass Function(Term):\n    pass\n\ndef unify(term1, term2, subst=None):\n    # Find most general unifier\n    # Returns: substitution dict or None if unification fails\n    pass\n\ndef apply_subst(term, subst):\n    # Apply substitution to term\n    pass",
    "solution": "class Term:\n    \"\"\"Base class for FOL terms.\"\"\"\n    pass\n\nclass Constant(Term):\n    def __init__(self, name):\n        self.name = name\n\n    def __eq__(self, other):\n        return isinstance(other, Constant) and self.name == other.name\n\n    def __hash__(self):\n        return hash(('const', self.name))\n\n    def __repr__(self):\n        return self.name\n\nclass Variable(Term):\n    def __init__(self, name):\n        self.name = name\n\n    def __eq__(self, other):\n        return isinstance(other, Variable) and self.name == other.name\n\n    def __hash__(self):\n        return hash(('var', self.name))\n\n    def __repr__(self):\n        return f\"?{self.name}\"\n\nclass Function(Term):\n    def __init__(self, name, args):\n        self.name = name\n        self.args = tuple(args)  # List of terms\n\n    def __eq__(self, other):\n        return (isinstance(other, Function) and\n                self.name == other.name and\n                self.args == other.args)\n\n    def __hash__(self):\n        return hash(('func', self.name, self.args))\n\n    def __repr__(self):\n        args_str = \", \".join(str(arg) for arg in self.args)\n        return f\"{self.name}({args_str})\"\n\ndef apply_subst(term, subst):\n    \"\"\"Apply substitution to term.\"\"\"\n    if isinstance(term, Constant):\n        return term\n    elif isinstance(term, Variable):\n        if term in subst:\n            # Recursively apply (for transitive substitutions)\n            return apply_subst(subst[term], subst)\n        return term\n    elif isinstance(term, Function):\n        new_args = [apply_subst(arg, subst) for arg in term.args]\n        return Function(term.name, new_args)\n\ndef occur_check(var, term, subst):\n    \"\"\"Check if var occurs in term (prevents infinite structures).\"\"\"\n    if var == term:\n        return True\n    elif isinstance(term, Variable) and term in subst:\n        return occur_check(var, subst[term], subst)\n    elif isinstance(term, Function):\n        return any(occur_check(var, arg, subst) for arg in term.args)\n    return False\n\ndef unify(term1, term2, subst=None):\n    \"\"\"\n    Find most general unifier for two terms.\n    Returns: substitution dict or None if unification fails\n    \"\"\"\n    if subst is None:\n        subst = {}\n\n    # Apply current substitution\n    term1 = apply_subst(term1, subst)\n    term2 = apply_subst(term2, subst)\n\n    # If identical, already unified\n    if term1 == term2:\n        return subst\n\n    # Variable unification\n    if isinstance(term1, Variable):\n        if occur_check(term1, term2, subst):\n            return None  # Fail\n        subst[term1] = term2\n        return subst\n\n    if isinstance(term2, Variable):\n        if occur_check(term2, term1, subst):\n            return None  # Fail\n        subst[term2] = term1\n        return subst\n\n    # Function unification\n    if isinstance(term1, Function) and isinstance(term2, Function):\n        # Must have same functor and arity\n        if term1.name != term2.name or len(term1.args) != len(term2.args):\n            return None  # Fail\n\n        # Unify arguments pairwise\n        for arg1, arg2 in zip(term1.args, term2.args):\n            subst = unify(arg1, arg2, subst)\n            if subst is None:\n                return None  # Fail\n\n        return subst\n\n    # Constants or mismatched types\n    return None  # Fail\n\n# Test cases\nprint(\"=\"*50)\nprint(\"First-Order Logic Unification Tests\")\nprint(\"=\"*50)\n\n# Test 1: Constant unification\nprint(\"\\nTest 1: Unify constants\")\nc1 = Constant('a')\nc2 = Constant('a')\nc3 = Constant('b')\n\nsubst = unify(c1, c2)\nprint(f\"unify({c1}, {c2}) = {subst}\")\n\nsubst = unify(c1, c3)\nprint(f\"unify({c1}, {c3}) = {subst}\")\n\n# Test 2: Variable unification\nprint(\"\\nTest 2: Unify variables\")\nx = Variable('X')\ny = Variable('Y')\na = Constant('a')\n\nsubst = unify(x, a)\nprint(f\"unify({x}, {a}) = {subst}\")\n\nsubst = unify(x, y)\nprint(f\"unify({x}, {y}) = {subst}\")\n\n# Test 3: Function unification\nprint(\"\\nTest 3: Unify functions\")\n# f(X, a) and f(b, Y)\nf1 = Function('f', [Variable('X'), Constant('a')])\nf2 = Function('f', [Constant('b'), Variable('Y')])\n\nsubst = unify(f1, f2)\nprint(f\"unify({f1}, {f2}) = {subst}\")\n\nif subst:\n    print(f\"  {f1} becomes {apply_subst(f1, subst)}\")\n    print(f\"  {f2} becomes {apply_subst(f2, subst)}\")\n\n# Test 4: Nested functions\nprint(\"\\nTest 4: Nested functions\")\n# g(X, f(X)) and g(f(Y), Y)\nt1 = Function('g', [Variable('X'), Function('f', [Variable('X')])])\nt2 = Function('g', [Function('f', [Variable('Y')]), Variable('Y')])\n\nsubst = unify(t1, t2)\nprint(f\"unify({t1}, {t2}) = {subst}\")\n\nif subst:\n    print(f\"  {t1} becomes {apply_subst(t1, subst)}\")\n    print(f\"  {t2} becomes {apply_subst(t2, subst)}\")\n\n# Test 5: Occur check (should fail)\nprint(\"\\nTest 5: Occur check\")\n# Unify X with f(X) - should fail\nx = Variable('X')\nfx = Function('f', [x])\n\nsubst = unify(x, fx)\nprint(f\"unify({x}, {fx}) = {subst}\")\nprint(\"  (Should fail due to occur check)\")\n\n# Test 6: Complex example\nprint(\"\\nTest 6: Complex example\")\n# loves(X, f(X)) and loves(g(Y), Z)\nt1 = Function('loves', [Variable('X'), Function('f', [Variable('X')])])\nt2 = Function('loves', [Function('g', [Variable('Y')]), Variable('Z')])\n\nsubst = unify(t1, t2)\nprint(f\"unify({t1}, {t2}) = {subst}\")\n\nif subst:\n    print(f\"  {t1} becomes {apply_subst(t1, subst)}\")\n    print(f\"  {t2} becomes {apply_subst(t2, subst)}\")",
    "testCases": [
      {
        "input": "unify(Function(\"f\", [Variable(\"X\")]), Function(\"f\", [Constant(\"a\")]))",
        "isHidden": false,
        "description": "Test unifying function terms"
      },
      {
        "input": "unify(Variable(\"X\"), Variable(\"Y\"))",
        "isHidden": false,
        "description": "Test unifying two variables"
      },
      {
        "input": "unify(Variable(\"X\"), Function(\"f\", [Variable(\"X\")]))",
        "isHidden": false,
        "description": "Test occur check prevents infinite structures"
      }
    ],
    "hints": [
      "Constants unify only with identical constants",
      "Variables unify with anything (but check occur check first)",
      "Functions unify if they have the same name, arity, and all arguments unify",
      "Occur check: variable X cannot unify with a term containing X (like f(X))"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t7-ex01",
    "subjectId": "cs406",
    "topicId": "cs406-topic-7",
    "title": "Bayesian Network Inference",
    "difficulty": 2,
    "description": "Implement exact inference in a simple Bayesian network.\n\nYour implementation should:\n- Represent conditional probability tables (CPTs)\n- Compute joint probabilities\n- Perform inference using enumeration\n- Calculate P(X|evidence)",
    "starterCode": "class BayesianNetwork:\n    def __init__(self):\n        self.variables = []\n        self.parents = {}  # var -> list of parents\n        self.cpts = {}  # var -> CPT\n\n    def add_variable(self, var, parents, cpt):\n        # Add variable with its CPT\n        pass\n\n    def probability(self, var, value, evidence):\n        # Compute P(var=value | evidence)\n        pass\n\n# Example: Simple alarm network\n# Burglary -> Alarm <- Earthquake\n# Alarm -> Call",
    "solution": "from itertools import product\n\nclass BayesianNetwork:\n    def __init__(self):\n        self.variables = []\n        self.parents = {}\n        self.cpts = {}\n\n    def add_variable(self, var, parents, cpt):\n        \"\"\"Add variable with parents and CPT.\"\"\"\n        self.variables.append(var)\n        self.parents[var] = parents\n        self.cpts[var] = cpt\n\n    def get_probability(self, var, value, parent_values):\n        \"\"\"Get P(var=value | parent_values) from CPT.\"\"\"\n        cpt = self.cpts[var]\n\n        if not self.parents[var]:\n            # No parents, unconditional probability\n            return cpt.get(value, 0.0)\n\n        # Convert parent values to tuple key\n        key = tuple(parent_values[p] for p in self.parents[var])\n\n        return cpt.get((key, value), 0.0)\n\n    def enumerate_all(self, variables, evidence):\n        \"\"\"Enumerate all assignments consistent with evidence.\"\"\"\n        if not variables:\n            return 1.0\n\n        var = variables[0]\n        rest = variables[1:]\n\n        if var in evidence:\n            # Variable is observed\n            parent_vals = evidence\n            prob = self.get_probability(var, evidence[var], parent_vals)\n            return prob * self.enumerate_all(rest, evidence)\n        else:\n            # Sum over all possible values\n            total = 0.0\n\n            for value in [True, False]:\n                extended = evidence.copy()\n                extended[var] = value\n\n                parent_vals = extended\n                prob = self.get_probability(var, value, parent_vals)\n                total += prob * self.enumerate_all(rest, extended)\n\n            return total\n\n    def probability(self, query_var, query_value, evidence):\n        \"\"\"Compute P(query_var=query_value | evidence) using enumeration.\"\"\"\n\n        # P(X|e) = P(X,e) / P(e)\n\n        # Compute P(query_var=query_value, evidence)\n        extended = evidence.copy()\n        extended[query_var] = query_value\n        p_x_e = self.enumerate_all(self.variables, extended)\n\n        # Compute P(evidence)\n        p_e = self.enumerate_all(self.variables, evidence)\n\n        if p_e == 0:\n            return 0.0\n\n        return p_x_e / p_e\n\n# Build alarm network\n# Burglary -> Alarm <- Earthquake\n#              |\n#              v\n#            Call\n\nbn = BayesianNetwork()\n\n# P(Burglary)\nbn.add_variable('Burglary', [], {\n    True: 0.001,\n    False: 0.999\n})\n\n# P(Earthquake)\nbn.add_variable('Earthquake', [], {\n    True: 0.002,\n    False: 0.998\n})\n\n# P(Alarm | Burglary, Earthquake)\nbn.add_variable('Alarm', ['Burglary', 'Earthquake'], {\n    ((True, True), True): 0.95,\n    ((True, True), False): 0.05,\n    ((True, False), True): 0.94,\n    ((True, False), False): 0.06,\n    ((False, True), True): 0.29,\n    ((False, True), False): 0.71,\n    ((False, False), True): 0.001,\n    ((False, False), False): 0.999\n})\n\n# P(Call | Alarm)\nbn.add_variable('Call', ['Alarm'], {\n    ((True,), True): 0.70,\n    ((True,), False): 0.30,\n    ((False,), True): 0.05,\n    ((False,), False): 0.95\n})\n\n# Queries\nprint(\"Alarm Network Inference\")\nprint(\"=\"*50)\n\n# Prior probability of burglary\np_b = bn.probability('Burglary', True, {})\nprint(f\"P(Burglary) = {p_b:.4f}\")\n\n# P(Burglary | Call)\np_b_call = bn.probability('Burglary', True, {'Call': True})\nprint(f\"P(Burglary | Call) = {p_b_call:.4f}\")\n\n# P(Burglary | Call, ¬Earthquake)\np_b_call_no_eq = bn.probability('Burglary', True, {'Call': True, 'Earthquake': False})\nprint(f\"P(Burglary | Call, ¬Earthquake) = {p_b_call_no_eq:.4f}\")\n\n# P(Alarm | Burglary, ¬Earthquake)\np_a_b = bn.probability('Alarm', True, {'Burglary': True, 'Earthquake': False})\nprint(f\"P(Alarm | Burglary, ¬Earthquake) = {p_a_b:.4f}\")\n\nprint(\"\\nNote: Burglary probability increases from prior when we observe Call\")",
    "testCases": [
      {
        "input": "bn.probability(\"Burglary\", True, {})",
        "isHidden": false,
        "description": "Test prior probability calculation"
      },
      {
        "input": "bn.probability(\"Burglary\", True, {\"Call\": True})",
        "isHidden": false,
        "description": "Test posterior probability given evidence"
      },
      {
        "input": "bn.enumerate_all(variables, evidence)",
        "isHidden": false,
        "description": "Test enumeration over all variables"
      }
    ],
    "hints": [
      "Use enumeration: sum over all possible assignments consistent with evidence",
      "P(X|e) = P(X,e) / P(e) where P(X,e) and P(e) are computed by enumeration",
      "For each variable, if observed use its value, otherwise sum over both True and False"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t7-ex02",
    "subjectId": "cs406",
    "topicId": "cs406-topic-7",
    "title": "Hidden Markov Model - Viterbi Algorithm",
    "difficulty": 3,
    "description": "Implement the Viterbi algorithm for HMMs to find the most likely state sequence.\n\nYour implementation should:\n- Represent HMM with transition and emission probabilities\n- Use dynamic programming to find best path\n- Backtrack to recover full state sequence\n- Handle log probabilities to avoid underflow",
    "starterCode": "class HMM:\n    def __init__(self, states, observations):\n        self.states = states\n        self.observations = observations\n        self.start_prob = {}\n        self.trans_prob = {}\n        self.emit_prob = {}\n\n    def viterbi(self, observations):\n        # Find most likely state sequence\n        # Returns: (probability, state_sequence)\n        pass",
    "solution": "import math\n\nclass HMM:\n    def __init__(self, states, observations):\n        self.states = states\n        self.observations = observations\n        self.start_prob = {}  # state -> probability\n        self.trans_prob = {}  # (state, state) -> probability\n        self.emit_prob = {}   # (state, observation) -> probability\n\n    def viterbi(self, observations):\n        \"\"\"\n        Viterbi algorithm to find most likely state sequence.\n        Returns: (log_probability, state_sequence)\n        \"\"\"\n        T = len(observations)\n        N = len(self.states)\n\n        # DP table: viterbi[t][state] = (max_prob, best_prev_state)\n        viterbi = [{} for _ in range(T)]\n        backpointer = [{} for _ in range(T)]\n\n        # Initialize (t=0)\n        for state in self.states:\n            start_p = self.start_prob.get(state, 1e-10)\n            emit_p = self.emit_prob.get((state, observations[0]), 1e-10)\n\n            # Use log probabilities to avoid underflow\n            viterbi[0][state] = math.log(start_p) + math.log(emit_p)\n            backpointer[0][state] = None\n\n        # Forward pass (t=1 to T-1)\n        for t in range(1, T):\n            for curr_state in self.states:\n                max_prob = float('-inf')\n                best_prev = None\n\n                for prev_state in self.states:\n                    trans_p = self.trans_prob.get((prev_state, curr_state), 1e-10)\n                    prob = viterbi[t-1][prev_state] + math.log(trans_p)\n\n                    if prob > max_prob:\n                        max_prob = prob\n                        best_prev = prev_state\n\n                emit_p = self.emit_prob.get((curr_state, observations[t]), 1e-10)\n                viterbi[t][curr_state] = max_prob + math.log(emit_p)\n                backpointer[t][curr_state] = best_prev\n\n        # Find best final state\n        max_prob = float('-inf')\n        best_final = None\n\n        for state in self.states:\n            if viterbi[T-1][state] > max_prob:\n                max_prob = viterbi[T-1][state]\n                best_final = state\n\n        # Backtrack to recover path\n        path = [best_final]\n        for t in range(T-1, 0, -1):\n            path.insert(0, backpointer[t][path[0]])\n\n        return max_prob, path\n\n    def forward(self, observations):\n        \"\"\"\n        Forward algorithm to compute P(observations).\n        Returns: log probability\n        \"\"\"\n        T = len(observations)\n\n        # Forward table: forward[t][state] = P(o_1:t, state_t)\n        forward = [{} for _ in range(T)]\n\n        # Initialize\n        for state in self.states:\n            start_p = self.start_prob.get(state, 1e-10)\n            emit_p = self.emit_prob.get((state, observations[0]), 1e-10)\n            forward[0][state] = math.log(start_p) + math.log(emit_p)\n\n        # Forward pass\n        for t in range(1, T):\n            for curr_state in self.states:\n                log_sum = float('-inf')\n\n                for prev_state in self.states:\n                    trans_p = self.trans_prob.get((prev_state, curr_state), 1e-10)\n                    prob = forward[t-1][prev_state] + math.log(trans_p)\n\n                    # Log-sum-exp trick\n                    if log_sum == float('-inf'):\n                        log_sum = prob\n                    else:\n                        log_sum = max(log_sum, prob) + math.log(1 + math.exp(min(log_sum, prob) - max(log_sum, prob)))\n\n                emit_p = self.emit_prob.get((curr_state, observations[t]), 1e-10)\n                forward[t][curr_state] = log_sum + math.log(emit_p)\n\n        # Sum over final states\n        total = float('-inf')\n        for state in self.states:\n            if total == float('-inf'):\n                total = forward[T-1][state]\n            else:\n                p = forward[T-1][state]\n                total = max(total, p) + math.log(1 + math.exp(min(total, p) - max(total, p)))\n\n        return total\n\n# Example: Weather HMM\n# States: Sunny, Rainy\n# Observations: Walk, Shop, Clean\n\nhmm = HMM(\n    states=['Sunny', 'Rainy'],\n    observations=['Walk', 'Shop', 'Clean']\n)\n\n# Start probabilities\nhmm.start_prob = {\n    'Sunny': 0.6,\n    'Rainy': 0.4\n}\n\n# Transition probabilities\nhmm.trans_prob = {\n    ('Sunny', 'Sunny'): 0.7,\n    ('Sunny', 'Rainy'): 0.3,\n    ('Rainy', 'Sunny'): 0.4,\n    ('Rainy', 'Rainy'): 0.6\n}\n\n# Emission probabilities\nhmm.emit_prob = {\n    ('Sunny', 'Walk'): 0.6,\n    ('Sunny', 'Shop'): 0.3,\n    ('Sunny', 'Clean'): 0.1,\n    ('Rainy', 'Walk'): 0.1,\n    ('Rainy', 'Shop'): 0.4,\n    ('Rainy', 'Clean'): 0.5\n}\n\n# Observations\nobs = ['Walk', 'Shop', 'Clean']\n\nprint(\"Weather HMM\")\nprint(\"=\"*50)\nprint(f\"Observations: {obs}\")\nprint()\n\n# Viterbi - most likely state sequence\nlog_prob, path = hmm.viterbi(obs)\nprint(f\"Most likely state sequence: {path}\")\nprint(f\"Log probability: {log_prob:.4f}\")\nprint(f\"Probability: {math.exp(log_prob):.6e}\")\nprint()\n\n# Forward algorithm - total probability\ntotal_log_prob = hmm.forward(obs)\nprint(f\"Total P(observations) [log]: {total_log_prob:.4f}\")\nprint(f\"Total P(observations): {math.exp(total_log_prob):.6e}\")\n\n# Try different observation sequence\nobs2 = ['Clean', 'Clean', 'Clean']\nlog_prob2, path2 = hmm.viterbi(obs2)\nprint(f\"\\nObservations: {obs2}\")\nprint(f\"Most likely state sequence: {path2}\")\nprint(f\"Probability: {math.exp(log_prob2):.6e}\")",
    "testCases": [
      {
        "input": "hmm.viterbi(observations)",
        "isHidden": false,
        "description": "Test Viterbi finds most likely state sequence"
      },
      {
        "input": "hmm.forward(observations)",
        "isHidden": false,
        "description": "Test forward algorithm computes total probability"
      },
      {
        "input": "viterbi with different observations",
        "isHidden": false,
        "description": "Test Viterbi produces sensible paths"
      }
    ],
    "hints": [
      "Use dynamic programming: viterbi[t][s] = max probability of path ending in state s at time t",
      "Use log probabilities to avoid numerical underflow (multiply becomes add in log space)",
      "Store backpointers to recover the best path after forward pass completes",
      "Backtrack from the best final state to reconstruct the complete path"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t7-ex03",
    "subjectId": "cs406",
    "topicId": "cs406-topic-7",
    "title": "Particle Filter for Robot Localization",
    "difficulty": 4,
    "description": "Implement a particle filter for robot localization.\n\nYour implementation should:\n- Represent belief as a set of weighted particles\n- Implement prediction step (motion model)\n- Implement update step (sensor model)\n- Resample particles based on weights\n- Estimate position from particle cloud",
    "starterCode": "import random\nimport math\n\nclass Particle:\n    def __init__(self, x, y, weight=1.0):\n        self.x = x\n        self.y = y\n        self.weight = weight\n\nclass ParticleFilter:\n    def __init__(self, num_particles, world_size):\n        self.particles = []\n        self.world_size = world_size\n\n    def predict(self, motion):\n        # Move particles according to motion model\n        pass\n\n    def update(self, measurement, landmarks):\n        # Update particle weights based on sensor measurement\n        pass\n\n    def resample(self):\n        # Resample particles based on weights\n        pass\n\n    def estimate(self):\n        # Estimate robot position from particles\n        pass",
    "solution": "import random\nimport math\n\nclass Particle:\n    def __init__(self, x, y, weight=1.0):\n        self.x = x\n        self.y = y\n        self.weight = weight\n\n    def __repr__(self):\n        return f\"Particle({self.x:.2f}, {self.y:.2f}, w={self.weight:.4f})\"\n\nclass ParticleFilter:\n    def __init__(self, num_particles, world_size):\n        self.num_particles = num_particles\n        self.world_size = world_size\n\n        # Initialize particles uniformly\n        self.particles = []\n        for _ in range(num_particles):\n            x = random.uniform(0, world_size[0])\n            y = random.uniform(0, world_size[1])\n            self.particles.append(Particle(x, y, 1.0 / num_particles))\n\n    def predict(self, motion, noise_std=1.0):\n        \"\"\"\n        Move particles according to motion model.\n        motion: (dx, dy) movement\n        noise_std: standard deviation of motion noise\n        \"\"\"\n        dx, dy = motion\n\n        for particle in self.particles:\n            # Add noise to motion\n            noise_x = random.gauss(0, noise_std)\n            noise_y = random.gauss(0, noise_std)\n\n            particle.x += dx + noise_x\n            particle.y += dy + noise_y\n\n            # Keep particles in world bounds (wrap around or clip)\n            particle.x = particle.x % self.world_size[0]\n            particle.y = particle.y % self.world_size[1]\n\n    def update(self, measurement, landmarks, sensor_noise=2.0):\n        \"\"\"\n        Update particle weights based on sensor measurement.\n        measurement: list of distances to landmarks\n        landmarks: list of (x, y) landmark positions\n        sensor_noise: standard deviation of sensor noise\n        \"\"\"\n        weights = []\n\n        for particle in self.particles:\n            # Compute expected measurements for this particle\n            weight = 1.0\n\n            for i, (lx, ly) in enumerate(landmarks):\n                # Expected distance to landmark\n                expected_dist = math.sqrt((particle.x - lx)**2 + (particle.y - ly)**2)\n\n                # Actual measured distance\n                measured_dist = measurement[i]\n\n                # Compute likelihood using Gaussian\n                diff = expected_dist - measured_dist\n                weight *= math.exp(-(diff**2) / (2 * sensor_noise**2))\n\n            particle.weight = weight\n            weights.append(weight)\n\n        # Normalize weights\n        total_weight = sum(weights)\n\n        if total_weight > 0:\n            for particle in self.particles:\n                particle.weight /= total_weight\n\n    def resample(self):\n        \"\"\"Resample particles based on weights (importance sampling).\"\"\"\n        weights = [p.weight for p in self.particles]\n\n        # Systematic resampling\n        new_particles = []\n\n        # Compute cumulative weights\n        cumulative = []\n        cum_sum = 0\n        for w in weights:\n            cum_sum += w\n            cumulative.append(cum_sum)\n\n        # Generate uniform random samples\n        step = 1.0 / self.num_particles\n        start = random.uniform(0, step)\n\n        for i in range(self.num_particles):\n            target = start + i * step\n\n            # Find particle\n            for j, cum_w in enumerate(cumulative):\n                if target <= cum_w:\n                    # Copy particle\n                    p = self.particles[j]\n                    new_particles.append(Particle(p.x, p.y, 1.0 / self.num_particles))\n                    break\n\n        self.particles = new_particles\n\n    def estimate(self):\n        \"\"\"Estimate robot position as weighted mean of particles.\"\"\"\n        x_est = sum(p.x * p.weight for p in self.particles)\n        y_est = sum(p.y * p.weight for p in self.particles)\n\n        return (x_est, y_est)\n\n    def effective_sample_size(self):\n        \"\"\"Compute effective sample size (measure of particle diversity).\"\"\"\n        return 1.0 / sum(p.weight**2 for p in self.particles)\n\n# Simulation\nprint(\"Particle Filter for Robot Localization\")\nprint(\"=\"*50)\n\n# World setup\nworld_size = (100, 100)\nlandmarks = [(20, 20), (80, 20), (20, 80), (80, 80)]\n\n# True robot position\ntrue_x, true_y = 50, 50\n\n# Create particle filter\npf = ParticleFilter(num_particles=1000, world_size=world_size)\n\nprint(f\"World size: {world_size}\")\nprint(f\"Landmarks: {landmarks}\")\nprint(f\"True position: ({true_x}, {true_y})\")\nprint()\n\n# Initial estimate\nest_x, est_y = pf.estimate()\nerror = math.sqrt((est_x - true_x)**2 + (est_y - true_y)**2)\nprint(f\"Initial estimate: ({est_x:.2f}, {est_y:.2f})\")\nprint(f\"Initial error: {error:.2f}\")\nprint(f\"Effective sample size: {pf.effective_sample_size():.1f}\")\nprint()\n\n# Simulate robot motion and sensing\nmotions = [(5, 0), (0, 5), (-5, 0), (0, -5)]\n\nfor step, motion in enumerate(motions):\n    print(f\"Step {step + 1}: Motion {motion}\")\n\n    # Update true position\n    true_x += motion[0]\n    true_y += motion[1]\n\n    # Predict step\n    pf.predict(motion, noise_std=1.0)\n\n    # Generate measurement (with noise)\n    measurement = []\n    for lx, ly in landmarks:\n        true_dist = math.sqrt((true_x - lx)**2 + (true_y - ly)**2)\n        noisy_dist = true_dist + random.gauss(0, 2.0)\n        measurement.append(noisy_dist)\n\n    # Update step\n    pf.update(measurement, landmarks, sensor_noise=2.0)\n\n    # Resample if needed (when particle diversity is low)\n    if pf.effective_sample_size() < pf.num_particles / 2:\n        pf.resample()\n\n    # Estimate\n    est_x, est_y = pf.estimate()\n    error = math.sqrt((est_x - true_x)**2 + (est_y - true_y)**2)\n\n    print(f\"  True position: ({true_x:.2f}, {true_y:.2f})\")\n    print(f\"  Estimated position: ({est_x:.2f}, {est_y:.2f})\")\n    print(f\"  Error: {error:.2f}\")\n    print(f\"  Effective sample size: {pf.effective_sample_size():.1f}\")\n    print()\n\nprint(\"Note: Error decreases as particles converge to true position\")",
    "testCases": [
      {
        "input": "pf.predict(motion, noise_std)",
        "isHidden": false,
        "description": "Test prediction step moves particles according to motion model"
      },
      {
        "input": "pf.update(measurement, landmarks, sensor_noise)",
        "isHidden": false,
        "description": "Test update step weights particles by measurement likelihood"
      },
      {
        "input": "pf.resample()",
        "isHidden": false,
        "description": "Test resampling concentrates particles in high-probability regions"
      }
    ],
    "hints": [
      "Prediction: move each particle according to motion command plus Gaussian noise",
      "Update: weight each particle by how well its expected measurements match actual measurements",
      "Use Gaussian likelihood: exp(-(difference²)/(2*sigma²)) for sensor model",
      "Resample when effective sample size drops below threshold to avoid particle depletion"
    ],
    "language": "python"
  }
]