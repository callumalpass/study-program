[
  {
    "id": "cs405-exam-midterm",
    "subjectId": "cs405",
    "title": "CS405 Midterm Exam",
    "durationMinutes": 90,
    "instructions": [
      "Covers Topics 1-3: Cloud Fundamentals, Virtualization, Containers/Docker",
      "Mixture of multiple choice, true/false, code analysis, and short answer questions",
      "Passing score: 70% or higher",
      "Read questions carefully and select the BEST answer"
    ],
    "questions": [
      {
        "id": "mid-q1",
        "type": "multiple_choice",
        "prompt": "According to the NIST definition, which characteristic means cloud resources can be rapidly provisioned and released with minimal management effort?",
        "options": [
          "Broad network access",
          "Resource pooling",
          "Rapid elasticity",
          "Measured service"
        ],
        "correctAnswer": 2,
        "explanation": "Rapid elasticity means resources can be quickly scaled up or down, often automatically, to meet changing demand. This is a core characteristic that enables cloud's flexibility."
      },
      {
        "id": "mid-q2",
        "type": "multiple_choice",
        "prompt": "Which service model gives customers the MOST control over infrastructure configuration?",
        "options": [
          "SaaS",
          "PaaS",
          "IaaS",
          "FaaS"
        ],
        "correctAnswer": 2,
        "explanation": "IaaS provides virtual machines where customers control the OS, middleware, and applications. PaaS abstracts the OS, SaaS provides complete applications, and FaaS is event-driven code execution."
      },
      {
        "id": "mid-q3",
        "type": "true_false",
        "prompt": "In a hybrid cloud deployment model, data and applications can move between on-premises and public cloud environments.",
        "correctAnswer": true,
        "explanation": "Hybrid cloud integrates on-premises infrastructure with public cloud, enabling workload portability and data movement between environments for flexibility and optimization."
      },
      {
        "id": "mid-q4",
        "type": "multiple_choice",
        "prompt": "What is the primary advantage of converting IT costs from CapEx to OpEx?",
        "options": [
          "Eliminates all costs",
          "Better cash flow and no large upfront investment",
          "Reduces employee count",
          "Guarantees cost savings"
        ],
        "correctAnswer": 1,
        "explanation": "Converting to OpEx eliminates large capital investments for hardware, improving cash flow and aligning costs with actual usage. Organizations pay as they go rather than investing upfront."
      },
      {
        "id": "mid-q5",
        "type": "multiple_choice",
        "prompt": "Reserved instances typically offer what discount range compared to on-demand pricing?",
        "options": [
          "5-10%",
          "10-30%",
          "30-75%",
          "80-90%"
        ],
        "correctAnswer": 2,
        "explanation": "Reserved instances offer 30-75% discounts in exchange for 1 or 3-year commitments. Actual discount depends on commitment length, payment option, and instance convertibility."
      },
      {
        "id": "mid-q6",
        "type": "true_false",
        "prompt": "In the shared responsibility model for IaaS, the cloud provider is responsible for patching the guest operating system.",
        "correctAnswer": false,
        "explanation": "In IaaS, customers are responsible for managing the guest OS including patching. The provider manages the hypervisor and physical infrastructure, but OS maintenance is a customer responsibility."
      },
      {
        "id": "mid-q7",
        "type": "multiple_choice",
        "prompt": "Which cloud provider currently holds the largest market share?",
        "options": [
          "Microsoft Azure",
          "Google Cloud Platform",
          "Amazon Web Services",
          "IBM Cloud"
        ],
        "correctAnswer": 2,
        "explanation": "AWS holds approximately 32% of the cloud infrastructure market, followed by Azure (~23%) and Google Cloud (~11%). AWS pioneered cloud computing with EC2 and S3 in 2006."
      },
      {
        "id": "mid-q8",
        "type": "fill_blank",
        "prompt": "The practice of bringing financial accountability to cloud spending through collaboration between engineering, finance, and business is called ____.",
        "correctAnswer": "finops",
        "explanation": "FinOps (Cloud Financial Operations) is the practice of managing cloud costs through collaboration, visibility, and optimization, treating cloud spending as a variable cost model."
      },
      {
        "id": "mid-q9",
        "type": "code_output",
        "prompt": "If a service has 99.95% uptime SLA, approximately how many hours of downtime per year is allowed?",
        "codeSnippet": "# Year = 365 days = 8,760 hours\n# Downtime = 8,760 × (1 - 0.9995)\n# = 8,760 × 0.0005",
        "correctAnswer": "4.38",
        "explanation": "99.95% uptime means 0.05% downtime is acceptable. For a year: 8,760 hours × 0.0005 = 4.38 hours of allowable downtime annually."
      },
      {
        "id": "mid-q10",
        "type": "multiple_choice",
        "prompt": "What is the main difference between multi-cloud and hybrid cloud?",
        "options": [
          "Multi-cloud uses multiple public clouds, hybrid combines public and private",
          "Multi-cloud is cheaper than hybrid",
          "Hybrid cloud uses only one provider",
          "They are the same thing"
        ],
        "correctAnswer": 0,
        "explanation": "Multi-cloud uses multiple public cloud providers (AWS + Azure + GCP). Hybrid cloud combines public cloud with private/on-premises infrastructure."
      },
      {
        "id": "mid-q11",
        "type": "multiple_choice",
        "prompt": "What is the key architectural difference between Type 1 and Type 2 hypervisors?",
        "options": [
          "Type 1 is faster than Type 2",
          "Type 1 runs on bare metal, Type 2 runs on a host OS",
          "Type 1 is open source, Type 2 is commercial",
          "Type 1 supports more VMs than Type 2"
        ],
        "correctAnswer": 1,
        "explanation": "Type 1 hypervisors run directly on hardware (VMware ESXi, Hyper-V, KVM). Type 2 hypervisors run as applications on a host OS (VirtualBox, VMware Workstation)."
      },
      {
        "id": "mid-q12",
        "type": "true_false",
        "prompt": "Hardware-assisted virtualization (Intel VT-x, AMD-V) is required for modern virtualization to function.",
        "correctAnswer": false,
        "explanation": "While hardware assistance greatly improves performance (reducing overhead to 2-10%), virtualization can work without it using binary translation or paravirtualization, albeit with higher overhead."
      },
      {
        "id": "mid-q13",
        "type": "multiple_choice",
        "prompt": "Which hypervisor is built into the Linux kernel?",
        "options": [
          "VMware ESXi",
          "Xen",
          "KVM",
          "Hyper-V"
        ],
        "correctAnswer": 2,
        "explanation": "KVM (Kernel-based Virtual Machine) is integrated into the Linux kernel, turning Linux into a Type 1 hypervisor. It's used by many cloud providers."
      },
      {
        "id": "mid-q14",
        "type": "multiple_choice",
        "prompt": "What does vMotion (Live Migration) enable?",
        "options": [
          "Faster VM boot times",
          "Moving running VMs between hosts without downtime",
          "Automatic VM backups",
          "Converting physical to virtual machines"
        ],
        "correctAnswer": 1,
        "explanation": "vMotion/Live Migration moves running VMs from one host to another with minimal downtime (milliseconds), enabling maintenance without service interruption."
      },
      {
        "id": "mid-q15",
        "type": "true_false",
        "prompt": "Memory ballooning allows the hypervisor to reclaim unused memory from VMs.",
        "correctAnswer": true,
        "explanation": "Memory ballooning uses a driver in the guest OS to \"inflate\" and reclaim memory when the host needs it, avoiding hypervisor swapping. The guest decides what memory to release."
      },
      {
        "id": "mid-q16",
        "type": "multiple_choice",
        "prompt": "What is a vCPU?",
        "options": [
          "A physical CPU core",
          "A virtual CPU thread allocated to a VM",
          "A type of CPU cache",
          "A CPU scheduling algorithm"
        ],
        "correctAnswer": 1,
        "explanation": "A vCPU (virtual CPU) represents CPU resources allocated to a VM. Each vCPU maps to physical CPU threads, and the hypervisor time-slices physical CPUs among vCPUs."
      },
      {
        "id": "mid-q17",
        "type": "fill_blank",
        "prompt": "Allocating more virtual resources (vCPUs, memory) than physically available is called ____.",
        "correctAnswer": "over-subscription",
        "explanation": "Over-subscription (or over-commitment) allocates more virtual resources than physical capacity, betting that not all VMs will use maximum resources simultaneously."
      },
      {
        "id": "mid-q18",
        "type": "multiple_choice",
        "prompt": "Which provisioning type allocates disk space on-demand as data is written?",
        "options": [
          "Thick provisioning",
          "Thin provisioning",
          "Lazy zeroed",
          "Eager zeroed"
        ],
        "correctAnswer": 1,
        "explanation": "Thin provisioning allocates space on demand, improving efficiency. Thick provisioning allocates full space upfront. Thin requires monitoring to avoid datastore exhaustion."
      },
      {
        "id": "mid-q19",
        "type": "true_false",
        "prompt": "Snapshots are an appropriate long-term backup solution for virtual machines.",
        "correctAnswer": false,
        "explanation": "Snapshots are point-in-time captures for short-term use (testing, updates). They can degrade performance and shouldn't replace proper backups. Delete after use."
      },
      {
        "id": "mid-q20",
        "type": "multiple_choice",
        "prompt": "What is the typical CPU overhead of modern virtualization with hardware support?",
        "options": [
          "0%",
          "2-10%",
          "20-30%",
          "50%"
        ],
        "correctAnswer": 1,
        "explanation": "Modern hypervisors with hardware virtualization extensions (VT-x/AMD-V) introduce only 2-10% CPU overhead, making virtualization very efficient."
      },
      {
        "id": "mid-q21",
        "type": "multiple_choice",
        "prompt": "What is the fundamental difference between containers and virtual machines?",
        "options": [
          "Containers are slower than VMs",
          "Containers share the host OS kernel, VMs have separate kernels",
          "VMs use less resources than containers",
          "Containers require a hypervisor, VMs do not"
        ],
        "correctAnswer": 1,
        "explanation": "Containers share the host OS kernel and provide process-level isolation (via namespaces). VMs have complete OS with separate kernels, providing stronger but heavier isolation."
      },
      {
        "id": "mid-q22",
        "type": "true_false",
        "prompt": "Linux namespaces provide resource limiting (CPU, memory) for containers.",
        "correctAnswer": false,
        "explanation": "Namespaces provide isolation (PID, network, filesystem, etc.). Control groups (cgroups) provide resource limiting. Together they form the foundation of Linux containers."
      },
      {
        "id": "mid-q23",
        "type": "multiple_choice",
        "prompt": "What is the relationship between Docker images and containers?",
        "options": [
          "They are identical",
          "Image is the running instance, container is the template",
          "Image is the template, container is the running instance",
          "Images are bigger than containers"
        ],
        "correctAnswer": 2,
        "explanation": "A Docker image is a read-only template with application code and dependencies. A container is a running instance of an image with a writable layer on top."
      },
      {
        "id": "mid-q24",
        "type": "multiple_choice",
        "prompt": "Which Dockerfile instruction creates a new image layer?",
        "options": [
          "ENV",
          "RUN",
          "EXPOSE",
          "LABEL"
        ],
        "correctAnswer": 1,
        "explanation": "RUN, COPY, and ADD create new layers. ENV, EXPOSE, LABEL, and other metadata instructions don't create layers. Minimize layers by combining RUN commands."
      },
      {
        "id": "mid-q25",
        "type": "true_false",
        "prompt": "Multi-stage Dockerfile builds help reduce final image size by leaving build tools in earlier stages.",
        "correctAnswer": true,
        "explanation": "Multi-stage builds allow using compilers and build tools in early stages while copying only necessary artifacts (binaries, dependencies) to the final stage, significantly reducing image size."
      },
      {
        "id": "mid-q26",
        "type": "multiple_choice",
        "prompt": "What is the default Docker networking mode?",
        "options": [
          "host",
          "bridge",
          "overlay",
          "none"
        ],
        "correctAnswer": 1,
        "explanation": "Bridge is the default network mode. Containers get private IPs on a virtual bridge network and can communicate with each other. Port mapping (-p) exposes ports to the host."
      },
      {
        "id": "mid-q27",
        "type": "fill_blank",
        "prompt": "The recommended way to persist data in Docker that survives container deletion is using Docker ____.",
        "correctAnswer": "volumes",
        "explanation": "Docker volumes are the recommended persistence mechanism. They're managed by Docker, exist outside containers, and survive container deletion. Bind mounts are an alternative."
      },
      {
        "id": "mid-q28",
        "type": "true_false",
        "prompt": "Data in a container's writable layer persists after the container is removed.",
        "correctAnswer": false,
        "explanation": "Data in the container's writable layer is ephemeral and deleted with the container. Use volumes or bind mounts for persistence."
      },
      {
        "id": "mid-q29",
        "type": "multiple_choice",
        "prompt": "Why should containers avoid running as root user?",
        "options": [
          "Root containers run slower",
          "Security risk if container is compromised",
          "Root is not supported in Docker",
          "Better performance with non-root"
        ],
        "correctAnswer": 1,
        "explanation": "Running as root is a security risk. If a container is compromised, the attacker has root privileges which could affect the host. Use USER instruction for non-root users."
      },
      {
        "id": "mid-q30",
        "type": "multiple_choice",
        "prompt": "What does Docker Compose primarily enable?",
        "options": [
          "Building Docker images faster",
          "Defining and running multi-container applications",
          "Monitoring container performance",
          "Scanning images for vulnerabilities"
        ],
        "correctAnswer": 1,
        "explanation": "Docker Compose uses YAML files to define multi-container applications with their networks, volumes, and configurations. It enables easy deployment and management of complete application stacks."
      }
    ]
  },
  {
    "id": "cs405-exam-final",
    "subjectId": "cs405",
    "title": "CS405 Final Exam",
    "durationMinutes": 120,
    "instructions": [
      "Comprehensive exam covering all seven topics",
      "Mixture of multiple choice, true/false, code analysis, and written questions",
      "Passing score: 70% or higher",
      "Focus on practical application of cloud computing concepts"
    ],
    "questions": [
      {
        "id": "final-q1",
        "type": "multiple_choice",
        "prompt": "Which characteristic best distinguishes cloud computing from traditional data centers?",
        "options": [
          "Uses virtualization",
          "Has internet connectivity",
          "On-demand self-service with pay-per-use",
          "Located off-site"
        ],
        "correctAnswer": 2,
        "explanation": "While virtualization and internet access are common, on-demand self-service with pay-per-use pricing is the defining characteristic. Users provision resources instantly without human interaction and pay only for what they consume."
      },
      {
        "id": "final-q2",
        "type": "multiple_choice",
        "prompt": "In which service model does the customer have NO access to the underlying operating system?",
        "options": [
          "IaaS",
          "PaaS",
          "SaaS",
          "DaaS"
        ],
        "correctAnswer": 2,
        "explanation": "In SaaS, the provider manages everything including the OS, and customers only interact with the application. In PaaS, the provider manages the OS. In IaaS, customers have full OS access."
      },
      {
        "id": "final-q3",
        "type": "written",
        "prompt": "Explain the difference between horizontal and vertical scaling in cloud computing. When would you use each?",
        "correctAnswer": "horizontal",
        "explanation": "Vertical scaling (scaling up/down) increases/decreases the size of individual resources (more CPU/RAM). Horizontal scaling (scaling out/in) adds/removes resource instances. Use vertical scaling for legacy apps that can't distribute across multiple instances. Use horizontal scaling for cloud-native apps that can distribute load, as it scales better and provides redundancy.",
        "modelAnswer": "Horizontal scaling adds or removes instances of resources (e.g., adding more web servers), while vertical scaling increases or decreases the size of existing resources (e.g., upgrading from 4 to 8 CPU cores). Horizontal scaling is preferred for cloud-native applications because it provides better fault tolerance, theoretically unlimited scale, and is more cost-effective. Use vertical scaling for legacy applications that can't distribute across multiple instances, or when specific workloads need more powerful single machines (large databases). Horizontal scaling requires applications designed for distribution (stateless, load-balanced), while vertical has limits based on the largest instance size available."
      },
      {
        "id": "final-q4",
        "type": "true_false",
        "prompt": "Spot instances/preemptible VMs can be interrupted by the provider with 2 minutes notice.",
        "correctAnswer": true,
        "explanation": "Spot/preemptible instances use spare capacity at steep discounts but can be reclaimed by the provider with short notice (typically 2 minutes on AWS). Design fault-tolerant workloads to use them effectively."
      },
      {
        "id": "final-q5",
        "type": "multiple_choice",
        "prompt": "Which factor most commonly causes cloud security breaches?",
        "options": [
          "Provider infrastructure failures",
          "DDoS attacks",
          "Customer misconfiguration",
          "Encryption weaknesses"
        ],
        "correctAnswer": 2,
        "explanation": "Customer misconfiguration (public S3 buckets, overly permissive security groups, weak access controls) causes the majority of cloud breaches, not provider infrastructure issues."
      },
      {
        "id": "final-q6",
        "type": "multiple_choice",
        "prompt": "Which technology would you choose for maximum isolation between workloads?",
        "options": [
          "Containers",
          "Virtual machines",
          "Processes",
          "Threads"
        ],
        "correctAnswer": 1,
        "explanation": "VMs provide the strongest isolation with separate kernels and hardware virtualization. Containers share the host kernel (process-level isolation), while processes and threads offer even less isolation."
      },
      {
        "id": "final-q7",
        "type": "true_false",
        "prompt": "More vCPUs always leads to better VM performance.",
        "correctAnswer": false,
        "explanation": "Too many vCPUs can hurt performance due to scheduling overhead and increased CPU ready time. Right-size based on actual usage. Start small and scale up based on monitoring."
      },
      {
        "id": "final-q8",
        "type": "multiple_choice",
        "prompt": "What is the primary requirement for live migration (vMotion) between hosts?",
        "options": [
          "Same CPU manufacturer",
          "Shared storage for VM disks",
          "Same amount of RAM",
          "Same hypervisor version"
        ],
        "correctAnswer": 1,
        "explanation": "Live migration requires shared storage so both hosts can access VM disks. This enables moving only memory/CPU state. Storage vMotion can migrate storage separately."
      },
      {
        "id": "final-q9",
        "type": "fill_blank",
        "prompt": "The hypervisor memory management technique that identifies and merges identical memory pages across VMs is called ____.",
        "correctAnswer": "transparent page sharing",
        "explanation": "Transparent Page Sharing (TPS) or memory deduplication identifies identical memory pages and stores only one copy, reclaiming duplicate memory. Common with multiple same-OS VMs."
      },
      {
        "id": "final-q10",
        "type": "multiple_choice",
        "prompt": "Which hypervisor would be best for a Windows-heavy environment with Microsoft investments?",
        "options": [
          "VMware ESXi",
          "Microsoft Hyper-V",
          "KVM",
          "Xen"
        ],
        "correctAnswer": 1,
        "explanation": "Hyper-V is included with Windows Server, integrates with Active Directory and System Center, supports Azure hybrid benefits, and is optimized for Windows workloads."
      },
      {
        "id": "final-q11",
        "type": "multiple_choice",
        "prompt": "What is the main advantage of containers over VMs for microservices?",
        "options": [
          "Better security",
          "Faster startup and higher density",
          "Can run different operating systems",
          "Built-in orchestration"
        ],
        "correctAnswer": 1,
        "explanation": "Containers start in seconds vs minutes for VMs and use MB vs GB of resources, enabling higher density (100+ containers vs 10-100 VMs per host). Critical for microservices with many small services."
      },
      {
        "id": "final-q12",
        "type": "written",
        "prompt": "Explain the purpose of multi-stage builds in Docker and how they reduce image size.",
        "correctAnswer": "tools",
        "explanation": "Multi-stage builds use multiple FROM statements in a Dockerfile. Early stages include build tools (compilers, build systems) to compile code. Final stage copies only the compiled binaries/artifacts from earlier stages, leaving behind build tools and reducing final image size significantly.",
        "modelAnswer": "Multi-stage builds allow using multiple FROM instructions in a Dockerfile to create separate build stages. Early stages typically include all build dependencies, compilers, and development tools needed to build the application. The final stage starts from a minimal base image and copies only the necessary compiled artifacts or binaries from earlier stages. This dramatically reduces the final image size because build tools, source code, and intermediate files are discarded. For example, a Go application might use a golang:1.20 image to compile (1GB+), then copy the binary to an alpine:latest image (5MB), resulting in a final image under 20MB instead of over 1GB."
      },
      {
        "id": "final-q13",
        "type": "true_false",
        "prompt": "Docker volumes persist data even after the container is deleted.",
        "correctAnswer": true,
        "explanation": "Volumes exist independently of containers and persist after container deletion. They must be explicitly deleted. This is the recommended way to persist data in Docker."
      },
      {
        "id": "final-q14",
        "type": "multiple_choice",
        "prompt": "Which Docker networking mode removes network isolation entirely?",
        "options": [
          "bridge",
          "host",
          "overlay",
          "none"
        ],
        "correctAnswer": 1,
        "explanation": "Host mode gives containers direct access to the host network stack, removing isolation. No port mapping needed but less secure. Useful for network-intensive applications."
      },
      {
        "id": "final-q15",
        "type": "multiple_choice",
        "prompt": "What security practice should be followed for container images?",
        "options": [
          "Always use latest tag",
          "Run as root for maximum privileges",
          "Scan for vulnerabilities and use specific versions",
          "Include build tools in production images"
        ],
        "correctAnswer": 2,
        "explanation": "Best practices: scan images for vulnerabilities, use specific version tags (not latest), run as non-root user, remove build tools from final images, and regularly update base images."
      },
      {
        "id": "final-q16",
        "type": "multiple_choice",
        "prompt": "What is the smallest deployable unit in Kubernetes?",
        "options": [
          "Container",
          "Pod",
          "Deployment",
          "ReplicaSet"
        ],
        "correctAnswer": 1,
        "explanation": "A Pod is the smallest deployable unit in Kubernetes. Pods can contain one or more containers that share network and storage. You deploy and manage Pods, not individual containers."
      },
      {
        "id": "final-q17",
        "type": "true_false",
        "prompt": "A Kubernetes Service provides a stable endpoint for accessing Pods even as they are created and destroyed.",
        "correctAnswer": true,
        "explanation": "Services provide stable IPs and DNS names for accessing Pods. As Pods are created/destroyed (scaling, updates, failures), the Service automatically updates its endpoints, providing stable discovery."
      },
      {
        "id": "final-q18",
        "type": "multiple_choice",
        "prompt": "Which Kubernetes object type should be used for stateful applications like databases?",
        "options": [
          "Deployment",
          "StatefulSet",
          "DaemonSet",
          "Job"
        ],
        "correctAnswer": 1,
        "explanation": "StatefulSets manage stateful applications, providing stable network identities, persistent storage, and ordered deployment/scaling. Deployments are for stateless applications."
      },
      {
        "id": "final-q19",
        "type": "fill_blank",
        "prompt": "Kubernetes ____ are used to store non-sensitive configuration data that can be consumed by Pods.",
        "correctAnswer": "configmaps",
        "explanation": "ConfigMaps store configuration data as key-value pairs. For sensitive data like passwords and API keys, use Secrets instead. Both can be consumed as environment variables or mounted as files."
      },
      {
        "id": "final-q20",
        "type": "multiple_choice",
        "prompt": "What does Helm provide for Kubernetes?",
        "options": [
          "Container runtime",
          "Package manager and templating",
          "Network policy enforcement",
          "Storage provisioning"
        ],
        "correctAnswer": 1,
        "explanation": "Helm is a package manager for Kubernetes that uses charts (packages) with templating to deploy applications. It simplifies deploying and managing complex applications with many Kubernetes resources."
      },
      {
        "id": "final-q21",
        "type": "multiple_choice",
        "prompt": "What is the primary benefit of serverless computing (FaaS)?",
        "options": [
          "Servers are not involved",
          "No server management, pay only per execution",
          "Always faster than containers",
          "Guaranteed low latency"
        ],
        "correctAnswer": 1,
        "explanation": "Serverless eliminates server management and charges only for actual execution time (not idle time). The name is misleading—servers exist but are fully abstracted from developers."
      },
      {
        "id": "final-q22",
        "type": "true_false",
        "prompt": "Cold starts in serverless functions occur when a function hasn't been invoked recently and infrastructure must be provisioned.",
        "correctAnswer": true,
        "explanation": "Cold starts happen when the platform provisions resources for a function that hasn't run recently. This adds latency (100ms to several seconds). Keeping functions warm or using provisioned concurrency mitigates this."
      },
      {
        "id": "final-q23",
        "type": "multiple_choice",
        "prompt": "What is a typical maximum execution time for AWS Lambda functions?",
        "options": [
          "1 minute",
          "5 minutes",
          "15 minutes",
          "1 hour"
        ],
        "correctAnswer": 2,
        "explanation": "AWS Lambda has a 15-minute maximum execution time. Azure Functions: 10 minutes (consumption plan), Google Cloud Functions: 9 minutes. Long-running processes need alternative solutions."
      },
      {
        "id": "final-q24",
        "type": "written",
        "prompt": "Explain why serverless functions should be designed as stateless. How do you handle state if needed?",
        "correctAnswer": "external",
        "explanation": "Functions are stateless because instances can be created and destroyed at any time, and you can't guarantee the same instance handles subsequent requests. State must be stored externally in databases, object storage, or cache services (Redis, DynamoDB). Each invocation retrieves needed state, performs work, and saves updated state externally.",
        "modelAnswer": "Serverless functions should be stateless because the platform may create or destroy function instances at any time, and different instances may handle subsequent requests from the same user or process. There's no guarantee which instance will handle the next request or that a particular instance will persist. To handle state when needed, use external services: databases (DynamoDB, Cosmos DB) for persistent state, object storage (S3) for files, in-memory caches (Redis, Memcached) for temporary state, or message queues for workflow state. Each function invocation retrieves necessary state from external storage, performs its operation, and saves updated state back. This design ensures functions scale horizontally and remain reliable."
      },
      {
        "id": "final-q25",
        "type": "multiple_choice",
        "prompt": "Which pattern is most suitable for serverless architectures?",
        "options": [
          "Monolithic applications",
          "Event-driven microservices",
          "Long-running batch jobs",
          "Stateful sessions"
        ],
        "correctAnswer": 1,
        "explanation": "Event-driven microservices are ideal for serverless. Functions respond to events (HTTP, queue messages, file uploads), execute quickly, and terminate. Monolithic apps and long-running jobs don't fit well."
      },
      {
        "id": "final-q26",
        "type": "multiple_choice",
        "prompt": "Which storage type is most appropriate for a relational database?",
        "options": [
          "Object storage",
          "Block storage",
          "File storage",
          "Glacier"
        ],
        "correctAnswer": 1,
        "explanation": "Block storage (EBS, Azure Disks) provides low-latency, high-IOPS storage suitable for databases. Object storage has higher latency, file storage can work but block is optimal for databases."
      },
      {
        "id": "final-q27",
        "type": "true_false",
        "prompt": "Object storage like S3 is optimized for frequent updates to existing files.",
        "correctAnswer": false,
        "explanation": "Object storage is write-once, read-many. It's optimized for storing and retrieving complete objects, not frequent updates. Block storage is better for frequently modified files."
      },
      {
        "id": "final-q28",
        "type": "multiple_choice",
        "prompt": "What is the main advantage of storage lifecycle policies?",
        "options": [
          "Faster access times",
          "Automatic cost optimization by transitioning data to cheaper tiers",
          "Better security",
          "Increased durability"
        ],
        "correctAnswer": 1,
        "explanation": "Lifecycle policies automatically transition objects to cheaper storage classes (Infrequent Access, Glacier) after specified periods, optimizing costs without manual intervention."
      },
      {
        "id": "final-q29",
        "type": "fill_blank",
        "prompt": "The cloud storage service designed for infrequent access with retrieval times of minutes to hours is typically called ____.",
        "correctAnswer": "glacier",
        "explanation": "Glacier (AWS), Archive (Azure), and Archive (Google) provide very low-cost storage for infrequent access with retrieval times from minutes to hours, suitable for compliance and backups."
      },
      {
        "id": "final-q30",
        "type": "multiple_choice",
        "prompt": "Which database type is best suited for unstructured data and horizontal scaling?",
        "options": [
          "Relational (SQL)",
          "NoSQL",
          "In-memory",
          "Data warehouse"
        ],
        "correctAnswer": 1,
        "explanation": "NoSQL databases (DynamoDB, MongoDB, Cosmos DB) handle unstructured/semi-structured data well and scale horizontally. Relational databases excel at structured data with complex queries."
      },
      {
        "id": "final-q31",
        "type": "multiple_choice",
        "prompt": "Which principle from the 12-Factor App methodology emphasizes treating backing services (databases, queues) as attached resources?",
        "options": [
          "Codebase",
          "Dependencies",
          "Backing services",
          "Config"
        ],
        "correctAnswer": 2,
        "explanation": "The Backing Services factor treats databases, queues, and other services as attached resources accessed via URLs. This enables swapping implementations without code changes."
      },
      {
        "id": "final-q32",
        "type": "true_false",
        "prompt": "Microservices architecture typically results in more complex deployment and operations compared to monolithic applications.",
        "correctAnswer": true,
        "explanation": "Microservices increase operational complexity with distributed systems, inter-service communication, distributed transactions, and more deployment units. This is traded for development scalability and service independence."
      },
      {
        "id": "final-q33",
        "type": "written",
        "prompt": "Explain the purpose of a service mesh and name one benefit it provides.",
        "correctAnswer": "communication",
        "explanation": "A service mesh manages service-to-service communication in microservices architectures, providing features like load balancing, service discovery, encryption (mTLS), retries, circuit breaking, and observability without changing application code.",
        "modelAnswer": "A service mesh is an infrastructure layer that manages communication between microservices. It uses sidecar proxies (typically Envoy) deployed alongside each service to handle all network communication. Benefits include: automatic mutual TLS for secure service-to-service communication, intelligent load balancing and traffic routing, automatic retries and circuit breaking for resilience, distributed tracing for observability, and fine-grained access control. The service mesh centralizes these concerns, removing them from application code and applying them consistently across all services. Popular service meshes include Istio, Linkerd, and Consul Connect."
      },
      {
        "id": "final-q34",
        "type": "multiple_choice",
        "prompt": "What deployment strategy involves running two production environments and switching traffic between them?",
        "options": [
          "Rolling deployment",
          "Canary deployment",
          "Blue-green deployment",
          "A/B testing"
        ],
        "correctAnswer": 2,
        "explanation": "Blue-green deployment runs two identical production environments. Deploy new version to inactive environment (green), test, then switch traffic from blue to green. Enables instant rollback."
      },
      {
        "id": "final-q35",
        "type": "multiple_choice",
        "prompt": "Which observability pillar captures individual request flows through distributed systems?",
        "options": [
          "Metrics",
          "Logs",
          "Traces",
          "Events"
        ],
        "correctAnswer": 2,
        "explanation": "Distributed tracing follows requests through microservices, showing the path, timing, and dependencies. Metrics show aggregates, logs show events, but traces show request flows."
      }
    ]
  }
]