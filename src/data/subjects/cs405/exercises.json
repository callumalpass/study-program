[
  {
    "id": "cs405-ex-1-1",
    "subjectId": "cs405",
    "topicId": "cs405-topic-1",
    "title": "Cloud Cost Calculator",
    "difficulty": 2,
    "description": "Create a Python script that calculates monthly cloud costs based on resource usage. Input: instance type, hours running, storage GB, data transfer GB. Calculate costs using simplified pricing.",
    "starterCode": "# Cloud Cost Calculator\ndef calculate_monthly_cost(instance_type, hours, storage_gb, transfer_gb):\n    \"\"\"\n    Calculate estimated monthly cloud costs\n    \n    Args:\n        instance_type: string ('small', 'medium', 'large')\n        hours: hours running per month\n        storage_gb: storage in gigabytes\n        transfer_gb: data transfer in gigabytes\n    \n    Returns:\n        float: total monthly cost in dollars\n    \"\"\"\n    # Pricing (simplified)\n    # Instance: small=$0.05/hr, medium=$0.10/hr, large=$0.20/hr\n    # Storage: $0.10/GB-month\n    # Transfer: $0.09/GB\n    \n    # TODO: Implement cost calculation\n    pass\n\n# Test cases\nprint(calculate_monthly_cost('small', 730, 100, 50))  # Should return ~97.0\nprint(calculate_monthly_cost('medium', 730, 200, 100))  # Should return ~102.0",
    "solution": "def calculate_monthly_cost(instance_type, hours, storage_gb, transfer_gb):\n    # Instance pricing\n    instance_prices = {\n        'small': 0.05,\n        'medium': 0.10,\n        'large': 0.20\n    }\n    \n    # Calculate costs\n    instance_cost = instance_prices[instance_type] * hours\n    storage_cost = storage_gb * 0.10\n    transfer_cost = transfer_gb * 0.09\n    \n    total_cost = instance_cost + storage_cost + transfer_cost\n    return round(total_cost, 2)\n\nprint(calculate_monthly_cost('small', 730, 100, 50))  # 36.5 + 10 + 4.5 = 51.0\nprint(calculate_monthly_cost('medium', 730, 200, 100))  # 73 + 20 + 9 = 102.0",
    "testCases": [
      {
        "input": "calculate_monthly_cost('small', 730, 100, 50)",
        "expectedOutput": "51.0",
        "isHidden": false,
        "description": "Small instance full month"
      },
      {
        "input": "calculate_monthly_cost('medium', 730, 200, 100)",
        "expectedOutput": "102.0",
        "isHidden": false,
        "description": "Medium instance full month"
      },
      {
        "input": "calculate_monthly_cost('large', 365, 500, 200)",
        "expectedOutput": "141.0",
        "isHidden": true,
        "description": "Large instance half month"
      }
    ],
    "hints": [
      "Create a dictionary for instance type pricing",
      "Multiply instance price by hours for compute cost",
      "Add storage cost (GB * price per GB)",
      "Add transfer cost (GB * price per GB)"
    ],
    "language": "python"
  },
  {
    "id": "cs405-ex-1-2",
    "subjectId": "cs405",
    "topicId": "cs405-topic-1",
    "title": "SLA Uptime Calculator",
    "difficulty": 2,
    "description": "Build a Python script that calculates allowed downtime for different SLA percentages (monthly and yearly). Input an SLA percentage and output: minutes/month, hours/month, hours/year.",
    "starterCode": "# SLA Uptime Calculator\ndef calculate_downtime(sla_percentage):\n    \"\"\"\n    Calculate allowed downtime for given SLA\n\n    Args:\n        sla_percentage: float (e.g., 99.9 for 99.9%)\n\n    Returns:\n        dict: downtime in different units\n    \"\"\"\n    # Constants\n    # Month: 30 days = 43,200 minutes = 720 hours\n    # Year: 365 days = 525,600 minutes = 8,760 hours\n\n    # TODO: Calculate downtime\n    pass\n\n# Test cases\nprint(calculate_downtime(99.9))   # {'min_month': 43.2, 'hrs_month': 0.72, 'hrs_year': 8.76}\nprint(calculate_downtime(99.99))  # {'min_month': 4.32, 'hrs_month': 0.07, 'hrs_year': 0.88}",
    "solution": "def calculate_downtime(sla_percentage):\n    \"\"\"Calculate allowed downtime for given SLA\"\"\"\n    # Constants\n    MINUTES_PER_MONTH = 43200  # 30 days\n    HOURS_PER_MONTH = 720\n    HOURS_PER_YEAR = 8760  # 365 days\n\n    # Calculate downtime percentage\n    downtime_pct = (100 - sla_percentage) / 100\n\n    # Calculate downtime in different units\n    minutes_per_month = MINUTES_PER_MONTH * downtime_pct\n    hours_per_month = HOURS_PER_MONTH * downtime_pct\n    hours_per_year = HOURS_PER_YEAR * downtime_pct\n\n    return {\n        'min_month': round(minutes_per_month, 2),\n        'hrs_month': round(hours_per_month, 2),\n        'hrs_year': round(hours_per_year, 2)\n    }\n\n# Test cases\nprint(calculate_downtime(99.9))   # 43.2 min/month\nprint(calculate_downtime(99.99))  # 4.32 min/month\nprint(calculate_downtime(99.999)) # 0.43 min/month",
    "testCases": [
      {
        "input": "calculate_downtime(99.9)",
        "expectedOutput": "{'min_month': 43.2, 'hrs_month': 0.72, 'hrs_year': 8.76}",
        "isHidden": false,
        "description": "99.9% SLA downtime calculation"
      },
      {
        "input": "calculate_downtime(99.99)",
        "expectedOutput": "{'min_month': 4.32, 'hrs_month': 0.07, 'hrs_year': 0.88}",
        "isHidden": false,
        "description": "99.99% SLA downtime calculation"
      },
      {
        "input": "calculate_downtime(99.999)",
        "expectedOutput": "{'min_month': 0.43, 'hrs_month': 0.01, 'hrs_year': 0.09}",
        "isHidden": true,
        "description": "99.999% SLA downtime calculation"
      }
    ],
    "hints": [
      "Calculate downtime percentage as (100 - SLA) / 100",
      "Month = 30 days = 43,200 minutes",
      "Year = 365 days = 8,760 hours",
      "Multiply time periods by downtime percentage"
    ],
    "language": "python"
  },
  {
    "id": "cs405-ex-1-3",
    "subjectId": "cs405",
    "topicId": "cs405-topic-1",
    "title": "Cloud Service Model Classifier",
    "difficulty": 3,
    "description": "Create a Python program that classifies cloud services into IaaS, PaaS, or SaaS based on characteristics. Given service features, determine the service model and explain why.",
    "starterCode": "# Cloud Service Model Classifier\ndef classify_service(features):\n    \"\"\"\n    Classify cloud service into IaaS, PaaS, or SaaS\n\n    Args:\n        features: dict with boolean keys:\n            - manages_infrastructure: Provider manages infrastructure\n            - manages_os: Provider manages OS\n            - manages_runtime: Provider manages runtime/middleware\n            - manages_application: Provider manages application\n\n    Returns:\n        tuple: (service_model, explanation)\n    \"\"\"\n    # TODO: Implement classification logic\n    pass\n\n# Test case\nfeatures1 = {\n    'manages_infrastructure': True,\n    'manages_os': False,\n    'manages_runtime': False,\n    'manages_application': False\n}\nprint(classify_service(features1))  # Should return ('IaaS', explanation)",
    "solution": "def classify_service(features):\n    \"\"\"Classify cloud service into IaaS, PaaS, or SaaS\"\"\"\n\n    manages_infra = features['manages_infrastructure']\n    manages_os = features['manages_os']\n    manages_runtime = features['manages_runtime']\n    manages_app = features['manages_application']\n\n    # SaaS: Provider manages everything\n    if manages_infra and manages_os and manages_runtime and manages_app:\n        return ('SaaS', 'Provider manages entire stack including application. '\n                'User only configures and uses the software.')\n\n    # PaaS: Provider manages up to runtime, user manages app\n    elif manages_infra and manages_os and manages_runtime and not manages_app:\n        return ('PaaS', 'Provider manages infrastructure, OS, and runtime. '\n                'User deploys and manages applications.')\n\n    # IaaS: Provider manages only infrastructure\n    elif manages_infra and not manages_os and not manages_runtime and not manages_app:\n        return ('IaaS', 'Provider manages only infrastructure (compute, storage, network). '\n                'User manages OS, runtime, and applications.')\n\n    else:\n        return ('Unknown', 'Service does not fit standard IaaS/PaaS/SaaS model')\n\n# Test cases\niaas = {'manages_infrastructure': True, 'manages_os': False,\n        'manages_runtime': False, 'manages_application': False}\nprint(\"IaaS example (EC2):\", classify_service(iaas))\n\npaas = {'manages_infrastructure': True, 'manages_os': True,\n        'manages_runtime': True, 'manages_application': False}\nprint(\"PaaS example (Heroku):\", classify_service(paas))\n\nsaas = {'manages_infrastructure': True, 'manages_os': True,\n        'manages_runtime': True, 'manages_application': True}\nprint(\"SaaS example (Gmail):\", classify_service(saas))",
    "testCases": [
      {
        "input": "classify_service({'manages_infrastructure': True, 'manages_os': False, 'manages_runtime': False, 'manages_application': False})",
        "expectedOutput": "('IaaS', explanation)",
        "isHidden": false,
        "description": "Classify IaaS service"
      },
      {
        "input": "classify_service({'manages_infrastructure': True, 'manages_os': True, 'manages_runtime': True, 'manages_application': False})",
        "expectedOutput": "('PaaS', explanation)",
        "isHidden": false,
        "description": "Classify PaaS service"
      },
      {
        "input": "classify_service({'manages_infrastructure': True, 'manages_os': True, 'manages_runtime': True, 'manages_application': True})",
        "expectedOutput": "('SaaS', explanation)",
        "isHidden": false,
        "description": "Classify SaaS service"
      }
    ],
    "hints": [
      "SaaS: Provider manages everything",
      "PaaS: Provider manages infrastructure + OS + runtime",
      "IaaS: Provider manages only infrastructure",
      "Use conditional logic to check combinations"
    ],
    "language": "python"
  },
  {
    "id": "cs405-ex-1-4",
    "subjectId": "cs405",
    "topicId": "cs405-topic-1",
    "title": "Multi-Cloud Cost Comparison",
    "difficulty": 3,
    "description": "Build a tool that compares costs across AWS, Azure, and GCP for identical workloads. Calculate monthly costs for compute, storage, and data transfer across providers.",
    "starterCode": "# Multi-Cloud Cost Comparison\ndef compare_cloud_costs(compute_hours, storage_gb, transfer_gb):\n    \"\"\"\n    Compare costs across AWS, Azure, and GCP\n\n    Args:\n        compute_hours: hours of t2.medium equivalent\n        storage_gb: GB of block storage\n        transfer_gb: GB of data transfer out\n\n    Returns:\n        dict: costs per provider\n    \"\"\"\n    # Simplified pricing (per month)\n    pricing = {\n        'AWS': {'compute': 0.0464, 'storage': 0.10, 'transfer': 0.09},\n        'Azure': {'compute': 0.0496, 'storage': 0.12, 'transfer': 0.087},\n        'GCP': {'compute': 0.0475, 'storage': 0.10, 'transfer': 0.085}\n    }\n\n    # TODO: Calculate costs for each provider\n    pass\n\n# Test\nprint(compare_cloud_costs(730, 100, 50))",
    "solution": "def compare_cloud_costs(compute_hours, storage_gb, transfer_gb):\n    \"\"\"Compare costs across AWS, Azure, and GCP\"\"\"\n\n    # Simplified pricing (per hour for compute, per GB-month for storage/transfer)\n    pricing = {\n        'AWS': {'compute': 0.0464, 'storage': 0.10, 'transfer': 0.09},\n        'Azure': {'compute': 0.0496, 'storage': 0.12, 'transfer': 0.087},\n        'GCP': {'compute': 0.0475, 'storage': 0.10, 'transfer': 0.085}\n    }\n\n    results = {}\n\n    for provider, prices in pricing.items():\n        compute_cost = compute_hours * prices['compute']\n        storage_cost = storage_gb * prices['storage']\n        transfer_cost = transfer_gb * prices['transfer']\n\n        total = compute_cost + storage_cost + transfer_cost\n\n        results[provider] = {\n            'compute': round(compute_cost, 2),\n            'storage': round(storage_cost, 2),\n            'transfer': round(transfer_cost, 2),\n            'total': round(total, 2)\n        }\n\n    # Find cheapest\n    cheapest = min(results.items(), key=lambda x: x[1]['total'])\n    results['recommendation'] = cheapest[0]\n\n    return results\n\n# Test with full month (730 hours)\ncosts = compare_cloud_costs(730, 100, 50)\nfor provider, breakdown in costs.items():\n    if provider != 'recommendation':\n        print(f\"{provider}: ${breakdown['total']:.2f}\")\nprint(f\"\\nRecommendation: {costs['recommendation']}\")",
    "testCases": [
      {
        "input": "compare_cloud_costs(730, 100, 50)",
        "expectedOutput": "Cost comparison with recommendation",
        "isHidden": false,
        "description": "Full month comparison"
      },
      {
        "input": "compare_cloud_costs(365, 500, 100)",
        "expectedOutput": "Half month with higher storage",
        "isHidden": false,
        "description": "Different workload comparison"
      }
    ],
    "hints": [
      "Calculate each cost component separately",
      "Sum all components for total cost",
      "Use min() to find cheapest provider",
      "Return detailed breakdown per provider"
    ],
    "language": "python"
  },
  {
    "id": "cs405-ex-1-5",
    "subjectId": "cs405",
    "topicId": "cs405-topic-1",
    "title": "Reserved Instance Savings Calculator",
    "difficulty": 2,
    "description": "Calculate potential savings from using Reserved Instances vs On-Demand. Compare 1-year and 3-year commitments with different payment options.",
    "starterCode": "# Reserved Instance Savings Calculator\ndef calculate_ri_savings(on_demand_monthly_cost, commitment_years=1):\n    \"\"\"\n    Calculate RI savings vs on-demand\n\n    Args:\n        on_demand_monthly_cost: monthly on-demand cost\n        commitment_years: 1 or 3\n\n    Returns:\n        dict: savings breakdown\n    \"\"\"\n    # RI discounts (approximate)\n    # 1-year: 40% off, 3-year: 60% off\n\n    # TODO: Calculate savings\n    pass\n\nprint(calculate_ri_savings(1000, 1))\nprint(calculate_ri_savings(1000, 3))",
    "solution": "def calculate_ri_savings(on_demand_monthly_cost, commitment_years=1):\n    \"\"\"Calculate RI savings vs on-demand\"\"\"\n\n    # RI discounts\n    discounts = {\n        1: 0.40,  # 40% off for 1-year\n        3: 0.60   # 60% off for 3-year\n    }\n\n    if commitment_years not in discounts:\n        raise ValueError(\"Commitment must be 1 or 3 years\")\n\n    discount = discounts[commitment_years]\n\n    # Calculate costs\n    months = commitment_years * 12\n    total_on_demand = on_demand_monthly_cost * months\n\n    ri_monthly_cost = on_demand_monthly_cost * (1 - discount)\n    total_ri_cost = ri_monthly_cost * months\n\n    total_savings = total_on_demand - total_ri_cost\n    savings_percentage = (total_savings / total_on_demand) * 100\n\n    return {\n        'commitment_years': commitment_years,\n        'on_demand_monthly': round(on_demand_monthly_cost, 2),\n        'ri_monthly': round(ri_monthly_cost, 2),\n        'total_on_demand': round(total_on_demand, 2),\n        'total_ri': round(total_ri_cost, 2),\n        'total_savings': round(total_savings, 2),\n        'savings_pct': round(savings_percentage, 2)\n    }\n\n# Examples\nprint(\"1-Year RI Savings:\")\nresult = calculate_ri_savings(1000, 1)\nprint(f\"  Monthly: ${result['ri_monthly']} (was ${result['on_demand_monthly']})\")\nprint(f\"  Total savings: ${result['total_savings']} ({result['savings_pct']}%)\")\n\nprint(\"\\n3-Year RI Savings:\")\nresult = calculate_ri_savings(1000, 3)\nprint(f\"  Monthly: ${result['ri_monthly']} (was ${result['on_demand_monthly']})\")\nprint(f\"  Total savings: ${result['total_savings']} ({result['savings_pct']}%)\"),",
    "testCases": [
      {
        "input": "calculate_ri_savings(1000, 1)['total_savings']",
        "expectedOutput": "4800.0",
        "isHidden": false,
        "description": "1-year RI savings"
      },
      {
        "input": "calculate_ri_savings(1000, 3)['total_savings']",
        "expectedOutput": "21600.0",
        "isHidden": false,
        "description": "3-year RI savings"
      }
    ],
    "hints": [
      "1-year RI: ~40% discount",
      "3-year RI: ~60% discount",
      "Total savings = on-demand cost - RI cost",
      "Calculate over entire commitment period"
    ],
    "language": "python"
  },
  {
    "id": "cs405-ex-1-6",
    "subjectId": "cs405",
    "topicId": "cs405-topic-1",
    "title": "Deployment Model Selector",
    "difficulty": 2,
    "description": "Create a decision tree tool that recommends cloud deployment model (Public, Private, Hybrid, Multi-cloud) based on organization requirements.",
    "starterCode": "# Deployment Model Selector\ndef recommend_deployment_model(requirements):\n    \"\"\"\n    Recommend deployment model based on requirements\n\n    Args:\n        requirements: dict with:\n            - data_sensitivity: 'low', 'medium', 'high'\n            - budget: 'low', 'medium', 'high'\n            - has_legacy_systems: bool\n            - requires_multiple_providers: bool\n            - compliance_strict: bool\n\n    Returns:\n        tuple: (model, reasoning)\n    \"\"\"\n    # TODO: Implement decision logic\n    pass\n\n# Test cases\nreq1 = {\n    'data_sensitivity': 'low',\n    'budget': 'low',\n    'has_legacy_systems': False,\n    'requires_multiple_providers': False,\n    'compliance_strict': False\n}\nprint(recommend_deployment_model(req1))",
    "solution": "def recommend_deployment_model(requirements):\n    \"\"\"Recommend deployment model based on requirements\"\"\"\n\n    sensitivity = requirements['data_sensitivity']\n    budget = requirements['budget']\n    has_legacy = requirements['has_legacy_systems']\n    multi_provider = requirements['requires_multiple_providers']\n    strict_compliance = requirements['compliance_strict']\n\n    # Multi-cloud: Explicitly needs multiple providers\n    if multi_provider:\n        return (\n            'Multi-Cloud',\n            'Organization requires multiple cloud providers for redundancy, '\n            'vendor lock-in avoidance, or specific service needs.'\n        )\n\n    # Private Cloud: High sensitivity or strict compliance\n    if sensitivity == 'high' or strict_compliance:\n        return (\n            'Private Cloud',\n            'High data sensitivity or strict compliance requirements necessitate '\n            'dedicated infrastructure with maximum control and security.'\n        )\n\n    # Hybrid Cloud: Has legacy systems or medium sensitivity with budget\n    if has_legacy or (sensitivity == 'medium' and budget in ['medium', 'high']):\n        return (\n            'Hybrid Cloud',\n            'Combination of on-premises systems (legacy or sensitive data) '\n            'with public cloud for scalability and flexibility.'\n        )\n\n    # Public Cloud: Default for low sensitivity and cost efficiency\n    return (\n        'Public Cloud',\n        'Public cloud offers best cost-efficiency, scalability, and ease of use '\n        'for workloads without special security or compliance requirements.'\n    )\n\n# Test cases\nscenarios = [\n    {\n        'name': 'Startup',\n        'reqs': {\n            'data_sensitivity': 'low',\n            'budget': 'low',\n            'has_legacy_systems': False,\n            'requires_multiple_providers': False,\n            'compliance_strict': False\n        }\n    },\n    {\n        'name': 'Healthcare',\n        'reqs': {\n            'data_sensitivity': 'high',\n            'budget': 'high',\n            'has_legacy_systems': True,\n            'requires_multiple_providers': False,\n            'compliance_strict': True\n        }\n    },\n    {\n        'name': 'Enterprise Migration',\n        'reqs': {\n            'data_sensitivity': 'medium',\n            'budget': 'high',\n            'has_legacy_systems': True,\n            'requires_multiple_providers': False,\n            'compliance_strict': False\n        }\n    },\n    {\n        'name': 'Global Company',\n        'reqs': {\n            'data_sensitivity': 'medium',\n            'budget': 'high',\n            'has_legacy_systems': False,\n            'requires_multiple_providers': True,\n            'compliance_strict': False\n        }\n    }\n]\n\nfor scenario in scenarios:\n    model, reason = recommend_deployment_model(scenario['reqs'])\n    print(f\"{scenario['name']}: {model}\")\n    print(f\"  Reason: {reason}\\n\")",
    "testCases": [
      {
        "input": "recommend_deployment_model({'data_sensitivity': 'low', 'budget': 'low', 'has_legacy_systems': False, 'requires_multiple_providers': False, 'compliance_strict': False})[0]",
        "expectedOutput": "'Public Cloud'",
        "isHidden": false,
        "description": "Simple startup scenario"
      },
      {
        "input": "recommend_deployment_model({'data_sensitivity': 'high', 'budget': 'high', 'has_legacy_systems': False, 'requires_multiple_providers': False, 'compliance_strict': True})[0]",
        "expectedOutput": "'Private Cloud'",
        "isHidden": false,
        "description": "Strict compliance scenario"
      },
      {
        "input": "recommend_deployment_model({'data_sensitivity': 'medium', 'budget': 'high', 'has_legacy_systems': True, 'requires_multiple_providers': False, 'compliance_strict': False})[0]",
        "expectedOutput": "'Hybrid Cloud'",
        "isHidden": false,
        "description": "Legacy systems scenario"
      }
    ],
    "hints": [
      "Multi-cloud: multiple providers needed",
      "Private: high sensitivity or strict compliance",
      "Hybrid: legacy systems or medium sensitivity",
      "Public: default for standard workloads"
    ],
    "language": "python"
  },
  {
    "id": "cs405-ex-2-1",
    "subjectId": "cs405",
    "topicId": "cs405-topic-2",
    "title": "Create and Configure Virtual Machines",
    "difficulty": 2,
    "description": "Create a script that uses the VirtualBox CLI (VBoxManage) or libvirt/virsh to:\n\n1. Create a new virtual machine with specified resources\n2. Configure network settings (bridged and NAT)\n3. Attach a virtual disk\n4. List all configured VMs and their states\n\nRequirements:\n- VM should have 2GB RAM and 2 vCPUs\n- Create a 20GB virtual disk\n- Configure both bridged and NAT network adapters\n- Output VM configuration details",
    "starterCode": "#!/bin/bash\n# VM Management Script\n\nVM_NAME=\"test-vm\"\n# TODO: Define VM parameters\n\n# Function to create VM\ncreate_vm() {\n    # TODO: Implement VM creation\n    echo \"Creating VM...\"\n}\n\n# Function to configure networking\nconfigure_network() {\n    # TODO: Configure network adapters\n    echo \"Configuring network...\"\n}\n\n# Function to attach storage\nattach_storage() {\n    # TODO: Create and attach virtual disk\n    echo \"Attaching storage...\"\n}\n\n# Function to list VMs\nlist_vms() {\n    # TODO: List all VMs and their status\n    echo \"Listing VMs...\"\n}\n\n# Main execution\ncreate_vm\nconfigure_network\nattach_storage\nlist_vms",
    "solution": "#!/bin/bash\n# Complete VM Management Script using VirtualBox\n\nVM_NAME=\"test-vm\"\nVM_RAM=2048  # MB\nVM_CPUS=2\nDISK_SIZE=20480  # MB\nOSTYPE=\"Ubuntu_64\"\n\n# Function to create VM\ncreate_vm() {\n    echo \"Creating VM: $VM_NAME\"\n\n    # Create VM\n    VBoxManage createvm --name \"$VM_NAME\" --ostype \"$OSTYPE\" --register\n\n    # Configure resources\n    VBoxManage modifyvm \"$VM_NAME\" \\\n        --memory $VM_RAM \\\n        --cpus $VM_CPUS \\\n        --vram 128 \\\n        --boot1 dvd \\\n        --boot2 disk \\\n        --boot3 none \\\n        --boot4 none\n\n    echo \"VM created successfully\"\n}\n\n# Function to configure networking\nconfigure_network() {\n    echo \"Configuring network adapters...\"\n\n    # NAT adapter (adapter 1)\n    VBoxManage modifyvm \"$VM_NAME\" \\\n        --nic1 nat \\\n        --nictype1 82540EM \\\n        --cableconnected1 on\n\n    # Bridged adapter (adapter 2)\n    VBoxManage modifyvm \"$VM_NAME\" \\\n        --nic2 bridged \\\n        --bridgeadapter2 eth0 \\\n        --nictype2 82540EM \\\n        --cableconnected2 on\n\n    echo \"Network configured: NAT + Bridged\"\n}\n\n# Function to attach storage\nattach_storage() {\n    echo \"Creating and attaching virtual disk...\"\n\n    # Create storage controller\n    VBoxManage storagectl \"$VM_NAME\" \\\n        --name \"SATA Controller\" \\\n        --add sata \\\n        --controller IntelAhci\n\n    # Create virtual disk\n    DISK_PATH=\"$HOME/VirtualBox VMs/$VM_NAME/$VM_NAME.vdi\"\n    VBoxManage createhd \\\n        --filename \"$DISK_PATH\" \\\n        --size $DISK_SIZE \\\n        --format VDI\n\n    # Attach disk to VM\n    VBoxManage storageattach \"$VM_NAME\" \\\n        --storagectl \"SATA Controller\" \\\n        --port 0 \\\n        --device 0 \\\n        --type hdd \\\n        --medium \"$DISK_PATH\"\n\n    echo \"Storage attached: ${DISK_SIZE}MB disk\"\n}\n\n# Function to list VMs\nlist_vms() {\n    echo \"\\n=== Virtual Machines ===\"\n    VBoxManage list vms\n\n    echo \"\\n=== Running VMs ===\"\n    VBoxManage list runningvms\n\n    echo \"\\n=== VM Details: $VM_NAME ===\"\n    VBoxManage showvminfo \"$VM_NAME\" --machinereadable | grep -E \"(memory|cpus|nic)\"\n}\n\n# Main execution\necho \"Starting VM setup...\"\ncreate_vm\nconfigure_network\nattach_storage\nlist_vms\n\necho \"\\nVM setup complete!\"\necho \"To start the VM: VBoxManage startvm $VM_NAME --type headless\"\necho \"To stop the VM: VBoxManage controlvm $VM_NAME poweroff\"",
    "hints": [
      "Use VBoxManage or virsh for VM operations",
      "Configure networking before starting the VM",
      "Check VM state with list commands",
      "Use --machinereadable for parseable output"
    ],
    "testCases": [
      {
        "input": "create_vm",
        "expectedOutput": "VM created with 2GB RAM, 2 vCPUs",
        "isHidden": false,
        "description": "Create VM with specified resources"
      },
      {
        "input": "configure_network",
        "expectedOutput": "NAT and Bridged adapters configured",
        "isHidden": false,
        "description": "Configure network adapters"
      },
      {
        "input": "attach_storage",
        "expectedOutput": "20GB VDI disk attached via SATA",
        "isHidden": false,
        "description": "Attach virtual storage"
      }
    ],
    "language": "bash"
  },
  {
    "id": "cs405-ex-2-2",
    "subjectId": "cs405",
    "topicId": "cs405-topic-2",
    "title": "VM Snapshots and Cloning",
    "difficulty": 3,
    "description": "Create a script that demonstrates VM snapshot and cloning capabilities:\n\n1. Create a snapshot of an existing VM\n2. Restore from a snapshot\n3. Create a full clone of a VM\n4. Create a linked clone\n5. List all snapshots\n\nInclude error handling and validation.",
    "starterCode": "#!/bin/bash\n# VM Snapshot and Clone Manager\n\nVM_NAME=\"test-vm\"\n\ncreate_snapshot() {\n    # TODO: Create snapshot\n    echo \"Creating snapshot...\"\n}\n\nrestore_snapshot() {\n    # TODO: Restore from snapshot\n    echo \"Restoring snapshot...\"\n}\n\nclone_vm() {\n    # TODO: Clone VM\n    echo \"Cloning VM...\"\n}",
    "solution": "#!/bin/bash\n# Complete VM Snapshot and Clone Manager\n\nVM_NAME=\"test-vm\"\n\n# Colors for output\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nNC='\\033[0m' # No Color\n\n# Error handling\nset -e\ntrap 'echo \"${RED}Error occurred. Exiting...${NC}\"' ERR\n\n# Check if VM exists\ncheck_vm_exists() {\n    if ! VBoxManage list vms | grep -q \"$1\"; then\n        echo \"${RED}VM $1 does not exist${NC}\"\n        return 1\n    fi\n    return 0\n}\n\n# Create snapshot\ncreate_snapshot() {\n    local vm_name=$1\n    local snapshot_name=$2\n\n    echo \"Creating snapshot: $snapshot_name for VM: $vm_name\"\n\n    if ! check_vm_exists \"$vm_name\"; then\n        return 1\n    fi\n\n    VBoxManage snapshot \"$vm_name\" take \"$snapshot_name\" \\\n        --description \"Snapshot created on $(date)\" \\\n        --live  # Live snapshot if VM is running\n\n    echo \"${GREEN}Snapshot created successfully${NC}\"\n}\n\n# List snapshots\nlist_snapshots() {\n    local vm_name=$1\n\n    echo \"Snapshots for VM: $vm_name\"\n\n    if ! check_vm_exists \"$vm_name\"; then\n        return 1\n    fi\n\n    VBoxManage snapshot \"$vm_name\" list --details\n}\n\n# Restore snapshot\nrestore_snapshot() {\n    local vm_name=$1\n    local snapshot_name=$2\n\n    echo \"Restoring snapshot: $snapshot_name\"\n\n    if ! check_vm_exists \"$vm_name\"; then\n        return 1\n    fi\n\n    # Power off VM if running\n    if VBoxManage list runningvms | grep -q \"$vm_name\"; then\n        echo \"Powering off VM...\"\n        VBoxManage controlvm \"$vm_name\" poweroff\n        sleep 2\n    fi\n\n    VBoxManage snapshot \"$vm_name\" restore \"$snapshot_name\"\n\n    echo \"${GREEN}Snapshot restored successfully${NC}\"\n}\n\n# Delete snapshot\ndelete_snapshot() {\n    local vm_name=$1\n    local snapshot_name=$2\n\n    echo \"Deleting snapshot: $snapshot_name\"\n\n    VBoxManage snapshot \"$vm_name\" delete \"$snapshot_name\"\n\n    echo \"${GREEN}Snapshot deleted${NC}\"\n}\n\n# Create full clone\ncreate_full_clone() {\n    local source_vm=$1\n    local clone_name=$2\n\n    echo \"Creating full clone: $clone_name from $source_vm\"\n\n    if ! check_vm_exists \"$source_vm\"; then\n        return 1\n    fi\n\n    VBoxManage clonevm \"$source_vm\" \\\n        --name \"$clone_name\" \\\n        --mode machine \\\n        --options link \\\n        --register\n\n    echo \"${GREEN}Full clone created: $clone_name${NC}\"\n}\n\n# Create linked clone from snapshot\ncreate_linked_clone() {\n    local source_vm=$1\n    local snapshot_name=$2\n    local clone_name=$3\n\n    echo \"Creating linked clone: $clone_name\"\n\n    if ! check_vm_exists \"$source_vm\"; then\n        return 1\n    fi\n\n    VBoxManage clonevm \"$source_vm\" \\\n        --snapshot \"$snapshot_name\" \\\n        --name \"$clone_name\" \\\n        --options link \\\n        --register\n\n    echo \"${GREEN}Linked clone created: $clone_name${NC}\"\n}\n\n# Demo workflow\ndemo_workflow() {\n    echo \"=== VM Snapshot and Clone Demo ===\"\n\n    # Create snapshots\n    create_snapshot \"$VM_NAME\" \"clean-install\"\n    create_snapshot \"$VM_NAME\" \"after-updates\"\n\n    # List all snapshots\n    list_snapshots \"$VM_NAME\"\n\n    # Create full clone\n    create_full_clone \"$VM_NAME\" \"${VM_NAME}-clone-full\"\n\n    # Create linked clone\n    create_linked_clone \"$VM_NAME\" \"clean-install\" \"${VM_NAME}-clone-linked\"\n\n    # Restore to earlier snapshot\n    restore_snapshot \"$VM_NAME\" \"clean-install\"\n\n    echo \"${GREEN}Demo complete!${NC}\"\n}\n\n# Main execution\ncase \"${1:-demo}\" in\n    create-snapshot)\n        create_snapshot \"$2\" \"$3\"\n        ;;\n    list-snapshots)\n        list_snapshots \"$2\"\n        ;;\n    restore-snapshot)\n        restore_snapshot \"$2\" \"$3\"\n        ;;\n    delete-snapshot)\n        delete_snapshot \"$2\" \"$3\"\n        ;;\n    clone-full)\n        create_full_clone \"$2\" \"$3\"\n        ;;\n    clone-linked)\n        create_linked_clone \"$2\" \"$3\" \"$4\"\n        ;;\n    demo)\n        demo_workflow\n        ;;\n    *)\n        echo \"Usage: $0 {create-snapshot|list-snapshots|restore-snapshot|clone-full|clone-linked|demo} [args]\"\n        exit 1\n        ;;\nesac",
    "hints": [
      "Power off VM before restoring snapshots",
      "Use --live for snapshots of running VMs",
      "Linked clones save disk space",
      "Full clones are independent",
      "Always validate VM exists before operations"
    ],
    "testCases": [
      {
        "input": "create_snapshot test-vm snapshot-1",
        "expectedOutput": "Snapshot created with timestamp",
        "isHidden": false,
        "description": "Create a snapshot of a VM"
      },
      {
        "input": "list_snapshots test-vm",
        "expectedOutput": "All snapshots listed with details",
        "isHidden": false,
        "description": "List all snapshots for a VM"
      },
      {
        "input": "create_full_clone test-vm clone-1",
        "expectedOutput": "Independent clone created and registered",
        "isHidden": false,
        "description": "Create a full clone of the VM"
      }
    ],
    "language": "bash"
  },
  {
    "id": "cs405-ex-3-1",
    "subjectId": "cs405",
    "topicId": "cs405-topic-3",
    "title": "Build Multi-Stage Dockerfile",
    "difficulty": 3,
    "description": "Create a multi-stage Dockerfile for a Node.js application that:\n\n1. Uses separate build and production stages\n2. Installs dependencies in build stage\n3. Runs tests in build stage\n4. Creates minimal production image\n5. Runs as non-root user\n6. Includes health check\n\nAlso create a .dockerignore file to exclude unnecessary files.",
    "starterCode": "# Dockerfile\n# TODO: Build stage\n\n# TODO: Production stage\n\n# .dockerignore\n# TODO: Add files to exclude",
    "solution": "# Dockerfile\n# Multi-stage build for Node.js application\n\n# Stage 1: Dependencies\nFROM node:18-alpine AS dependencies\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production && \\\n    npm cache clean --force\n\n# Stage 2: Build\nFROM node:18-alpine AS builder\nWORKDIR /app\n\n# Copy package files\nCOPY package*.json ./\n\n# Install all dependencies (including dev)\nRUN npm ci\n\n# Copy source code\nCOPY . .\n\n# Run tests\nRUN npm test\n\n# Build application\nRUN npm run build\n\n# Stage 3: Production\nFROM node:18-alpine\n\n# Install dumb-init for proper signal handling\nRUN apk add --no-cache dumb-init\n\n# Create non-root user\nRUN addgroup -g 1001 -S nodejs && \\\n    adduser -S nodejs -u 1001\n\n# Set working directory\nWORKDIR /app\n\n# Copy production dependencies from dependencies stage\nCOPY --from=dependencies --chown=nodejs:nodejs /app/node_modules ./node_modules\n\n# Copy built application from builder stage\nCOPY --from=builder --chown=nodejs:nodejs /app/dist ./dist\nCOPY --from=builder --chown=nodejs:nodejs /app/package*.json ./\n\n# Switch to non-root user\nUSER nodejs\n\n# Expose port\nEXPOSE 3000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n  CMD node -e \"require('http').get('http://localhost:3000/health', (res) => process.exit(res.statusCode === 200 ? 0 : 1))\"\n\n# Use dumb-init to handle signals\nENTRYPOINT [\"dumb-init\", \"--\"]\n\n# Start application\nCMD [\"node\", \"dist/index.js\"]\n\n# .dockerignore file\nnode_modules\nnpm-debug.log\n.git\n.gitignore\nREADME.md\n.env\n.env.local\n.env.*.local\n.vscode\n.idea\n*.md\ncoverage\n.nyc_output\ndist\nbuild\n.DS_Store\nThumbs.db\n*.log\n.dockerignore\nDockerfile\ndocker-compose.yml",
    "hints": [
      "Use Alpine images for smaller size",
      "Copy package.json before source code for better caching",
      "Clean npm cache in same RUN command",
      "Use COPY --chown to set ownership",
      "Health checks enable automatic restart"
    ],
    "testCases": [
      {
        "input": "docker build -t myapp .",
        "expectedOutput": "Image built successfully, tests pass",
        "isHidden": false,
        "description": "Build multi-stage Docker image"
      },
      {
        "input": "docker run myapp",
        "expectedOutput": "Container runs as non-root user",
        "isHidden": false,
        "description": "Run container with non-root user"
      },
      {
        "input": "docker inspect myapp",
        "expectedOutput": "Health check configured",
        "isHidden": false,
        "description": "Verify health check is configured"
      }
    ],
    "language": "dockerfile"
  },
  {
    "id": "cs405-ex-3-2",
    "subjectId": "cs405",
    "topicId": "cs405-topic-3",
    "title": "Docker Compose Full-Stack Application",
    "difficulty": 4,
    "description": "Create a docker-compose.yml for a full-stack application with:\n\n1. Frontend (React/Vue)\n2. Backend API (Node.js/Python)\n3. Database (PostgreSQL)\n4. Redis cache\n5. Nginx reverse proxy\n\nInclude:\n- Custom networks (frontend, backend)\n- Named volumes for data persistence\n- Environment variables\n- Health checks\n- Resource limits\n- Proper service dependencies",
    "starterCode": "version: '3.8'\n\nservices:\n  # TODO: Define all services\n\nnetworks:\n  # TODO: Define networks\n\nvolumes:\n  # TODO: Define volumes",
    "solution": "version: '3.8'\n\nservices:\n  # PostgreSQL Database\n  postgres:\n    image: postgres:15-alpine\n    container_name: app-database\n    restart: unless-stopped\n    environment:\n      POSTGRES_DB: ${DB_NAME:-appdb}\n      POSTGRES_USER: ${DB_USER:-appuser}\n      POSTGRES_PASSWORD: ${DB_PASSWORD:-secret}\n      PGDATA: /var/lib/postgresql/data/pgdata\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n      - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro\n    networks:\n      - backend\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U ${DB_USER:-appuser} -d ${DB_NAME:-appdb}\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n      start_period: 10s\n    deploy:\n      resources:\n        limits:\n          cpus: '1'\n          memory: 1G\n        reservations:\n          cpus: '0.5'\n          memory: 512M\n\n  # Redis Cache\n  redis:\n    image: redis:7-alpine\n    container_name: app-cache\n    restart: unless-stopped\n    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-redissecret}\n    volumes:\n      - redis-data:/data\n    networks:\n      - backend\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"--raw\", \"incr\", \"ping\"]\n      interval: 10s\n      timeout: 3s\n      retries: 5\n    deploy:\n      resources:\n        limits:\n          cpus: '0.5'\n          memory: 512M\n\n  # Backend API\n  backend:\n    build:\n      context: ./backend\n      dockerfile: Dockerfile\n      target: production\n    container_name: app-backend\n    restart: unless-stopped\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    environment:\n      NODE_ENV: production\n      PORT: 5000\n      DB_HOST: postgres\n      DB_PORT: 5432\n      DB_NAME: ${DB_NAME:-appdb}\n      DB_USER: ${DB_USER:-appuser}\n      DB_PASSWORD: ${DB_PASSWORD:-secret}\n      REDIS_HOST: redis\n      REDIS_PORT: 6379\n      REDIS_PASSWORD: ${REDIS_PASSWORD:-redissecret}\n      JWT_SECRET: ${JWT_SECRET:-change-me-in-production}\n    volumes:\n      - ./backend/uploads:/app/uploads\n    networks:\n      - frontend\n      - backend\n    healthcheck:\n      test: [\"CMD\", \"node\", \"-e\", \"require('http').get('http://localhost:5000/health')\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 40s\n    deploy:\n      resources:\n        limits:\n          cpus: '1'\n          memory: 1G\n        reservations:\n          cpus: '0.5'\n          memory: 512M\n\n  # Frontend\n  frontend:\n    build:\n      context: ./frontend\n      dockerfile: Dockerfile\n      args:\n        REACT_APP_API_URL: /api\n    container_name: app-frontend\n    restart: unless-stopped\n    depends_on:\n      backend:\n        condition: service_healthy\n    networks:\n      - frontend\n    deploy:\n      resources:\n        limits:\n          cpus: '0.5'\n          memory: 512M\n\n  # Nginx Reverse Proxy\n  nginx:\n    image: nginx:alpine\n    container_name: app-nginx\n    restart: unless-stopped\n    depends_on:\n      - frontend\n      - backend\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro\n      - ./nginx/ssl:/etc/nginx/ssl:ro\n      - nginx-cache:/var/cache/nginx\n    networks:\n      - frontend\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--quiet\", \"--tries=1\", \"--spider\", \"http://localhost/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n    deploy:\n      resources:\n        limits:\n          cpus: '0.5'\n          memory: 256M\n\n  # PGAdmin (optional, for development)\n  pgadmin:\n    image: dpage/pgadmin4:latest\n    container_name: app-pgadmin\n    restart: unless-stopped\n    profiles:\n      - dev  # Only runs when using --profile dev\n    environment:\n      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL:-admin@example.com}\n      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD:-admin}\n    ports:\n      - \"5050:80\"\n    volumes:\n      - pgadmin-data:/var/lib/pgadmin\n    networks:\n      - backend\n    depends_on:\n      - postgres\n\nnetworks:\n  frontend:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.0.0/16\n\n  backend:\n    driver: bridge\n    internal: true  # No external access\n    ipam:\n      config:\n        - subnet: 172.21.0.0/16\n\nvolumes:\n  postgres-data:\n    driver: local\n  redis-data:\n    driver: local\n  nginx-cache:\n    driver: local\n  pgadmin-data:\n    driver: local\n\n# nginx/nginx.conf\n# events {\n#     worker_connections 1024;\n# }\n#\n# http {\n#     upstream backend {\n#         server backend:5000;\n#     }\n#\n#     upstream frontend {\n#         server frontend:3000;\n#     }\n#\n#     server {\n#         listen 80;\n#         server_name localhost;\n#\n#         # Frontend\n#         location / {\n#             proxy_pass http://frontend;\n#             proxy_http_version 1.1;\n#             proxy_set_header Upgrade $http_upgrade;\n#             proxy_set_header Connection 'upgrade';\n#             proxy_set_header Host $host;\n#             proxy_cache_bypass $http_upgrade;\n#         }\n#\n#         # Backend API\n#         location /api {\n#             rewrite ^/api/(.*) /$1 break;\n#             proxy_pass http://backend;\n#             proxy_http_version 1.1;\n#             proxy_set_header Host $host;\n#             proxy_set_header X-Real-IP $remote_addr;\n#             proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n#             proxy_set_header X-Forwarded-Proto $scheme;\n#         }\n#\n#         # Health check\n#         location /health {\n#             access_log off;\n#             return 200 \"healthy\\n\";\n#             add_header Content-Type text/plain;\n#         }\n#     }\n# }\n\n# .env file\n# DB_NAME=appdb\n# DB_USER=appuser\n# DB_PASSWORD=secret-password-change-me\n# REDIS_PASSWORD=redis-secret-change-me\n# JWT_SECRET=jwt-secret-change-me\n# PGADMIN_EMAIL=admin@example.com\n# PGADMIN_PASSWORD=admin",
    "hints": [
      "Use depends_on with condition: service_healthy",
      "Internal networks prevent external access",
      "Named volumes persist data across restarts",
      "Resource limits prevent resource exhaustion",
      "Profiles enable optional services"
    ],
    "testCases": [
      {
        "input": "docker compose up",
        "expectedOutput": "All services start in correct order",
        "isHidden": false,
        "description": "Start all services with Docker Compose"
      },
      {
        "input": "docker compose ps",
        "expectedOutput": "All services running and healthy",
        "isHidden": false,
        "description": "Check service status"
      },
      {
        "input": "curl http://localhost",
        "expectedOutput": "Frontend accessible via Nginx",
        "isHidden": false,
        "description": "Test frontend access through Nginx"
      }
    ],
    "language": "yaml"
  },
  {
    "id": "cs405-ex-4-1",
    "subjectId": "cs405",
    "topicId": "cs405-topic-4",
    "title": "Deploy Application on Kubernetes",
    "difficulty": 3,
    "description": "Create Kubernetes manifests to deploy a web application with:\n\n1. Deployment with 3 replicas\n2. Service (LoadBalancer)\n3. ConfigMap for configuration\n4. Secret for sensitive data\n5. Health probes (liveness and readiness)\n6. Resource requests and limits",
    "starterCode": "# deployment.yaml\n# TODO: Create Deployment\n\n# service.yaml\n# TODO: Create Service\n\n# configmap.yaml\n# TODO: Create ConfigMap\n\n# secret.yaml\n# TODO: Create Secret",
    "solution": "# configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\n  labels:\n    app: webapp\ndata:\n  app.properties: |\n    server.port=8080\n    logging.level=INFO\n    feature.enabled=true\n  database.host: \"postgres.default.svc.cluster.local\"\n  cache.host: \"redis.default.svc.cluster.local\"\n\n---\n# secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\n  labels:\n    app: webapp\ntype: Opaque\nstringData:\n  database-password: \"changeme-in-production\"\n  api-key: \"secret-api-key-here\"\n  jwt-secret: \"jwt-signing-secret\"\n\n---\n# deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webapp\n  labels:\n    app: webapp\n    version: v1\nspec:\n  replicas: 3\n  revisionHistoryLimit: 5\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      app: webapp\n  template:\n    metadata:\n      labels:\n        app: webapp\n        version: v1\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 1000\n        fsGroup: 2000\n      containers:\n      - name: webapp\n        image: myapp:1.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        env:\n        - name: NODE_ENV\n          value: \"production\"\n        - name: DB_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: database.host\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: app-secrets\n              key: database-password\n        - name: API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: app-secrets\n              key: api-key\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: cache\n          mountPath: /tmp\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n      volumes:\n      - name: config\n        configMap:\n          name: app-config\n      - name: cache\n        emptyDir: {}\n\n---\n# service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: webapp-service\n  labels:\n    app: webapp\nspec:\n  type: LoadBalancer\n  selector:\n    app: webapp\n  ports:\n  - name: http\n    protocol: TCP\n    port: 80\n    targetPort: 8080\n  sessionAffinity: ClientIP\n  sessionAffinityConfig:\n    clientIP:\n      timeoutSeconds: 10800\n\n---\n# hpa.yaml (Horizontal Pod Autoscaler)\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: webapp-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: webapp\n  minReplicas: 3\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n      - type: Percent\n        value: 50\n        periodSeconds: 60\n    scaleUp:\n      stabilizationWindowSeconds: 0\n      policies:\n      - type: Percent\n        value: 100\n        periodSeconds: 30\n\n---\n# pdb.yaml (Pod Disruption Budget)\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: webapp-pdb\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: webapp",
    "hints": [
      "Always set resource requests and limits",
      "Use health probes for reliability",
      "ConfigMaps for config, Secrets for sensitive data",
      "Rolling updates prevent downtime",
      "HPA requires metrics-server"
    ],
    "testCases": [
      {
        "input": "kubectl apply -f .",
        "expectedOutput": "All resources created successfully",
        "isHidden": false,
        "description": "Apply all Kubernetes manifests"
      },
      {
        "input": "kubectl get pods",
        "expectedOutput": "3 pods running and ready",
        "isHidden": false,
        "description": "Check pod status"
      },
      {
        "input": "kubectl get svc",
        "expectedOutput": "LoadBalancer service with external IP",
        "isHidden": false,
        "description": "Verify service configuration"
      }
    ],
    "language": "yaml"
  },
  {
    "id": "cs405-ex-5-1",
    "subjectId": "cs405",
    "topicId": "cs405-topic-5",
    "title": "Build Serverless REST API",
    "difficulty": 3,
    "description": "Create a serverless REST API using AWS Lambda and API Gateway (or similar) with:\n\n1. CRUD endpoints for a resource\n2. DynamoDB for data storage\n3. Input validation\n4. Error handling\n5. Proper HTTP status codes\n\nUse Serverless Framework or SAM.",
    "starterCode": "# serverless.yml\n# TODO: Define service configuration\n\n# handler.py\n# TODO: Implement Lambda functions",
    "solution": "# serverless.yml\nservice: users-api\n\nprovider:\n  name: aws\n  runtime: python3.9\n  region: us-east-1\n  stage: ${opt:stage, 'dev'}\n  environment:\n    USERS_TABLE: ${self:service}-${self:provider.stage}-users\n    STAGE: ${self:provider.stage}\n  iam:\n    role:\n      statements:\n        - Effect: Allow\n          Action:\n            - dynamodb:Query\n            - dynamodb:Scan\n            - dynamodb:GetItem\n            - dynamodb:PutItem\n            - dynamodb:UpdateItem\n            - dynamodb:DeleteItem\n          Resource:\n            - !GetAtt UsersTable.Arn\n            - !Join ['/', [!GetAtt UsersTable.Arn, 'index', '*']]\n\nfunctions:\n  createUser:\n    handler: handler.create_user\n    events:\n      - http:\n          path: /users\n          method: post\n          cors: true\n\n  getUser:\n    handler: handler.get_user\n    events:\n      - http:\n          path: /users/{user_id}\n          method: get\n          cors: true\n\n  listUsers:\n    handler: handler.list_users\n    events:\n      - http:\n          path: /users\n          method: get\n          cors: true\n\n  updateUser:\n    handler: handler.update_user\n    events:\n      - http:\n          path: /users/{user_id}\n          method: put\n          cors: true\n\n  deleteUser:\n    handler: handler.delete_user\n    events:\n      - http:\n          path: /users/{user_id}\n          method: delete\n          cors: true\n\nresources:\n  Resources:\n    UsersTable:\n      Type: AWS::DynamoDB::Table\n      Properties:\n        TableName: ${self:provider.environment.USERS_TABLE}\n        AttributeDefinitions:\n          - AttributeName: user_id\n            AttributeType: S\n          - AttributeName: email\n            AttributeType: S\n        KeySchema:\n          - AttributeName: user_id\n            KeyType: HASH\n        GlobalSecondaryIndexes:\n          - IndexName: EmailIndex\n            KeySchema:\n              - AttributeName: email\n                KeyType: HASH\n            Projection:\n              ProjectionType: ALL\n            ProvisionedThroughput:\n              ReadCapacityUnits: 5\n              WriteCapacityUnits: 5\n        BillingMode: PROVISIONED\n        ProvisionedThroughput:\n          ReadCapacityUnits: 5\n          WriteCapacityUnits: 5\n\n# handler.py\nimport json\nimport os\nimport uuid\nfrom datetime import datetime\nimport boto3\nfrom boto3.dynamodb.conditions import Key\nfrom botocore.exceptions import ClientError\n\ndynamodb = boto3.resource('dynamodb')\ntable_name = os.environ['USERS_TABLE']\ntable = dynamodb.Table(table_name)\n\ndef response(status_code, body):\n    \"\"\"Helper function to create HTTP response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*',\n            'Access-Control-Allow-Credentials': True\n        },\n        'body': json.dumps(body)\n    }\n\ndef validate_user(data):\n    \"\"\"Validate user data\"\"\"\n    required_fields = ['name', 'email']\n    for field in required_fields:\n        if field not in data or not data[field]:\n            return False, f\"Missing required field: {field}\"\n\n    # Basic email validation\n    if '@' not in data['email']:\n        return False, \"Invalid email format\"\n\n    return True, None\n\ndef create_user(event, context):\n    \"\"\"Create a new user\"\"\"\n    try:\n        data = json.loads(event['body'])\n\n        # Validate input\n        is_valid, error_msg = validate_user(data)\n        if not is_valid:\n            return response(400, {'error': error_msg})\n\n        # Check if email already exists\n        existing = table.query(\n            IndexName='EmailIndex',\n            KeyConditionExpression=Key('email').eq(data['email'])\n        )\n        if existing['Items']:\n            return response(409, {'error': 'Email already exists'})\n\n        # Create user\n        user = {\n            'user_id': str(uuid.uuid4()),\n            'name': data['name'],\n            'email': data['email'],\n            'created_at': datetime.utcnow().isoformat(),\n            'updated_at': datetime.utcnow().isoformat()\n        }\n\n        table.put_item(Item=user)\n\n        return response(201, user)\n\n    except json.JSONDecodeError:\n        return response(400, {'error': 'Invalid JSON'})\n    except Exception as e:\n        print(f\"Error creating user: {str(e)}\")\n        return response(500, {'error': 'Internal server error'})\n\ndef get_user(event, context):\n    \"\"\"Get a single user by ID\"\"\"\n    try:\n        user_id = event['pathParameters']['user_id']\n\n        result = table.get_item(Key={'user_id': user_id})\n\n        if 'Item' not in result:\n            return response(404, {'error': 'User not found'})\n\n        return response(200, result['Item'])\n\n    except Exception as e:\n        print(f\"Error getting user: {str(e)}\")\n        return response(500, {'error': 'Internal server error'})\n\ndef list_users(event, context):\n    \"\"\"List all users with pagination\"\"\"\n    try:\n        # Get query parameters\n        params = event.get('queryStringParameters') or {}\n        limit = int(params.get('limit', 10))\n        last_key = params.get('last_key')\n\n        # Build scan parameters\n        scan_params = {'Limit': limit}\n        if last_key:\n            scan_params['ExclusiveStartKey'] = {'user_id': last_key}\n\n        result = table.scan(**scan_params)\n\n        response_body = {\n            'users': result['Items'],\n            'count': len(result['Items'])\n        }\n\n        if 'LastEvaluatedKey' in result:\n            response_body['last_key'] = result['LastEvaluatedKey']['user_id']\n\n        return response(200, response_body)\n\n    except Exception as e:\n        print(f\"Error listing users: {str(e)}\")\n        return response(500, {'error': 'Internal server error'})\n\ndef update_user(event, context):\n    \"\"\"Update an existing user\"\"\"\n    try:\n        user_id = event['pathParameters']['user_id']\n        data = json.loads(event['body'])\n\n        # Check if user exists\n        existing = table.get_item(Key={'user_id': user_id})\n        if 'Item' not in existing:\n            return response(404, {'error': 'User not found'})\n\n        # Build update expression\n        update_expr = \"SET updated_at = :updated_at\"\n        expr_values = {':updated_at': datetime.utcnow().isoformat()}\n\n        if 'name' in data:\n            update_expr += \", #n = :name\"\n            expr_values[':name'] = data['name']\n\n        if 'email' in data:\n            # Check if new email already exists\n            email_check = table.query(\n                IndexName='EmailIndex',\n                KeyConditionExpression=Key('email').eq(data['email'])\n            )\n            if email_check['Items'] and email_check['Items'][0]['user_id'] != user_id:\n                return response(409, {'error': 'Email already exists'})\n\n            update_expr += \", email = :email\"\n            expr_values[':email'] = data['email']\n\n        # Update user\n        result = table.update_item(\n            Key={'user_id': user_id},\n            UpdateExpression=update_expr,\n            ExpressionAttributeNames={'#n': 'name'} if 'name' in data else {},\n            ExpressionAttributeValues=expr_values,\n            ReturnValues='ALL_NEW'\n        )\n\n        return response(200, result['Attributes'])\n\n    except json.JSONDecodeError:\n        return response(400, {'error': 'Invalid JSON'})\n    except Exception as e:\n        print(f\"Error updating user: {str(e)}\")\n        return response(500, {'error': 'Internal server error'})\n\ndef delete_user(event, context):\n    \"\"\"Delete a user\"\"\"\n    try:\n        user_id = event['pathParameters']['user_id']\n\n        # Check if user exists\n        existing = table.get_item(Key={'user_id': user_id})\n        if 'Item' not in existing:\n            return response(404, {'error': 'User not found'})\n\n        table.delete_item(Key={'user_id': user_id})\n\n        return response(204, {})\n\n    except Exception as e:\n        print(f\"Error deleting user: {str(e)}\")\n        return response(500, {'error': 'Internal server error'})\n\n# requirements.txt\n# boto3==1.26.137\n# botocore==1.29.137",
    "hints": [
      "Use environment variables for table names",
      "Validate input before database operations",
      "Return appropriate HTTP status codes",
      "Use GSI for non-key queries",
      "Implement pagination for list operations"
    ],
    "testCases": [
      {
        "input": "POST /users {\"name\":\"John\",\"email\":\"john@test.com\"}",
        "expectedOutput": "201 Created with user object",
        "isHidden": false,
        "description": "Create a new user"
      },
      {
        "input": "GET /users/USER_ID",
        "expectedOutput": "200 OK with user details",
        "isHidden": false,
        "description": "Get user by ID"
      },
      {
        "input": "DELETE /users/USER_ID",
        "expectedOutput": "204 No Content",
        "isHidden": false,
        "description": "Delete a user"
      }
    ],
    "language": "python"
  },
  {
    "id": "cs405-ex-6-1",
    "subjectId": "cs405",
    "topicId": "cs405-topic-6",
    "title": "Implement S3 Lifecycle Policies",
    "difficulty": 2,
    "description": "Create AWS CLI scripts to:\n\n1. Create S3 bucket with versioning\n2. Configure lifecycle policies (transition to IA after 30 days, Glacier after 90 days)\n3. Enable encryption\n4. Set up bucket policy for public read\n5. Upload files and verify transitions",
    "starterCode": "#!/bin/bash\n# TODO: Create bucket\n# TODO: Enable versioning\n# TODO: Configure lifecycle\n# TODO: Enable encryption\n# TODO: Set bucket policy",
    "solution": "#!/bin/bash\n# S3 Lifecycle Management Script\n\nBUCKET_NAME=\"my-lifecycle-bucket-$(date +%s)\"\nREGION=\"us-east-1\"\n\n# Create bucket\necho \"Creating bucket: $BUCKET_NAME\"\naws s3api create-bucket \\\n  --bucket $BUCKET_NAME \\\n  --region $REGION\n\n# Enable versioning\necho \"Enabling versioning...\"\naws s3api put-bucket-versioning \\\n  --bucket $BUCKET_NAME \\\n  --versioning-configuration Status=Enabled\n\n# Enable encryption\necho \"Enabling encryption...\"\naws s3api put-bucket-encryption \\\n  --bucket $BUCKET_NAME \\\n  --server-side-encryption-configuration '{\n    \"Rules\": [{\n      \"ApplyServerSideEncryptionByDefault\": {\n        \"SSEAlgorithm\": \"AES256\"\n      }\n    }]\n  }'\n\n# Configure lifecycle policy\necho \"Configuring lifecycle policy...\"\ncat > lifecycle-policy.json << 'POLICY'\n{\n  \"Rules\": [\n    {\n      \"Id\": \"TransitionToIA\",\n      \"Status\": \"Enabled\",\n      \"Transitions\": [\n        {\n          \"Days\": 30,\n          \"StorageClass\": \"STANDARD_IA\"\n        },\n        {\n          \"Days\": 90,\n          \"StorageClass\": \"GLACIER\"\n        }\n      ]\n    },\n    {\n      \"Id\": \"DeleteOldVersions\",\n      \"Status\": \"Enabled\",\n      \"NoncurrentVersionExpiration\": {\n        \"NoncurrentDays\": 180\n      }\n    },\n    {\n      \"Id\": \"AbortIncompleteMultipartUpload\",\n      \"Status\": \"Enabled\",\n      \"AbortIncompleteMultipartUpload\": {\n        \"DaysAfterInitiation\": 7\n      }\n    }\n  ]\n}\nPOLICY\n\naws s3api put-bucket-lifecycle-configuration \\\n  --bucket $BUCKET_NAME \\\n  --lifecycle-configuration file://lifecycle-policy.json\n\n# Set bucket policy for public read (optional)\necho \"Setting bucket policy...\"\ncat > bucket-policy.json << POLICY\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"PublicReadGetObject\",\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": \"arn:aws:s3:::$BUCKET_NAME/*\"\n    }\n  ]\n}\nPOLICY\n\naws s3api put-bucket-policy \\\n  --bucket $BUCKET_NAME \\\n  --policy file://bucket-policy.json\n\n# Upload test files\necho \"Uploading test files...\"\necho \"Test content\" > test-file.txt\naws s3 cp test-file.txt s3://$BUCKET_NAME/\n\n# Verify configuration\necho \"\\nBucket configuration:\"\naws s3api get-bucket-versioning --bucket $BUCKET_NAME\naws s3api get-bucket-lifecycle-configuration --bucket $BUCKET_NAME\naws s3api get-bucket-encryption --bucket $BUCKET_NAME\n\necho \"\\nBucket created: $BUCKET_NAME\"\necho \"Files will transition to IA after 30 days, Glacier after 90 days\"\necho \"Old versions deleted after 180 days\"",
    "hints": [
      "Enable versioning before lifecycle policies",
      "Use lifecycle transitions for cost optimization",
      "Always enable encryption",
      "Clean up incomplete multipart uploads"
    ],
    "testCases": [
      {
        "input": "aws s3api get-bucket-lifecycle",
        "expectedOutput": "Lifecycle rules configured",
        "isHidden": false,
        "description": "Verify lifecycle policies are configured"
      },
      {
        "input": "aws s3api get-bucket-versioning",
        "expectedOutput": "Versioning enabled",
        "isHidden": false,
        "description": "Verify bucket versioning is enabled"
      }
    ],
    "language": "bash"
  },
  {
    "id": "cs405-ex-7-1",
    "subjectId": "cs405",
    "topicId": "cs405-topic-7",
    "title": "Implement 12-Factor App",
    "difficulty": 3,
    "description": "Create a cloud-native application following 12-factor principles:\n\n1. Configuration via environment variables\n2. Stateless design\n3. Proper logging\n4. Graceful shutdown\n5. Health check endpoints\n6. Containerized deployment\n\nInclude Dockerfile and deployment manifests.",
    "starterCode": "# app.py\n# TODO: Implement 12-factor app\n\n# Dockerfile\n# TODO: Create Dockerfile\n\n# k8s/deployment.yaml\n# TODO: Create Kubernetes deployment",
    "solution": "# app.py\nimport os\nimport sys\nimport signal\nimport logging\nfrom flask import Flask, jsonify, request\nimport redis\nfrom datetime import datetime\n\n# Configure logging (Factor 11: Logs as event streams)\nlogging.basicConfig(\n    level=os.environ.get('LOG_LEVEL', 'INFO'),\n    format='{\"timestamp\":\"%(asctime)s\",\"level\":\"%(levelname)s\",\"message\":\"%(message)s\"}',\n    stream=sys.stdout\n)\nlogger = logging.getLogger(__name__)\n\napp = Flask(__name__)\n\n# Factor 3: Config in environment\nCONFIG = {\n    'REDIS_HOST': os.environ.get('REDIS_HOST', 'localhost'),\n    'REDIS_PORT': int(os.environ.get('REDIS_PORT', 6379)),\n    'PORT': int(os.environ.get('PORT', 8080)),\n    'WORKERS': int(os.environ.get('WORKERS', 4))\n}\n\n# Factor 4: Backing services as attached resources\nredis_client = None\n\ndef init_redis():\n    \"\"\"Initialize Redis connection\"\"\"\n    global redis_client\n    try:\n        redis_client = redis.Redis(\n            host=CONFIG['REDIS_HOST'],\n            port=CONFIG['REDIS_PORT'],\n            decode_responses=True,\n            socket_connect_timeout=5\n        )\n        redis_client.ping()\n        logger.info(f\"Connected to Redis at {CONFIG['REDIS_HOST']}:{CONFIG['REDIS_PORT']}\")\n    except Exception as e:\n        logger.error(f\"Failed to connect to Redis: {str(e)}\")\n        redis_client = None\n\n# Factor 9: Disposability (fast startup)\ninit_redis()\n\n# Health check endpoint\n@app.route('/health')\ndef health():\n    \"\"\"Health check endpoint\"\"\"\n    health_status = {\n        'status': 'healthy',\n        'timestamp': datetime.utcnow().isoformat(),\n        'redis': 'connected' if redis_client and redis_client.ping() else 'disconnected'\n    }\n    status_code = 200 if health_status['redis'] == 'connected' else 503\n    return jsonify(health_status), status_code\n\n# Readiness check\n@app.route('/ready')\ndef ready():\n    \"\"\"Readiness check endpoint\"\"\"\n    if redis_client and redis_client.ping():\n        return jsonify({'status': 'ready'}), 200\n    return jsonify({'status': 'not ready'}), 503\n\n# Factor 6: Stateless processes\n@app.route('/counter', methods=['GET', 'POST'])\ndef counter():\n    \"\"\"Stateless counter using Redis\"\"\"\n    if not redis_client:\n        return jsonify({'error': 'Redis not available'}), 503\n\n    if request.method == 'POST':\n        count = redis_client.incr('counter')\n        logger.info(f\"Counter incremented to {count}\")\n        return jsonify({'count': count}), 200\n    else:\n        count = redis_client.get('counter') or 0\n        return jsonify({'count': int(count)}), 200\n\n# Factor 9: Graceful shutdown\nshutdown_flag = False\n\ndef signal_handler(signum, frame):\n    \"\"\"Handle shutdown signals\"\"\"\n    global shutdown_flag\n    logger.info(f\"Received signal {signum}, initiating graceful shutdown...\")\n    shutdown_flag = True\n\n    # Close Redis connection\n    if redis_client:\n        redis_client.close()\n        logger.info(\"Redis connection closed\")\n\n    logger.info(\"Shutdown complete\")\n    sys.exit(0)\n\nsignal.signal(signal.SIGTERM, signal_handler)\nsignal.signal(signal.SIGINT, signal_handler)\n\nif __name__ == '__main__':\n    logger.info(f\"Starting application on port {CONFIG['PORT']}\")\n    # Factor 7: Port binding\n    app.run(\n        host='0.0.0.0',\n        port=CONFIG['PORT'],\n        debug=False\n    )\n\n# Dockerfile\nFROM python:3.11-slim as base\n\n# Factor 2: Dependencies explicitly declared\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Factor 5: Build, release, run (build stage)\nFROM python:3.11-slim\n\n# Install dumb-init for proper signal handling\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends dumb-init && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Create non-root user\nRUN useradd -m -u 1000 appuser\n\nWORKDIR /app\n\n# Copy dependencies\nCOPY --from=base /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages\n\n# Copy application\nCOPY --chown=appuser:appuser app.py .\n\nUSER appuser\n\n# Factor 7: Port binding\nEXPOSE 8080\n\n# Factor 9: Disposability\nENTRYPOINT [\"dumb-init\", \"--\"]\nCMD [\"python\", \"app.py\"]\n\n# requirements.txt\nFlask==2.3.2\nredis==4.5.5\ngunicorn==20.1.0\n\n# k8s/deployment.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  LOG_LEVEL: \"INFO\"\n  WORKERS: \"4\"\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cloud-native-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cloud-native-app\n  template:\n    metadata:\n      labels:\n        app: cloud-native-app\n    spec:\n      containers:\n      - name: app\n        image: cloud-native-app:1.0\n        ports:\n        - containerPort: 8080\n        env:\n        # Factor 3: Config from environment\n        - name: PORT\n          value: \"8080\"\n        - name: REDIS_HOST\n          value: redis\n        - name: REDIS_PORT\n          value: \"6379\"\n        - name: LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: LOG_LEVEL\n        # Factor 10: Dev/prod parity\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"500m\"\n        # Factor 9: Fast shutdown\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"sleep\", \"5\"]\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: cloud-native-app\nspec:\n  selector:\n    app: cloud-native-app\n  ports:\n  - port: 80\n    targetPort: 8080\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:7-alpine\n        ports:\n        - containerPort: 6379\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis\nspec:\n  selector:\n    app: redis\n  ports:\n  - port: 6379\n    targetPort: 6379",
    "hints": [
      "Use environment variables for all configuration",
      "Design stateless processes",
      "Implement graceful shutdown",
      "Log to stdout/stderr",
      "Fast startup and shutdown"
    ],
    "testCases": [
      {
        "input": "curl /health",
        "expectedOutput": "200 OK with health status",
        "isHidden": false,
        "description": "Test health check endpoint"
      },
      {
        "input": "SIGTERM signal",
        "expectedOutput": "Graceful shutdown within 5 seconds",
        "isHidden": false,
        "description": "Verify graceful shutdown handling"
      }
    ],
    "language": "python"
  }
]