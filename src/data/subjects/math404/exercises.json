[
  {
    "id": "math404-t1-ex1",
    "subjectId": "math404",
    "topicId": "topic-1",
    "type": "written",
    "title": "Problem Formulation Basics",
    "description": "Formulate an optimization problem from a word description, identifying decision variables, objective function, and constraints.",
    "difficulty": 2,
    "hints": [
      "Decision variables are the quantities you can control",
      "The objective is what you want to maximize or minimize",
      "Constraints are limitations on the decision variables",
      "Include non-negativity constraints when appropriate"
    ],
    "solution": "To formulate an optimization problem:\n\n1. **Decision Variables**: Identify quantities you can choose\n   Let x₁ = units of product A, x₂ = units of product B\n\n2. **Objective Function**: Write what to optimize\n   max f(x) = 5x₁ + 3x₂ (profit)\n\n3. **Constraints**: List all restrictions\n   2x₁ + x₂ ≤ 100 (labor hours)\n   x₁ + 2x₂ ≤ 80 (material)\n\n4. **Non-negativity**: x₁, x₂ ≥ 0\n\nComplete formulation:\nmax 5x₁ + 3x₂\ns.t. 2x₁ + x₂ ≤ 100\n     x₁ + 2x₂ ≤ 80\n     x₁, x₂ ≥ 0"
  },
  {
    "id": "math404-t1-ex2",
    "subjectId": "math404",
    "topicId": "topic-1",
    "type": "written",
    "title": "Convexity Verification",
    "description": "Determine whether the function f(x) = x₁² + x₂² - x₁x₂ is convex by analyzing its Hessian matrix.",
    "difficulty": 3,
    "hints": [
      "Compute the Hessian matrix of second partial derivatives",
      "A symmetric matrix is PSD if all eigenvalues are non-negative",
      "For 2×2 matrices, check det(H) ≥ 0 and trace(H) ≥ 0"
    ],
    "solution": "f(x) = x₁² + x₂² - x₁x₂\n\n**Step 1: Compute the Hessian**\n∂f/∂x₁ = 2x₁ - x₂\n∂f/∂x₂ = 2x₂ - x₁\n∂²f/∂x₁² = 2\n∂²f/∂x₂² = 2\n∂²f/∂x₁∂x₂ = -1\n\nH = [[2, -1], [-1, 2]]\n\n**Step 2: Check eigenvalues**\ndet(H - λI) = (2-λ)² - 1 = 0\nλ² - 4λ + 3 = 0\nλ = (4 ± 2)/2 → λ₁ = 1, λ₂ = 3\n\n**Conclusion**: Both eigenvalues are positive, so H is positive definite.\nTherefore f is strictly convex."
  },
  {
    "id": "math404-t1-ex3",
    "subjectId": "math404",
    "topicId": "topic-1",
    "type": "written",
    "title": "First-Order Optimality Conditions",
    "description": "Find and classify the stationary points of f(x,y) = x³ - 3xy + y³.",
    "difficulty": 4,
    "hints": [
      "Set ∇f = 0 and solve the system of equations",
      "Use substitution to solve the nonlinear system",
      "Classify using the second derivative test (Hessian)"
    ],
    "solution": "**Finding stationary points:**\n∂f/∂x = 3x² - 3y = 0 → y = x²\n∂f/∂y = -3x + 3y² = 0 → x = y²\n\nSubstituting y = x² into x = y²:\nx = (x²)² = x⁴\nx⁴ - x = 0\nx(x³ - 1) = 0\n\nSo x = 0 or x = 1\nPoints: (0,0) and (1,1)\n\n**Classification using Hessian:**\nH = [[6x, -3], [-3, 6y]]\n\nAt (0,0): H = [[0, -3], [-3, 0]]\ndet(H) = -9 < 0 → **Saddle point**\n\nAt (1,1): H = [[6, -3], [-3, 6]]\ndet(H) = 36 - 9 = 27 > 0\ntrace(H) = 12 > 0 → **Local minimum**\n\nf(1,1) = 1 - 3 + 1 = -1"
  },
  {
    "id": "math404-t1-ex04",
    "subjectId": "math404",
    "topicId": "topic-1",
    "type": "written",
    "title": "Feasible Region Analysis",
    "description": "Determine the feasible region for a system of constraints and verify if given points are feasible.",
    "difficulty": 2,
    "hints": [
      "Check each constraint individually",
      "A point is feasible only if it satisfies ALL constraints",
      "Sketch the region if working in 2D"
    ],
    "solution": "**Constraints:**\nx₁ + 2x₂ ≤ 10\n3x₁ + x₂ ≤ 12\nx₁, x₂ ≥ 0\n\n**Test point (2, 3):**\n- x₁ + 2x₂ = 2 + 6 = 8 ≤ 10 ✓\n- 3x₁ + x₂ = 6 + 3 = 9 ≤ 12 ✓\n- x₁ = 2 ≥ 0, x₂ = 3 ≥ 0 ✓\n**Result:** Feasible\n\n**Test point (5, 4):**\n- x₁ + 2x₂ = 5 + 8 = 13 ≤ 10 ✗\n**Result:** Infeasible\n\n**Vertices of feasible region:**\n- (0, 0)\n- (4, 0)\n- (0, 5)\n- (2, 4): intersection of x₁ + 2x₂ = 10 and 3x₁ + x₂ = 12\n\nThe feasible region is a bounded polyhedron (quadrilateral)."
  },
  {
    "id": "math404-t1-ex05",
    "subjectId": "math404",
    "topicId": "topic-1",
    "type": "written",
    "title": "Constraint Qualification - LICQ",
    "description": "Verify whether LICQ (Linear Independence Constraint Qualification) holds at a given point.",
    "difficulty": 3,
    "hints": [
      "LICQ requires active constraint gradients to be linearly independent",
      "Find which constraints are active (equality holds)",
      "Compute gradient vectors and check rank"
    ],
    "solution": "**Problem:** min f(x) s.t.\ng₁(x) = x₁² + x₂² - 1 ≤ 0\ng₂(x) = x₁ + x₂ - 1 ≤ 0\nh(x) = x₁ - x₂ = 0\n\n**At point x* = (1/√2, 1/√2):**\n\n**Active constraints:**\n- g₁(x*) = 1/2 + 1/2 - 1 = 0 (active)\n- g₂(x*) = 1/√2 + 1/√2 - 1 = √2 - 1 ≈ 0.414 > 0 (inactive)\n- h(x*) = 0 (always active)\n\n**Gradients:**\n∇g₁ = [2x₁, 2x₂] = [√2, √2]\n∇h = [1, -1]\n\n**Check linear independence:**\nForm matrix: [[√2, √2], [1, -1]]\ndet = -√2 - √2 = -2√2 ≠ 0\n\n**Conclusion:** LICQ holds at x* since the gradients are linearly independent."
  },
  {
    "id": "math404-t1-ex06",
    "subjectId": "math404",
    "topicId": "topic-1",
    "type": "written",
    "title": "Local vs Global Minimum",
    "description": "Determine whether a local minimum is also a global minimum for a given function.",
    "difficulty": 3,
    "hints": [
      "Check if the function is convex",
      "For convex functions: local minimum = global minimum",
      "For non-convex: need to check all critical points"
    ],
    "solution": "**Example 1:** f(x) = x² on ℝ\n- f''(x) = 2 > 0 → convex\n- Critical point: f'(x) = 2x = 0 → x* = 0\n- f(0) = 0 is both local and global minimum ✓\n\n**Example 2:** f(x) = x³ - 3x on [-2, 2]\n- f'(x) = 3x² - 3 = 0 → x = ±1\n- f(-1) = -1 + 3 = 2 (local max)\n- f(1) = 1 - 3 = -2 (local min)\n- Boundary: f(-2) = -8 + 6 = -2, f(2) = 8 - 6 = 2\n- Global minimum: min{f(-2), f(1), f(2)} = -2 at x = ±1, -2\n\n**Example 3:** f(x,y) = x² - y² (saddle function)\n- Not convex (indefinite Hessian)\n- (0,0) is stationary but NOT a local minimum\n- No global minimum (unbounded below)\n\n**Key insight:** Convexity guarantees local = global."
  },
  {
    "id": "math404-t1-ex07",
    "subjectId": "math404",
    "topicId": "topic-1",
    "type": "written",
    "title": "Portfolio Optimization Formulation",
    "description": "Formulate a portfolio optimization problem with risk and return constraints.",
    "difficulty": 3,
    "hints": [
      "Decision variables: portfolio weights wᵢ",
      "Objective: minimize risk (variance) or maximize return",
      "Constraints: weights sum to 1, possibly bounds on weights"
    ],
    "solution": "**Problem Statement:**\nInvestor has n assets with expected returns μᵢ and covariance matrix Σ.\nGoal: Find portfolio weights to maximize return for given risk level.\n\n**Decision Variables:**\nw = [w₁, w₂, ..., wₙ]ᵀ (fraction invested in each asset)\n\n**Objective:**\nMaximize expected return: max wᵀμ\nOR minimize risk: min wᵀΣw\n\n**Constraints:**\n1. Σwᵢ = 1 (weights sum to 100%)\n2. w ≥ 0 (no short selling)\n3. wᵀΣw ≤ σ² (risk limit)\n4. wᵀμ ≥ r_target (minimum return)\n\n**Markowitz Mean-Variance:**\nmin (1/2)wᵀΣw - λwᵀμ\ns.t. Σwᵢ = 1, w ≥ 0\n\nwhere λ controls risk-return tradeoff.\n\n**Example with 2 assets:**\nw₁ + w₂ = 1\nmin σ₁²w₁² + σ₂²w₂² + 2ρσ₁σ₂w₁w₂\n\nThis is a convex QP."
  },
  {
    "id": "math404-t1-ex08",
    "subjectId": "math404",
    "topicId": "topic-1",
    "type": "written",
    "title": "Strict vs Non-Strict Convexity",
    "description": "Distinguish between strictly convex and convex functions, and identify implications for optimization.",
    "difficulty": 3,
    "hints": [
      "Strict convexity: strict inequality in definition",
      "Strictly convex functions have unique minimizers",
      "Check second derivative test"
    ],
    "solution": "**Definitions:**\n\n**Convex:** f(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y)\n**Strictly convex:** f(λx + (1-λ)y) < λf(x) + (1-λ)f(y) for x ≠ y, λ ∈ (0,1)\n\n**Second-order characterization:**\n- Convex: ∇²f(x) ⪰ 0 (PSD)\n- Strictly convex: ∇²f(x) ≻ 0 (PD)\n\n**Examples:**\n\n1. **f(x) = x²** (strictly convex)\n   - f''(x) = 2 > 0\n   - Unique minimizer at x = 0\n\n2. **f(x) = |x|** (convex but not strictly)\n   - Convex but not differentiable at 0\n   - Unique minimizer at x = 0\n\n3. **f(x,y) = x²** (convex but not strictly)\n   - ∇²f = [[2, 0], [0, 0]] (PSD not PD)\n   - Minimizers: entire y-axis (not unique)\n\n**Implications:**\n- Strictly convex → unique global minimum\n- Convex → possibly multiple minimizers (but all optimal)\n- Non-convex → multiple local minima possible"
  },
  {
    "id": "math404-t1-ex09",
    "subjectId": "math404",
    "topicId": "topic-1",
    "type": "written",
    "title": "Production Planning Formulation",
    "description": "Formulate an optimization problem for production planning with capacity and demand constraints.",
    "difficulty": 2,
    "hints": [
      "Variables: production quantities for each product",
      "Objective: maximize profit or minimize cost",
      "Constraints: resource limits, demand requirements"
    ],
    "solution": "**Scenario:**\nFactory produces 3 products using 2 machines.\n- Product A: profit $5, needs 2h machine 1, 1h machine 2\n- Product B: profit $3, needs 1h machine 1, 2h machine 2\n- Product C: profit $4, needs 1h machine 1, 1h machine 2\n- Machine 1: 100 hours available\n- Machine 2: 80 hours available\n- Demand: at least 10 units of product A\n\n**Decision Variables:**\nx₁ = units of product A\nx₂ = units of product B\nx₃ = units of product C\n\n**Objective:**\nmax 5x₁ + 3x₂ + 4x₃\n\n**Constraints:**\n2x₁ + x₂ + x₃ ≤ 100  (machine 1 capacity)\nx₁ + 2x₂ + x₃ ≤ 80   (machine 2 capacity)\nx₁ ≥ 10              (minimum demand)\nx₁, x₂, x₃ ≥ 0       (non-negativity)\n\nThis is a **linear program** that can be solved by simplex or interior point methods."
  },
  {
    "id": "math404-t1-ex10",
    "subjectId": "math404",
    "topicId": "topic-1",
    "type": "written",
    "title": "Level Sets and Contours",
    "description": "Analyze the level sets of a function and their relationship to optimization.",
    "difficulty": 3,
    "hints": [
      "Level set: {x : f(x) = c}",
      "Gradient is perpendicular to level sets",
      "At optimum, gradient equals zero (flat level set)"
    ],
    "solution": "**Function:** f(x,y) = x² + 4y²\n\n**Level sets:** {(x,y) : x² + 4y² = c}\nThese are ellipses with semi-axes √c and √c/2.\n\n**Analysis:**\n- c = 1: ellipse x²/1 + y²/(1/4) = 1\n- c = 4: ellipse x²/4 + y²/1 = 1\n- c → ∞: increasingly large ellipses\n- c = 0: single point (0,0) - the minimum\n\n**Gradient:** ∇f = [2x, 8y]\nAt (2, 1): ∇f = [4, 8], perpendicular to level curve\n\n**Optimization geometry:**\nFor min f(x,y) s.t. g(x,y) = 0:\n- Optimal when ∇f parallel to ∇g\n- This is Lagrange multiplier condition: ∇f = λ∇g\n\n**Example:** min x² + 4y² s.t. x + y = 1\n∇f = [2x, 8y], ∇g = [1, 1]\n[2x, 8y] = λ[1, 1]\n→ 2x = λ, 8y = λ\n→ 2x = 8y → x = 4y\nSubstituting: 4y + y = 1 → y = 1/5, x = 4/5\nOptimal: (4/5, 1/5), f* = 16/25 + 4/25 = 4/5"
  },
  {
    "id": "math404-t1-ex11",
    "subjectId": "math404",
    "topicId": "topic-1",
    "type": "written",
    "title": "Second-Order Sufficient Conditions",
    "description": "Apply second-order sufficient conditions to verify local optimality.",
    "difficulty": 4,
    "hints": [
      "First-order: ∇f(x*) = 0 (necessary)",
      "Second-order: ∇²f(x*) ≻ 0 for minimum (sufficient)",
      "Check eigenvalues or leading principal minors"
    ],
    "solution": "**Problem:** f(x,y) = x³ + y³ - 3xy\n\n**First-order (necessary):**\n∂f/∂x = 3x² - 3y = 0 → y = x²\n∂f/∂y = 3y² - 3x = 0 → x = y²\n\nSolving: x = (x²)² = x⁴ → x(x³ - 1) = 0\nPoints: (0,0) and (1,1)\n\n**Second-order (sufficient):**\n∇²f = [[6x, -3], [-3, 6y]]\n\n**At (0,0):**\nH = [[0, -3], [-3, 0]]\nEigenvalues: λ² - 0 + 9 = 0 → λ = ±3\nOne negative → **saddle point** (not minimum)\n\n**At (1,1):**\nH = [[6, -3], [-3, 6]]\n- Leading minors: M₁ = 6 > 0, M₂ = 36 - 9 = 27 > 0\n- Both positive → H ≻ 0\n**Conclusion:** (1,1) is a strict local minimum by second-order sufficient conditions\n\n**Verification:** f(1,1) = 1 + 1 - 3 = -1\nNearby: f(1.1, 1.1) = 1.331 + 1.331 - 3.63 = -0.968 > -1 ✓"
  },
  {
    "id": "math404-t1-ex12",
    "subjectId": "math404",
    "topicId": "topic-1",
    "type": "written",
    "title": "Multi-Objective Optimization Setup",
    "description": "Formulate a multi-objective optimization problem and discuss solution approaches.",
    "difficulty": 4,
    "hints": [
      "Multiple objectives may conflict",
      "Pareto optimality: no objective can improve without worsening another",
      "Scalarization: combine objectives with weights"
    ],
    "solution": "**Problem:** Design optimization\n- Minimize cost: f₁(x)\n- Maximize performance: f₂(x)\n- Minimize weight: f₃(x)\n\n**Approaches:**\n\n**1. Weighted Sum (Scalarization):**\nmin w₁f₁(x) - w₂f₂(x) + w₃f₃(x)\nwhere w₁ + w₂ + w₃ = 1, wᵢ ≥ 0\n\n**2. ε-Constraint Method:**\nmin f₁(x)\ns.t. f₂(x) ≥ ε₂\n     f₃(x) ≤ ε₃\n\n**3. Pareto Front:**\nSolution x* is Pareto optimal if no x exists such that:\n- fᵢ(x) ≤ fᵢ(x*) for all i\n- fⱼ(x) < fⱼ(x*) for some j\n\n**Example:**\nf₁(x) = x², f₂(x) = (x-2)² on [0,2]\n- Minimize f₁: x = 0 (but f₂ = 4)\n- Minimize f₂: x = 2 (but f₁ = 4)\n- Pareto front: all x ∈ [0,2]\n- Compromise: x = 1 (f₁ = f₂ = 1)\n\n**With weights:** min w₁x² + w₂(x-2)²\nOptimal: x* = 2w₂/(w₁ + w₂)"
  },
  {
    "id": "math404-t1-ex13",
    "subjectId": "math404",
    "topicId": "topic-1",
    "type": "written",
    "title": "Boundedness and Existence of Solutions",
    "description": "Determine when an optimization problem has a solution using the extreme value theorem.",
    "difficulty": 3,
    "hints": [
      "Extreme value theorem: continuous f on compact set has min/max",
      "Compact = closed and bounded",
      "Check domain and objective function properties"
    ],
    "solution": "**Extreme Value Theorem:**\nIf f is continuous and domain S is compact (closed + bounded), then f attains its minimum and maximum on S.\n\n**Example 1:** min x² on [0,1]\n- f(x) = x² is continuous\n- [0,1] is compact\n- **Solution exists:** x* = 0, f* = 0 ✓\n\n**Example 2:** min x² on (0,1)\n- f(x) = x² is continuous\n- (0,1) is NOT closed\n- **No minimum attained** (infimum is 0, not achieved) ✗\n\n**Example 3:** min 1/x on [1,∞)\n- f unbounded domain\n- **No minimum** (approaches 0 as x→∞, never achieved) ✗\n\n**Example 4:** min (x-1)² + (y-2)² s.t. x² + y² ≤ 4\n- f continuous\n- Feasible set is closed ball (compact)\n- **Solution exists** ✓\n\n**Checking compactness:**\n- Closed: includes boundary (≤ not <)\n- Bounded: fits in a ball\n- ℝⁿ is NOT compact\n- Closed ball is compact\n- Open ball is NOT compact\n\n**For unbounded domains:**\nAdd coercivity: f(x) → ∞ as ||x|| → ∞\nThen sublevel sets {x : f(x) ≤ c} are compact."
  },
  {
    "id": "math404-t1-ex14",
    "subjectId": "math404",
    "topicId": "topic-1",
    "type": "written",
    "title": "Diet Problem Formulation",
    "description": "Formulate the classic diet problem as a linear optimization model.",
    "difficulty": 2,
    "hints": [
      "Variables: amount of each food",
      "Objective: minimize cost",
      "Constraints: meet nutritional requirements"
    ],
    "solution": "**Problem:**\nSelect foods to meet daily nutritional needs at minimum cost.\n\n**Data:**\n- n foods: costs cᵢ per unit\n- m nutrients: minimum requirements bⱼ\n- aᵢⱼ = amount of nutrient j in food i\n\n**Decision Variables:**\nxᵢ = units of food i to consume\n\n**Objective:**\nmin Σᵢ cᵢxᵢ (total cost)\n\n**Constraints:**\nΣᵢ aᵢⱼxᵢ ≥ bⱼ  for all nutrients j\nxᵢ ≥ 0\n\n**Concrete Example:**\nFoods: Bread ($2/loaf), Milk ($3/gallon)\nNutrients: Calories (≥2000), Protein (≥50g), Calcium (≥1000mg)\n\n| Food  | Cal | Prot | Calc | Cost |\n|-------|-----|------|------|------|\n| Bread | 1200| 30g  | 200mg| $2   |\n| Milk  | 800 | 40g  | 1200mg|$3   |\n\n**Formulation:**\nmin 2x₁ + 3x₂\ns.t. 1200x₁ + 800x₂ ≥ 2000  (calories)\n     30x₁ + 40x₂ ≥ 50      (protein)\n     200x₁ + 1200x₂ ≥ 1000  (calcium)\n     x₁, x₂ ≥ 0\n\nThis is a linear program."
  },
  {
    "id": "math404-t1-ex15",
    "subjectId": "math404",
    "topicId": "topic-1",
    "type": "written",
    "title": "Equality vs Inequality Constraints",
    "description": "Compare optimization problems with equality versus inequality constraints and their geometric interpretation.",
    "difficulty": 3,
    "hints": [
      "Equality constrains to a manifold (lower dimension)",
      "Inequality defines a region",
      "Active inequalities behave like equalities at optimum"
    ],
    "solution": "**Equality Constraints:** h(x) = 0\n- Reduces dimensionality\n- No \"slack\" - must hold exactly\n- Always active\n\n**Inequality Constraints:** g(x) ≤ 0\n- Defines feasible region\n- May be active (g(x*) = 0) or inactive (g(x*) < 0)\n- Active at boundary, inactive in interior\n\n**Example 1: Equality**\nmin x² + y² s.t. x + y = 1\n- Constraint is a line in ℝ²\n- Feasible set is 1-dimensional\n- Solution: minimize distance to origin from line\n- Optimal: (1/2, 1/2)\n\n**Example 2: Inequality**\nmin x² + y² s.t. x + y ≤ 1\n- Constraint is a half-plane\n- Feasible set is 2-dimensional\n- Unconstrained minimum (0,0) is feasible\n- Optimal: (0, 0) (constraint inactive)\n\n**Example 3: Inequality becomes active**\nmin x² + y² s.t. x + y ≥ 1\n- Unconstrained minimum (0,0) is infeasible\n- Optimal on boundary: (1/2, 1/2)\n- Constraint is active, behaves like equality\n\n**Key insight:**\nAt optimum, active inequalities + equalities determine the solution.\nInactive inequalities have no effect (could remove them).\n\n**Lagrange multipliers:**\n- Equality: λ unrestricted\n- Inequality: λ ≥ 0\n- Complementary slackness: λg(x) = 0"
  },
  {
    "id": "math404-t1-ex16",
    "subjectId": "math404",
    "topicId": "topic-1",
    "type": "written",
    "title": "Jensen's Inequality Application",
    "description": "Apply Jensen's inequality to prove bounds and verify convexity properties.",
    "difficulty": 4,
    "hints": [
      "Jensen: f(E[X]) ≤ E[f(X)] for convex f",
      "Strict inequality if f strictly convex and X non-constant",
      "Can prove AM-GM inequality as special case"
    ],
    "solution": "**Jensen's Inequality:**\nFor convex f and random variable X:\nf(E[X]) ≤ E[f(X)]\n\nEquality iff f is linear or X is constant.\n\n**Application 1: Arithmetic-Geometric Mean**\nLet f(x) = -log(x) (convex for x > 0)\nFor x₁,...,xₙ > 0 with equal weights:\n\nf((x₁+...+xₙ)/n) ≤ (f(x₁)+...+f(xₙ))/n\n-log((x₁+...+xₙ)/n) ≤ -(log x₁+...+log xₙ)/n\nlog((x₁+...+xₙ)/n) ≥ log((x₁···xₙ)^(1/n))\n\nTherefore: **(x₁+...+xₙ)/n ≥ (x₁···xₙ)^(1/n)** (AM-GM)\n\n**Application 2: Variance bound**\nLet f(x) = x² (convex)\nE[X²] ≥ (E[X])²\nTherefore: **Var(X) = E[X²] - (E[X])² ≥ 0**\n\n**Application 3: Log-sum-exp**\nf(x) = eˣ is convex\nFor x₁,...,xₙ:\ne^((x₁+...+xₙ)/n) ≤ (e^x₁+...+e^xₙ)/n\n\n**Application 4: Optimization**\nmin f(x) s.t. x ∈ C (convex)\nAny convex combination of feasible points is feasible.\nIf x*, y* both optimal, then (x*+y*)/2 is also optimal (for strictly convex, this gives uniqueness).\n\n**Reverse Jensen:** For concave f:\nf(E[X]) ≥ E[f(X)]\n\nExample: log is concave\nlog(E[X]) ≥ E[log(X)]"
  },
  {
    "id": "math404-t2-ex1",
    "subjectId": "math404",
    "topicId": "topic-2",
    "type": "written",
    "title": "LP Standard Form Conversion",
    "description": "Convert a linear program with inequality constraints to standard form.",
    "difficulty": 2,
    "hints": [
      "Standard form requires Ax = b with x ≥ 0",
      "Add slack variables for ≤ constraints",
      "Subtract surplus variables for ≥ constraints",
      "Replace free variables x with x⁺ - x⁻"
    ],
    "solution": "**Problem:** Convert to standard form:\nmax 3x₁ + 2x₂\ns.t. x₁ + x₂ ≤ 4\n     2x₁ + x₂ ≥ 2\n     x₁ ≥ 0, x₂ free\n\n**Solution:**\n1. Convert max to min: min -3x₁ - 2x₂\n2. Add slack s₁: x₁ + x₂ + s₁ = 4\n3. Subtract surplus s₂: 2x₁ + x₂ - s₂ = 2\n4. Replace x₂ = x₂⁺ - x₂⁻\n\n**Standard form:**\nmin -3x₁ - 2x₂⁺ + 2x₂⁻\ns.t. x₁ + x₂⁺ - x₂⁻ + s₁ = 4\n     2x₁ + x₂⁺ - x₂⁻ - s₂ = 2\n     x₁, x₂⁺, x₂⁻, s₁, s₂ ≥ 0"
  },
  {
    "id": "math404-t2-ex2",
    "subjectId": "math404",
    "topicId": "topic-2",
    "type": "written",
    "title": "Simplex Method Iteration",
    "description": "Perform one iteration of the simplex method on a given tableau.",
    "difficulty": 3,
    "hints": [
      "Select entering variable with most negative coefficient in objective row",
      "Use minimum ratio test to select leaving variable",
      "Pivot to create new basic feasible solution"
    ],
    "solution": "**Simplex Iteration:**\n\nInitial tableau:\n| BV  | x₁ | x₂ | s₁ | s₂ | RHS |\n|-----|----|----|----|----|-----|\n| s₁  | 1  | 2  | 1  | 0  | 8   |\n| s₂  | 4  | 1  | 0  | 1  | 12  |\n| z   | -3 | -2 | 0  | 0  | 0   |\n\n1. **Entering**: x₁ (most negative: -3)\n2. **Leaving**: min(8/1, 12/4) = 3 → s₂ leaves\n3. **Pivot** on row 2, column 1\n\nAfter pivoting:\n| BV  | x₁ | x₂   | s₁ | s₂  | RHS |\n|-----|----|----- |----|----|------|\n| s₁  | 0  | 7/4  | 1  |-1/4| 5    |\n| x₁  | 1  | 1/4  | 0  |1/4 | 3    |\n| z   | 0  |-5/4  | 0  |3/4 | 9    |\n\nCurrent solution: x₁ = 3, x₂ = 0, z = 9"
  },
  {
    "id": "math404-t2-ex3",
    "subjectId": "math404",
    "topicId": "topic-2",
    "type": "written",
    "title": "LP Infeasibility and Unboundedness",
    "description": "Identify whether an LP is infeasible, unbounded, or has a unique optimal solution.",
    "difficulty": 4,
    "hints": [
      "Check if Phase I finds feasible solution",
      "Unbounded if entering variable has no positive coefficients",
      "Multiple optima if non-basic variable has zero reduced cost"
    ],
    "solution": "**Analysis Procedure:**\n\n**Case 1: Infeasibility**\nmax 2x₁ + 3x₂, s.t. x₁ + x₂ ≤ 2, x₁ + x₂ ≥ 4, x ≥ 0\nNo x satisfies both constraints → **Infeasible**\n\n**Case 2: Unboundedness**\nmax 2x₁ + 3x₂, s.t. x₁ - x₂ ≤ 1, x ≥ 0\nMoving along direction (0, 1) increases objective unboundedly → **Unbounded**\n\n**Case 3: Unique Optimal**\nmax x₁ + x₂, s.t. x₁ + 2x₂ ≤ 4, 2x₁ + x₂ ≤ 4, x ≥ 0\nOptimal at (4/3, 4/3), objective = 8/3 → **Unique optimal**\n\n**Detection in Simplex:**\n- Infeasible: Phase I terminates with artificial variables > 0\n- Unbounded: All coefficients in pivot column ≤ 0\n- Multiple optima: Zero reduced cost for non-basic variable"
  },
  {
    "id": "math404-t2-ex04",
    "subjectId": "math404",
    "topicId": "topic-2",
    "type": "written",
    "title": "Basic Feasible Solutions",
    "description": "Identify all basic feasible solutions for a given LP and determine which are vertices of the feasible region.",
    "difficulty": 3,
    "hints": [
      "BFS: Set n-m variables to zero, solve for remaining m",
      "Check if solution satisfies all constraints",
      "Each BFS corresponds to a vertex of the polytope"
    ],
    "solution": "**Problem:** x₁ + x₂ + s₁ = 4, 2x₁ + x₂ + s₂ = 6, x, s ≥ 0\n(n=4 variables, m=2 equations → 2 basic, 2 nonbasic)\n\n**Finding all BFS:**\n\n**1. x₁, x₂ basic (s₁=s₂=0):**\nx₁ + x₂ = 4, 2x₁ + x₂ = 6\n→ x₁ = 2, x₂ = 2\nCheck: x ≥ 0 ✓ → **BFS at (2,2)**\n\n**2. x₁, s₁ basic (x₂=s₂=0):**\nx₁ + s₁ = 4, 2x₁ = 6\n→ x₁ = 3, s₁ = 1\nCheck: x₁, s₁ ≥ 0 ✓ → **BFS at (3,0)**\n\n**3. x₁, s₂ basic (x₂=s₁=0):**\nx₁ = 4, 2x₁ + s₂ = 6\n→ x₁ = 4, s₂ = -2\nCheck: s₂ < 0 ✗ → **Not feasible**\n\n**4. x₂, s₁ basic (x₁=s₂=0):**\nx₂ + s₁ = 4, x₂ = 6\n→ x₂ = 6, s₁ = -2\nCheck: s₁ < 0 ✗ → **Not feasible**\n\n**5. x₂, s₂ basic (x₁=s₁=0):**\nx₂ = 4, x₂ + s₂ = 6\n→ x₂ = 4, s₂ = 2\nCheck: x₂, s₂ ≥ 0 ✓ → **BFS at (0,4)**\n\n**6. s₁, s₂ basic (x₁=x₂=0):**\ns₁ = 4, s₂ = 6\nCheck: s ≥ 0 ✓ → **BFS at (0,0)**\n\n**Vertices:** (0,0), (3,0), (2,2), (0,4)"
  },
  {
    "id": "math404-t2-ex05",
    "subjectId": "math404",
    "topicId": "topic-2",
    "type": "written",
    "title": "Graphical LP Solution",
    "description": "Solve a 2-variable LP problem graphically by plotting constraints and identifying the optimal vertex.",
    "difficulty": 2,
    "hints": [
      "Plot each constraint as a line",
      "Shade the feasible region",
      "Evaluate objective at each vertex",
      "Optimal solution is at a vertex"
    ],
    "solution": "**Problem:** max 3x₁ + 2x₂\ns.t. x₁ + x₂ ≤ 4\n     2x₁ + x₂ ≤ 6\n     x₁, x₂ ≥ 0\n\n**Step 1: Plot constraints**\n- x₁ + x₂ = 4: passes through (4,0) and (0,4)\n- 2x₁ + x₂ = 6: passes through (3,0) and (0,6)\n- x₁ ≥ 0: right of y-axis\n- x₂ ≥ 0: above x-axis\n\n**Step 2: Find vertices**\n- (0, 0): origin\n- (3, 0): intersection of 2x₁ + x₂ = 6 and x₂ = 0\n- (2, 2): intersection of x₁ + x₂ = 4 and 2x₁ + x₂ = 6\n  Solving: x₁ + x₂ = 4, 2x₁ + x₂ = 6 → x₁ = 2, x₂ = 2\n- (0, 4): intersection of x₁ + x₂ = 4 and x₁ = 0\n\n**Step 3: Evaluate objective**\n| Vertex | z = 3x₁ + 2x₂ |\n|--------|---------------|\n| (0,0)  | 0             |\n| (3,0)  | 9             |\n| (2,2)  | 10            |\n| (0,4)  | 8             |\n\n**Optimal:** x* = (2, 2), z* = 10"
  },
  {
    "id": "math404-t2-ex06",
    "subjectId": "math404",
    "topicId": "topic-2",
    "type": "written",
    "title": "Slack Variables and Standard Form",
    "description": "Convert an LP with mixed constraints to standard form by introducing slack and surplus variables.",
    "difficulty": 2,
    "hints": [
      "Add slack for ≤ constraints",
      "Subtract surplus for ≥ constraints",
      "Replace free variables with difference of non-negative variables"
    ],
    "solution": "**Original Problem:**\nmax 2x₁ + 3x₂ - x₃\ns.t. x₁ + 2x₂ + x₃ ≤ 10\n     3x₁ - x₂ ≥ 5\n     x₁ - x₃ = 2\n     x₁, x₂ ≥ 0, x₃ free\n\n**Step 1: Convert to minimization**\nmin -2x₁ - 3x₂ + x₃\n\n**Step 2: Handle constraints**\n- ≤ constraint: add slack s₁\n  x₁ + 2x₂ + x₃ + s₁ = 10, s₁ ≥ 0\n- ≥ constraint: subtract surplus s₂\n  3x₁ - x₂ - s₂ = 5, s₂ ≥ 0\n- = constraint: keep as is\n  x₁ - x₃ = 2\n\n**Step 3: Handle free variable**\nReplace x₃ = x₃⁺ - x₃⁻ where x₃⁺, x₃⁻ ≥ 0\n\n**Standard Form:**\nmin -2x₁ - 3x₂ + x₃⁺ - x₃⁻\ns.t. x₁ + 2x₂ + x₃⁺ - x₃⁻ + s₁ = 10\n     3x₁ - x₂ - s₂ = 5\n     x₁ - x₃⁺ + x₃⁻ = 2\n     x₁, x₂, x₃⁺, x₃⁻, s₁, s₂ ≥ 0\n\n**Matrix form:** min cᵀx s.t. Ax = b, x ≥ 0\nwhere x = [x₁, x₂, x₃⁺, x₃⁻, s₁, s₂]ᵀ"
  },
  {
    "id": "math404-t2-ex07",
    "subjectId": "math404",
    "topicId": "topic-2",
    "type": "written",
    "title": "Degeneracy in Simplex",
    "description": "Identify degenerate basic feasible solutions and explain their impact on the simplex algorithm.",
    "difficulty": 3,
    "hints": [
      "Degeneracy: more than n-m variables are zero",
      "Can cause cycling in simplex",
      "Bland's rule prevents cycling"
    ],
    "solution": "**Definition:**\nA BFS is degenerate if more than n-m variables are zero (some basic variables = 0).\n\n**Example:** Consider x₁ + x₂ + s₁ = 4, x₁ + x₂ + s₂ = 4, x ≥ 0\n\n**BFS with x₁, s₁ basic:**\nSet x₂ = s₂ = 0: x₁ + s₁ = 4, x₁ = 4\n→ x₁ = 4, s₁ = 0\nThis is **degenerate** (3 variables zero instead of 2)\n\n**Impact on Simplex:**\n1. **Zero pivot:** Minimum ratio test may give tie\n2. **No improvement:** Objective may not change\n3. **Cycling possible:** Algorithm may revisit same basis\n\n**Example of cycling:**\nConsider highly degenerate LP:\nmax -3x₄ + 20x₅ - (1/2)x₆ + 6x₇\ns.t. (1/4)x₄ - 8x₅ - x₆ + 9x₇ ≤ 0\n     (1/2)x₄ - 12x₅ - (1/2)x₆ + 3x₇ ≤ 0\n     x₇ ≤ 1\n\nWithout anti-cycling rules, simplex can cycle through:\nBasis 1 → Basis 2 → Basis 3 → ... → Basis 1 (never terminating)\n\n**Prevention: Bland's Rule**\n- Choose smallest index entering variable among candidates\n- Choose smallest index leaving variable in case of ties\n- Guarantees finite termination\n\n**Geometric interpretation:**\nDegeneracy occurs when more than n constraints intersect at a vertex."
  },
  {
    "id": "math404-t2-ex08",
    "subjectId": "math404",
    "topicId": "topic-2",
    "type": "written",
    "title": "Revised Simplex Method",
    "description": "Apply the revised simplex method using basis matrix operations.",
    "difficulty": 4,
    "hints": [
      "Maintain basis matrix B and its inverse B⁻¹",
      "Compute reduced costs: c̄ⱼ = cⱼ - cᴮᵀB⁻¹Aⱼ",
      "Update B⁻¹ efficiently using rank-1 updates"
    ],
    "solution": "**Problem:** min cᵀx s.t. Ax = b, x ≥ 0\n\n**Revised Simplex Key Ideas:**\nInstead of full tableau, maintain only:\n- Basis B and B⁻¹\n- Basic solution xᴮ = B⁻¹b\n- Dual variables y = cᴮᵀB⁻¹\n\n**Algorithm:**\n\n**1. Pricing:**\nFor each nonbasic j:\n  c̄ⱼ = cⱼ - yᵀAⱼ = cⱼ - cᴮᵀB⁻¹Aⱼ\nIf all c̄ⱼ ≥ 0: STOP (optimal)\nElse: select entering variable k with c̄ₖ < 0\n\n**2. Ratio test:**\nCompute d = B⁻¹Aₖ (direction)\nθ = min{xᴮᵢ/dᵢ : dᵢ > 0}\nSelect leaving variable at position achieving minimum\n\n**3. Update basis:**\nReplace leaving column in B with Aₖ\nUpdate B⁻¹ using Sherman-Morrison or recompute\n\n**Example:**\nA = [[1,1,1,0], [2,1,0,1]], b = [4,6], c = [-3,-2,0,0]\nInitial basis: B = [[1,0],[0,1]] (columns 3,4)\n\n**Iteration 1:**\n- xᴮ = B⁻¹b = [4,6]\n- y = cᴮᵀB⁻¹ = [0,0]\n- c̄₁ = -3 - 0 = -3 < 0 → enter x₁\n- d = B⁻¹A₁ = [1,2]\n- θ = min{4/1, 6/2} = 3 → x₄ leaves\n- New basis: columns {3,1}\n\n**Advantage:** Sparse matrices → much faster for large LPs"
  },
  {
    "id": "math404-t2-ex09",
    "subjectId": "math404",
    "topicId": "topic-2",
    "type": "written",
    "title": "Sensitivity Analysis: RHS Changes",
    "description": "Analyze how changes in the right-hand side affect the optimal solution and objective value.",
    "difficulty": 3,
    "hints": [
      "Shadow price = dual variable value",
      "Valid for changes keeping basis optimal",
      "Recompute xᴮ = B⁻¹b for new b"
    ],
    "solution": "**Optimal Tableau:**\n| BV | x₁ | x₂ | s₁ | s₂ | RHS |\n|----|----|----|----|----|-----|\n| x₁ | 1  | 0  | 2  | -1 | 2   |\n| x₂ | 0  | 1  | -1 | 2  | 3   |\n| z  | 0  | 0  | 1  | 2  | 13  |\n\nDual variables: y₁ = 1, y₂ = 2 (shadow prices)\n\n**Question:** What if b₁ changes from 4 to 4 + Δ₁?\n\n**Analysis:**\nNew RHS = B⁻¹(b + Δ)\nFrom tableau: B⁻¹ = [[2,-1],[-1,2]]\n\nIf b₁ increases by 1:\nNew xᴮ = [[2,-1],[-1,2]][[5],[6]] = [[4],[7]]\n\n**Feasibility range:**\nNeed xᴮ ≥ 0:\n2(4+Δ₁) - 6 ≥ 0 → Δ₁ ≥ -1\n-(4+Δ₁) + 2·6 ≥ 0 → Δ₁ ≤ 8\nRange: -1 ≤ Δ₁ ≤ 8\n\n**Objective change:**\nΔz = y₁Δ₁ = 1·Δ₁\nIf Δ₁ = 2: z increases by 2\n\n**Multiple RHS changes:**\nΔz = y₁Δ₁ + y₂Δ₂\nValid only if new xᴮ ≥ 0\n\n**Economic interpretation:**\nShadow price yᵢ = marginal value of resource i\nIncreasing bᵢ by 1 unit increases optimal profit by yᵢ (within valid range)."
  },
  {
    "id": "math404-t2-ex10",
    "subjectId": "math404",
    "topicId": "topic-2",
    "type": "written",
    "title": "Sensitivity Analysis: Objective Coefficients",
    "description": "Determine the range of an objective coefficient that maintains the current optimal basis.",
    "difficulty": 3,
    "hints": [
      "Reduced cost c̄ⱼ = cⱼ - cᴮᵀB⁻¹Aⱼ",
      "Need c̄ⱼ ≥ 0 for all nonbasic j",
      "For basic variable: affects cᴮ, recalculate all c̄ⱼ"
    ],
    "solution": "**Optimal Tableau:**\n| BV | x₁ | x₂ | s₁ | s₂ | RHS | c̄ⱼ |\n|----|----|----|----|----|-----|-----|\n| x₁ | 1  | 0  | 2  | -1 | 2   |  -  |\n| x₂ | 0  | 1  | -1 | 2  | 3   |  -  |\n| z  | 0  | 0  | 1  | 2  | 13  | 0   |\n\nOriginal: max 3x₁ + 2x₂\nCurrent basis: {x₁, x₂}\n\n**Question 1: Range for c₁**\nLet c₁ = 3 + Δ\nNeed: c̄ⱼ ≥ 0 for all nonbasic {s₁, s₂}\n\nc̄ₛ₁ = 0 - [(3+Δ), 2][[2],[-1]] = -(6+2Δ-2) = -(4+2Δ)\nNeed: 4+2Δ ≥ 0 → Δ ≥ -2 → **c₁ ≥ 1**\n\nc̄ₛ₂ = 0 - [(3+Δ), 2][[-1],[2]] = -(-3-Δ+4) = -(1-Δ)\nNeed: 1-Δ ≥ 0 → Δ ≤ 1 → **c₁ ≤ 4**\n\n**Range: 1 ≤ c₁ ≤ 4**\n\n**Question 2: Range for c₂**\nLet c₂ = 2 + Δ\n\nc̄ₛ₁ = 0 - [3, (2+Δ)][[2],[-1]] = -(6-2-Δ) = -(4-Δ)\nNeed: 4-Δ ≥ 0 → **c₂ ≤ 6**\n\nc̄ₛ₂ = 0 - [3, (2+Δ)][[-1],[2]] = -(-3+4+2Δ) = -(1+2Δ)\nNeed: 1+2Δ ≥ 0 → **c₂ ≥ -1/2**\n\n**Range: -1/2 ≤ c₂ ≤ 6**\n\n**Outside range:** Basis changes, need to re-solve LP"
  },
  {
    "id": "math404-t2-ex11",
    "subjectId": "math404",
    "topicId": "topic-2",
    "type": "written",
    "title": "Two-Phase Simplex Method",
    "description": "Apply the two-phase simplex method to find an initial basic feasible solution.",
    "difficulty": 4,
    "hints": [
      "Phase I: Minimize sum of artificial variables",
      "Add artificial variables for = and ≥ constraints",
      "Phase II: Use Phase I solution as starting BFS"
    ],
    "solution": "**Problem:**\nmin 2x₁ + 3x₂\ns.t. x₁ + x₂ = 4\n     2x₁ + x₂ ≥ 5\n     x₁, x₂ ≥ 0\n\n**Phase I: Find initial BFS**\n\nAdd artificial variables a₁, a₂:\nmin a₁ + a₂\ns.t. x₁ + x₂ + a₁ = 4\n     2x₁ + x₂ - s₂ + a₂ = 5\n     x, s, a ≥ 0\n\n**Initial tableau:**\n| BV | x₁ | x₂ | s₂ | a₁ | a₂ | RHS |\n|----|----|----|----|----|-----|-----|\n| a₁ | 1  | 1  | 0  | 1  | 0   | 4   |\n| a₂ | 2  | 1  | -1 | 0  | 1   | 5   |\n| w  | -3 | -2 | 1  | 0  | 0   | -9  |\n\n(w-row formed by: 0 - (1+2)x₁ - (1+1)x₂ - (-1)s₂)\n\n**Iteration 1:** x₁ enters, a₂ leaves\n**Iteration 2:** x₂ enters, a₁ leaves\n\n**Final Phase I tableau:**\nAll artificial variables nonbasic, w = 0\nBFS: x₁ = 1, x₂ = 3\n\n**Phase II: Optimize original objective**\nUse x₁ = 1, x₂ = 3 as starting BFS\nMinimize 2x₁ + 3x₂\n\n**Result:**\nOptimal: x₁ = 1, x₂ = 3, z* = 11\n\n**Note:** If Phase I ends with artificial variables > 0, problem is infeasible."
  },
  {
    "id": "math404-t2-ex12",
    "subjectId": "math404",
    "topicId": "topic-2",
    "type": "written",
    "title": "Big-M Method",
    "description": "Solve an LP using the Big-M method as an alternative to two-phase simplex.",
    "difficulty": 3,
    "hints": [
      "Add artificial variables with large penalty M",
      "Single phase with modified objective",
      "M should be large enough to force artificials to zero"
    ],
    "solution": "**Problem:**\nmin x₁ + 2x₂\ns.t. x₁ + x₂ = 3\n     2x₁ + x₂ ≥ 4\n     x₁, x₂ ≥ 0\n\n**Big-M Formulation:**\nAdd artificials a₁, a₂ with penalty M:\nmin x₁ + 2x₂ + M·a₁ + M·a₂\ns.t. x₁ + x₂ + a₁ = 3\n     2x₁ + x₂ - s + a₂ = 4\n     x, s, a ≥ 0\n\n**Initial tableau:**\n| BV | x₁    | x₂    | s  | a₁ | a₂ | RHS |\n|----|-------|-------|----|----|-----|-----|\n| a₁ | 1     | 1     | 0  | 1  | 0   | 3   |\n| a₂ | 2     | 1     | -1 | 0  | 1   | 4   |\n| z  |-1-3M |-2-2M  | M  | 0  | 0   | -7M |\n\n**Simplex iterations:**\nM is very large → most negative coefficient is -1-3M\nEnter x₁, leave a₂\n\nAfter pivoting:\n| BV | x₁ | x₂   | s    | a₁ | a₂   | RHS |\n|----|-----|------|------|----|------|-----|\n| a₁ | 0  | 1/2  | 1/2  | 1  |-1/2  | 1   |\n| x₁ | 1  | 1/2  |-1/2  | 0  | 1/2  | 2   |\n| z  | 0  |-1-M/2| M/2-1| 0  |1+3M/2| 2-4M|\n\nContinue until all artificials are driven out.\n\n**Final:** x₁ = 2, x₂ = 1, z* = 4\n(Artificials a₁ = a₂ = 0)\n\n**Practical M:** M = 10⁶ typically sufficient\n**Numerical issues:** Large M can cause ill-conditioning"
  },
  {
    "id": "math404-t2-ex13",
    "subjectId": "math404",
    "topicId": "topic-2",
    "type": "written",
    "title": "Transportation Problem",
    "description": "Formulate and solve a transportation problem as a special structured LP.",
    "difficulty": 3,
    "hints": [
      "Variables: xᵢⱼ = amount shipped from source i to destination j",
      "Supply constraints: Σⱼ xᵢⱼ = sᵢ",
      "Demand constraints: Σᵢ xᵢⱼ = dⱼ",
      "Special structure allows efficient algorithms"
    ],
    "solution": "**Problem:**\n3 factories supply 4 warehouses\nFactories: supplies s₁=20, s₂=30, s₃=25\nWarehouses: demands d₁=15, d₂=20, d₃=25, d₄=15\nCost cᵢⱼ to ship from factory i to warehouse j\n\n**LP Formulation:**\nmin Σᵢ Σⱼ cᵢⱼxᵢⱼ\n\ns.t. Σⱼ xᵢⱼ = sᵢ  for i=1,2,3  (supply)\n     Σᵢ xᵢⱼ = dⱼ  for j=1,2,3,4  (demand)\n     xᵢⱼ ≥ 0\n\n**Matrix form (cost table):**\n|     | W1 | W2 | W3 | W4 | Supply |\n|-----|----|----|----|----|--------|\n| F1  | 8  | 6  | 10 | 9  | 20     |\n| F2  | 9  | 12 | 13 | 7  | 30     |\n| F3  | 14 | 9  | 16 | 5  | 25     |\n|Demand| 15 | 20 | 25 | 15 | 75     |\n\n**Special Properties:**\n- Totally unimodular → BFS has integer values\n- m+n-1 basic variables (3+4-1 = 6)\n- Network structure → efficient algorithms\n\n**Solution Methods:**\n1. **Northwest Corner:** Start at (1,1), allocate max possible\n2. **Vogel's Approximation:** Greedy based on penalties\n3. **Stepping Stone:** Improvement method\n4. **Network Simplex:** Specialized simplex\n\n**Example Solution:**\nx₁₁=15, x₁₂=5, x₂₂=15, x₂₄=15, x₃₃=25\nTotal cost = 8·15 + 6·5 + 12·15 + 7·15 + 16·25 = 805"
  },
  {
    "id": "math404-t2-ex14",
    "subjectId": "math404",
    "topicId": "topic-2",
    "type": "written",
    "title": "Assignment Problem",
    "description": "Formulate the assignment problem as a special case of LP and explain the Hungarian algorithm.",
    "difficulty": 3,
    "hints": [
      "Binary variables: xᵢⱼ ∈ {0,1}",
      "Each worker assigned to exactly one job",
      "Each job assigned to exactly one worker",
      "LP relaxation has integer optimal solution"
    ],
    "solution": "**Problem:**\nAssign n workers to n jobs to minimize total cost.\nCost cᵢⱼ for worker i to do job j.\n\n**LP Formulation:**\nmin Σᵢ Σⱼ cᵢⱼxᵢⱼ\ns.t. Σⱼ xᵢⱼ = 1  for all i  (each worker assigned once)\n     Σᵢ xᵢⱼ = 1  for all j  (each job done once)\n     xᵢⱼ ≥ 0\n\n**Remarkably:** LP relaxation (dropping xᵢⱼ ∈ {0,1}) automatically gives integer solution!\n\n**Cost Matrix Example:**\n|        | Job1 | Job2 | Job3 |\n|--------|------|------|------|\n| Worker1| 10   | 19   | 8    |\n| Worker2| 12   | 15   | 13   |\n| Worker3| 14   | 13   | 12   |\n\n**Hungarian Algorithm:**\n\n**Step 1: Row reduction**\nSubtract minimum of each row:\n|   | J1 | J2 | J3 |\n|---|----|----|-----|\n| W1| 2  | 11 | 0   |\n| W2| 0  | 3  | 1   |\n| W3| 2  | 1  | 0   |\n\n**Step 2: Column reduction**\nSubtract minimum of each column:\n|   | J1 | J2 | J3 |\n|---|----|----|-----|\n| W1| 2  | 10 | 0   |\n| W2| 0  | 2  | 1   |\n| W3| 2  | 0  | 0   |\n\n**Step 3: Cover zeros**\nTry to match using minimum lines. If < n lines needed, continue.\n\n**Step 4: Optimal assignment**\nW1→J3, W2→J1, W3→J2\nTotal cost = 8 + 12 + 13 = 33\n\n**Complexity:** O(n³) vs O(n!) brute force"
  },
  {
    "id": "math404-t2-ex15",
    "subjectId": "math404",
    "topicId": "topic-2",
    "type": "written",
    "title": "Barrier Function for LP",
    "description": "Apply the logarithmic barrier method to solve a simple LP and trace the central path.",
    "difficulty": 4,
    "hints": [
      "Barrier: φ(x) = -Σ log(bᵢ - aᵢᵀx)",
      "Minimize f(x) = cᵀx + (1/t)φ(x)",
      "As t → ∞, solution approaches LP optimum"
    ],
    "solution": "**Problem:** min x s.t. x ≥ 0, x ≤ 1\n\n**Barrier function:**\nφ(x) = -log(x) - log(1-x)\n\n**Barrier problem:**\nmin_t(x) = x - (1/t)[log(x) + log(1-x)]\n\n**Optimality condition:**\nf'(x) = 1 + (1/t)[1/x - 1/(1-x)] = 0\n\nSolving:\n1 + (1/t)[(1-x-x)/(x(1-x))] = 0\nt = -(1-2x)/(x(1-x))\nt·x(1-x) = 2x - 1\ntx - tx² = 2x - 1\ntx² - (t-2)x - 1 = 0\n\n**Central path x*(t):**\nx*(t) = [(t-2) + √((t-2)² + 4t)]/(2t)\n\n**Asymptotic behavior:**\nAs t → ∞:\nx*(t) ≈ [(t-2) + √(t²)]/(2t) = [(t-2) + t]/(2t) = (2t-2)/(2t) = 1 - 1/t → 1\n\n**Duality gap:**\nGap = cᵀx - b'ᵀy ≈ 1/t → 0 as t → ∞\n\n**Example values:**\n| t   | x*(t) | f(x*) |\n|-----|-------|-------|\n| 1   | 0.5   | 0.5   |\n| 10  | 0.9   | 0.9   |\n| 100 | 0.99  | 0.99  |\n| ∞   | 1.0   | 1.0   |\n\n**Central path:** Smooth curve from analytic center (t=0: x=0.5) to optimum (x=1)"
  },
  {
    "id": "math404-t2-ex16",
    "subjectId": "math404",
    "topicId": "topic-2",
    "type": "written",
    "title": "Primal-Dual Interior Point Method",
    "description": "Explain the primal-dual interior point algorithm for LP and its advantages over simplex.",
    "difficulty": 4,
    "hints": [
      "Simultaneously solve primal and dual",
      "Maintain strict feasibility",
      "Newton steps toward KKT conditions",
      "Polynomial complexity: O(√n log(1/ε))"
    ],
    "solution": "**LP Primal-Dual Pair:**\nPrimal: min cᵀx s.t. Ax = b, x ≥ 0\nDual: max bᵀy s.t. Aᵀy + s = c, s ≥ 0\n\n**KKT Conditions:**\n1. Aᵀy + s = c\n2. Ax = b\n3. xᵢsᵢ = 0 for all i (complementarity)\n4. x, s ≥ 0\n\n**Primal-Dual Algorithm:**\n\nReplace complementarity xᵢsᵢ = 0 with xᵢsᵢ = μ (barrier parameter)\n\n**Perturbed KKT:**\nF(x,y,s,μ) = [Aᵀy + s - c, Ax - b, XSe - μe] = 0\nwhere X = diag(x), S = diag(s), e = [1,...,1]ᵀ\n\n**Newton Step:**\nSolve for (Δx, Δy, Δs):\n[0   Aᵀ  I] [Δx]   [rₐ]\n[A   0   0] [Δy] = -[rᵦ]\n[S   0   X] [Δs]   [rμ]\n\nwhere rₐ = Aᵀy + s - c\n      rᵦ = Ax - b\n      rμ = XSe - μe\n\n**Update:**\n(x,y,s) ← (x,y,s) + α(Δx,Δy,Δs)\nα chosen to maintain x,s > 0\n\n**Reduce μ:**\nμ ← σμ (typically σ = 0.1)\n\n**Convergence:**\n- Each outer iteration: reduce μ by constant factor\n- Each outer: O(√n) Newton steps\n- Total: O(√n log(1/ε)) iterations\n\n**Advantages over Simplex:**\n1. Polynomial worst-case (simplex is exponential)\n2. Warm-start for similar problems\n3. Exploits sparsity efficiently\n4. Parallelizable\n\n**Software:** CPLEX, Gurobi, MOSEK use interior point for large LPs"
  },
  {
    "id": "math404-t3-ex01",
    "subjectId": "math404",
    "topicId": "topic-3",
    "type": "written",
    "title": "Dual LP Formulation",
    "description": "Construct the dual of a given linear program using the standard rules.",
    "difficulty": 2,
    "hints": [
      "Each primal constraint → one dual variable",
      "Each primal variable → one dual constraint",
      "Transpose coefficient matrix and swap c, b vectors"
    ],
    "solution": "**Primal (max):**\nmax 4x₁ + 3x₂\ns.t. 2x₁ + x₂ ≤ 10\n     x₁ + 3x₂ ≤ 12\n     x₁ + x₂ = 5\n     x₁ ≥ 0, x₂ free\n\n**Dual Construction:**\n\n| Primal (max) | Dual (min) |\n|--------------|------------|\n| ≤ constraint | y ≥ 0      |\n| = constraint | y free     |\n| x ≥ 0        | ≥ constraint|\n| x free       | = constraint|\n\n**Dual (min):**\nmin 10y₁ + 12y₂ + 5y₃\ns.t. 2y₁ + y₂ + y₃ ≥ 4   (from x₁ ≥ 0)\n     y₁ + 3y₂ + y₃ = 3   (from x₂ free)\n     y₁, y₂ ≥ 0, y₃ free\n\n**Verification:**\n- Primal has 2 variables, dual has 2+1=3 constraints (counting = as 2)\n- Primal has 2+2=4 constraints, dual has 3 variables\n- Coefficient matrix transposes: A → Aᵀ"
  },
  {
    "id": "math404-t3-ex02",
    "subjectId": "math404",
    "topicId": "topic-3",
    "type": "written",
    "title": "Weak Duality Theorem",
    "description": "Prove weak duality and use it to bound an LP's optimal value.",
    "difficulty": 2,
    "hints": [
      "For feasible primal x and dual y: cᵀx ≤ bᵀy (max) or cᵀx ≥ bᵀy (min)",
      "Use constraint multiplications",
      "Provides stopping criterion: if cᵀx = bᵀy, both optimal"
    ],
    "solution": "**Weak Duality Proof:**\n\n**Primal (P):** max cᵀx s.t. Ax ≤ b, x ≥ 0\n**Dual (D):** min bᵀy s.t. Aᵀy ≥ c, y ≥ 0\n\nFor feasible x (primal) and y (dual):\ncᵀx ≤ (Aᵀy)ᵀx = yᵀ(Ax) ≤ yᵀb = bᵀy\n\nTherefore: **cᵀx ≤ bᵀy** (primal ≤ dual)\n\n**Application:**\n\nPrimal: max 2x₁ + 3x₂, s.t. x₁ + 2x₂ ≤ 8, 3x₁ + 2x₂ ≤ 12, x ≥ 0\nDual: min 8y₁ + 12y₂, s.t. y₁ + 3y₂ ≥ 2, 2y₁ + 2y₂ ≥ 3, y ≥ 0\n\n**Feasible solutions:**\n- Primal: x = (2, 3) → z_P = 2·2 + 3·3 = 13 (lower bound on optimal)\n- Dual: y = (1, 1) → z_D = 8·1 + 12·1 = 20 (upper bound on optimal)\n\n**Conclusion:** 13 ≤ z* ≤ 20\n\n**Better dual:** y = (1/2, 1/2)\nCheck: 1/2 + 3/2 = 2 ≥ 2 ✓, 1 + 1 = 2 < 3 ✗ (infeasible)\n\nTry y = (0, 1): 0 + 3 = 3 ≥ 2 ✓, 0 + 2 = 2 < 3 ✗\nTry y = (1, 1/3): 1 + 1 = 2 ≥ 2 ✓, 2 + 2/3 = 8/3 ≥ 3 ✗\n\n**If we find cᵀx* = bᵀy*:** Both optimal by weak duality!"
  },
  {
    "id": "math404-t3-ex03",
    "subjectId": "math404",
    "topicId": "topic-3",
    "type": "written",
    "title": "Strong Duality Verification",
    "description": "Verify strong duality by showing the optimal values of primal and dual are equal.",
    "difficulty": 3,
    "hints": [
      "Solve both primal and dual",
      "Check that p* = d* (zero duality gap)",
      "Strong duality holds for LPs when feasible"
    ],
    "solution": "**Primal:**\nmax 3x₁ + 2x₂\ns.t. x₁ + x₂ ≤ 4\n     2x₁ + x₂ ≤ 6\n     x ≥ 0\n\n**Solve by simplex:**\nOptimal: x* = (2, 2), z*_P = 3·2 + 2·2 = 10\n\n**Dual:**\nmin 4y₁ + 6y₂\ns.t. y₁ + 2y₂ ≥ 3\n     y₁ + y₂ ≥ 2\n     y ≥ 0\n\n**Solve dual:**\nGraphically or by simplex:\nActive constraints: y₁ + 2y₂ = 3, y₁ + y₂ = 2\nSolving: y₂ = 1, y₁ = 1\nOptimal: y* = (1, 1), z*_D = 4·1 + 6·1 = 10\n\n**Strong Duality:** z*_P = z*_D = 10 ✓\n\n**Complementary Slackness Verification:**\nPrimal:\n- x₁* = 2 > 0 → dual constraint 1 active: y₁ + 2y₂ = 3 ✓\n- x₂* = 2 > 0 → dual constraint 2 active: y₁ + y₂ = 2 ✓\n\nDual:\n- y₁* = 1 > 0 → primal constraint 1 active: x₁ + x₂ = 4 ✓\n- y₂* = 1 > 0 → primal constraint 2 active: 2x₁ + x₂ = 6 ✓\n\nAll complementary slackness conditions hold!"
  },
  {
    "id": "math404-t3-ex04",
    "subjectId": "math404",
    "topicId": "topic-3",
    "type": "written",
    "title": "Complementary Slackness Conditions",
    "description": "Use complementary slackness to find the optimal dual solution from a known primal solution.",
    "difficulty": 3,
    "hints": [
      "If xⱼ > 0, then j-th dual constraint is tight",
      "If i-th primal constraint is slack, then yᵢ = 0",
      "Solve system of equations from active constraints"
    ],
    "solution": "**Problem:**\nmax 5x₁ + 4x₂ + 3x₃\ns.t. 2x₁ + 3x₂ + x₃ ≤ 5   (constraint 1)\n     4x₁ + x₂ + 2x₃ ≤ 11  (constraint 2)\n     3x₁ + 4x₂ + 2x₃ ≤ 8   (constraint 3)\n     x ≥ 0\n\n**Given:** Primal optimal x* = (2, 0, 1), z* = 13\n\n**Verify feasibility:**\n2·2 + 0 + 1 = 5 ≤ 5 (active)\n4·2 + 0 + 2·1 = 10 ≤ 11 (slack = 1)\n3·2 + 0 + 2·1 = 8 ≤ 8 (active)\n\n**Dual:**\nmin 5y₁ + 11y₂ + 8y₃\ns.t. 2y₁ + 4y₂ + 3y₃ ≥ 5   (from x₁)\n     3y₁ + y₂ + 4y₃ ≥ 4   (from x₂)\n     y₁ + 2y₂ + 2y₃ ≥ 3   (from x₃)\n     y ≥ 0\n\n**Complementary Slackness:**\n- x₁* = 2 > 0 → 2y₁ + 4y₂ + 3y₃ = 5\n- x₂* = 0 → no requirement\n- x₃* = 1 > 0 → y₁ + 2y₂ + 2y₃ = 3\n- Constraint 2 slack → y₂* = 0\n- Constraints 1,3 active → y₁*, y₃* ≥ 0\n\n**Solve:**\nWith y₂ = 0:\n2y₁ + 3y₃ = 5\ny₁ + 2y₃ = 3\n\nFrom second: y₁ = 3 - 2y₃\nInto first: 2(3-2y₃) + 3y₃ = 5\n6 - 4y₃ + 3y₃ = 5\ny₃ = 1, y₁ = 1\n\n**Dual optimal:** y* = (1, 0, 1), z*_D = 5·1 + 0 + 8·1 = 13 ✓"
  },
  {
    "id": "math404-t3-ex05",
    "subjectId": "math404",
    "topicId": "topic-3",
    "type": "written",
    "title": "Economic Interpretation of Dual Variables",
    "description": "Interpret dual variables as shadow prices in a resource allocation problem.",
    "difficulty": 2,
    "hints": [
      "Dual variable = marginal value of resource",
      "If yᵢ = 5, increasing bᵢ by 1 increases profit by 5",
      "Zero dual → resource is not scarce (slack in primal)"
    ],
    "solution": "**Production Problem:**\nMaximize profit from products A, B:\nmax 40x_A + 30x_B\ns.t. 2x_A + x_B ≤ 100  (hours of machine time)\n     x_A + 2x_B ≤ 80   (kg of raw material)\n     x_A, x_B ≥ 0\n\n**Optimal Solution:**\nx* = (40, 20), profit = 40·40 + 30·20 = $2200\n\nBoth constraints active:\n2·40 + 20 = 100 (full machine time)\n40 + 2·20 = 80 (full material)\n\n**Dual Variables (Shadow Prices):**\ny₁* = 10 (machine time)\ny₂* = 10 (material)\n\n**Economic Interpretation:**\n\n**1. Machine Time (y₁ = 10):**\n- Worth $10 per hour\n- Increasing machine time from 100→101 hours increases profit by ~$10\n- Decreasing to 99 hours decreases profit by ~$10\n\n**2. Material (y₂ = 10):**\n- Worth $10 per kg\n- Should acquire more material if cost < $10/kg\n- Should sell material if can get > $10/kg\n\n**Example:** Suppose we can:\n- Rent extra machine: $8/hour → DO IT! (net gain $2/hour)\n- Buy material: $12/kg → DON'T! (net loss $2/kg)\n\n**Dual Optimal Value:**\n100·10 + 80·10 = 2200 = primal optimal (strong duality)\n\n**If constraint has slack:** y = 0\nExample: Add constraint x_A ≤ 50 (not binding)\nThen y₃ = 0 (no value to increasing this limit)"
  },
  {
    "id": "math404-t3-ex06",
    "subjectId": "math404",
    "topicId": "topic-3",
    "type": "written",
    "title": "Dual of Dual is Primal",
    "description": "Prove that taking the dual of the dual LP returns the original primal problem.",
    "difficulty": 3,
    "hints": [
      "Start with primal in standard form",
      "Construct dual systematically",
      "Take dual of the dual using same rules"
    ],
    "solution": "**Primal (P):**\nmax cᵀx\ns.t. Ax ≤ b\n     x ≥ 0\n\n**Dual (D):**\nmin bᵀy\ns.t. Aᵀy ≥ c\n     y ≥ 0\n\n**Dual of Dual (DD):**\nRewrite (D) in max form:\nmax -bᵀy\ns.t. -Aᵀy ≤ -c\n     y ≥ 0\n\n**Take dual of this:**\nmin -cᵀz\ns.t. (-Aᵀ)ᵀz ≥ -b\n     z ≥ 0\n\nSimplify:\nmin -cᵀz\ns.t. -Az ≥ -b  (equivalently: Az ≤ b)\n     z ≥ 0\n\nMultiply objective by -1 (max → min):\n**max cᵀz s.t. Az ≤ b, z ≥ 0**\n\nThis is identical to (P) with z replacing x! ∎\n\n**Example:**\n\nP: max 3x₁ + 2x₂, s.t. x₁ + x₂ ≤ 4, x ≥ 0\nD: min 4y, s.t. y ≥ 3, y ≥ 2, y ≥ 0\n   Simplifying: min 4y, s.t. y ≥ 3\nDD: Taking dual again returns to P\n\n**Intuition:** Duality is a symmetric relationship."
  },
  {
    "id": "math404-t3-ex07",
    "subjectId": "math404",
    "topicId": "topic-3",
    "type": "written",
    "title": "Farkas' Lemma Application",
    "description": "Use Farkas' Lemma to prove a system of linear inequalities is infeasible.",
    "difficulty": 4,
    "hints": [
      "Farkas: Exactly one holds: (1) Ax ≤ b, x ≥ 0 OR (2) Aᵀy ≥ 0, bᵀy < 0, y ≥ 0",
      "To prove (1) infeasible, find y satisfying (2)",
      "This is the certificate of infeasibility"
    ],
    "solution": "**Farkas' Lemma:**\nExactly one of the following holds:\n1. ∃x: Ax ≤ b, x ≥ 0\n2. ∃y: Aᵀy ≥ 0, bᵀy < 0, y ≥ 0\n\n**Problem:** Prove infeasible:\nx₁ + 2x₂ ≤ 3\n2x₁ + x₂ ≤ 3\n-x₁ - x₂ ≤ -4\nx ≥ 0\n\n**In matrix form:**\nA = [[1, 2], [2, 1], [-1, -1]], b = [3, 3, -4]ᵀ\n\n**Find certificate y:**\nNeed: Aᵀy ≥ 0, bᵀy < 0, y ≥ 0\n\nAᵀy = [[1,2,-1], [2,1,-1]][[y₁],[y₂],[y₃]]\n     = [[y₁+2y₂-y₃], [2y₁+y₂-y₃]]\n\nTry y₁ = y₂ = y₃ = 1:\nAᵀy = [1+2-1, 2+1-1] = [2, 2] ≥ 0 ✓\nbᵀy = 3·1 + 3·1 + (-4)·1 = 2 ≮ 0 ✗\n\nTry y₁ = y₂ = 0, y₃ = 1:\nAᵀy = [-1, -1] ≮ 0 ✗\n\nTry y₁ = y₂ = 1, y₃ = 2:\nAᵀy = [1+2-2, 2+1-2] = [1, 1] ≥ 0 ✓\nbᵀy = 3 + 3 - 8 = -2 < 0 ✓\n\n**Certificate found:** y* = (1, 1, 2)\nThis proves system is **infeasible**.\n\n**Geometric interpretation:**\ny gives a linear combination of constraints:\n1·(x₁+2x₂) + 1·(2x₁+x₂) + 2·(-x₁-x₂) ≤ 1·3 + 1·3 + 2·(-4)\nx₁ + 2x₂ + 2x₁ + x₂ - 2x₁ - 2x₂ ≤ -2\nx₁ + x₂ ≤ -2\n\nBut x ≥ 0 implies x₁ + x₂ ≥ 0, contradiction!"
  },
  {
    "id": "math404-t3-ex08",
    "subjectId": "math404",
    "topicId": "topic-3",
    "type": "written",
    "title": "Dual Simplex Method",
    "description": "Apply the dual simplex method to solve an LP starting from dual feasible but primal infeasible basis.",
    "difficulty": 4,
    "hints": [
      "Dual simplex maintains dual feasibility",
      "Selects leaving variable with most negative RHS",
      "Ratio test ensures dual feasibility maintained"
    ],
    "solution": "**Problem:**\nmin 2x₁ + 3x₂\ns.t. x₁ + x₂ ≥ 3\n     2x₁ + x₂ ≥ 4\n     x ≥ 0\n\n**Standard form (slack/surplus):**\nmin 2x₁ + 3x₂\ns.t. x₁ + x₂ - s₁ = 3\n     2x₁ + x₂ - s₂ = 4\n     x, s ≥ 0\n\n**Initial tableau (basis s₁, s₂):**\n| BV | x₁ | x₂ | s₁ | s₂ | RHS |\n|----|----|----|----|----|-----|\n| s₁ | 1  | 1  | -1 | 0  | 3   |\n| s₂ | 2  | 1  | 0  | -1 | 4   |\n| z  | -2 | -3 | 0  | 0  | 0   |\n\n**Problem:** RHS values 3, 4 are positive, but s₁, s₂ coefficients are -1\nAfter converting: -s₁ = -3, -s₂ = -4 (primal infeasible)\n\n**Multiply rows by -1:**\n| BV | x₁ | x₂ | s₁ | s₂ | RHS  |\n|----|----|----|----|----|------|\n| s₁ | -1 | -1 | 1  | 0  | -3   |\n| s₂ | -2 | -1 | 0  | 1  | -4   |\n| z  | -2 | -3 | 0  | 0  | 0    |\n\n**Dual Simplex:**\n\n**Iteration 1:**\n- Leaving: most negative RHS = -4 (row 2, s₂ leaves)\n- Entering: ratio test on objective row: min{-2/-2, -3/-1} = min{1, 3} = 1\n  → x₁ enters\n\n**Pivot on a₂₁ = -2:**\nDivide row 2 by -2:\n| x₁ | x₂  | s₁ | s₂  | RHS |\n|----|-----|----|-----|-----|\n| 1  | 1/2 | 0  |-1/2 | 2   |\n\nUpdate other rows...\n\n**Final tableau:**\n| BV | x₁ | x₂ | s₁ | s₂ | RHS |\n|----|----|----|----|----|-----|\n| x₂ | 0  | 1  | 2  | -1 | 1   |\n| x₁ | 1  | 0  |-1  | 1  | 1   |\n| z  | 0  | 0  | 1  | 1  | 7   |\n\n**Optimal:** x* = (1, 1), z* = 7\n\n**Dual Simplex Use Cases:**\n1. After adding cutting planes in integer programming\n2. Re-optimization after constraint changes\n3. When dual feasible starting basis available"
  },
  {
    "id": "math404-t3-ex09",
    "subjectId": "math404",
    "topicId": "topic-3",
    "type": "written",
    "title": "Duality Gap Analysis",
    "description": "Analyze the duality gap for a sequence of approximations to an LP.",
    "difficulty": 3,
    "hints": [
      "Duality gap = |primal objective - dual objective|",
      "Gap = 0 at optimality (strong duality)",
      "Can use gap as stopping criterion"
    ],
    "solution": "**Problem:**\nPrimal (P): max 3x₁ + 4x₂, s.t. x₁ + 2x₂ ≤ 8, 2x₁ + x₂ ≤ 8, x ≥ 0\nDual (D): min 8y₁ + 8y₂, s.t. y₁ + 2y₂ ≥ 3, 2y₁ + y₂ ≥ 4, y ≥ 0\n\n**Iteration sequence:**\n\n| Iter | Primal x        | z_P | Dual y      | z_D | Gap |\n|------|-----------------|-----|-------------|-----|-----|\n| 0    | (0, 0)          | 0   | (3, 2)      | 40  | 40  |\n| 1    | (4, 0)          | 12  | (2, 1)      | 24  | 12  |\n| 2    | (0, 4)          | 16  | (1, 2)      | 24  | 8   |\n| 3    | (2.67, 2.67)    | 18.7| (1.33,1.33) | 21.3| 2.6 |\n| 4    | (8/3, 8/3)      | 56/3| (4/3, 4/3)  | 64/3| 8/3 |\n| *    | (8/3, 8/3)      | 56/3| (4/3, 4/3)  | 64/3| 0   |\n\nWait, let me recalculate optimum:\n\n**Solve P graphically:**\nVertices: (0,0), (4,0), (0,4), (8/3,8/3)\nEvaluate:\n- (0,0): 0\n- (4,0): 12\n- (0,4): 16\n- (8/3,8/3): 3·8/3 + 4·8/3 = 24/3 + 32/3 = 56/3 ≈ 18.67\n\nOptimal P: x* = (8/3, 8/3), z*_P = 56/3\n\n**Solve D:**\nActive: y₁ + 2y₂ = 3, 2y₁ + y₂ = 4\nSolving: y₁ = 5/3, y₂ = 2/3\nz*_D = 8·5/3 + 8·2/3 = 40/3 + 16/3 = 56/3\n\n**Duality gap at optimum = 0** ✓\n\n**Practical use:**\nStop when gap < ε:\n|z_P - z_D| < 10⁻⁶\nThis guarantees both within 10⁻⁶ of true optimum."
  },
  {
    "id": "math404-t3-ex10",
    "subjectId": "math404",
    "topicId": "topic-3",
    "type": "written",
    "title": "Unbounded Primal and Dual",
    "description": "Analyze the relationship between primal and dual when one is unbounded.",
    "difficulty": 3,
    "hints": [
      "If primal unbounded → dual infeasible",
      "If dual unbounded → primal infeasible",
      "Both can be infeasible simultaneously"
    ],
    "solution": "**Theorem:** For LP primal-dual pair:\n1. If P is unbounded → D is infeasible\n2. If D is unbounded → P is infeasible\n3. Both P and D can be infeasible\n4. Exactly one holds: both optimal, one unbounded (other infeasible), both infeasible\n\n**Example 1: Unbounded Primal**\nP: max x₁ + x₂, s.t. -x₁ + x₂ ≤ 1, x ≥ 0\nD: min y, s.t. -y ≥ 1, y ≥ 1, y ≥ 0\n\nFor P: Move along direction (1,1): objective unbounded\nFor D: Need y ≥ 1 and -y ≥ 1, impossible!\n**P unbounded, D infeasible** ✓\n\n**Example 2: Both Infeasible**\nP: max x, s.t. x ≤ -1, x ≥ 0\n   No x satisfies x ≥ 0 and x ≤ -1\n\nD: min -y, s.t. y ≥ 1, y ≥ 0\n   This simplifies to max y, s.t. y ≥ 1, y ≥ 0\n   Unbounded!\n\nActually by duality: D infeasible too.\nLet me reconsider...\n\nP: max x₁, s.t. x₁ ≤ 1, -x₁ ≤ -2, x₁ ≥ 0\n   Need x₁ ≤ 1 and x₁ ≥ 2 → infeasible\n\nD: min y₁ - 2y₂, s.t. y₁ - y₂ ≥ 1, y ≥ 0\n   Set y₁ = 0, y₂ → ∞: objective → -∞ → unbounded\n\n**Both P and D infeasible:**\nP: max x, s.t. x ≤ 1, x ≥ 2\nD: min y₁ + 2y₂, s.t. y₁ + y₂ ≥ 1, y₁ - y₂ = 0, y ≥ 0\n   From y₁ = y₂: 2y₁ ≥ 1 → y₁ ≥ 1/2\n   But objective= y₁ + 2y₁ = 3y₁, minimum at y₁ = 1/2, gives 3/2\n\nLet me use standard example:\nP: max x₁ - x₂, s.t. x₁ - x₂ ≤ 1, -x₁ + x₂ ≤ -2, x ≥ 0\nD: min y₁ - 2y₂, s.t. y₁ - y₂ ≥ 1, -y₁ + y₂ ≥ -1, y ≥ 0\n\nFrom constraints: y₁ - y₂ ≥ 1 and y₁ - y₂ ≤ 1 → y₁ - y₂ = 1\nObjective unbounded below.\n\n**Summary:**\n- Weak duality prevents both from being unbounded\n- Both can be infeasible\n- If one feasible and bounded, strong duality ensures both optimal"
  },
  {
    "id": "math404-t3-ex11",
    "subjectId": "math404",
    "topicId": "topic-3",
    "type": "written",
    "title": "Lagrangian Dual for QP",
    "description": "Derive the Lagrangian dual of a quadratic programming problem.",
    "difficulty": 4,
    "hints": [
      "Form Lagrangian L(x,λ) = f(x) + λᵀg(x)",
      "Dual function g(λ) = inf_x L(x,λ)",
      "Dual problem: max g(λ) s.t. λ ≥ 0"
    ],
    "solution": "**Primal QP:**\nmin (1/2)xᵀQx + cᵀx\ns.t. Ax ≥ b\n     x ≥ 0\n\nwhere Q ⪰ 0 (positive semidefinite)\n\n**Lagrangian:**\nL(x,λ,μ) = (1/2)xᵀQx + cᵀx - λᵀ(Ax - b) - μᵀx\n\n**Dual Function:**\ng(λ,μ) = inf_x L(x,λ,μ)\n\n**Find minimum over x:**\n∇_x L = Qx + c - Aᵀλ - μ = 0\n→ Qx = Aᵀλ + μ - c\n\nIf Q ≻ 0 (positive definite):\nx*(λ,μ) = Q⁻¹(Aᵀλ + μ - c)\n\n**Substitute back:**\ng(λ,μ) = (1/2)(Aᵀλ+μ-c)ᵀQ⁻¹(Aᵀλ+μ-c) + cᵀQ⁻¹(Aᵀλ+μ-c)\n         - λᵀ(AQ⁻¹(Aᵀλ+μ-c) - b) - μᵀQ⁻¹(Aᵀλ+μ-c)\n\n**Simplify:** (algebra omitted)\ng(λ,μ) = -(1/2)(Aᵀλ+μ-c)ᵀQ⁻¹(Aᵀλ+μ-c) + λᵀb\n\n**Dual Problem:**\nmax -(1/2)(Aᵀλ+μ-c)ᵀQ⁻¹(Aᵀλ+μ-c) + λᵀb\ns.t. λ, μ ≥ 0\n\n**For QP with only equality constraints (Ax = b):**\nSimpler form:\nmax -(1/2)λᵀAQ⁻¹Aᵀλ + (b + AQ⁻¹c)ᵀλ\n\n**Example:**\nmin x₁² + x₂² - 2x₁ - 4x₂, s.t. x₁ + x₂ ≤ 3, x ≥ 0\n\nQ = [[2,0],[0,2]], c = [-2,-4], A = [-1,-1], b = -3\n\nDual becomes simpler to solve than primal in some cases!"
  },
  {
    "id": "math404-t3-ex12",
    "subjectId": "math404",
    "topicId": "topic-3",
    "type": "written",
    "title": "Bidual Problem",
    "description": "Formulate the bidual (dual of the Lagrangian dual) and relate it to the primal.",
    "difficulty": 4,
    "hints": [
      "Start with Lagrangian dual",
      "Take dual of the dual problem",
      "For convex problems, bidual = primal"
    ],
    "solution": "**Primal:**\nmin f(x)\ns.t. gᵢ(x) ≤ 0, i=1,...,m\n     hⱼ(x) = 0, j=1,...,p\n\n**Lagrangian Dual:**\nmax g(λ,ν) = inf_x L(x,λ,ν)\ns.t. λ ≥ 0\n\nwhere L(x,λ,ν) = f(x) + Σλᵢgᵢ(x) + Σνⱼhⱼ(x)\n\n**For convex problem:** Bidual = Primal (strong duality)\n\n**Example: QP**\nP: min (1/2)xᵀQx + cᵀx, s.t. Ax = b\n\n**Lagrangian:**\nL(x,ν) = (1/2)xᵀQx + cᵀx + νᵀ(Ax - b)\n\n**Dual function:**\ng(ν) = inf_x L = -(1/2)(Aᵀν+c)ᵀQ⁻¹(Aᵀν+c) + cᵀQ⁻¹(Aᵀν+c) - νᵀb\n\n**Dual:**\nD: max g(ν)\n\n**Bidual:**\ninf_x sup_ν L(x,ν)\n\n**Von Neumann Minimax Theorem:**\nFor convex-concave L:\ninf_x sup_ν L(x,ν) = sup_ν inf_x L(x,ν)\n\nLeft side = Primal\nRight side = Dual\n\n**Strong Duality ⟺ Minimax Equality**\n\n**Slater's Condition:**\nIf ∃x̃: gᵢ(x̃) < 0 for all i (strictly feasible), then strong duality holds.\n\n**Summary:**\nBidual recovers primal for convex problems with constraint qualification.\nThis is fundamental result in convex analysis!"
  },
  {
    "id": "math404-t3-ex13",
    "subjectId": "math404",
    "topicId": "topic-3",
    "type": "written",
    "title": "Sensitivity from Dual Solution",
    "description": "Use the optimal dual solution to perform sensitivity analysis on constraint RHS.",
    "difficulty": 3,
    "hints": [
      "Dual optimal gives shadow prices",
      "Valid in neighborhood of optimum",
      "Δz ≈ Σyᵢ*Δbᵢ for small changes"
    ],
    "solution": "**Problem:**\nmax 5x₁ + 4x₂ + 6x₃\ns.t. x₁ + x₂ + x₃ ≤ 20   (resource 1)\n     2x₁ + x₂ + 3x₃ ≤ 30  (resource 2)\n     x ≥ 0\n\n**Optimal Solutions:**\nPrimal: x* = (0, 15, 5), z* = 90\nDual: y* = (0, 2), z*_D = 60\n\nWait, that doesn't match. Let me recalculate.\n\nActually solving:\nOptimal: x* = (10, 0, 10/3), z* = 50 + 20 = 70\nDual: y* = (1, 2), verify: y₁ + 2y₂ = 5 ✓, y₁ + y₂ = 3 ✗\n\nLet me use correct values:\nPrimal optimal: x* = (15, 0, 5), z* = 75 + 30 = 105\nCheck: 15 + 5 = 20 ✓, 30 + 15 = 45 > 30 ✗\n\n**Correct Example:**\nmax 3x₁ + 2x₂\ns.t. x₁ + x₂ ≤ 4    (b₁ = 4)\n     2x₁ + x₂ ≤ 6   (b₂ = 6)\n     x ≥ 0\n\nOptimal: x* = (2, 2), z* = 10\nDual: y* = (1, 1)\n\n**Sensitivity Analysis:**\n\n**Question 1:** Increase b₁ from 4 to 5?\nΔz ≈ y₁*Δb₁ = 1·1 = 1\nNew objective ≈ 10 + 1 = 11\n\n**Verify:** Resolving with b₁ = 5:\nNew optimal: x = (2.5, 2.5) (if both stay active)\nWait: x₁ + x₂ = 5, 2x₁ + x₂ = 6 → x₁ = 1, x₂ = 4\nCheck: 1 + 4 = 5 ✓, 2 + 4 = 6 ✓\nz = 3·1 + 2·4 = 11 ✓ Matches!\n\n**Question 2:** Increase b₂ from 6 to 7?\nΔz ≈ y₂*Δb₂ = 1·1 = 1\nNew z ≈ 11\n\n**Verify:** x₁ + x₂ = 4, 2x₁ + x₂ = 7\n→ x₁ = 3, x₂ = 1\nz = 3·3 + 2·1 = 11 ✓\n\n**Multiple changes:**\nΔb₁ = +1, Δb₂ = -0.5\nΔz ≈ 1·1 + 1·(-0.5) = 0.5\nNew z ≈ 10.5\n\n**Valid range:** Shadow prices valid while basis unchanged."
  },
  {
    "id": "math404-t3-ex14",
    "subjectId": "math404",
    "topicId": "topic-3",
    "type": "written",
    "title": "Reduced Cost Interpretation",
    "description": "Interpret reduced costs in terms of the dual problem and optimality.",
    "difficulty": 3,
    "hints": [
      "Reduced cost c̄ⱼ = cⱼ - yᵀAⱼ",
      "c̄ⱼ = rate of objective change if xⱼ increases from 0",
      "c̄ⱼ = 0 for basic variables, c̄ⱼ ≥ 0 for optimality"
    ],
    "solution": "**Optimal Tableau:**\n| BV | x₁ | x₂ | x₃ | s₁ | s₂ | RHS | c̄ⱼ |\n|----|----|----|----|----|----|-----|-----|\n| x₁ | 1  | 0  | 2  | 1  | 0  | 3   | 0   |\n| x₂ | 0  | 1  | -1 | 0  | 1  | 2   | 0   |\n| z  | 0  | 0  | 3  | 1  | 2  | 13  |     |\n\nOriginal: max 3x₁ + 2x₂ + 4x₃\n\n**Reduced Costs:**\n- c̄₁ = 0 (basic)\n- c̄₂ = 0 (basic)\n- c̄₃ = 3 (from z-row)\n- c̄_s₁ = 1\n- c̄_s₂ = 2\n\n**Interpretation:**\n\n**1. For nonbasic variable x₃ (c̄₃ = 3):**\n- Currently x₃ = 0\n- If we force x₃ to increase by 1:\n  * Must maintain feasibility: adjust basic variables\n  * From tableau: x₁ decreases by 2, x₂ increases by 1\n  * Objective change: -3·2 + 2·1 + 4·1 = -2\n\nWait, that's negative. Let me recalculate.\nActually c̄₃ = 3 > 0 means current solution is NOT optimal!\nShould bring x₃ into basis.\n\nLet me assume we're at optimum with c̄₃ = -3 (for min):\n\n**For minimization:**\nc̄₃ = -3 < 0 → can improve by increasing x₃\nc̄₃ = 0 → indifferent\nc̄₃ > 0 → increasing x₃ worsens objective\n\n**Connection to Dual:**\nReduced cost = primal objective coef - dual constraint value\nc̄ⱼ = cⱼ - yᵀAⱼ\n\nwhere y = dual variables = shadow prices\n\n**Example:**\nIf c̄₃ = 2 in max problem:\n- Increasing x₃ by 1 improves objective by 2 (in isolation)\n- But maintaining feasibility requires changes\n- Net effect: would decrease objective by 2 (that's why it's nonbasic)\n\n**Economic interpretation:**\nc̄ⱼ = opportunity cost of bringing nonbasic variable into solution.\nIf c̄ⱼ > 0: cost exceeds benefit → stay at zero\nIf c̄ⱼ < 0: benefit exceeds cost → should increase (not optimal)\nIf c̄ⱼ = 0: alternative optimal solution"
  },
  {
    "id": "math404-t3-ex15",
    "subjectId": "math404",
    "topicId": "topic-3",
    "type": "written",
    "title": "Duality in Network Flow",
    "description": "Formulate the dual of a minimum cost flow problem and interpret the dual variables.",
    "difficulty": 4,
    "hints": [
      "Primal variables: flow on each arc",
      "Dual variables: potential at each node",
      "Complementary slackness: flow only on shortest paths"
    ],
    "solution": "**Minimum Cost Flow Primal:**\nmin Σ_{(i,j)∈A} cᵢⱼxᵢⱼ  (cost)\ns.t. Σⱼ xᵢⱼ - Σⱼ xⱼᵢ = bᵢ  for all nodes i  (flow conservation)\n     0 ≤ xᵢⱼ ≤ uᵢⱼ  (capacity)\n\nwhere bᵢ > 0 (supply), bᵢ < 0 (demand), bᵢ = 0 (transship)\n\n**Dual:**\nmax Σᵢ bᵢπᵢ - Σ_{(i,j)} uᵢⱼyᵢⱼ\ns.t. πᵢ - πⱼ - yᵢⱼ ≤ cᵢⱼ  for all arcs (i,j)\n     yᵢⱼ ≥ 0\n\nwhere πᵢ = node potential (price/height)\n\n**Reduced Cost Interpretation:**\nc̄ᵢⱼ = cᵢⱼ - (πᵢ - πⱼ)\nThis is the reduced cost of arc (i,j)\n\n**Complementary Slackness:**\n1. If 0 < xᵢⱼ < uᵢⱼ: c̄ᵢⱼ = 0 → πᵢ - πⱼ = cᵢⱼ\n2. If xᵢⱼ = 0: c̄ᵢⱼ ≥ 0 → πᵢ - πⱼ ≤ cᵢⱼ\n3. If xᵢⱼ = uᵢⱼ: c̄ᵢⱼ ≤ 0 → πᵢ - πⱼ ≥ cᵢⱼ\n\n**Interpretation:**\n- πᵢ = \"price\" at node i\n- Flow goes from high price to low price\n- c̄ᵢⱼ = actual cost - price differential\n- Flow only uses arcs with c̄ᵢⱼ = 0 (zero reduced cost)\n\n**Example: Shortest Path**\nSingle source s, single sink t, unit supply/demand\nDual potentials are shortest path distances!\n\n**Network Simplex:**\nMaintains spanning tree basis\nDual variables = node potentials\nOptimality: all nonbasic arcs have c̄ᵢⱼ ≥ 0"
  },
  {
    "id": "math404-t3-ex16",
    "subjectId": "math404",
    "topicId": "topic-3",
    "type": "written",
    "title": "Dual Degeneracy and Uniqueness",
    "description": "Analyze the relationship between primal degeneracy, dual degeneracy, and uniqueness of optimal solutions.",
    "difficulty": 4,
    "hints": [
      "Primal degeneracy ↔ multiple dual optima",
      "Dual degeneracy ↔ multiple primal optima",
      "Strict complementarity ↔ unique solutions"
    ],
    "solution": "**Definitions:**\n\n**Primal Degeneracy:** Basic variable = 0\n**Dual Degeneracy:** Nonbasic variable has c̄ⱼ = 0\n\n**Theorems:**\n\n1. **Primal degenerate ↔ Dual has multiple optima**\n   - If basic xᵢ* = 0, corresponding dual constraint can be slack\n   - Multiple dual solutions satisfy optimality\n\n2. **Dual degenerate ↔ Primal has multiple optima**\n   - If c̄ⱼ = 0 for nonbasic xⱼ, can bring xⱼ into basis\n   - Objective unchanged → alternative optimal solution\n\n**Example 1: Primal Degeneracy**\nmax 3x₁ + 2x₂\ns.t. x₁ + x₂ ≤ 4\n     x₁ ≤ 2\n     x₂ ≤ 2\n     x ≥ 0\n\nOptimal: x* = (2, 2), slack s₃ = 0 (degenerate basis)\n\nDual:\nmin 4y₁ + 2y₂ + 2y₃\ns.t. y₁ + y₂ ≥ 3\n     y₁ + y₃ ≥ 2\n     y ≥ 0\n\nMultiple dual optima:\ny = (2, 1, 0): z = 8 + 2 = 10\ny = (3, 0, 0): z = 12 ≠ 10 ✗\n\nActually y* = (1, 2, 0) gives 4 + 4 = 8\nCheck: 1 + 2 = 3 ≥ 3 ✓, 1 + 0 = 1 < 2 ✗\n\nLet me recalculate. With x* = (2,2), z* = 10\nActive constraints: x₁ + x₂ = 4, x₁ = 2, x₂ = 2\n\nDue to redundancy, multiple dual solutions possible.\n\n**Example 2: Dual Degeneracy**\nmax x₁ + x₂\ns.t. x₁ + x₂ ≤ 2\n     x ≥ 0\n\n**Optimal:** Any point on line x₁ + x₂ = 2 is optimal\nThis is dual degeneracy → multiple primal optima\n\n**Strict Complementarity:**\nDefinition: For every constraint i:\nEither sᵢ > 0 OR λᵢ > 0 (not both = 0)\n\n**If strict complementarity holds:**\n→ Unique primal solution\n→ Unique dual solution\n→ No degeneracy\n\n**Summary Table:**\n| Primal | Dual   | Implications          |\n|--------|--------|-----------------------|\n| Unique | Unique | Strict complementarity|\n| Unique | Mult.  | Primal degeneracy     |\n| Mult.  | Unique | Dual degeneracy       |\n| Mult.  | Mult.  | Both degenerate       |"
  },
  {
    "id": "math404-t4-ex01",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "type": "written",
    "title": "Convex Set Verification",
    "description": "Prove that a given set is convex by showing it satisfies the definition of convexity.",
    "difficulty": 1,
    "hints": [
      "A set is convex if for any two points x, y in the set, the line segment connecting them is also in the set",
      "Mathematically: λx + (1-λ)y ∈ S for all λ ∈ [0,1]",
      "Show the convex combination satisfies the defining inequality"
    ],
    "solution": "**Problem:** Prove the set S = {x ∈ ℝ² : x₁ + 2x₂ ≤ 4} is convex.\n\n**Proof:**\nLet x, y ∈ S and λ ∈ [0,1].\nWe need to show z = λx + (1-λ)y ∈ S.\n\n**Step 1:** Since x ∈ S: x₁ + 2x₂ ≤ 4\n**Step 2:** Since y ∈ S: y₁ + 2y₂ ≤ 4\n\n**Step 3:** Check z = λx + (1-λ)y:\nz₁ + 2z₂ = λx₁ + (1-λ)y₁ + 2λx₂ + 2(1-λ)y₂\n         = λ(x₁ + 2x₂) + (1-λ)(y₁ + 2y₂)\n         ≤ λ·4 + (1-λ)·4\n         = 4\n\n**Conclusion:** z ∈ S, so S is convex. ∎\n\n**Geometric interpretation:** S is a halfspace, and all halfspaces are convex."
  },
  {
    "id": "math404-t4-ex02",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "type": "written",
    "title": "Non-Convex Set Example",
    "description": "Show that the union of two disjoint intervals is not convex by providing a counterexample.",
    "difficulty": 1,
    "hints": [
      "Find two points in the set such that a point on the line segment between them is not in the set",
      "For unions, points in different components often work",
      "The midpoint is a good candidate"
    ],
    "solution": "**Set:** S = [0,1] ∪ [2,3] ⊂ ℝ\n\n**Counterexample:**\nTake x = 0.5 ∈ [0,1] ⊂ S\nTake y = 2.5 ∈ [2,3] ⊂ S\n\n**Midpoint:** z = 0.5·0.5 + 0.5·2.5 = 1.5\n\n**Check:** 1.5 ∉ [0,1] and 1.5 ∉ [2,3]\n\nTherefore z = 1.5 ∉ S.\n\n**Conclusion:** We found x, y ∈ S and λ = 0.5 such that λx + (1-λ)y ∉ S.\nTherefore S is NOT convex. ∎\n\n**Key insight:** The union of convex sets is generally not convex, unlike intersection which always preserves convexity."
  },
  {
    "id": "math404-t4-ex03",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "type": "written",
    "title": "Intersection of Convex Sets",
    "description": "Prove that the intersection of any collection of convex sets is convex.",
    "difficulty": 2,
    "hints": [
      "Let {Cᵢ} be a collection of convex sets",
      "Take any two points in the intersection and show their convex combination is in the intersection",
      "A point is in the intersection iff it is in every Cᵢ"
    ],
    "solution": "**Theorem:** If {Cᵢ}ᵢ∈I is a collection of convex sets, then C = ∩ᵢ∈I Cᵢ is convex.\n\n**Proof:**\nLet x, y ∈ C and λ ∈ [0,1].\nWe need to show z = λx + (1-λ)y ∈ C.\n\n**Step 1:** Since x ∈ C, we have x ∈ Cᵢ for all i ∈ I.\n**Step 2:** Since y ∈ C, we have y ∈ Cᵢ for all i ∈ I.\n\n**Step 3:** Fix any i ∈ I.\n- x ∈ Cᵢ and y ∈ Cᵢ\n- Cᵢ is convex\n- Therefore z = λx + (1-λ)y ∈ Cᵢ\n\n**Step 4:** Since z ∈ Cᵢ for all i ∈ I, we have z ∈ ∩ᵢ∈I Cᵢ = C.\n\n**Conclusion:** C is convex. ∎\n\n**Applications:**\n- Feasible region of LP = intersection of halfspaces (convex)\n- Positive semidefinite cone = intersection of halfspaces in eigenvalue space"
  },
  {
    "id": "math404-t4-ex04",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "type": "written",
    "title": "Convex Hull Computation",
    "description": "Find the convex hull of a finite set of points and describe it as the intersection of halfspaces.",
    "difficulty": 2,
    "hints": [
      "Convex hull = smallest convex set containing all points",
      "For finite points, it is a polyhedron",
      "Can express as {x : Ax ≤ b} for appropriate A, b"
    ],
    "solution": "**Points:** P = {(0,0), (2,0), (0,2)}\n\n**Step 1: Identify convex hull**\nconv(P) = {λ₁(0,0) + λ₂(2,0) + λ₃(0,2) : λᵢ ≥ 0, Σλᵢ = 1}\n\nThis is a triangle with vertices at the three points.\n\n**Step 2: Find halfspace representation**\nThe triangle is bounded by three lines:\n- x₁ = 0 (left edge)\n- x₂ = 0 (bottom edge)\n- x₁ + x₂ = 2 (hypotenuse)\n\n**Halfspace form:**\nconv(P) = {(x₁,x₂) : x₁ ≥ 0, x₂ ≥ 0, x₁ + x₂ ≤ 2}\n\n**Matrix form:**\nA = [[-1, 0], [0, -1], [1, 1]], b = [0, 0, 2]ᵀ\nconv(P) = {x : Ax ≤ b}\n\n**Verification:**\n- (0,0): [-0, 0, 0] ≤ [0, 0, 2] ✓\n- (2,0): [-2, 0, 2] ≤ [0, 0, 2] ✓\n- (0,2): [0, -2, 2] ≤ [0, 0, 2] ✓\n- (1,0.5): [-1, -0.5, 1.5] ≤ [0, 0, 2] ✓ (interior point)"
  },
  {
    "id": "math404-t4-ex05",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "type": "written",
    "title": "Convex Function Verification via Definition",
    "description": "Prove that f(x) = eˣ is convex using the definition of convexity.",
    "difficulty": 2,
    "hints": [
      "Need to show f(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y)",
      "Use properties of the exponential function",
      "Consider using the weighted AM-GM inequality"
    ],
    "solution": "**Claim:** f(x) = eˣ is convex on ℝ.\n\n**Proof using definition:**\nFor any x, y ∈ ℝ and λ ∈ [0,1], we need to show:\ne^(λx + (1-λ)y) ≤ λeˣ + (1-λ)eʸ\n\n**Method 1: Using AM-GM inequality**\nThe weighted arithmetic mean-geometric mean inequality states:\naᵏb¹⁻ᵏ ≤ λa + (1-λ)b for a,b > 0 and λ ∈ [0,1]\n\nLet a = eˣ and b = eʸ:\n(eˣ)ᵏ(eʸ)¹⁻ᵏ ≤ λeˣ + (1-λ)eʸ\ne^(λx)e^((1-λ)y) ≤ λeˣ + (1-λ)eʸ\ne^(λx + (1-λ)y) ≤ λeˣ + (1-λ)eʸ ✓\n\n**Method 2: Second derivative test**\nf'(x) = eˣ\nf''(x) = eˣ > 0 for all x ∈ ℝ\n\nSince f''(x) > 0, the function is strictly convex.\n\n**Conclusion:** f(x) = eˣ is convex (in fact, strictly convex). ∎"
  },
  {
    "id": "math404-t4-ex06",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "type": "written",
    "title": "Hessian Test for Convexity",
    "description": "Determine whether f(x,y) = x² + xy + y² is convex by analyzing its Hessian matrix.",
    "difficulty": 3,
    "hints": [
      "Compute the Hessian matrix of second partial derivatives",
      "Check if the Hessian is positive semidefinite",
      "A 2×2 symmetric matrix is PSD if trace ≥ 0 and det ≥ 0"
    ],
    "solution": "**Function:** f(x,y) = x² + xy + y²\n\n**Step 1: Compute gradient**\n∂f/∂x = 2x + y\n∂f/∂y = x + 2y\n\n**Step 2: Compute Hessian**\nH = [[∂²f/∂x², ∂²f/∂x∂y], [∂²f/∂y∂x, ∂²f/∂y²]]\nH = [[2, 1], [1, 2]]\n\n**Step 3: Check positive semidefiniteness**\n\n**Method 1: Eigenvalue test**\ndet(H - λI) = (2-λ)² - 1 = λ² - 4λ + 3 = 0\nλ = (4 ± 2)/2 → λ₁ = 1, λ₂ = 3\n\nBoth eigenvalues > 0 → H is positive definite → f is strictly convex.\n\n**Method 2: Leading principal minors**\nM₁ = 2 > 0\nM₂ = det(H) = 4 - 1 = 3 > 0\n\nSince all leading principal minors > 0 → H is positive definite.\n\n**Method 3: Quadratic form**\nvᵀHv = 2v₁² + 2v₁v₂ + 2v₂² = v₁² + (v₁ + v₂)² + v₂² ≥ 0\n\nwith equality only when v = 0 → strictly convex.\n\n**Conclusion:** f is strictly convex. ∎"
  },
  {
    "id": "math404-t4-ex07",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "type": "written",
    "title": "First-Order Condition for Convexity",
    "description": "Use the first-order characterization to prove convexity of a differentiable function.",
    "difficulty": 3,
    "hints": [
      "First-order condition: f(y) ≥ f(x) + ∇f(x)ᵀ(y-x)",
      "This says the function lies above all its tangent lines",
      "Equivalent to convexity for differentiable functions"
    ],
    "solution": "**Theorem:** A differentiable function f is convex iff:\nf(y) ≥ f(x) + ∇f(x)ᵀ(y-x) for all x, y\n\n**Example:** Prove f(x) = ||x||² = xᵀx is convex using first-order condition.\n\n**Gradient:** ∇f(x) = 2x\n\n**First-order condition:**\nf(y) ≥ f(x) + ∇f(x)ᵀ(y-x)\n||y||² ≥ ||x||² + 2xᵀ(y-x)\n||y||² ≥ ||x||² + 2xᵀy - 2||x||²\n||y||² ≥ 2xᵀy - ||x||²\n\n**Rearranging:**\n||y||² - 2xᵀy + ||x||² ≥ 0\n(y - x)ᵀ(y - x) ≥ 0\n||y - x||² ≥ 0 ✓\n\nThis is always true! ∎\n\n**Geometric interpretation:**\nThe tangent hyperplane at x is z = f(x) + ∇f(x)ᵀ(y-x).\nFor convex f, the graph of f lies above this hyperplane.\nFor strictly convex f, the graph lies strictly above (except at x)."
  },
  {
    "id": "math404-t4-ex08",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "type": "written",
    "title": "Operations Preserving Convexity - Sum",
    "description": "Prove that the sum of two convex functions is convex.",
    "difficulty": 2,
    "hints": [
      "Apply the definition of convexity to each function",
      "Add the inequalities together",
      "Use linearity of addition"
    ],
    "solution": "**Theorem:** If f and g are convex, then h = f + g is convex.\n\n**Proof:**\nLet x, y ∈ dom(h) and λ ∈ [0,1].\n\n**Step 1:** Since f is convex:\nf(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y)\n\n**Step 2:** Since g is convex:\ng(λx + (1-λ)y) ≤ λg(x) + (1-λ)g(y)\n\n**Step 3:** Add the inequalities:\nf(λx + (1-λ)y) + g(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y) + λg(x) + (1-λ)g(y)\n\nh(λx + (1-λ)y) ≤ λ(f(x) + g(x)) + (1-λ)(f(y) + g(y))\n\nh(λx + (1-λ)y) ≤ λh(x) + (1-λ)h(y) ✓\n\n**Conclusion:** h = f + g is convex. ∎\n\n**Extension:** More generally, if f₁,...,fₙ are convex and w₁,...,wₙ ≥ 0, then:\nh = Σᵢ wᵢfᵢ is convex.\n\n**Example:** f(x) = x² + |x| + eˣ is convex (sum of three convex functions)."
  },
  {
    "id": "math404-t4-ex09",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "type": "written",
    "title": "Pointwise Maximum of Convex Functions",
    "description": "Prove that the pointwise maximum of a set of convex functions is convex.",
    "difficulty": 3,
    "hints": [
      "Define h(x) = max{f₁(x), f₂(x), ...}",
      "Use that the max of two numbers ≤ max of each",
      "The maximum preserves the convexity inequality"
    ],
    "solution": "**Theorem:** If f₁,...,fₘ are convex, then h(x) = max{f₁(x),...,fₘ(x)} is convex.\n\n**Proof:**\nLet x, y ∈ dom(h) and λ ∈ [0,1].\nLet z = λx + (1-λ)y.\n\n**Step 1:** For any i:\nfᵢ(z) ≤ λfᵢ(x) + (1-λ)fᵢ(y)  (convexity of fᵢ)\n      ≤ λ·max_j fⱼ(x) + (1-λ)·max_j fⱼ(y)  (fᵢ ≤ max)\n      = λh(x) + (1-λ)h(y)\n\n**Step 2:** Since this holds for all i:\nmax_i fᵢ(z) ≤ λh(x) + (1-λ)h(y)\nh(z) ≤ λh(x) + (1-λ)h(y) ✓\n\n**Conclusion:** h is convex. ∎\n\n**Applications:**\n1. **Piecewise linear:** max{a₁ᵀx + b₁, ..., aₘᵀx + bₘ}\n2. **Spectral norm:** ||A|| = max_i σᵢ(A)\n3. **Max eigenvalue:** λ_max(A) = max_||x||=1 xᵀAx\n\n**Example:** f(x) = max{x, -x, 0} = |x|⁺ = max(|x|, 0) = |x|\nThis shows |x| is convex as max of linear functions."
  },
  {
    "id": "math404-t4-ex10",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "type": "written",
    "title": "Conjugate Function Computation",
    "description": "Compute the conjugate function f*(y) = sup_x{yᵀx - f(x)} for a quadratic function.",
    "difficulty": 4,
    "hints": [
      "Set derivative of yᵀx - f(x) to zero to find maximizer",
      "Substitute back to get f*(y)",
      "For quadratics, conjugate is also quadratic"
    ],
    "solution": "**Problem:** Find f*(y) where f(x) = (1/2)xᵀQx for Q ≻ 0.\n\n**Definition:** f*(y) = sup_x{yᵀx - f(x)} = sup_x{yᵀx - (1/2)xᵀQx}\n\n**Step 1: Find maximizer**\n∂/∂x [yᵀx - (1/2)xᵀQx] = y - Qx = 0\nx* = Q⁻¹y\n\n**Step 2: Substitute back**\nf*(y) = yᵀ(Q⁻¹y) - (1/2)(Q⁻¹y)ᵀQ(Q⁻¹y)\n      = yᵀQ⁻¹y - (1/2)yᵀQ⁻¹QQ⁻¹y\n      = yᵀQ⁻¹y - (1/2)yᵀQ⁻¹y\n      = (1/2)yᵀQ⁻¹y\n\n**Result:** f*(y) = (1/2)yᵀQ⁻¹y\n\n**Observations:**\n1. f*(y) is also quadratic\n2. The Hessian of f is Q; the Hessian of f* is Q⁻¹\n3. Taking conjugate again: f**(x) = f(x) (for convex f)\n\n**Special case:** f(x) = (1/2)x² → f*(y) = (1/2)y²\n\n**General property:** If f has strong convexity parameter m and smoothness L, then f* has strong convexity 1/L and smoothness 1/m."
  },
  {
    "id": "math404-t4-ex11",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "type": "written",
    "title": "Sublevel Sets and Convexity",
    "description": "Show that sublevel sets of a convex function are convex, and provide a counterexample for the converse.",
    "difficulty": 3,
    "hints": [
      "Sublevel set: Sα = {x : f(x) ≤ α}",
      "Take two points in Sα and show their convex combination is also in Sα",
      "For counterexample, find a non-convex f with convex sublevel sets"
    ],
    "solution": "**Part 1: Sublevel sets of convex f are convex**\n\n**Proof:**\nLet f be convex and Sα = {x : f(x) ≤ α}.\nTake x, y ∈ Sα and λ ∈ [0,1].\n\nSince x, y ∈ Sα: f(x) ≤ α and f(y) ≤ α\n\nBy convexity of f:\nf(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y)\n              ≤ λα + (1-λ)α = α\n\nTherefore λx + (1-λ)y ∈ Sα. ∎\n\n**Part 2: Counterexample (converse is false)**\n\n**Function:** f(x) = x³ on ℝ\n\n**Check sublevel sets:**\nS₀ = {x : x³ ≤ 0} = {x : x ≤ 0} = (-∞, 0] (convex!)\nSα = {x : x³ ≤ α} = (-∞, α^(1/3)] for α > 0 (convex!)\n\nAll sublevel sets are intervals = convex!\n\n**Check convexity of f:**\nf''(x) = 6x\nf''(x) < 0 for x < 0 (concave there)\n\nf is NOT convex, even though all sublevel sets are convex.\n\n**Definition:** A function with convex sublevel sets is called **quasiconvex**.\nEvery convex function is quasiconvex, but not vice versa."
  },
  {
    "id": "math404-t4-ex12",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "type": "written",
    "title": "Epigraph and Convexity",
    "description": "Prove that a function is convex if and only if its epigraph is a convex set.",
    "difficulty": 4,
    "hints": [
      "Epigraph: epi(f) = {(x,t) : f(x) ≤ t}",
      "For \"if\": take (x,t), (y,s) in epi(f) and show convex combination is too",
      "For \"only if\": use definition of convex function"
    ],
    "solution": "**Theorem:** f: ℝⁿ → ℝ is convex ⟺ epi(f) = {(x,t) : f(x) ≤ t} is convex.\n\n**Proof (⟹):** Assume f is convex.\nTake (x,t), (y,s) ∈ epi(f) and λ ∈ [0,1].\nWe need: (λx + (1-λ)y, λt + (1-λ)s) ∈ epi(f)\n\nCheck: f(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y)  (convexity of f)\n                     ≤ λt + (1-λ)s          (since f(x) ≤ t, f(y) ≤ s)\n\nTherefore (λx + (1-λ)y, λt + (1-λ)s) ∈ epi(f). ∎\n\n**Proof (⟸):** Assume epi(f) is convex.\nTake x, y ∈ dom(f) and λ ∈ [0,1].\n\nLet t = f(x) and s = f(y). Then (x, t), (y, s) ∈ epi(f).\n\nBy convexity of epi(f):\n(λx + (1-λ)y, λt + (1-λ)s) ∈ epi(f)\n\nThis means: f(λx + (1-λ)y) ≤ λt + (1-λ)s = λf(x) + (1-λ)f(y)\n\nTherefore f is convex. ∎\n\n**Applications:**\n- This characterization allows extending convexity to non-differentiable functions\n- Useful for showing operations preserve convexity\n- epi(f + g) = intersection of translates of epi(f) and epi(g)"
  },
  {
    "id": "math404-t4-ex13",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "type": "written",
    "title": "Separation Hyperplane",
    "description": "Apply the separating hyperplane theorem to separate a point from a convex set.",
    "difficulty": 4,
    "hints": [
      "If x₀ ∉ C (convex), there exists hyperplane separating them",
      "Find the closest point in C to x₀",
      "The hyperplane is perpendicular to the line joining them"
    ],
    "solution": "**Separating Hyperplane Theorem:**\nIf C is closed convex and x₀ ∉ C, then ∃ a ≠ 0 and b such that:\naᵀx ≤ b for all x ∈ C, and aᵀx₀ > b\n\n**Example:** C = {x ∈ ℝ² : ||x|| ≤ 1} (unit ball), x₀ = (2, 0)\n\n**Step 1: Find closest point**\nProject x₀ onto C: x_closest = x₀/||x₀|| = (1, 0)\n\n**Step 2: Find separating hyperplane**\nNormal direction: a = x₀ - x_closest = (1, 0)\nMidpoint: (x₀ + x_closest)/2 = (1.5, 0)\nHyperplane: aᵀx = aᵀ(midpoint) = 1.5\n\n**Separating hyperplane:** x₁ = 1.5\n\n**Verification:**\n- For x ∈ C: ||x|| ≤ 1 → x₁ ≤ 1 < 1.5 ✓\n- For x₀ = (2,0): x₁ = 2 > 1.5 ✓\n\n**Strict separation:**\nFor any b ∈ (1, 1.5): aᵀx < b for all x ∈ C, and aᵀx₀ > b\n\n**General formula:**\na = x₀ - P_C(x₀)  (projection direction)\nb = aᵀ(x₀ + P_C(x₀))/2  (midpoint)\n\nwhere P_C(x₀) = argmin_{x∈C} ||x - x₀||."
  },
  {
    "id": "math404-t4-ex14",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "type": "written",
    "title": "Perspective Function Convexity",
    "description": "Show that the perspective of a convex function preserves convexity.",
    "difficulty": 4,
    "hints": [
      "Perspective of f: g(x,t) = tf(x/t) for t > 0",
      "The perspective operation preserves convexity",
      "Check using definition or epi(g)"
    ],
    "solution": "**Definition:** The perspective of f: ℝⁿ → ℝ is:\ng(x,t) = tf(x/t) for t > 0\n\n**Theorem:** If f is convex, then g is convex.\n\n**Proof:**\nLet (x₁, t₁), (x₂, t₂) with t₁, t₂ > 0 and λ ∈ [0,1].\nLet μ = λt₁/(λt₁ + (1-λ)t₂).\n\n**Key calculation:**\nλx₁/t₁ + (1-λ)x₂/t₂ can be written as:\n= [λt₁ · (x₁/t₁) + (1-λ)t₂ · (x₂/t₂)] / [λt₁ + (1-λ)t₂]\n= μ(x₁/t₁) + (1-μ)(x₂/t₂)\n\n**Check convexity of g:**\ng(λ(x₁,t₁) + (1-λ)(x₂,t₂))\n= g(λx₁ + (1-λ)x₂, λt₁ + (1-λ)t₂)\n= (λt₁ + (1-λ)t₂) · f((λx₁ + (1-λ)x₂)/(λt₁ + (1-λ)t₂))\n= (λt₁ + (1-λ)t₂) · f(μ(x₁/t₁) + (1-μ)(x₂/t₂))\n≤ (λt₁ + (1-λ)t₂) · [μf(x₁/t₁) + (1-μ)f(x₂/t₂)]  (convexity of f)\n= λt₁f(x₁/t₁) + (1-λ)t₂f(x₂/t₂)\n= λg(x₁,t₁) + (1-λ)g(x₂,t₂) ✓\n\n**Example:**\nf(x) = x² → g(x,t) = tx²/t² = x²/t (convex for t > 0)\n\n**Application:** The relative entropy D(p||q) = Σpᵢlog(pᵢ/qᵢ) is a perspective of -log, hence convex."
  },
  {
    "id": "math404-t4-ex15",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "type": "written",
    "title": "Jensen's Inequality",
    "description": "State and prove Jensen's inequality, then apply it to prove the AM-GM inequality.",
    "difficulty": 4,
    "hints": [
      "Jensen: f(E[X]) ≤ E[f(X)] for convex f",
      "For AM-GM, use f(x) = -log(x) or f(x) = eˣ",
      "Consider uniform distribution on the values"
    ],
    "solution": "**Jensen's Inequality:**\nIf f is convex and X is a random variable with E[X] ∈ dom(f), then:\nf(E[X]) ≤ E[f(X)]\n\n**Proof (finite case):**\nLet X take values x₁,...,xₙ with probabilities p₁,...,pₙ.\nE[X] = Σpᵢxᵢ\n\nBy induction on convexity definition:\nf(Σpᵢxᵢ) ≤ Σpᵢf(xᵢ) = E[f(X)] ✓\n\n**AM-GM Inequality:**\nFor a₁,...,aₙ > 0: (a₁ + ... + aₙ)/n ≥ (a₁ · ... · aₙ)^(1/n)\n\n**Proof using Jensen:**\nLet f(x) = -log(x), which is convex (f''(x) = 1/x² > 0).\nLet X take values a₁,...,aₙ each with probability 1/n.\n\nJensen gives: f(E[X]) ≤ E[f(X)]\n-log((Σaᵢ)/n) ≤ (1/n)Σ(-log(aᵢ))\n-log((Σaᵢ)/n) ≤ -log((Πaᵢ)^(1/n))\nlog((Σaᵢ)/n) ≥ log((Πaᵢ)^(1/n))\n\nTaking exponentials:\n**(Σaᵢ)/n ≥ (Πaᵢ)^(1/n)** ∎\n\n**Other applications:**\n- Variance: E[X²] ≥ (E[X])² (use f(x) = x²)\n- Log-sum-exp: log(Σeˣⁱ) ≥ (1/n)Σxᵢ"
  },
  {
    "id": "math404-t4-ex16",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "type": "written",
    "title": "Log-Concavity and Log-Convexity",
    "description": "Prove that the Gaussian density is log-concave and discuss implications.",
    "difficulty": 5,
    "hints": [
      "A function is log-concave if log(f) is concave",
      "The Gaussian pdf is proportional to exp(-x²/2)",
      "Log-concave functions have nice optimization properties"
    ],
    "solution": "**Definition:**\n- f is **log-concave** if log(f) is concave\n- f is **log-convex** if log(f) is convex\n\n**Gaussian Density:**\nf(x) = (1/√(2π)) exp(-x²/2)\n\n**Claim:** f is log-concave.\n\n**Proof:**\nlog(f(x)) = -log(√(2π)) - x²/2\n\nd²(log f)/dx² = -1 < 0\n\nSince second derivative is negative, log(f) is concave.\nTherefore f is log-concave. ∎\n\n**Properties of log-concave functions:**\n\n1. **Product rule:** If f, g log-concave, then fg is log-concave.\n   Proof: log(fg) = log(f) + log(g) (sum of concave = concave)\n\n2. **Convolution:** If f, g log-concave, then f * g is log-concave.\n   (Prékopa-Leindler theorem)\n\n3. **Marginals:** Marginal of log-concave density is log-concave.\n\n4. **Optimization:** Maximizing log-concave function is a concave maximization (convex problem).\n\n**Examples of log-concave:**\n- Gaussian: exp(-||x-μ||²/(2σ²))\n- Exponential: exp(-λx) for x ≥ 0\n- Uniform: 1 on [a,b]\n- Laplace: exp(-|x|)\n\n**Example of log-convex:**\n- Gamma function: Γ(x) for x > 0"
  },
  {
    "id": "math404-t5-ex01",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "type": "written",
    "title": "Gradient Descent Update",
    "description": "Perform gradient descent iterations on a simple quadratic function.",
    "difficulty": 1,
    "hints": [
      "The update rule is x_{k+1} = x_k - α∇f(x_k)",
      "For quadratic f, gradient is linear in x",
      "Choose step size carefully to ensure convergence"
    ],
    "solution": "**Problem:** Minimize f(x) = x² starting from x₀ = 4, step size α = 0.3\n\n**Gradient:** ∇f(x) = 2x\n\n**Iteration 1:**\nx₁ = x₀ - α∇f(x₀)\n   = 4 - 0.3 × 2 × 4\n   = 4 - 2.4\n   = 1.6\n\n**Iteration 2:**\nx₂ = x₁ - α∇f(x₁)\n   = 1.6 - 0.3 × 2 × 1.6\n   = 1.6 - 0.96\n   = 0.64\n\n**Iteration 3:**\nx₃ = 0.64 - 0.3 × 2 × 0.64\n   = 0.64 - 0.384\n   = 0.256\n\n**Pattern:** x_k = (1 - 2α)^k × x₀ = (0.4)^k × 4\n\n**Convergence:** |1 - 2α| = 0.4 < 1, so converges.\n\n**After k iterations:** x_k = 4 × (0.4)^k → 0 as k → ∞\n\n**Verification:** Optimal x* = 0, f(x*) = 0 ✓"
  },
  {
    "id": "math404-t5-ex02",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "type": "written",
    "title": "Gradient Computation",
    "description": "Compute the gradient of a multivariate function and verify it using the definition.",
    "difficulty": 1,
    "hints": [
      "Gradient is the vector of partial derivatives",
      "For f: ℝⁿ → ℝ, ∇f = [∂f/∂x₁, ..., ∂f/∂xₙ]ᵀ",
      "Check by computing directional derivative"
    ],
    "solution": "**Function:** f(x,y) = x²y + 3xy² - 2x + y\n\n**Compute gradient:**\n∂f/∂x = 2xy + 3y² - 2\n∂f/∂y = x² + 6xy + 1\n\n**Gradient:** ∇f(x,y) = [2xy + 3y² - 2, x² + 6xy + 1]ᵀ\n\n**At point (1, 2):**\n∇f(1,2) = [2(1)(2) + 3(4) - 2, 1 + 6(1)(2) + 1]ᵀ\n        = [4 + 12 - 2, 1 + 12 + 1]ᵀ\n        = [14, 14]ᵀ\n\n**Verification using definition:**\nDirectional derivative in direction u = [1,0]:\nlim_{h→0} [f(1+h, 2) - f(1,2)]/h\n= lim_{h→0} [(1+h)²(2) + 3(1+h)(4) - 2(1+h) + 2 - (2 + 12 - 2 + 2)]/h\n= lim_{h→0} [2(1+2h+h²) + 12 + 12h - 2 - 2h + 2 - 14]/h\n= lim_{h→0} [4h + 2h² + 12h - 2h]/h = 14 ✓\n\n**Steepest ascent direction:** ∇f(1,2)/||∇f(1,2)|| = [1/√2, 1/√2]ᵀ"
  },
  {
    "id": "math404-t5-ex03",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "type": "written",
    "title": "Condition Number and Convergence",
    "description": "Analyze how the condition number affects gradient descent convergence rate.",
    "difficulty": 2,
    "hints": [
      "Condition number κ = λ_max/λ_min for quadratic functions",
      "Convergence rate is (κ-1)/(κ+1)",
      "Higher condition number means slower convergence"
    ],
    "solution": "**Problem:** f(x) = (1/2)xᵀQx where Q has eigenvalues 1 and 100.\n\n**Condition number:**\nκ = λ_max/λ_min = 100/1 = 100\n\n**Convergence rate for gradient descent:**\nr = (κ-1)/(κ+1) = 99/101 ≈ 0.98\n\n**Analysis:**\nAfter k iterations: ||x_k - x*|| ≤ r^k ||x₀ - x*||\n\n**For ε-accuracy (error ≤ ε||x₀ - x*||):**\nNeed r^k ≤ ε\nk ≥ log(1/ε)/log(1/r)\nk ≥ log(1/ε)/(1-r) ≈ log(1/ε) × (κ+1)/2\n\n**Example:**\nFor ε = 10⁻⁶:\nk ≥ 6 × ln(10) × 101/2 ≈ 697 iterations\n\n**Comparison with well-conditioned problem:**\nIf κ = 2: r = 1/3, k ≥ 6 × ln(10) × 3/2 ≈ 21 iterations\n\n**Conclusion:**\n- High condition number = slow convergence\n- This is the \"zig-zag\" behavior in elongated level sets\n- Preconditioning or Newton's method can help"
  },
  {
    "id": "math404-t5-ex04",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "type": "written",
    "title": "Newton's Method Iteration",
    "description": "Perform one iteration of Newton's method for optimization and compare to gradient descent.",
    "difficulty": 2,
    "hints": [
      "Newton update: x_{k+1} = x_k - H⁻¹∇f",
      "For quadratics, Newton converges in one step",
      "Need to compute and invert the Hessian"
    ],
    "solution": "**Problem:** Minimize f(x,y) = x² + 2y² - xy\n\n**Gradient:**\n∇f = [2x - y, 4y - x]ᵀ\n\n**Hessian:**\nH = [[2, -1], [-1, 4]]\n\n**Hessian inverse:**\ndet(H) = 8 - 1 = 7\nH⁻¹ = (1/7)[[4, 1], [1, 2]]\n\n**Starting point:** x₀ = (3, 1)\n\n**Newton iteration:**\n∇f(x₀) = [2(3) - 1, 4(1) - 3]ᵀ = [5, 1]ᵀ\n\nΔx = -H⁻¹∇f(x₀) = -(1/7)[[4,1],[1,2]][5,1]ᵀ\n   = -(1/7)[20+1, 5+2]ᵀ = -(1/7)[21, 7]ᵀ = [-3, -1]ᵀ\n\nx₁ = x₀ + Δx = (3,1) + (-3,-1) = (0, 0)\n\n**Verify optimality:**\n∇f(0,0) = [0, 0]ᵀ ✓\n\n**Newton found exact optimum in ONE iteration!**\n\n**Gradient descent comparison:**\nWith α = 0.2:\nx₁ = (3,1) - 0.2[5,1] = (2, 0.8)\nStill far from optimum after 1 iteration.\n\n**Convergence:**\n- Newton: quadratic convergence (error squares each iteration)\n- GD: linear convergence (error reduces by constant factor)"
  },
  {
    "id": "math404-t5-ex05",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "type": "written",
    "title": "Exact Line Search",
    "description": "Derive the optimal step size for gradient descent using exact line search.",
    "difficulty": 3,
    "hints": [
      "Exact line search: α* = argmin_α f(x - α∇f(x))",
      "For quadratics, this has a closed-form solution",
      "Set derivative with respect to α to zero"
    ],
    "solution": "**Problem:** f(x) = (1/2)xᵀQx - bᵀx\n\nFind optimal step size α at point x with descent direction d = -∇f(x) = b - Qx.\n\n**Line search problem:**\nmin_α f(x + αd)\n\n**Expand:**\nf(x + αd) = (1/2)(x + αd)ᵀQ(x + αd) - bᵀ(x + αd)\n          = (1/2)xᵀQx + α xᵀQd + (α²/2)dᵀQd - bᵀx - αbᵀd\n\n**Differentiate with respect to α:**\nd/dα f(x + αd) = xᵀQd + αdᵀQd - bᵀd\n               = dᵀQx + αdᵀQd - dᵀb\n               = dᵀ(Qx - b) + αdᵀQd\n               = -dᵀd + αdᵀQd  (since d = b - Qx)\n\n**Wait, let me recalculate with d = -∇f = -(Qx - b) = b - Qx:**\nd/dα = (Qx - b)ᵀd + αdᵀQd = -||d||² + αdᵀQd\n\n**Set to zero:**\nα* = ||d||²/(dᵀQd) = ||∇f||²/(∇fᵀQ∇f)\n\n**For steepest descent (d = -∇f):**\n**α* = ∇fᵀ∇f / (∇fᵀQ∇f)**\n\n**Example:**\nQ = [[2, 0], [0, 8]], x = (1, 1)\n∇f = Qx = [2, 8]\nα* = (4 + 64)/(4×2 + 64×8) = 68/520 ≈ 0.131\n\n**Properties:**\n- Exact line search guarantees descent\n- Successive gradients are orthogonal: ∇f_{k+1}ᵀ∇f_k = 0\n- Optimal for quadratics with exact arithmetic"
  },
  {
    "id": "math404-t5-ex06",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "type": "written",
    "title": "Backtracking Line Search",
    "description": "Implement the Armijo backtracking line search and verify sufficient decrease.",
    "difficulty": 3,
    "hints": [
      "Armijo condition: f(x - αd) ≤ f(x) - cα∇fᵀd",
      "Start with α = 1 and reduce by factor β until condition holds",
      "Typical values: c = 0.0001, β = 0.5"
    ],
    "solution": "**Armijo-Goldstein Condition:**\nf(x_k - α d_k) ≤ f(x_k) - c α ∇f(x_k)ᵀd_k\n\nwhere d_k is descent direction, typically d_k = ∇f(x_k).\n\n**Algorithm:**\n1. Set α = 1, c = 10⁻⁴, β = 0.5\n2. While f(x - αd) > f(x) - cα||d||²:\n     α ← βα\n3. Return α\n\n**Example:**\nf(x) = (x-2)⁴, x₀ = 0, d = -f'(x₀) = -4(0-2)³ = 32\n\n**Check α = 1:**\nf(0 - 1×32) = f(-32) = (-34)⁴ ≈ 1.3 × 10⁶\nf(0) - 0.0001×1×32² = 16 - 0.1 = 15.9\n1.3×10⁶ > 15.9 ✗\n\n**Check α = 0.5:**\nf(-16) = (-18)⁴ = 104,976\nStill much > 15.9 ✗\n\n**Continue reducing...**\n\n**α = 0.0625:**\nf(-2) = (-4)⁴ = 256\nf(0) - 0.0001×0.0625×1024 = 16 - 0.0064 ≈ 16\n256 > 16 ✗\n\n**α = 0.03125:**\nf(-1) = (-3)⁴ = 81\n16 - 0.0032 ≈ 16\n81 > 16 ✗\n\n**α = 0.01:**\nf(-0.32) = (-2.32)⁴ ≈ 29\n16 - 0.001 ≈ 16\n29 > 16 ✗\n\nEventually we find suitable α.\n\n**Key insight:** Backtracking ensures sufficient decrease per iteration."
  },
  {
    "id": "math404-t5-ex07",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "type": "written",
    "title": "Momentum in Gradient Descent",
    "description": "Compare gradient descent with and without momentum on a poorly conditioned problem.",
    "difficulty": 3,
    "hints": [
      "Momentum update: v_{k+1} = βv_k + ∇f(x_k), x_{k+1} = x_k - αv_{k+1}",
      "Momentum accelerates convergence in consistent directions",
      "Typical β = 0.9"
    ],
    "solution": "**Problem:** f(x,y) = (1/2)(x² + 100y²) (condition number κ = 100)\n\n**Standard GD with α = 0.01:**\nStarting at (10, 1):\n∇f = [10, 200]\nx₁ = (10, 1) - 0.01[10, 200] = (9.9, -1)\nx₂ = (9.9, -1) - 0.01[9.9, -200] = (9.801, 1)\n...oscillates slowly\n\n**Gradient Descent with Momentum (β = 0.9):**\nv₀ = 0\n∇f(x₀) = [10, 200]\n\n**Iteration 1:**\nv₁ = 0.9×0 + [10, 200] = [10, 200]\nx₁ = (10, 1) - 0.01[10, 200] = (9.9, -1)\n\n**Iteration 2:**\nv₂ = 0.9[10, 200] + [9.9, -200] = [9 + 9.9, 180 - 200] = [18.9, -20]\nx₂ = (9.9, -1) - 0.01[18.9, -20] = (9.711, -0.8)\n\n**Iteration 3:**\nv₃ = 0.9[18.9, -20] + [9.711, -80] = [17.01 + 9.711, -18 - 80] = [26.7, -98]\nx₃ = (9.711, -0.8) - 0.01[26.7, -98] = (9.44, 0.18)\n\n**Comparison:**\n- Without momentum: x-coordinate decreases slowly\n- With momentum: acceleration in x-direction, damping in y-direction\n\n**Theory:**\nWith optimal momentum, convergence rate improves from:\n(κ-1)/(κ+1) to (√κ-1)/(√κ+1)\n\nFor κ = 100:\n- Without momentum: r = 0.98\n- With momentum: r = 0.82"
  },
  {
    "id": "math404-t5-ex08",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "type": "written",
    "title": "Quasi-Newton BFGS Update",
    "description": "Perform one BFGS update to approximate the inverse Hessian.",
    "difficulty": 4,
    "hints": [
      "BFGS maintains an approximation H_k ≈ (∇²f)⁻¹",
      "Update uses secant condition: H_{k+1}y_k = s_k",
      "Formula: H_{k+1} = (I - ρsy^T)H_k(I - ρys^T) + ρss^T"
    ],
    "solution": "**BFGS Update Formula:**\nH_{k+1} = (I - ρsyᵀ)H_k(I - ρysᵀ) + ρssᵀ\n\nwhere:\n- s = x_{k+1} - x_k (step)\n- y = ∇f_{k+1} - ∇f_k (gradient change)\n- ρ = 1/(yᵀs)\n\n**Example:**\nH₀ = I (identity initialization)\nx₀ = (2, 1), x₁ = (1, 0.5)\n∇f₀ = (4, 2), ∇f₁ = (2, 1)\n\n**Compute s and y:**\ns = x₁ - x₀ = (-1, -0.5)\ny = ∇f₁ - ∇f₀ = (-2, -1)\n\n**Compute ρ:**\nyᵀs = (-2)(-1) + (-1)(-0.5) = 2 + 0.5 = 2.5\nρ = 1/2.5 = 0.4\n\n**Compute intermediate terms:**\nsyᵀ = [[-1], [-0.5]] × [[-2, -1]] = [[2, 1], [1, 0.5]]\nysᵀ = [[-2], [-1]] × [[-1, -0.5]] = [[2, 1], [1, 0.5]]\nssᵀ = [[1, 0.5], [0.5, 0.25]]\n\n**Update:**\n(I - ρsyᵀ) = [[1-0.8, -0.4], [-0.4, 1-0.2]] = [[0.2, -0.4], [-0.4, 0.8]]\n\nH₁ = [[0.2, -0.4], [-0.4, 0.8]] × I × [[0.2, -0.4], [-0.4, 0.8]]ᵀ + 0.4×ssᵀ\n   = [[0.2, -0.4], [-0.4, 0.8]]² + 0.4[[1, 0.5], [0.5, 0.25]]\n   = [[0.04+0.16, -0.08-0.32], [-0.08-0.32, 0.16+0.64]] + [[0.4, 0.2], [0.2, 0.1]]\n   = [[0.2+0.4, -0.4+0.2], [-0.4+0.2, 0.8+0.1]]\n   = [[0.6, -0.2], [-0.2, 0.9]]\n\n**Verification:** H₁y = s\n[[0.6, -0.2], [-0.2, 0.9]][[-2],[-1]] = [[-1.2+0.2], [0.4-0.9]] = [[-1], [-0.5]] = s ✓"
  },
  {
    "id": "math404-t5-ex09",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "type": "written",
    "title": "Conjugate Gradient Method",
    "description": "Apply conjugate gradient to minimize a quadratic function.",
    "difficulty": 4,
    "hints": [
      "CG generates Q-conjugate directions: dᵢᵀQdⱼ = 0",
      "For n-dimensional quadratic, converges in at most n steps",
      "Update: d_k = -g_k + β_k d_{k-1}"
    ],
    "solution": "**Problem:** Minimize f(x) = (1/2)xᵀQx - bᵀx\nwhere Q = [[2, 1], [1, 2]], b = [1, 0]ᵀ\n\n**Conjugate Gradient Algorithm:**\n\n**Initialization:** x₀ = (0, 0)\ng₀ = Qx₀ - b = -[1, 0]ᵀ = [-1, 0]ᵀ\nd₀ = -g₀ = [1, 0]ᵀ\n\n**Iteration 1:**\nα₀ = g₀ᵀg₀ / (d₀ᵀQd₀)\n   = 1 / ([1,0][[2,1],[1,2]][1,0]ᵀ)\n   = 1 / [1,0][2,1]ᵀ = 1/2\n\nx₁ = x₀ + α₀d₀ = (0,0) + 0.5(1,0) = (0.5, 0)\n\ng₁ = Qx₁ - b = [[2,1],[1,2]][0.5,0]ᵀ - [1,0]ᵀ = [1,0.5]ᵀ - [1,0]ᵀ = [0, 0.5]ᵀ\n\nβ₁ = g₁ᵀg₁ / g₀ᵀg₀ = 0.25 / 1 = 0.25\n\nd₁ = -g₁ + β₁d₀ = [0, -0.5]ᵀ + 0.25[1, 0]ᵀ = [0.25, -0.5]ᵀ\n\n**Iteration 2:**\nα₁ = g₁ᵀg₁ / (d₁ᵀQd₁)\nd₁ᵀQd₁ = [0.25, -0.5][[2,1],[1,2]][0.25, -0.5]ᵀ\n       = [0.25, -0.5][0.5-0.5, 0.25-1]ᵀ = [0.25, -0.5][0, -0.75]ᵀ = 0.375\n\nα₁ = 0.25/0.375 = 2/3\n\nx₂ = x₁ + α₁d₁ = (0.5, 0) + (2/3)(0.25, -0.5) = (0.5 + 1/6, -1/3) = (2/3, -1/3)\n\n**Verify:** g₂ = Q x₂ - b = [[2,1],[1,2]][2/3,-1/3]ᵀ - [1,0]ᵀ = [4/3-1/3, 2/3-2/3]ᵀ - [1,0]ᵀ = [0,0]ᵀ ✓\n\n**Solution:** x* = (2/3, -1/3)\nCG converged exactly in n = 2 iterations!"
  },
  {
    "id": "math404-t5-ex10",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "type": "written",
    "title": "Convergence Rate Analysis",
    "description": "Prove the linear convergence rate of gradient descent for strongly convex functions.",
    "difficulty": 4,
    "hints": [
      "Strong convexity: f(y) ≥ f(x) + ∇f(x)ᵀ(y-x) + (m/2)||y-x||²",
      "Smoothness: ||∇f(x) - ∇f(y)|| ≤ L||x-y||",
      "Contraction: ||x_{k+1} - x*|| ≤ (1 - m/L)||x_k - x*||"
    ],
    "solution": "**Assumptions:**\n- f is m-strongly convex: ∇²f ⪰ mI\n- f is L-smooth: ∇²f ⪯ LI\n- Condition number: κ = L/m\n\n**Gradient Descent:** x_{k+1} = x_k - (1/L)∇f(x_k)\n\n**Theorem:** ||x_{k+1} - x*||² ≤ (1 - 1/κ)||x_k - x*||²\n\n**Proof sketch:**\nBy L-smoothness:\nf(x_{k+1}) ≤ f(x_k) + ∇f(x_k)ᵀ(x_{k+1} - x_k) + (L/2)||x_{k+1} - x_k||²\n         = f(x_k) - (1/L)||∇f(x_k)||² + (1/2L)||∇f(x_k)||²\n         = f(x_k) - (1/2L)||∇f(x_k)||²\n\nBy strong convexity:\n||∇f(x_k)||² ≥ 2m(f(x_k) - f*)\n\nCombining:\nf(x_{k+1}) - f* ≤ (1 - m/L)(f(x_k) - f*)\n\n**Corollary:** After k iterations:\nf(x_k) - f* ≤ (1 - 1/κ)^k (f(x₀) - f*)\n\n**For ε-accuracy:**\nk ≥ κ log(1/ε)\n\n**Example:**\nκ = 100, ε = 10⁻⁶\nk ≥ 100 × 6 × ln(10) ≈ 1382 iterations\n\n**Optimal step size:** α = 1/L (not 2/(m+L))\nWith α = 2/(m+L): rate = (κ-1)/(κ+1) (slightly better)"
  },
  {
    "id": "math404-t5-ex11",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "type": "written",
    "title": "Stochastic Gradient Descent",
    "description": "Analyze the convergence of stochastic gradient descent for finite-sum problems.",
    "difficulty": 4,
    "hints": [
      "SGD uses a random gradient estimate instead of full gradient",
      "For f(x) = (1/n)Σfᵢ(x), use ∇fᵢ(x) for random i",
      "Convergence is O(1/k) for convex, O(1/√k) for general"
    ],
    "solution": "**Problem:** min f(x) = (1/n)Σᵢ₌₁ⁿ fᵢ(x)\n\n**SGD Update:**\n1. Sample i uniformly from {1,...,n}\n2. x_{k+1} = x_k - α_k ∇fᵢ(x_k)\n\n**Key property:** E[∇fᵢ(x)] = ∇f(x) (unbiased estimate)\n\n**Variance:** E[||∇fᵢ - ∇f||²] = σ²\n\n**Convergence for strongly convex f:**\n\n**Constant step size α = 1/L:**\nlim_{k→∞} E[f(x_k) - f*] = O(σ²/m)\n(Does not converge to optimum!)\n\n**Decreasing step size α_k = 1/(m·k):**\nE[f(x_k) - f*] ≤ O(σ²/(mk))\n(Converges but slowly)\n\n**Mini-batch SGD:**\nUse batch of size B: ∇ ≈ (1/B)Σⱼ∈B ∇fⱼ\nReduces variance by factor B\n\n**Comparison for n = 10⁶, ε = 10⁻⁴:**\n\n| Method | Cost per iteration | Iterations | Total cost |\n|--------|-------------------|------------|------------|\n| GD     | n                 | O(κ log 1/ε) | n × κ log 1/ε |\n| SGD    | 1                 | O(κ/ε)     | κ/ε        |\n\nSGD wins when n > κ log(1/ε)/ε\n\n**Variance reduction (SVRG, SAGA):**\nAchieve O(κ log 1/ε) complexity with O(1) per-iteration cost!"
  },
  {
    "id": "math404-t5-ex12",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "type": "written",
    "title": "Proximal Gradient Method",
    "description": "Apply the proximal gradient method to minimize a composite objective.",
    "difficulty": 5,
    "hints": [
      "For f(x) + g(x) where g is non-smooth, use prox_g",
      "Proximal operator: prox_g(y) = argmin_x{g(x) + (1/2)||x-y||²}",
      "Update: x_{k+1} = prox_{αg}(x_k - α∇f(x_k))"
    ],
    "solution": "**Problem:** min (1/2)||Ax - b||² + λ||x||₁\n\nf(x) = (1/2)||Ax - b||² (smooth)\ng(x) = λ||x||₁ (non-smooth)\n\n**Proximal operator for L1:**\nprox_{λ||·||₁}(y) = soft-threshold(y, λ)\n[prox(y)]ᵢ = sign(yᵢ) × max(|yᵢ| - λ, 0)\n\n**Proximal Gradient (ISTA):**\n1. Gradient step: z_k = x_k - α AᵀA(Ax_k - b)\n2. Proximal step: x_{k+1} = soft-threshold(z_k, αλ)\n\n**Example:** A = I, b = [3, -1]ᵀ, λ = 0.5, α = 1\n\n**Iteration 1:** x₀ = (0, 0)\nGradient step: z₀ = (0, 0) - 1×(0 - [3, -1]) = (3, -1)\nProximal step: x₁ = (max(3-0.5, 0), -max(1-0.5, 0)) = (2.5, -0.5)\n\n**Iteration 2:**\nz₁ = (2.5, -0.5) - 1×([2.5, -0.5] - [3, -1]) = (2.5, -0.5) - (-0.5, 0.5) = (3, -1)\nx₂ = (2.5, -0.5)\n\n**Converged!** x* = (2.5, -0.5)\n\n**Verification:**\n∇f(x*) = x* - b = (-0.5, 0.5)\nSubgradient of g: ∂g(x*) = λ × sign(x*) = (0.5, -0.5)\n∇f + ∂g ∋ 0 ✓\n\n**Acceleration (FISTA):** Achieves O(1/k²) vs O(1/k)"
  },
  {
    "id": "math404-t5-ex13",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "type": "written",
    "title": "Nesterov Acceleration",
    "description": "Derive and analyze Nesterov's accelerated gradient method.",
    "difficulty": 5,
    "hints": [
      "Achieves O(1/k²) for convex, vs O(1/k) for standard GD",
      "Uses momentum with look-ahead gradient",
      "Optimal for first-order methods"
    ],
    "solution": "**Nesterov Accelerated Gradient (NAG):**\n\n**Algorithm:**\ny_k = x_k + β_k(x_k - x_{k-1})  (momentum step)\nx_{k+1} = y_k - α∇f(y_k)        (gradient step)\n\nwhere β_k = (k-1)/(k+2) or β_k = (√κ - 1)/(√κ + 1)\n\n**Key insight:** Compute gradient at \"look-ahead\" point y_k, not x_k\n\n**Comparison:**\n\n**Standard GD:**\nError: f(x_k) - f* ≤ O(L||x₀ - x*||²/k)\nRate: O(1/k)\n\n**NAG:**\nError: f(x_k) - f* ≤ O(L||x₀ - x*||²/k²)\nRate: O(1/k²)\n\n**For strongly convex:**\n\n**Standard GD:**\nf(x_k) - f* ≤ (1 - 1/κ)^k (f(x₀) - f*)\n\n**NAG:**\nf(x_k) - f* ≤ (1 - 1/√κ)^k (f(x₀) - f*)\n\n**Example:** κ = 100\n- GD: need k ≈ 100 × ln(10⁶) ≈ 1382 for ε = 10⁻⁶\n- NAG: need k ≈ 10 × ln(10⁶) ≈ 138 for ε = 10⁻⁶\n\n**10× fewer iterations!**\n\n**Optimality:**\nNesterov showed this rate is optimal for first-order methods:\nAny method using only gradient information requires Ω(√κ log(1/ε)) iterations.\n\n**Implementation:**\n```\nx, x_old = x0, x0\nfor k in range(K):\n    y = x + (k-1)/(k+2) * (x - x_old)\n    x_old = x\n    x = y - alpha * grad_f(y)\n```"
  },
  {
    "id": "math404-t5-ex14",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "type": "written",
    "title": "Coordinate Descent",
    "description": "Apply coordinate descent to minimize a separable objective function.",
    "difficulty": 3,
    "hints": [
      "Update one coordinate at a time",
      "For separable problems, each update is a 1D optimization",
      "Cyclic or randomized coordinate selection"
    ],
    "solution": "**Problem:** min f(x₁, x₂) = x₁² + 2x₂² + x₁x₂\n\n**Coordinate Descent:**\nAt each step, minimize over one coordinate holding others fixed.\n\n**Update rule for coordinate i:**\nxᵢ ← argmin_{xᵢ} f(x₁, ..., xᵢ, ..., xₙ)\n\n**For our problem:**\n∂f/∂x₁ = 2x₁ + x₂ = 0 → x₁ = -x₂/2\n∂f/∂x₂ = 4x₂ + x₁ = 0 → x₂ = -x₁/4\n\n**Starting point:** x₀ = (4, 4)\n\n**Iteration 1:**\nUpdate x₁: x₁ = -x₂/2 = -4/2 = -2\nAfter: x = (-2, 4)\n\nUpdate x₂: x₂ = -x₁/4 = -(-2)/4 = 0.5\nAfter: x = (-2, 0.5)\n\n**Iteration 2:**\nUpdate x₁: x₁ = -0.5/2 = -0.25\nAfter: x = (-0.25, 0.5)\n\nUpdate x₂: x₂ = -(-0.25)/4 = 0.0625\nAfter: x = (-0.25, 0.0625)\n\n**Pattern:**\nAfter each full cycle, x₁ reduces by factor 1/8:\nx₁^(k) = (−1/8)^k × x₁^(0)\n\n**Convergence:** x* = (0, 0)\n\n**When coordinate descent works well:**\n1. Separable or nearly separable objectives\n2. Each coordinate update is cheap\n3. High-dimensional problems where full gradient is expensive\n\n**Randomized CD:**\nPick coordinate uniformly at random each iteration.\nOften better bounds than cyclic CD."
  },
  {
    "id": "math404-t5-ex15",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "type": "written",
    "title": "Trust Region Method",
    "description": "Solve a trust region subproblem and adjust the trust region radius.",
    "difficulty": 5,
    "hints": [
      "Solve min m_k(p) s.t. ||p|| ≤ Δ_k",
      "m_k(p) = f_k + g_kᵀp + (1/2)pᵀH_kp",
      "Adjust Δ based on actual vs predicted reduction"
    ],
    "solution": "**Trust Region Subproblem:**\nmin m(p) = f + gᵀp + (1/2)pᵀHp\ns.t. ||p|| ≤ Δ\n\n**Case 1: Unconstrained minimizer inside trust region**\nIf H ≻ 0 and ||H⁻¹g|| ≤ Δ:\n  p* = -H⁻¹g (Newton step)\n\n**Case 2: Constrained to boundary**\np*(λ) = -(H + λI)⁻¹g for some λ ≥ 0\nChoose λ such that ||p*(λ)|| = Δ\n\n**Example:**\nf = 10, g = [2, 1]ᵀ, H = [[1, 0], [0, 2]], Δ = 1\n\n**Check unconstrained:**\np_Newton = -H⁻¹g = -[[1,0],[0,0.5]][2,1]ᵀ = [-2, -0.5]ᵀ\n||p_Newton|| = √(4 + 0.25) ≈ 2.06 > 1 = Δ\n\n**Constrain to boundary:**\nNeed ||-(H + λI)⁻¹g|| = 1\n\nFor λ = 1:\nH + I = [[2,0],[0,3]]\np = -[[0.5,0],[0,0.33]][2,1]ᵀ = [-1, -0.33]ᵀ\n||p|| = √(1 + 0.11) ≈ 1.05 > 1\n\nFor λ = 1.2:\np ≈ [-0.91, -0.31]ᵀ, ||p|| ≈ 0.96 < 1\n\nSolve exactly: ||p(λ)|| = 1\n\n**Radius adjustment:**\nρ = (f(x) - f(x + p))/(m(0) - m(p)) (actual/predicted reduction)\n\nIf ρ < 0.25: Δ ← Δ/4 (contract)\nIf ρ > 0.75 and ||p|| = Δ: Δ ← min(2Δ, Δ_max) (expand)\nAccept step if ρ > η (typically η = 0.1)\n\n**Advantages:**\n- Global convergence guarantee\n- Handles indefinite Hessians\n- Naturally adapts step size"
  },
  {
    "id": "math404-t5-ex16",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "type": "written",
    "title": "L-BFGS Method",
    "description": "Describe the limited-memory BFGS method and its memory efficiency.",
    "difficulty": 5,
    "hints": [
      "Stores only last m pairs (s_i, y_i) instead of full Hessian",
      "Uses two-loop recursion to compute H_k∇f efficiently",
      "Typical m = 3-20"
    ],
    "solution": "**L-BFGS Motivation:**\n- BFGS stores n×n matrix H_k (O(n²) memory)\n- For large n, this is prohibitive\n- L-BFGS uses only O(mn) memory\n\n**Storage:**\nKeep last m pairs:\n{(s_{k-m}, y_{k-m}), ..., (s_{k-1}, y_{k-1})}\nwhere s_i = x_{i+1} - x_i, y_i = ∇f_{i+1} - ∇f_i\n\n**Two-Loop Recursion:**\n\n**Forward loop (compute coefficients):**\nq ← ∇f_k\nfor i = k-1, k-2, ..., k-m:\n  ρᵢ = 1/(yᵢᵀsᵢ)\n  αᵢ = ρᵢ sᵢᵀ q\n  q ← q - αᵢ yᵢ\n\n**Scale:**\nH⁰ = γI where γ = (s_{k-1}ᵀy_{k-1})/(y_{k-1}ᵀy_{k-1})\nr ← H⁰ q = γq\n\n**Backward loop (apply corrections):**\nfor i = k-m, ..., k-1:\n  β = ρᵢ yᵢᵀ r\n  r ← r + sᵢ(αᵢ - β)\n\nreturn -r\n\n**Complexity:**\n- Memory: O(mn)\n- Per iteration: O(mn) vs O(n²) for full BFGS\n\n**Typical parameters:**\nm = 10 works well for most problems\n\n**Comparison (n = 10,000):**\n| Method | Memory | Per-iteration |\n|--------|--------|---------------|\n| GD     | O(n)   | O(n)          |\n| BFGS   | O(n²)  | O(n²)         |\n| L-BFGS | O(mn)  | O(mn)         |\n\nFor n = 10,000, m = 10:\n- BFGS: 100M entries\n- L-BFGS: 200K entries (500× smaller)\n\n**When to use:**\n- Large-scale smooth optimization\n- When Hessian is too expensive to compute/store\n- Works well with line search"
  },
  {
    "id": "math404-t6-ex01",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "type": "written",
    "title": "Lagrangian Formulation",
    "description": "Write the Lagrangian for a constrained optimization problem with equality and inequality constraints.",
    "difficulty": 1,
    "hints": [
      "Lagrangian: L(x,λ,μ) = f(x) + Σλᵢgᵢ(x) + Σμⱼhⱼ(x)",
      "λᵢ for inequality constraints gᵢ(x) ≤ 0",
      "μⱼ for equality constraints hⱼ(x) = 0"
    ],
    "solution": "**Problem:**\nmin x² + y²\ns.t. x + y ≤ 2\n     x - y = 0\n\n**Identify constraints:**\nInequality: g(x,y) = x + y - 2 ≤ 0\nEquality: h(x,y) = x - y = 0\n\n**Lagrangian:**\nL(x, y, λ, μ) = x² + y² + λ(x + y - 2) + μ(x - y)\n\nwhere λ ≥ 0 (multiplier for inequality) and μ ∈ ℝ (for equality).\n\n**Expanded:**\nL = x² + y² + λx + λy - 2λ + μx - μy\n\n**Interpretation:**\n- The Lagrangian penalizes constraint violations\n- At optimum: ∇ₓL = 0 (stationarity)\n- Multipliers give sensitivity to constraint changes"
  },
  {
    "id": "math404-t6-ex02",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "type": "written",
    "title": "KKT Conditions - Statement",
    "description": "State the complete KKT conditions for a nonlinear optimization problem.",
    "difficulty": 2,
    "hints": [
      "KKT has four components: stationarity, primal feasibility, dual feasibility, complementary slackness",
      "These are necessary conditions for local optimality",
      "Sufficient for convex problems"
    ],
    "solution": "**Problem:** min f(x) s.t. gᵢ(x) ≤ 0, hⱼ(x) = 0\n\n**KKT Conditions:**\n\n**1. Stationarity:**\n∇f(x*) + Σλᵢ*∇gᵢ(x*) + Σμⱼ*∇hⱼ(x*) = 0\n\n**2. Primal Feasibility:**\ngᵢ(x*) ≤ 0 for all i\nhⱼ(x*) = 0 for all j\n\n**3. Dual Feasibility:**\nλᵢ* ≥ 0 for all i\n\n**4. Complementary Slackness:**\nλᵢ*gᵢ(x*) = 0 for all i\n\n**Interpretation:**\n- Either constraint is inactive (gᵢ < 0, λᵢ = 0)\n- Or constraint is active (gᵢ = 0, λᵢ ≥ 0)\n\n**When KKT conditions are sufficient:**\n- f convex, gᵢ convex, hⱼ affine\n- Any point satisfying KKT is a global minimum\n\n**When KKT conditions are necessary:**\n- Under constraint qualification (e.g., LICQ, Slater)\n- Any local minimum satisfies KKT"
  },
  {
    "id": "math404-t6-ex03",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "type": "written",
    "title": "KKT Solution - Equality Constraint",
    "description": "Solve a constrained problem with one equality constraint using KKT conditions.",
    "difficulty": 2,
    "hints": [
      "Form Lagrangian with μ for equality constraint",
      "Set gradient of Lagrangian to zero",
      "Solve system of equations"
    ],
    "solution": "**Problem:** min x² + y² s.t. x + y = 1\n\n**Lagrangian:**\nL(x, y, μ) = x² + y² + μ(x + y - 1)\n\n**KKT Conditions:**\n\n**Stationarity:**\n∂L/∂x = 2x + μ = 0  →  x = -μ/2\n∂L/∂y = 2y + μ = 0  →  y = -μ/2\n\n**Primal Feasibility:**\nx + y = 1\n(-μ/2) + (-μ/2) = 1\n-μ = 1\nμ = -1\n\n**Solution:**\nx* = y* = 1/2\nμ* = -1\n\n**Verification:**\n- Constraint: 1/2 + 1/2 = 1 ✓\n- Objective: f(1/2, 1/2) = 1/4 + 1/4 = 1/2\n\n**Geometric interpretation:**\nWe're finding the point on the line x + y = 1 closest to the origin.\nThe gradient ∇f = (1, 1) is perpendicular to the constraint."
  },
  {
    "id": "math404-t6-ex04",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "type": "written",
    "title": "KKT Solution - Inequality Constraint",
    "description": "Solve a constrained problem with one inequality constraint using KKT conditions.",
    "difficulty": 3,
    "hints": [
      "Check if unconstrained optimum is feasible",
      "If not, constraint must be active at optimum",
      "Use complementary slackness to determine λ"
    ],
    "solution": "**Problem:** min (x-2)² + (y-2)² s.t. x + y ≤ 2\n\n**Step 1: Check unconstrained optimum**\nUnconstrained: (x*, y*) = (2, 2)\nConstraint: 2 + 2 = 4 > 2 (infeasible!)\n\n**Step 2: Solve with active constraint**\nSince unconstrained optimum is infeasible, constraint must be active.\nBy complementary slackness: λ > 0 and x + y = 2\n\n**KKT Conditions:**\n∂L/∂x = 2(x-2) + λ = 0  →  x = 2 - λ/2\n∂L/∂y = 2(y-2) + λ = 0  →  y = 2 - λ/2\n\nx + y = 2:\n(2 - λ/2) + (2 - λ/2) = 2\n4 - λ = 2\nλ = 2\n\n**Solution:**\nx* = y* = 2 - 1 = 1\nλ* = 2\n\n**Verification:**\n- Primal: 1 + 1 = 2 ✓\n- Dual: λ* = 2 ≥ 0 ✓\n- Stationarity: ∇f(1,1) = (-2,-2), λ∇g = 2(1,1), sum = 0 ✓\n- Objective: f(1,1) = 1 + 1 = 2"
  },
  {
    "id": "math404-t6-ex05",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "type": "written",
    "title": "Multiple Inequality Constraints",
    "description": "Solve a problem with multiple inequality constraints, identifying which are active.",
    "difficulty": 3,
    "hints": [
      "Consider all possible combinations of active constraints",
      "Check feasibility and KKT for each case",
      "Active constraints have λ > 0"
    ],
    "solution": "**Problem:** min -x - y\ns.t. x² + y² ≤ 1\n     x ≥ 0\n     y ≥ 0\n\nRewrite: max x + y s.t. constraints\n\n**Lagrangian:**\nL = -x - y + λ₁(x² + y² - 1) - λ₂x - λ₃y\n\n**Analyze constraint activity:**\n\n**Case 1: Interior (all constraints inactive)**\n∇(-x-y) = (-1,-1) ≠ 0, so not stationary. ✗\n\n**Case 2: Only circle active**\nBy symmetry: x = y = 1/√2\nCheck bounds: x, y > 0 ✓\n\nKKT: -1 + 2λ₁(1/√2) = 0 → λ₁ = √2/2 ≈ 0.707 > 0 ✓\n\n**Solution:** (x*, y*) = (1/√2, 1/√2), λ₁ = √2/2\n\n**Verification:**\n- x² + y² = 1/2 + 1/2 = 1 ✓\n- x = y = 1/√2 > 0 ✓\n- Objective: x + y = √2 ≈ 1.414\n\n**Alternative cases checked:**\n- x = 0: optimal at (0, 1), value = 1 < √2\n- y = 0: optimal at (1, 0), value = 1 < √2\n- Corner (0,0): value = 0 < √2\n\n**Optimal:** (1/√2, 1/√2) with f* = -√2"
  },
  {
    "id": "math404-t6-ex06",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "type": "written",
    "title": "Constraint Qualification - LICQ",
    "description": "Verify Linear Independence Constraint Qualification at a candidate optimal point.",
    "difficulty": 3,
    "hints": [
      "LICQ: gradients of active constraints are linearly independent",
      "Find which constraints are active (equality holds)",
      "Check if gradient vectors are linearly independent"
    ],
    "solution": "**Problem:** min x² + y² s.t.\ng₁(x,y) = x + y - 1 ≤ 0\ng₂(x,y) = x - y ≤ 0\n\n**At point (1/2, 1/2):**\ng₁ = 1/2 + 1/2 - 1 = 0 (active)\ng₂ = 1/2 - 1/2 = 0 (active)\n\n**Gradients of active constraints:**\n∇g₁ = (1, 1)\n∇g₂ = (1, -1)\n\n**Check linear independence:**\nMatrix: [[1, 1], [1, -1]]\ndet = -1 - 1 = -2 ≠ 0\n\n**LICQ holds!** Gradients are linearly independent.\n\n**Consequence:**\nKKT conditions are necessary for optimality at this point.\n\n**Counterexample where LICQ fails:**\n\n**Problem:** min x s.t. x² ≤ 0, -x² ≤ 0\n\nAt x = 0: both constraints active\n∇g₁ = 2x = 0\n∇g₂ = -2x = 0\n\nBoth gradients are zero → LICQ fails!\n\n**Other constraint qualifications:**\n- Slater: ∃x with gᵢ(x) < 0 (strictly feasible)\n- Mangasarian-Fromovitz: weaker than LICQ\n- When CQ holds, KKT conditions are necessary"
  },
  {
    "id": "math404-t6-ex07",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "type": "written",
    "title": "Quadratic Programming with KKT",
    "description": "Solve a convex QP using KKT conditions.",
    "difficulty": 3,
    "hints": [
      "QP: min (1/2)xᵀQx + cᵀx s.t. Ax ≤ b",
      "Form Lagrangian and apply KKT conditions",
      "Use complementary slackness to identify active constraints"
    ],
    "solution": "**Problem:** min (1/2)(x₁² + x₂²) - x₁ - x₂\ns.t. x₁ + x₂ ≤ 1, x₁, x₂ ≥ 0\n\n**Standard form:**\ng₁: x₁ + x₂ - 1 ≤ 0\ng₂: -x₁ ≤ 0\ng₃: -x₂ ≤ 0\n\n**Lagrangian:**\nL = (1/2)(x₁² + x₂²) - x₁ - x₂ + λ₁(x₁ + x₂ - 1) - λ₂x₁ - λ₃x₂\n\n**KKT Stationarity:**\nx₁ - 1 + λ₁ - λ₂ = 0\nx₂ - 1 + λ₁ - λ₃ = 0\n\n**Try: interior solution (all λ = 0)**\nx₁ = x₂ = 1\nCheck: x₁ + x₂ = 2 > 1 (infeasible) ✗\n\n**Try: g₁ active, g₂, g₃ inactive (λ₁ > 0, λ₂ = λ₃ = 0)**\nx₁ - 1 + λ₁ = 0 → x₁ = 1 - λ₁\nx₂ - 1 + λ₁ = 0 → x₂ = 1 - λ₁\nx₁ + x₂ = 1 → 2(1 - λ₁) = 1 → λ₁ = 1/2\n\nx₁ = x₂ = 1/2 > 0 ✓\nλ₁ = 1/2 > 0 ✓\n\n**Solution:** x* = (1/2, 1/2), λ₁* = 1/2\n\n**Objective:** f* = (1/2)(1/2) - 1/2 - 1/2 = 1/4 - 1 = -3/4"
  },
  {
    "id": "math404-t6-ex08",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "type": "written",
    "title": "Penalty Method",
    "description": "Apply the quadratic penalty method to solve a constrained problem.",
    "difficulty": 3,
    "hints": [
      "Penalty function: P_ρ(x) = f(x) + (ρ/2)Σ[gᵢ(x)]₊² + (ρ/2)Σhⱼ(x)²",
      "Minimize for increasing ρ → ∞",
      "Solutions converge to constrained optimum"
    ],
    "solution": "**Problem:** min x² s.t. x = 1\n\n**Penalty function:**\nP_ρ(x) = x² + (ρ/2)(x - 1)²\n\n**Minimizing P_ρ:**\ndP_ρ/dx = 2x + ρ(x - 1) = 0\n2x + ρx - ρ = 0\nx(2 + ρ) = ρ\nx*(ρ) = ρ/(2 + ρ)\n\n**Convergence:**\n| ρ    | x*(ρ)      | Error |\n|------|------------|-------|\n| 1    | 1/3 ≈ 0.33 | 0.67  |\n| 10   | 10/12 ≈ 0.83| 0.17  |\n| 100  | 100/102 ≈ 0.98| 0.02|\n| 1000 | ≈ 0.998    | 0.002 |\n\nAs ρ → ∞: x*(ρ) → 1 ✓\n\n**Disadvantages:**\n1. Need large ρ for accuracy\n2. Large ρ → ill-conditioning\n3. Hessian of penalty: 2 + ρ → ∞\n\n**For inequality constraints:**\nP_ρ(x) = f(x) + (ρ/2)Σ[max(0, gᵢ(x))]²\n\nExample: min x s.t. x ≥ 1\nP_ρ(x) = x + (ρ/2)[max(0, 1-x)]²"
  },
  {
    "id": "math404-t6-ex09",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "type": "written",
    "title": "Barrier Method",
    "description": "Apply the log-barrier method to solve an inequality-constrained problem.",
    "difficulty": 4,
    "hints": [
      "Barrier function: φ(x) = -Σlog(-gᵢ(x))",
      "Solve min f(x) + (1/t)φ(x) for increasing t",
      "As t → ∞, solutions approach constrained optimum"
    ],
    "solution": "**Problem:** min x s.t. x ≥ 1\n\nRewrite: g(x) = 1 - x ≤ 0\n\n**Log-barrier:**\nφ(x) = -log(-(1-x)) = -log(x-1)\n\n**Barrier problem:**\nB_t(x) = x - (1/t)log(x-1)\n\n**Minimizing B_t:**\ndB_t/dx = 1 - 1/(t(x-1)) = 0\nt(x-1) = 1\nx*(t) = 1 + 1/t\n\n**Central path:**\n| t    | x*(t)  | Duality gap |\n|------|--------|-------------|\n| 1    | 2      | 1/t = 1     |\n| 10   | 1.1    | 0.1         |\n| 100  | 1.01   | 0.01        |\n| 1000 | 1.001  | 0.001       |\n\nAs t → ∞: x*(t) → 1 ✓\n\n**Advantages over penalty:**\n1. Stays strictly feasible\n2. Newton's method works well\n3. Polynomial-time complexity\n\n**For multiple constraints:**\nB_t(x) = f(x) - (1/t)Σlog(-gᵢ(x))\n\n**Duality gap:**\nGap ≈ m/t where m = number of constraints\nFor ε-accuracy: t = m/ε"
  },
  {
    "id": "math404-t6-ex10",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "type": "written",
    "title": "Augmented Lagrangian Method",
    "description": "Apply the augmented Lagrangian method to solve a constrained problem.",
    "difficulty": 4,
    "hints": [
      "Combines Lagrangian with penalty term",
      "L_ρ(x,λ) = f(x) + λᵀh(x) + (ρ/2)||h(x)||²",
      "Update multipliers: λ ← λ + ρh(x)"
    ],
    "solution": "**Problem:** min x² s.t. x = 1\n\n**Augmented Lagrangian:**\nL_ρ(x, λ) = x² + λ(x - 1) + (ρ/2)(x - 1)²\n\n**Algorithm:**\n1. Fix λ, minimize L_ρ over x\n2. Update λ ← λ + ρ(x - 1)\n3. Optionally increase ρ\n\n**Iteration 1:** λ₀ = 0, ρ = 1\n∂L_ρ/∂x = 2x + λ + ρ(x-1) = 2x + 0 + (x-1) = 3x - 1 = 0\nx₁ = 1/3\n\nλ₁ = λ₀ + ρ(x₁ - 1) = 0 + 1(1/3 - 1) = -2/3\n\n**Iteration 2:** λ₁ = -2/3, ρ = 1\n2x - 2/3 + (x-1) = 3x - 5/3 = 0\nx₂ = 5/9 ≈ 0.56\n\nλ₂ = -2/3 + 1(5/9 - 1) = -2/3 - 4/9 = -10/9\n\n**Iteration 3:**\n2x - 10/9 + (x-1) = 3x - 19/9 = 0\nx₃ = 19/27 ≈ 0.70\n\n**Pattern:** Converging to x* = 1, λ* = -2\n\n**Verification:** At optimum:\n∂L/∂x = 2x + λ = 2(1) + (-2) = 0 ✓\n\n**Advantages:**\n- Bounded penalty parameter ρ\n- Faster convergence than pure penalty\n- Less ill-conditioning"
  },
  {
    "id": "math404-t6-ex11",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "type": "written",
    "title": "Primal-Dual Interior Point Method",
    "description": "Describe the primal-dual interior point algorithm for QP.",
    "difficulty": 5,
    "hints": [
      "Solve perturbed KKT system",
      "Replace complementarity λᵢsᵢ = 0 with λᵢsᵢ = μ",
      "Use Newton to solve system, decrease μ"
    ],
    "solution": "**Primal QP:**\nmin (1/2)xᵀQx + cᵀx\ns.t. Ax ≤ b\n\n**KKT System:**\nQx + c + Aᵀλ = 0  (stationarity)\nAx + s = b        (primal feasibility)\nλᵢsᵢ = 0         (complementarity)\nλ, s ≥ 0         (dual feasibility)\n\n**Perturbed KKT (barrier parameter μ):**\nReplace λᵢsᵢ = 0 with λᵢsᵢ = μ\n\n**Newton System:**\n[Q    0   Aᵀ] [Δx]   [-(Qx + c + Aᵀλ)]\n[A    I   0 ] [Δs] = [-(Ax + s - b)   ]\n[0    Λ   S ] [Δλ]   [μe - ΛSe        ]\n\nwhere Λ = diag(λ), S = diag(s), e = (1,...,1)ᵀ\n\n**Algorithm:**\n1. Initialize x, s, λ > 0 (strictly feasible)\n2. While μ > ε:\n   a. Solve Newton system for (Δx, Δs, Δλ)\n   b. Line search to maintain s, λ > 0\n   c. Update: (x,s,λ) ← (x,s,λ) + α(Δx,Δs,Δλ)\n   d. Reduce μ ← σμ (e.g., σ = 0.1)\n\n**Complexity:**\nO(√m log(1/ε)) Newton iterations\nEach iteration: O(n³) for dense, much less for sparse\n\n**Advantages:**\n1. Polynomial worst-case complexity\n2. Very efficient in practice\n3. Warm-starting possible\n\n**Software:** MOSEK, Gurobi, CVXPY all use interior point"
  },
  {
    "id": "math404-t6-ex12",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "type": "written",
    "title": "Duality Gap and Optimality",
    "description": "Compute the duality gap and use it as an optimality certificate.",
    "difficulty": 4,
    "hints": [
      "Duality gap = primal objective - dual objective",
      "At optimality, gap = 0 (strong duality)",
      "Gap provides certificate: both within gap of optimal"
    ],
    "solution": "**Primal:** min cᵀx s.t. Ax ≤ b, x ≥ 0\n**Dual:** max bᵀy s.t. Aᵀy ≤ c, y ≥ 0\n\n**Given feasible solutions:**\nPrimal: x = (1, 2) with cᵀx = 5\nDual: y = (1, 0.5) with bᵀy = 4\n\n**Duality gap:** 5 - 4 = 1\n\n**Interpretation:**\n- Primal objective: 5 (upper bound on p*)\n- Dual objective: 4 (lower bound on p*)\n- True optimum: 4 ≤ p* ≤ 5\n\n**For ε-optimality:**\nIf gap < ε, both solutions within ε of optimal.\n\n**Interior Point Gap:**\nAt barrier parameter t with m constraints:\nGap ≈ m/t\n\nExample: m = 100 constraints, t = 10⁶\nGap ≈ 100/10⁶ = 10⁻⁴\n\n**Stopping criterion:**\nStop when gap < ε·(1 + |p|) for relative tolerance ε\n\n**Certificate of optimality:**\n(x*, y*) with:\n- Ax* ≤ b, x* ≥ 0 (primal feasible)\n- Aᵀy* ≤ c, y* ≥ 0 (dual feasible)\n- cᵀx* = bᵀy* (zero gap)\n\nThis certifies x* is optimal!"
  },
  {
    "id": "math404-t6-ex13",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "type": "written",
    "title": "Sequential Quadratic Programming",
    "description": "Perform one iteration of SQP for a nonlinear constrained problem.",
    "difficulty": 5,
    "hints": [
      "Approximate problem with QP at current point",
      "Solve QP to get step direction",
      "Use line search for step size"
    ],
    "solution": "**Problem:** min f(x) s.t. h(x) = 0\n\n**SQP subproblem at x_k:**\nmin ∇f(x_k)ᵀd + (1/2)dᵀB_kd\ns.t. h(x_k) + ∇h(x_k)ᵀd = 0\n\nwhere B_k ≈ ∇²L(x_k, λ_k) (Hessian of Lagrangian)\n\n**Example:**\nf(x) = x₁² + x₂²\nh(x) = x₁ + x₂ - 1 = 0\n\nAt x₀ = (0, 0):\n∇f = (0, 0)\n∇h = (1, 1)\nh(x₀) = -1\nB₀ = I (initial approximation)\n\n**QP subproblem:**\nmin (1/2)(d₁² + d₂²)\ns.t. -1 + d₁ + d₂ = 0\n\n**Lagrangian of QP:**\nL = (1/2)(d₁² + d₂²) + μ(d₁ + d₂ - 1)\n\n**KKT:**\nd₁ + μ = 0 → d₁ = -μ\nd₂ + μ = 0 → d₂ = -μ\nd₁ + d₂ = 1 → -2μ = 1 → μ = -1/2\n\n**Solution:** d = (1/2, 1/2)\n\n**Update:** x₁ = x₀ + d = (1/2, 1/2)\n\n**Check:** This is the optimal solution!\n\n**General SQP iteration:**\n1. Solve QP for d_k\n2. Line search: x_{k+1} = x_k + α_k d_k\n3. Update B_k using BFGS on Lagrangian\n4. Update multiplier estimates"
  },
  {
    "id": "math404-t6-ex14",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "type": "written",
    "title": "Active Set Method",
    "description": "Solve a QP using the active set method.",
    "difficulty": 4,
    "hints": [
      "Maintain a working set of constraints treated as equalities",
      "Solve equality-constrained problem at each iteration",
      "Add/remove constraints based on multipliers and feasibility"
    ],
    "solution": "**Problem:** min (1/2)(x₁² + x₂²) - 2x₁ - x₂\ns.t. x₁ + x₂ ≤ 2\n     x₁ ≥ 0\n     x₂ ≥ 0\n\n**Iteration 1:** Start at x₀ = (0, 0), Working set W = {x₁ = 0, x₂ = 0}\n\nSolve with equality: x₁ = 0, x₂ = 0\nAlready at this point.\n\nCheck multipliers from ∇L = 0:\n∇f = (x₁ - 2, x₂ - 1) = (-2, -1)\nNeed -2 - λ₂ = 0 → λ₂ = -2 < 0 (remove this constraint)\n\n**Iteration 2:** W = {x₂ = 0}\nSolve: min (1/2)(x₁² + x₂²) - 2x₁ - x₂ s.t. x₂ = 0\n\n∂/∂x₁ = x₁ - 2 = 0 → x₁ = 2\nCheck: x₁ + x₂ = 2 ≤ 2 ✓, x₁ = 2 ≥ 0 ✓\n\nMultiplier for x₂ = 0:\n∇f = (0, -1) at (2, 0)\nNeed -1 - λ₃ = 0 → λ₃ = -1 < 0 (remove this constraint)\n\n**Iteration 3:** W = {} (no active constraints)\nSolve unconstrained: x₁ = 2, x₂ = 1\nCheck: x₁ + x₂ = 3 > 2 ✗ (infeasible)\n\nAdd constraint x₁ + x₂ = 2 to working set.\n\n**Iteration 4:** W = {x₁ + x₂ = 2}\nSolve: min f s.t. x₁ + x₂ = 2\n\nKKT: x₁ - 2 + λ₁ = 0, x₂ - 1 + λ₁ = 0, x₁ + x₂ = 2\nFrom first two: x₁ = 2 - λ₁, x₂ = 1 - λ₁\nSubstitute: 3 - 2λ₁ = 2 → λ₁ = 1/2\n\nx* = (3/2, 1/2), λ₁* = 1/2 > 0 ✓\n\n**Optimal!** f* = (9/8 + 1/8) - 3 - 1/2 = 5/4 - 7/2 = -9/4"
  },
  {
    "id": "math404-t6-ex15",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "type": "written",
    "title": "Second-Order Sufficient Conditions",
    "description": "Verify second-order sufficient conditions for a constrained local minimum.",
    "difficulty": 5,
    "hints": [
      "SOSC: KKT holds + positive curvature on tangent space",
      "Tangent space = {d : ∇gᵢ(x)ᵀd = 0 for active i}",
      "Check dᵀ∇²L(x)d > 0 for all d in tangent space"
    ],
    "solution": "**Problem:** min f(x) = x₁² + x₂² - x₁x₂\ns.t. g(x) = x₁ + x₂ - 1 ≤ 0\n\n**Candidate:** x* = (1/2, 1/2), λ* = 1\n\n**Verify KKT:**\n∇f = (2x₁ - x₂, 2x₂ - x₁) = (1/2, 1/2)\n∇g = (1, 1)\n∇f + λ∇g = (1/2, 1/2) + 1(1, 1) = (3/2, 3/2) ≠ 0 ✗\n\nLet me recalculate...\n\nActually, for stationarity: ∇f + λ∇g = 0\n(2x₁ - x₂ + λ, 2x₂ - x₁ + λ) = 0\n\nAt x = (1/2, 1/2):\n2(1/2) - 1/2 + λ = 1/2 + λ = 0 → λ = -1/2\n\nBut λ must be ≥ 0 for inequality constraint!\n\n**Revised problem:** min f s.t. g ≥ 0 (or constraint is equality)\n\nFor equality h(x) = x₁ + x₂ - 1 = 0:\n∇f = (1/2, 1/2), ∇h = (1, 1)\n(1/2, 1/2) + μ(1, 1) = 0 → μ = -1/2 (OK for equality)\n\n**Second-Order Check:**\nHessian of Lagrangian:\n∇²L = ∇²f = [[2, -1], [-1, 2]]\n\nTangent space: {d : ∇hᵀd = 0} = {d : d₁ + d₂ = 0}\nTake d = (1, -1):\ndᵀ∇²Ld = [1,-1][[2,-1],[-1,2]][1,-1]ᵀ\n       = [1,-1][3,-3]ᵀ = 3 + 3 = 6 > 0 ✓\n\n**SOSC verified:** x* = (1/2, 1/2) is a strict local minimum"
  },
  {
    "id": "math404-t6-ex16",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "type": "written",
    "title": "Sensitivity Analysis",
    "description": "Analyze how the optimal value changes with respect to constraint perturbations.",
    "difficulty": 4,
    "hints": [
      "Optimal Lagrange multiplier = sensitivity to constraint",
      "For max problem: λ = ∂p*/∂b (marginal value)",
      "Valid for small perturbations"
    ],
    "solution": "**Problem:** min f(x) s.t. h(x) = b\n\n**Sensitivity Theorem:**\nIf (x*, λ*) solves the problem at b*, then:\n∂p*(b)/∂bᵢ = -λᵢ*\n\nwhere p*(b) is the optimal value as function of b.\n\n**Example:**\nmin x² + y² s.t. x + y = b\n\n**Solve:**\nL = x² + y² + μ(x + y - b)\n∇L = (2x + μ, 2y + μ) = 0 → x = y = -μ/2\nx + y = b → -μ = b → μ = -b\n\nx* = y* = b/2\np*(b) = (b/2)² + (b/2)² = b²/2\nλ* = μ = -b\n\n**Verify sensitivity:**\ndp*/db = b = -λ* ✓\n\n**Interpretation:**\n- λ* = -b is the cost of tightening constraint\n- If b = 1, λ* = -1: increasing b by Δ decreases optimal cost by Δ\n\n**For inequalities:**\nmin f(x) s.t. g(x) ≤ b\n\nIf constraint active: ∂p*/∂b = -λ* (shadow price)\nIf constraint inactive: ∂p*/∂b = 0 (no sensitivity)\n\n**Example application:**\nResource allocation: λ* = value of additional resource\nIf λ* = 5, should pay up to $5 for one more unit."
  },
  {
    "id": "math404-t7-ex01",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "type": "written",
    "title": "Second-Order Cone Constraint",
    "description": "Formulate a constraint as a second-order cone constraint and identify its structure.",
    "difficulty": 2,
    "hints": [
      "SOCP form: ||Ax + b||₂ ≤ cᵀx + d",
      "Can represent various nonlinear constraints",
      "Lorentz cone: K = {(t, x) : ||x||₂ ≤ t}"
    ],
    "solution": "**Second-Order Cone (Lorentz cone):**\nK = {(t, x) ∈ ℝ × ℝⁿ : ||x||₂ ≤ t}\n\n**Example 1: Norm constraint**\n||x|| ≤ 3\nSOCP form: (3, x) ∈ K\n\n**Example 2: Quadratic inequality**\nx² + y² ≤ z\nRewrite: ||(x, y)||₂ ≤ √z\nSOCP: Requires z ≥ 0, then (√z, x, y) ∈ K\nOr use rotated cone: x² + y² ≤ z·1\n\n**Example 3: Robust linear constraint**\n(a + u)ᵀx ≤ b for all ||u|| ≤ 1\nWorst case: aᵀx + ||x|| ≤ b\nSOCP: (b - aᵀx, x) ∈ K\n\n**Example 4: Euclidean distance**\n||x - c|| ≤ r\nSOCP: (r, x - c) ∈ K\n\n**Standard SOCP form:**\nmin cᵀx\ns.t. ||Aᵢx + bᵢ||₂ ≤ dᵢᵀx + eᵢ for i = 1,...,m\n\n**Complexity:** O(n² × m) per iteration with interior point methods"
  },
  {
    "id": "math404-t7-ex02",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "type": "written",
    "title": "Semidefinite Programming Formulation",
    "description": "Formulate a semidefinite programming (SDP) problem for matrix optimization.",
    "difficulty": 3,
    "hints": [
      "SDP minimizes a linear function over positive semidefinite matrices",
      "Write X ⪰ 0 to denote X is positive semidefinite",
      "Linear constraints use trace: tr(AᵢX) = bᵢ"
    ],
    "solution": "**SDP Standard Form:**\nmin tr(CX)\ns.t. tr(AᵢX) = bᵢ for i = 1,...,m\n     X ⪰ 0\n\nwhere X ⪰ 0 means X is positive semidefinite (all eigenvalues ≥ 0).\n\n**Example 1: Maximum eigenvalue**\nMinimize the largest eigenvalue of matrix A:\nmin t\ns.t. A ⪯ tI\n     (equivalently: tI - A ⪰ 0)\n\n**Example 2: Matrix completion**\nFind PSD matrix X matching known entries:\nmin ||X||_* (nuclear norm)\ns.t. Xᵢⱼ = Mᵢⱼ for (i,j) ∈ Ω\n     X ⪰ 0\n\n**Example 3: Minimum trace**\nmin tr(X)\ns.t. Xᵢⱼ = 1 for all i,j\n     X ⪰ 0\n\nSolution: X = eeᵀ where e = (1,...,1)ᵀ\n\n**Dual SDP:**\nmax bᵀy\ns.t. ΣyᵢAᵢ + S = C\n     S ⪰ 0\n\n**Strong duality holds when Slater's condition satisfied (strictly feasible point exists)."
  },
  {
    "id": "math404-t7-ex03",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "type": "written",
    "title": "MAX-CUT SDP Relaxation",
    "description": "Derive the SDP relaxation for the MAX-CUT problem and explain the Goemans-Williamson approximation.",
    "difficulty": 4,
    "hints": [
      "Start with labels xᵢ ∈ {-1, +1}",
      "Use X = xxᵀ, then relax to X ⪰ 0",
      "Diagonal entries Xᵢᵢ = 1 since xᵢ² = 1"
    ],
    "solution": "**MAX-CUT Integer Programming:**\nmax (1/4) Σ_{(i,j)∈E} wᵢⱼ(1 - xᵢxⱼ)\ns.t. xᵢ ∈ {-1, +1}\n\n**SDP Relaxation:**\nLet Xᵢⱼ = xᵢxⱼ. Note X = xxᵀ ⪰ 0 and Xᵢᵢ = 1.\n\nmax (1/4) Σ wᵢⱼ(1 - Xᵢⱼ)\ns.t. Xᵢᵢ = 1 for all i\n     X ⪰ 0\n\n**Matrix form:** max (1/4) tr(W(J - X))\nwhere J is the all-ones matrix.\n\n**Goemans-Williamson Rounding:**\n1. Solve SDP to get X*\n2. Decompose X* = VVᵀ (Cholesky-like)\n3. Choose random hyperplane r uniformly on unit sphere\n4. Set xᵢ = sign(vᵢᵀr)\n\n**Approximation guarantee:**\nE[cut value] ≥ α·OPT_SDP ≥ α·OPT_IP\n\nwhere α = min_{θ∈[0,π]} (2/π)θ/(1-cos(θ)) ≈ 0.878\n\n**Example:**\nGraph: Triangle with edge weights = 1\nOPT = 2 (cut one vertex from other two)\nSDP = 2 (exact in this case)\n\n**This 0.878 ratio is optimal under the Unique Games Conjecture!**"
  },
  {
    "id": "math404-t7-ex04",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "type": "written",
    "title": "Robust Linear Programming",
    "description": "Formulate a robust LP where constraint coefficients have ellipsoidal uncertainty.",
    "difficulty": 4,
    "hints": [
      "Uncertain constraint: (a + u)ᵀx ≤ b for all u ∈ U",
      "For ellipsoidal U = {u: ||u|| ≤ ρ}, find the worst case",
      "max_{||u||≤ρ} uᵀx = ρ||x||₂"
    ],
    "solution": "**Nominal LP:** min cᵀx, s.t. aᵀx ≤ b\n\n**Uncertain constraint:** (a + u)ᵀx ≤ b for all u ∈ U\nwhere U = {u : ||u||₂ ≤ ρ} (ellipsoidal uncertainty)\n\n**Robust counterpart:**\nThe constraint must hold for worst-case u:\naᵀx + max_{||u||≤ρ} uᵀx ≤ b\n\nSince max_{||u||≤ρ} uᵀx = ρ||x||₂ (achieved at u = ρx/||x||₂):\n\naᵀx + ρ||x||₂ ≤ b\n\n**Final robust LP:**\nmin cᵀx\ns.t. aᵀx + ρ||x||₂ ≤ b\n\n**SOCP formulation:**\nmin cᵀx\ns.t. (b - aᵀx, ρx) ∈ SOC\n\n**Example:**\nNominal: min x + y, s.t. x + y ≤ 2\nUncertainty: coefficients in ±0.1\n\nRobust: min x + y, s.t. x + y + 0.1||(x,y)|| ≤ 2\n\n**Trade-off:**\n- Larger ρ → more protection → worse objective\n- Smaller ρ → less protection → risk of infeasibility\n\n**Price of robustness:**\nRobust_obj - Nominal_obj = cost of hedging against uncertainty"
  },
  {
    "id": "math404-t7-ex05",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "type": "written",
    "title": "Integer Programming Formulation",
    "description": "Formulate a combinatorial optimization problem as an integer program.",
    "difficulty": 2,
    "hints": [
      "Use binary variables xᵢ ∈ {0,1} for selection decisions",
      "Model logical constraints with linear inequalities",
      "Common: xᵢ ≤ xⱼ (implication), xᵢ + xⱼ ≤ 1 (exclusive)"
    ],
    "solution": "**Knapsack Problem:**\nItems with values vᵢ and weights wᵢ, capacity W.\nSelect items to maximize total value within capacity.\n\n**IP Formulation:**\nmax Σᵢ vᵢxᵢ\ns.t. Σᵢ wᵢxᵢ ≤ W\n     xᵢ ∈ {0, 1}\n\n**Set Cover Problem:**\nSets S₁,...,Sₘ with costs cⱼ, cover universe U.\nFind minimum cost collection covering all elements.\n\n**IP Formulation:**\nmin Σⱼ cⱼxⱼ\ns.t. Σⱼ:i∈Sⱼ xⱼ ≥ 1 for all i ∈ U\n     xⱼ ∈ {0, 1}\n\n**Facility Location:**\nFacilities at locations j with costs fⱼ, clients i with demands dᵢ.\nCost cᵢⱼ to serve client i from facility j.\n\n**IP Formulation:**\nmin Σⱼ fⱼyⱼ + Σᵢ,ⱼ cᵢⱼxᵢⱼ\ns.t. Σⱼ xᵢⱼ = 1 for all clients i\n     xᵢⱼ ≤ yⱼ for all i,j\n     xᵢⱼ, yⱼ ∈ {0, 1}\n\n**Logical constraints as linear inequalities:**\n- AND: z = x₁ ∧ x₂ → z ≤ x₁, z ≤ x₂, z ≥ x₁ + x₂ - 1\n- OR: z = x₁ ∨ x₂ → z ≥ x₁, z ≥ x₂, z ≤ x₁ + x₂\n- Implication: x₁ → x₂ → x₁ ≤ x₂"
  },
  {
    "id": "math404-t7-ex06",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "type": "written",
    "title": "LP Relaxation and Integrality Gap",
    "description": "Compute the LP relaxation of an integer program and analyze the integrality gap.",
    "difficulty": 3,
    "hints": [
      "LP relaxation: replace xᵢ ∈ {0,1} with 0 ≤ xᵢ ≤ 1",
      "Integrality gap = OPT_IP / OPT_LP (for max) or vice versa",
      "Smaller gap means LP relaxation is tighter"
    ],
    "solution": "**Vertex Cover IP:**\nGiven graph G = (V, E), find minimum vertex set covering all edges.\n\nmin Σᵥ xᵥ\ns.t. xᵤ + xᵥ ≥ 1 for all (u,v) ∈ E\n     xᵥ ∈ {0, 1}\n\n**LP Relaxation:**\nmin Σᵥ xᵥ\ns.t. xᵤ + xᵥ ≥ 1 for all (u,v) ∈ E\n     0 ≤ xᵥ ≤ 1\n\n**Example: Triangle graph (K₃)**\nLP optimal: xᵥ = 1/2 for all v, value = 3/2\nIP optimal: select any 2 vertices, value = 2\n\n**Integrality gap:** 2 / (3/2) = 4/3\n\n**General bounds:**\nFor vertex cover: gap ≤ 2\n(Rounding x̂ᵥ = 1 if xᵥ ≥ 1/2 gives 2-approximation)\n\n**Tight example for 2-gap:**\nComplete bipartite graph Kₙ,ₙ:\n- LP optimal: all vertices = 1/2, value = n\n- IP optimal: one side, value = n\n- Gap = 1 (no gap for bipartite!)\n\nFor general graphs, gap → 2 as n → ∞.\n\n**Strengthening LP relaxations:**\n1. Add valid inequalities (cuts)\n2. Lift-and-project\n3. Semidefinite relaxation"
  },
  {
    "id": "math404-t7-ex07",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "type": "written",
    "title": "Branch and Bound",
    "description": "Apply the branch and bound algorithm to solve a small integer program.",
    "difficulty": 3,
    "hints": [
      "Solve LP relaxation at each node",
      "Branch on fractional variable",
      "Prune nodes with bound worse than incumbent"
    ],
    "solution": "**Problem:**\nmax 5x₁ + 4x₂\ns.t. x₁ + x₂ ≤ 5\n     10x₁ + 6x₂ ≤ 45\n     x₁, x₂ ∈ {0,1,2,3,...} (integer)\n\n**Node 0 (Root):** Solve LP relaxation\nLP optimal: x₁ = 3.75, x₂ = 1.25, z = 23.75\nFractional, so branch on x₁.\n\n**Node 1:** x₁ ≤ 3\nLP: x₁ = 3, x₂ = 2, z = 23\nInteger! Update incumbent: z* = 23, x* = (3, 2)\n\n**Node 2:** x₁ ≥ 4\nLP: x₁ = 4, x₂ = 0.83, z = 23.33\nz = 23.33 > z* = 23, but fractional.\nBranch on x₂.\n\n**Node 3:** x₁ ≥ 4, x₂ ≤ 0\nLP: x₁ = 4.5, x₂ = 0, z = 22.5\nz = 22.5 < z* = 23 → PRUNE\n\n**Node 4:** x₁ ≥ 4, x₂ ≥ 1\nLP: x₁ = 3.9, x₂ = 1, z = 23.5\nConstraint x₁ ≥ 4 violated → infeasible → PRUNE\n\n**Optimal:** x* = (3, 2), z* = 23\n\n**Key insights:**\n1. LP relaxation provides upper bound (for max)\n2. Integer solutions provide lower bound\n3. Prune when bound ≤ incumbent\n4. Choice of branching variable affects efficiency"
  },
  {
    "id": "math404-t7-ex08",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "type": "written",
    "title": "Cutting Planes",
    "description": "Derive a Gomory cutting plane from an optimal LP tableau.",
    "difficulty": 4,
    "hints": [
      "Gomory cut from row with fractional basic variable",
      "Use fractional parts of coefficients",
      "Cut eliminates current LP solution but preserves integer solutions"
    ],
    "solution": "**Optimal LP Tableau:**\n| BV | x₁ | x₂ | s₁ | s₂ | RHS  |\n|----|----|----|----|----|------|\n| x₁ | 1  | 0  | 2/3| -1/3| 7/3 |\n| x₂ | 0  | 1  |-1/3| 2/3 | 4/3 |\n\nCurrent LP optimal: x₁ = 7/3, x₂ = 4/3 (fractional)\n\n**Gomory Cut from Row 1:**\nx₁ + (2/3)s₁ + (-1/3)s₂ = 7/3\n\n**Fractional parts:**\nLet f(a) = a - ⌊a⌋\n\nf(7/3) = 1/3\nf(2/3) = 2/3\nf(-1/3) = 2/3 (since -1/3 - (-1) = 2/3)\n\n**Gomory cut:**\nf(2/3)s₁ + f(2/3)s₂ ≥ f(7/3)\n(2/3)s₁ + (2/3)s₂ ≥ 1/3\n\nMultiply by 3:\n2s₁ + 2s₂ ≥ 1\n\n**Add to tableau:**\ns₁ = 5 - x₁ - x₂\ns₂ = 45 - 10x₁ - 6x₂\n\n2(5 - x₁ - x₂) + 2(45 - 10x₁ - 6x₂) ≥ 1\n10 - 2x₁ - 2x₂ + 90 - 20x₁ - 12x₂ ≥ 1\n100 - 22x₁ - 14x₂ ≥ 1\n22x₁ + 14x₂ ≤ 99\n\n**This cut eliminates (7/3, 4/3) but keeps all integer solutions!**\n\n**Properties:**\n- Gomory cuts finite convergence for pure IP\n- Practical: combine with branch and bound (branch and cut)"
  },
  {
    "id": "math404-t7-ex09",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "type": "written",
    "title": "Geometric Programming",
    "description": "Formulate and solve a geometric programming problem.",
    "difficulty": 4,
    "hints": [
      "GP: minimize posynomial subject to posynomial ≤ 1",
      "Posynomial: sum of monomials with positive coefficients",
      "Transform to convex problem via log substitution"
    ],
    "solution": "**Geometric Programming Standard Form:**\nmin f₀(x)\ns.t. fᵢ(x) ≤ 1, i = 1,...,m\n     gⱼ(x) = 1, j = 1,...,p\n\nwhere fᵢ are posynomials (sum of monomials).\n\n**Monomial:** cxₐ₁¹...xₐₙⁿ with c > 0\n**Posynomial:** Σₖ cₖ xₐₖ¹¹...xₐₖⁿⁿ\n\n**Example: Box design**\nMaximize volume V = xyz subject to:\n- Surface area: 2(xy + yz + xz) ≤ A\n- Aspect ratio: x/y ≤ 2, y/x ≤ 2\n\n**GP formulation (minimize 1/V):**\nmin x⁻¹y⁻¹z⁻¹\ns.t. (2/A)(xy + yz + xz) ≤ 1\n     (1/2)xy⁻¹ ≤ 1\n     (1/2)x⁻¹y ≤ 1\n\n**Convex transformation:**\nLet yᵢ = log(xᵢ). Then xᵢ = e^{yᵢ}.\n\nMonomial: c·e^{a₁y₁+...+aₙyₙ} (log-sum-exp)\nlog of posynomial: log(Σₖ e^{aₖᵀy + bₖ}) (convex!)\n\n**Transformed problem:**\nmin -y₁ - y₂ - y₃\ns.t. log(e^{y₁+y₂} + e^{y₂+y₃} + e^{y₁+y₃}) ≤ log(A/2)\n     y₁ - y₂ ≤ log(2)\n     y₂ - y₁ ≤ log(2)\n\nThis is a convex optimization problem!\n\n**Solution:** For A = 6, optimal is x = y = z = 1 (cube), V = 1"
  },
  {
    "id": "math404-t7-ex10",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "type": "written",
    "title": "Conic Programming Hierarchy",
    "description": "Explain the hierarchy of conic programs: LP, SOCP, SDP.",
    "difficulty": 3,
    "hints": [
      "LP ⊂ SOCP ⊂ SDP in terms of problem classes",
      "Each cone generalizes the previous",
      "Complexity increases but more problems can be modeled"
    ],
    "solution": "**Cone Hierarchy:**\n\n**1. Linear Programming (LP)**\nCone: ℝⁿ₊ = {x : xᵢ ≥ 0}\nForm: min cᵀx s.t. Ax = b, x ≥ 0\n\n**2. Second-Order Cone Programming (SOCP)**\nCone: Kₛₒc = {(t,x) : ||x||₂ ≤ t}\nForm: min cᵀx s.t. ||Aᵢx + bᵢ|| ≤ dᵢᵀx + eᵢ\n\n**3. Semidefinite Programming (SDP)**\nCone: S₊ⁿ = {X : X ⪰ 0} (PSD matrices)\nForm: min tr(CX) s.t. tr(AᵢX) = bᵢ, X ⪰ 0\n\n**Inclusion relationships:**\n\n**LP as SOCP:**\nx ≥ 0 ↔ (x, 0) ∈ Kₛₒc\n\n**SOCP as SDP:**\n||x||₂ ≤ t ↔ [[tI, x], [xᵀ, t]] ⪰ 0 (Schur complement)\n\n**Complexity per interior-point iteration:**\n| Problem | Variables | Per-iteration |\n|---------|-----------|---------------|\n| LP      | n         | O(n²)         |\n| SOCP    | n         | O(n²)         |\n| SDP     | n×n       | O(n⁶)         |\n\n**Iteration count:** O(√n log(1/ε)) for all\n\n**Modeling power:**\n- LP: Linear constraints only\n- SOCP: Norm constraints, rotated cones\n- SDP: Eigenvalue constraints, matrix inequalities\n\n**Example progression:**\nPortfolio optimization:\n- LP: Linear utility, no risk constraint\n- SOCP: Quadratic risk constraint ||Σ^½w|| ≤ σ\n- SDP: More complex risk measures"
  },
  {
    "id": "math404-t7-ex11",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "type": "written",
    "title": "Convex Relaxation of Rank Constraint",
    "description": "Derive the nuclear norm relaxation for low-rank matrix optimization.",
    "difficulty": 5,
    "hints": [
      "Rank constraint is non-convex",
      "Nuclear norm ||X||_* = Σσᵢ(X) is convex envelope of rank",
      "Replace rank(X) ≤ k with ||X||_* ≤ τ"
    ],
    "solution": "**Low-Rank Matrix Optimization:**\nmin ||A(X) - b||²\ns.t. rank(X) ≤ k\n\nThis is NP-hard in general!\n\n**Nuclear Norm Relaxation:**\n||X||_* = Σᵢ σᵢ(X) = tr(√(XᵀX))\n\n**Theorem:** Nuclear norm is the convex envelope of rank over unit spectral norm ball.\n\n**Relaxed problem:**\nmin ||A(X) - b||² + λ||X||_*\n\nor equivalently (for some τ):\nmin ||X||_*\ns.t. ||A(X) - b||² ≤ ε\n\n**SDP formulation of nuclear norm:**\n||X||_* = min{(1/2)(tr(W₁) + tr(W₂)) : [[W₁, X], [Xᵀ, W₂]] ⪰ 0}\n\n**Matrix Completion Example:**\nObserve entries Ωᵢⱼ of rank-k matrix M.\nmin ||X||_*\ns.t. Xᵢⱼ = Mᵢⱼ for (i,j) ∈ Ω\n\n**Recovery guarantee (Candès-Recht):**\nIf M is rank-r and incoherent, with |Ω| ≥ Cn^{1.2}r log n,\nthen X* = M with high probability!\n\n**Applications:**\n1. Netflix recommendation (matrix completion)\n2. Video surveillance (background/foreground separation)\n3. Robust PCA"
  },
  {
    "id": "math404-t7-ex12",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "type": "written",
    "title": "Stochastic Programming",
    "description": "Formulate a two-stage stochastic program with recourse.",
    "difficulty": 4,
    "hints": [
      "First stage: decisions before uncertainty revealed",
      "Second stage: recourse decisions after observing ξ",
      "Objective: minimize first stage + expected second stage cost"
    ],
    "solution": "**Two-Stage Stochastic Program:**\n\n**First stage (here-and-now):**\nChoose x before uncertainty ξ is revealed.\nCost: cᵀx\n\n**Second stage (wait-and-see):**\nObserve ξ, then choose y(ξ).\nCost: Q(x, ξ) = min{q(ξ)ᵀy : W(ξ)y = h(ξ) - T(ξ)x, y ≥ 0}\n\n**Full problem:**\nmin cᵀx + E_ξ[Q(x, ξ)]\ns.t. Ax = b\n     x ≥ 0\n\n**Example: Newsvendor**\nOrder x newspapers (first stage, cost c per paper).\nDemand D is random. Sell at price p, salvage at s < c.\n\n**Second stage:** Given demand D and order x:\nQ(x, D) = -p·min(x, D) - s·max(x-D, 0)\n\n**Full problem:**\nmin cx + E_D[-p·min(x, D) - s·(x-D)⁺]\n\n**Optimal solution:** Order x* where:\nP(D ≥ x*) = (c - s)/(p - s)\n\n**Scenario-based formulation:**\nIf ξ ∈ {ξ₁,...,ξₛ} with probabilities π₁,...,πₛ:\n\nmin cᵀx + Σₛ πₛ qₛᵀyₛ\ns.t. Ax = b\n     Wₛyₛ = hₛ - Tₛx, for s = 1,...,S\n     x, yₛ ≥ 0\n\nThis is a large LP (solvable by decomposition methods)."
  },
  {
    "id": "math404-t7-ex13",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "type": "written",
    "title": "Benders Decomposition",
    "description": "Apply Benders decomposition to solve a structured optimization problem.",
    "difficulty": 5,
    "hints": [
      "Separate problem into master (x) and subproblem (y)",
      "Subproblem provides cuts for master",
      "Iterate until convergence"
    ],
    "solution": "**Original Problem:**\nmin cᵀx + dᵀy\ns.t. Ax = b\n     Bx + Dy = e\n     x, y ≥ 0\n\n**Decomposition:**\nFor fixed x, subproblem in y:\nQ(x) = min{dᵀy : Dy = e - Bx, y ≥ 0}\n\n**Master problem:**\nmin cᵀx + θ\ns.t. Ax = b\n     θ ≥ Q(x)  (implicitly)\n     x ≥ 0\n\n**Benders Cuts:**\nFrom dual of subproblem:\nQ(x) = max{πᵀ(e - Bx) : πᵀD ≤ dᵀ}\n\nExtreme points: π₁,...,πₖ\nExtreme rays: ρ₁,...,ρₘ\n\n**Feasibility cut (if subproblem infeasible):**\nρⱼᵀ(e - Bx) ≤ 0\n\n**Optimality cut (if subproblem bounded):**\nθ ≥ πⱼᵀ(e - Bx)\n\n**Algorithm:**\n1. Initialize master with no cuts\n2. Solve master → (x*, θ*)\n3. Solve subproblem with x = x*\n   - If infeasible: add feasibility cut\n   - If bounded: add optimality cut if πᵀ(e - Bx*) > θ*\n4. If no cut added: STOP (optimal)\n5. Else: go to step 2\n\n**Convergence:** Finite (finite number of extreme points/rays)\n\n**Application:** Two-stage stochastic programming\n- Master: first-stage decisions\n- Subproblems: second-stage for each scenario"
  },
  {
    "id": "math404-t7-ex14",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "type": "written",
    "title": "Sum-of-Squares Relaxation",
    "description": "Use sum-of-squares relaxation to check polynomial non-negativity.",
    "difficulty": 5,
    "hints": [
      "Polynomial p(x) ≥ 0 is hard to verify in general",
      "SOS: p(x) = Σqᵢ(x)² is sufficient for non-negativity",
      "SOS can be checked via SDP"
    ],
    "solution": "**Non-negativity Certification:**\nIs p(x) ≥ 0 for all x?\n\nThis is NP-hard for general polynomials (degree ≥ 4).\n\n**Sum-of-Squares (SOS):**\np(x) is SOS if p(x) = Σⱼ qⱼ(x)² for some polynomials qⱼ.\n\nSOS ⟹ non-negative (converse false in general)\n\n**SDP Formulation:**\np(x) = m(x)ᵀQm(x)\nwhere m(x) = [1, x, x², ..., x^d] (monomial basis)\n\np is SOS ⟺ Q ⪰ 0 (SDP constraint!)\n\n**Example:**\np(x) = x⁴ + 2x² + 1\nm(x) = [1, x, x²]\n\nFind Q ⪰ 0 such that:\nm(x)ᵀQm(x) = q₁₁ + 2q₁₂x + (2q₁₃ + q₂₂)x² + 2q₂₃x³ + q₃₃x⁴\n\nMatching: q₃₃ = 1, 2q₂₃ = 0, 2q₁₃ + q₂₂ = 2, 2q₁₂ = 0, q₁₁ = 1\n\nOne solution: Q = [[1,0,0], [0,2,0], [0,0,1]] ⪰ 0 ✓\n\nSo p(x) = 1 + 2x² + x⁴ = (1)² + (√2 x)² + (x²)²\n\n**Hierarchy of relaxations:**\nAs SOS degree increases, approximates true non-negativity better.\n\n**Applications:**\n- Control: Lyapunov function synthesis\n- Optimization: Global polynomial optimization\n- Robotics: Safety verification"
  },
  {
    "id": "math404-t7-ex15",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "type": "written",
    "title": "Distributed Optimization",
    "description": "Formulate a distributed optimization problem and describe ADMM algorithm.",
    "difficulty": 4,
    "hints": [
      "Decompose problem among agents",
      "Consensus constraint: all local copies agree",
      "ADMM: alternating minimization with dual update"
    ],
    "solution": "**Distributed Problem:**\nmin Σᵢ fᵢ(xᵢ)\ns.t. xᵢ = z for all i (consensus)\n\nEach agent i has local data and computes fᵢ.\n\n**ADMM (Alternating Direction Method of Multipliers):**\n\n**Augmented Lagrangian:**\nL_ρ(x, z, y) = Σᵢ[fᵢ(xᵢ) + yᵢᵀ(xᵢ - z) + (ρ/2)||xᵢ - z||²]\n\n**Algorithm:**\n1. **x-update (parallel):**\n   xᵢ^{k+1} = argmin_xᵢ{fᵢ(xᵢ) + yᵢ^kᵀxᵢ + (ρ/2)||xᵢ - z^k||²}\n\n2. **z-update (aggregation):**\n   z^{k+1} = (1/n)Σᵢ(xᵢ^{k+1} + yᵢ^k/ρ)\n\n3. **Dual update (parallel):**\n   yᵢ^{k+1} = yᵢ^k + ρ(xᵢ^{k+1} - z^{k+1})\n\n**Example: Distributed LASSO**\nmin (1/2)Σᵢ||Aᵢx - bᵢ||² + λ||x||₁\n\nSplit: fᵢ(xᵢ) = (1/2)||Aᵢxᵢ - bᵢ||², g(z) = λ||z||₁\n\n**ADMM updates:**\nxᵢ^{k+1} = (AᵢᵀAᵢ + ρI)⁻¹(Aᵢᵀbᵢ + ρz^k - yᵢ^k)\nz^{k+1} = soft_threshold(x̄^{k+1} + ȳ^k/ρ, λ/(nρ))\nyᵢ^{k+1} = yᵢ^k + ρ(xᵢ^{k+1} - z^{k+1})\n\n**Convergence:** O(1/k) for convex problems\n\n**Communication:** Only z needs to be shared (centralized) or exchanged (decentralized)"
  },
  {
    "id": "math404-t7-ex16",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "type": "written",
    "title": "Multi-Objective Optimization",
    "description": "Solve a bi-objective optimization problem and find the Pareto frontier.",
    "difficulty": 3,
    "hints": [
      "No single solution optimizes all objectives",
      "Pareto optimal: cannot improve one without worsening another",
      "Methods: weighted sum, ε-constraint, goal programming"
    ],
    "solution": "**Bi-Objective Problem:**\nmin (f₁(x), f₂(x))\ns.t. x ∈ X\n\n**Pareto Dominance:**\nx dominates y if:\n- f₁(x) ≤ f₁(y) and f₂(x) ≤ f₂(y)\n- At least one strict inequality\n\n**Pareto Optimal:** Not dominated by any feasible x.\n\n**Example:**\nmin (x², (x-2)²)\ns.t. x ∈ [0, 2]\n\n**Analysis:**\nf₁(x) = x² is minimized at x = 0\nf₂(x) = (x-2)² is minimized at x = 2\n\nFor x ∈ [0, 2], all points are Pareto optimal!\n\n**Pareto frontier:** {(x², (x-2)²) : x ∈ [0, 2]}\n= {(t, (2-√t)²) : t ∈ [0, 4]}\n\n**Method 1: Weighted Sum**\nmin w₁f₁(x) + w₂f₂(x), w₁ + w₂ = 1, w ≥ 0\n\nFor w₁ = w₂ = 0.5:\nmin 0.5x² + 0.5(x-2)² = 0.5(2x² - 4x + 4)\nd/dx = 2x - 2 = 0 → x* = 1\nSolution: (1, 1) on Pareto frontier\n\n**Method 2: ε-Constraint**\nmin f₁(x)\ns.t. f₂(x) ≤ ε\n     x ∈ X\n\nFor ε = 0.5:\nmin x² s.t. (x-2)² ≤ 0.5\nx ∈ [2-√0.5, 2] ≈ [1.29, 2]\nOptimal: x* ≈ 1.29\n\n**Varying ε traces the Pareto frontier!**\n\n**Applications:**\n- Cost vs quality\n- Risk vs return\n- Accuracy vs interpretability"
  }
]