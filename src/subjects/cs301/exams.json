[
  {
    "id": "cs301-exam-midterm",
    "subjectId": "cs301",
    "title": "CS301 Midterm Exam",
    "durationMinutes": 90,
    "instructions": [
      "This exam covers Process Management, Threads and Concurrency, CPU Scheduling, and Synchronization.",
      "Answer all questions.",
      "You may use pseudo-code or C-like syntax for written answers.",
      "Show your work for scheduling calculations."
    ],
    "questions": [
      {
        "id": "mid-q1",
        "type": "multiple_choice",
        "prompt": "Which of the following is NOT a valid process state?",
        "options": [
          "Running",
          "Ready",
          "Waiting",
          "Compiled"
        ],
        "correctAnswer": 3,
        "explanation": "Processes exist in states like New, Ready, Running, Waiting, and Terminated. Compiled is not a process state."
      },
      {
        "id": "mid-q2",
        "type": "true_false",
        "prompt": "A context switch involves saving the state of one process and loading the state of another.",
        "correctAnswer": true,
        "explanation": "Context switching saves the current process state (registers, PC, etc.) in its PCB and loads the saved state of the new process."
      },
      {
        "id": "mid-q3",
        "type": "multiple_choice",
        "prompt": "The Process Control Block (PCB) contains:",
        "options": [
          "Only the process ID",
          "Process state, program counter, CPU registers, memory management info",
          "Only memory addresses",
          "Only scheduling information"
        ],
        "correctAnswer": 1,
        "explanation": "The PCB contains comprehensive information including PID, process state, program counter, CPU registers, memory management info, and scheduling information."
      },
      {
        "id": "mid-q4",
        "type": "fill_blank",
        "prompt": "In shared memory IPC, processes communicate by reading and writing to a common ______ region.",
        "correctAnswer": "memory",
        "explanation": "Shared memory IPC involves creating a memory region that multiple processes can access for communication."
      },
      {
        "id": "mid-q5",
        "type": "written",
        "prompt": "Explain the difference between shared memory and message passing for inter-process communication. What are the advantages of each?",
        "correctAnswer": "shared memory fast synchronization message passing easier distributed",
        "explanation": "Shared memory is faster but requires synchronization. Message passing is easier to use and works across machines.",
        "modelAnswer": "SHARED MEMORY: Processes share a region of memory. Advantages: Very fast communication (no kernel involvement after setup), efficient for large data transfers. Disadvantages: Requires explicit synchronization (semaphores/mutexes), processes must be on same machine. MESSAGE PASSING: Processes communicate by sending/receiving messages through the kernel. Advantages: No synchronization needed (messages are ordered), works across distributed systems, cleaner abstraction. Disadvantages: Slower due to system calls, overhead for small frequent messages."
      },
      {
        "id": "mid-q6",
        "type": "multiple_choice",
        "prompt": "Which of the following is shared between threads of the same process?",
        "options": [
          "Stack",
          "Program counter",
          "Registers",
          "Code and data sections"
        ],
        "correctAnswer": 3,
        "explanation": "Threads share the code, data, and heap sections. Each thread has its own stack, registers, and program counter."
      },
      {
        "id": "mid-q7",
        "type": "multiple_choice",
        "prompt": "In the many-to-one threading model:",
        "options": [
          "Many user threads map to many kernel threads",
          "Many user threads map to one kernel thread",
          "One user thread maps to one kernel thread",
          "One user thread maps to many kernel threads"
        ],
        "correctAnswer": 1,
        "explanation": "Many-to-one maps multiple user threads to a single kernel thread. If one blocks, all block."
      },
      {
        "id": "mid-q8",
        "type": "true_false",
        "prompt": "Creating a new thread is typically faster than creating a new process.",
        "correctAnswer": true,
        "explanation": "Thread creation is faster because threads share the address space and resources of their parent process, requiring less overhead."
      },
      {
        "id": "mid-q9",
        "type": "coding",
        "prompt": "Write pthread code to create a thread that prints \"Hello from thread\" and waits for it to complete.",
        "starterCode": "#include <pthread.h>\n#include <stdio.h>\n\n// Define thread function and main",
        "language": "c",
        "solution": "#include <pthread.h>\n#include <stdio.h>\n\nvoid* thread_func(void* arg) {\n    printf(\"Hello from thread\\n\");\n    return NULL;\n}\n\nint main() {\n    pthread_t tid;\n    pthread_create(&tid, NULL, thread_func, NULL);\n    pthread_join(tid, NULL);\n    return 0;\n}",
        "testCases": [
          {
            "input": "",
            "expectedOutput": "Hello from thread",
            "isHidden": false,
            "description": "Thread prints message"
          }
        ],
        "correctAnswer": true,
        "explanation": "Create a thread function, use pthread_create to spawn the thread, and pthread_join to wait for completion."
      },
      {
        "id": "mid-q10",
        "type": "multiple_choice",
        "prompt": "A thread pool is beneficial because:",
        "options": [
          "It creates threads on demand for each request",
          "It pre-creates threads to handle requests, avoiding creation overhead",
          "It eliminates the need for synchronization",
          "It automatically detects deadlocks"
        ],
        "correctAnswer": 1,
        "explanation": "Thread pools pre-create a set of threads that can be reused, avoiding the overhead of creating and destroying threads for each request."
      },
      {
        "id": "mid-q11",
        "type": "multiple_choice",
        "prompt": "Which scheduling algorithm may cause starvation?",
        "options": [
          "Round Robin",
          "FCFS",
          "Shortest Job First",
          "Random"
        ],
        "correctAnswer": 2,
        "explanation": "SJF may cause starvation of long processes if short processes keep arriving. FCFS and RR do not cause starvation."
      },
      {
        "id": "mid-q12",
        "type": "fill_blank",
        "prompt": "In Round Robin scheduling, each process gets a fixed time ______ before being preempted.",
        "correctAnswer": "quantum",
        "explanation": "The time quantum (or time slice) is the fixed amount of CPU time allocated to each process before the scheduler moves to the next."
      },
      {
        "id": "mid-q13",
        "type": "multiple_choice",
        "prompt": "Given processes P1(burst=8), P2(burst=4), P3(burst=2) arriving at time 0, what is the average waiting time using SJF?",
        "codeSnippet": "Order: P3(2) → P2(4) → P1(8)\nP3 waits: 0\nP2 waits: 2\nP1 waits: 6\nAverage = ?",
        "options": [
          "2.67",
          "4.67",
          "8/3",
          "Both A and C"
        ],
        "correctAnswer": 3,
        "explanation": "Average = (0 + 2 + 6) / 3 = 8/3 ≈ 2.67. Both A and C represent the same value."
      },
      {
        "id": "mid-q14",
        "type": "multiple_choice",
        "prompt": "Preemptive Priority Scheduling differs from non-preemptive in that:",
        "options": [
          "It assigns priorities randomly",
          "A running process can be interrupted if a higher priority process arrives",
          "It never causes context switches",
          "It uses FCFS within priority levels"
        ],
        "correctAnswer": 1,
        "explanation": "In preemptive scheduling, a running process can be interrupted when a higher priority process becomes ready."
      },
      {
        "id": "mid-q15",
        "type": "true_false",
        "prompt": "In a multilevel feedback queue, a process can move between different queues based on its CPU usage behavior.",
        "correctAnswer": true,
        "explanation": "Multilevel feedback queues adjust a process's queue based on its behavior - CPU-bound processes may be demoted while I/O-bound processes may be promoted."
      },
      {
        "id": "mid-q16",
        "type": "written",
        "prompt": "Explain the convoy effect in FCFS scheduling and how it impacts system performance.",
        "correctAnswer": "convoy long short wait CPU bound I/O",
        "explanation": "The convoy effect occurs when short processes wait behind long processes, increasing average waiting time.",
        "modelAnswer": "The convoy effect occurs in FCFS scheduling when a long CPU-bound process holds the CPU while many shorter processes wait in the ready queue. All the short processes are stuck behind the long process like cars in a convoy behind a slow truck. Impact on performance: 1) Average waiting time increases significantly, 2) CPU and I/O device utilization drops because I/O-bound processes must wait for CPU even though they need little CPU time, 3) Overall system throughput decreases. Solution: Use preemptive scheduling like Round Robin or SJF to allow short processes to execute without waiting for long processes to complete."
      },
      {
        "id": "mid-q17",
        "type": "multiple_choice",
        "prompt": "A race condition occurs when:",
        "options": [
          "Processes execute too quickly",
          "Multiple processes access shared data concurrently and the outcome depends on execution order",
          "The CPU cannot keep up with I/O",
          "A process waits indefinitely"
        ],
        "correctAnswer": 1,
        "explanation": "A race condition occurs when the outcome of concurrent execution depends on the particular order of operations, leading to unpredictable results."
      },
      {
        "id": "mid-q18",
        "type": "multiple_choice",
        "prompt": "Which is NOT a requirement for a correct critical section solution?",
        "options": [
          "Mutual exclusion",
          "Progress",
          "Bounded waiting",
          "Simultaneous access"
        ],
        "correctAnswer": 3,
        "explanation": "A critical section solution must provide mutual exclusion (only one process in CS), progress (no deadlock), and bounded waiting (no starvation). Simultaneous access violates mutual exclusion."
      },
      {
        "id": "mid-q19",
        "type": "true_false",
        "prompt": "A mutex (mutual exclusion lock) can be held by multiple threads simultaneously.",
        "correctAnswer": false,
        "explanation": "By definition, a mutex provides mutual exclusion - only one thread can hold the lock at a time."
      },
      {
        "id": "mid-q20",
        "type": "multiple_choice",
        "prompt": "What is the difference between a binary semaphore and a counting semaphore?",
        "options": [
          "Binary semaphores can only be 0 or 1; counting semaphores can be any non-negative integer",
          "They are identical",
          "Counting semaphores are faster",
          "Binary semaphores allow multiple threads in the critical section"
        ],
        "correctAnswer": 0,
        "explanation": "A binary semaphore is restricted to values 0 and 1 (like a mutex), while a counting semaphore can be any non-negative integer, allowing control of access to multiple identical resources."
      },
      {
        "id": "mid-q21",
        "type": "coding",
        "prompt": "Write pseudo-code for the Producer-Consumer problem using semaphores (empty, full, mutex).",
        "starterCode": "// Assume: empty = N, full = 0, mutex = 1\n\nvoid producer() {\n    // Your code\n}\n\nvoid consumer() {\n    // Your code\n}",
        "language": "c",
        "solution": "// Producer\nvoid producer() {\n    while (true) {\n        item = produce_item();\n        wait(empty);      // Wait for empty slot\n        wait(mutex);      // Enter critical section\n        buffer[in] = item;\n        in = (in + 1) % N;\n        signal(mutex);    // Exit critical section\n        signal(full);     // Signal item available\n    }\n}\n\n// Consumer\nvoid consumer() {\n    while (true) {\n        wait(full);       // Wait for item\n        wait(mutex);      // Enter critical section\n        item = buffer[out];\n        out = (out + 1) % N;\n        signal(mutex);    // Exit critical section\n        signal(empty);    // Signal empty slot\n        consume_item(item);\n    }\n}",
        "testCases": [
          {
            "input": "",
            "expectedOutput": "Correct synchronization",
            "isHidden": false,
            "description": "Producer-consumer sync"
          }
        ],
        "correctAnswer": true,
        "explanation": "Use empty semaphore to track empty slots, full to track filled slots, and mutex for mutual exclusion when accessing the buffer."
      },
      {
        "id": "mid-q22",
        "type": "multiple_choice",
        "prompt": "In the Dining Philosophers problem, what causes deadlock?",
        "options": [
          "All philosophers pick up their left fork simultaneously",
          "One philosopher eats too fast",
          "The forks are too small",
          "There are too many philosophers"
        ],
        "correctAnswer": 0,
        "explanation": "If all philosophers simultaneously pick up their left fork and wait for the right fork, no one can eat and deadlock occurs."
      },
      {
        "id": "mid-q23",
        "type": "fill_blank",
        "prompt": "A ______ is a high-level synchronization construct that encapsulates shared data with procedures that operate on that data.",
        "correctAnswer": "monitor",
        "explanation": "Monitors encapsulate shared data with procedures, ensuring only one thread executes a monitor procedure at a time."
      },
      {
        "id": "mid-q24",
        "type": "multiple_choice",
        "prompt": "Which synchronization technique does NOT use blocking?",
        "options": [
          "Mutex locks",
          "Semaphores",
          "Compare-and-swap (CAS)",
          "Condition variables"
        ],
        "correctAnswer": 2,
        "explanation": "Compare-and-swap is a lock-free atomic operation that does not block threads. It uses busy-waiting or retrying instead."
      },
      {
        "id": "mid-q25",
        "type": "multiple_choice",
        "prompt": "What is the turnaround time for P2 using Round Robin (quantum=3) for: P1(burst=5), P2(burst=3), P3(burst=4) all arriving at time 0?",
        "codeSnippet": "Timeline:\n0-3: P1 (2 remaining)\n3-6: P2 (done)\n6-9: P3 (1 remaining)\n9-11: P1 (done)\n11-12: P3 (done)",
        "options": [
          "3",
          "6",
          "9",
          "12"
        ],
        "correctAnswer": 1,
        "explanation": "P2 completes at time 6. Turnaround time = completion - arrival = 6 - 0 = 6."
      },
      {
        "id": "mid-q26",
        "type": "true_false",
        "prompt": "The fork() system call creates a new thread within the same process.",
        "correctAnswer": false,
        "explanation": "fork() creates a new process (child) that is a copy of the parent. Thread creation uses pthread_create() or similar."
      }
    ]
  },
  {
    "id": "cs301-exam-final",
    "subjectId": "cs301",
    "title": "CS301 Final Exam",
    "durationMinutes": 150,
    "instructions": [
      "Comprehensive exam covering all Operating Systems topics.",
      "Topics: Process Management, Threads, Scheduling, Synchronization, Deadlock, Memory Management, Virtual Memory, and File Systems.",
      "Answer all questions. Show work where applicable.",
      "You may use pseudo-code or C-like syntax for algorithm implementations."
    ],
    "questions": [
      {
        "id": "fin-q1",
        "type": "multiple_choice",
        "prompt": "Which process state transition is NOT possible?",
        "options": [
          "Running → Waiting",
          "Ready → Running",
          "Waiting → Running",
          "Running → Ready"
        ],
        "correctAnswer": 2,
        "explanation": "A waiting process must first become ready before it can run. Direct Waiting → Running transition is not allowed."
      },
      {
        "id": "fin-q2",
        "type": "multiple_choice",
        "prompt": "The advantage of user-level threads over kernel-level threads is:",
        "options": [
          "They can take advantage of multiple CPUs",
          "Thread operations are faster because they don't require kernel intervention",
          "They handle blocking system calls better",
          "They provide better security"
        ],
        "correctAnswer": 1,
        "explanation": "User-level thread operations (create, switch) are faster because they don't involve system calls. However, they can't utilize multiple CPUs or handle blocking I/O well."
      },
      {
        "id": "fin-q3",
        "type": "fill_blank",
        "prompt": "Aging is a technique to prevent ______ by gradually increasing the priority of waiting processes.",
        "correctAnswer": "starvation",
        "explanation": "Aging increases the priority of processes that have been waiting for a long time, ensuring they eventually get CPU time."
      },
      {
        "id": "fin-q4",
        "type": "true_false",
        "prompt": "In real-time scheduling, a hard deadline must be met or the result is considered a failure.",
        "correctAnswer": true,
        "explanation": "Hard real-time systems require guaranteed deadline compliance. Missing a hard deadline can have catastrophic consequences."
      },
      {
        "id": "fin-q5",
        "type": "multiple_choice",
        "prompt": "Which is true about spinlocks?",
        "options": [
          "They are ideal for long critical sections",
          "They put the waiting thread to sleep",
          "They are efficient when the lock is expected to be held briefly",
          "They cannot be used in multiprocessor systems"
        ],
        "correctAnswer": 2,
        "explanation": "Spinlocks are efficient for short critical sections because the overhead of context switching would exceed the wait time. They waste CPU cycles spinning, so they're bad for long waits."
      },
      {
        "id": "fin-q6",
        "type": "coding",
        "prompt": "Implement Peterson's solution for mutual exclusion between two processes.",
        "starterCode": "int flag[2] = {0, 0};\nint turn;\n\nvoid enter_section(int i) {\n    // Your code\n}\n\nvoid leave_section(int i) {\n    // Your code\n}",
        "language": "c",
        "solution": "int flag[2] = {0, 0};\nint turn;\n\nvoid enter_section(int i) {\n    int j = 1 - i;\n    flag[i] = 1;        // I want to enter\n    turn = j;           // Give other process a chance\n    while (flag[j] && turn == j)\n        ;               // Busy wait\n}\n\nvoid leave_section(int i) {\n    flag[i] = 0;        // I'm leaving\n}",
        "testCases": [
          {
            "input": "",
            "expectedOutput": "Mutual exclusion achieved",
            "isHidden": false,
            "description": "Peterson's algorithm"
          }
        ],
        "correctAnswer": true,
        "explanation": "Peterson's solution uses two flags and a turn variable to ensure mutual exclusion, progress, and bounded waiting for two processes."
      },
      {
        "id": "fin-q7",
        "type": "multiple_choice",
        "prompt": "The Rate Monotonic scheduling algorithm assigns priorities based on:",
        "options": [
          "Process size",
          "Arrival time",
          "Period (shorter period = higher priority)",
          "Execution time"
        ],
        "correctAnswer": 2,
        "explanation": "Rate Monotonic assigns higher priority to tasks with shorter periods. It's optimal for preemptive static priority scheduling of periodic tasks."
      },
      {
        "id": "fin-q8",
        "type": "written",
        "prompt": "Compare and contrast semaphores and monitors as synchronization mechanisms.",
        "correctAnswer": "semaphore low-level monitor high-level encapsulation condition variable",
        "explanation": "Semaphores are low-level primitives; monitors provide higher-level abstraction with encapsulation.",
        "modelAnswer": "SEMAPHORES: Low-level synchronization primitive using wait/signal operations. Programmer must correctly place wait/signal calls. Easy to make errors (forgetting to signal, wrong order). Can be counting or binary. Very flexible but error-prone. MONITORS: High-level construct that encapsulates shared data with procedures. Only one process can execute a monitor procedure at a time (automatic mutual exclusion). Uses condition variables for waiting/signaling. Harder to misuse because synchronization is built into the structure. Less flexible but safer. Key differences: 1) Semaphores require manual management; monitors automate mutual exclusion. 2) Semaphores use signal to release; monitors use condition variables. 3) Monitors provide better encapsulation and are less error-prone."
      },
      {
        "id": "fin-q9",
        "type": "multiple_choice",
        "prompt": "Which is NOT a necessary condition for deadlock?",
        "options": [
          "Mutual exclusion",
          "Hold and wait",
          "Preemption",
          "Circular wait"
        ],
        "correctAnswer": 2,
        "explanation": "The four necessary conditions for deadlock are: Mutual exclusion, Hold and wait, NO preemption, and Circular wait. Preemption prevents deadlock."
      },
      {
        "id": "fin-q10",
        "type": "true_false",
        "prompt": "In the Banker's Algorithm, a state is safe if there exists at least one sequence in which all processes can finish.",
        "correctAnswer": true,
        "explanation": "A safe state guarantees that all processes can complete by finding a safe sequence of resource allocations."
      },
      {
        "id": "fin-q11",
        "type": "multiple_choice",
        "prompt": "Which deadlock handling strategy is used by most operating systems?",
        "options": [
          "Prevention",
          "Avoidance",
          "Detection and recovery",
          "Ostrich algorithm (ignore it)"
        ],
        "correctAnswer": 3,
        "explanation": "Most general-purpose operating systems ignore the deadlock problem (ostrich algorithm) because deadlocks are rare and the overhead of prevention/avoidance is high."
      },
      {
        "id": "fin-q12",
        "type": "multiple_choice",
        "prompt": "Given: Available = [3,3,2], Max = [[7,5,3],[3,2,2],[9,0,2],[2,2,2],[4,3,3]], Allocation = [[0,1,0],[2,0,0],[3,0,2],[2,1,1],[0,0,2]]. Is the system in a safe state?",
        "codeSnippet": "Need = Max - Allocation\nProcess  Need\nP0      [7,4,3]\nP1      [1,2,2]\nP2      [6,0,0]\nP3      [0,1,1]\nP4      [4,3,1]\n\nAvailable = [3,3,2]",
        "options": [
          "Yes, safe sequence: P1,P3,P4,P2,P0",
          "Yes, safe sequence: P1,P3,P4,P0,P2",
          "No, not safe",
          "Cannot determine"
        ],
        "correctAnswer": 0,
        "explanation": "Start with Available [3,3,2]: P1 can run (needs [1,2,2]), releasing to [5,3,2]. P3 can run, then P4, P2, P0. Safe sequence exists."
      },
      {
        "id": "fin-q13",
        "type": "fill_blank",
        "prompt": "Breaking the circular wait condition by imposing a total ordering on resources is an example of deadlock ______.",
        "correctAnswer": "prevention",
        "explanation": "Deadlock prevention ensures at least one necessary condition cannot hold. Ordering resources prevents circular wait."
      },
      {
        "id": "fin-q14",
        "type": "multiple_choice",
        "prompt": "In deadlock recovery by process termination, which approach minimizes work lost?",
        "options": [
          "Abort all deadlocked processes",
          "Abort one process at a time until deadlock is resolved",
          "Abort the process with highest priority",
          "Abort the newest process"
        ],
        "correctAnswer": 1,
        "explanation": "Aborting one process at a time and checking if deadlock is resolved minimizes the number of processes terminated and work lost."
      },
      {
        "id": "fin-q15",
        "type": "multiple_choice",
        "prompt": "External fragmentation is a problem in which allocation scheme?",
        "options": [
          "Paging",
          "Contiguous allocation",
          "Both",
          "Neither"
        ],
        "correctAnswer": 1,
        "explanation": "Contiguous allocation suffers from external fragmentation (free memory exists but not contiguously). Paging eliminates external fragmentation by using fixed-size frames."
      },
      {
        "id": "fin-q16",
        "type": "true_false",
        "prompt": "Internal fragmentation occurs in paging systems when a process doesn't use all of its last page.",
        "correctAnswer": true,
        "explanation": "Internal fragmentation is the wasted space within allocated units. In paging, the last page of a process may not be fully used."
      },
      {
        "id": "fin-q17",
        "type": "fill_blank",
        "prompt": "In paging, a logical address consists of a page number and a page ______.",
        "correctAnswer": "offset",
        "explanation": "The logical address is divided into the page number (to index into the page table) and the page offset (position within the page)."
      },
      {
        "id": "fin-q18",
        "type": "multiple_choice",
        "prompt": "Given page size = 4KB, how many bits are needed for the page offset?",
        "codeSnippet": "Page size = 4KB = 4 × 1024 = 4096 bytes\n4096 = 2^?",
        "options": [
          "10 bits",
          "11 bits",
          "12 bits",
          "13 bits"
        ],
        "correctAnswer": 2,
        "explanation": "4KB = 4096 = 2^12, so 12 bits are needed to address any byte within a 4KB page."
      },
      {
        "id": "fin-q19",
        "type": "multiple_choice",
        "prompt": "The Translation Lookaside Buffer (TLB) is used to:",
        "options": [
          "Store recently translated page table entries for fast lookup",
          "Buffer disk writes",
          "Cache frequently accessed files",
          "Store process scheduling information"
        ],
        "correctAnswer": 0,
        "explanation": "The TLB is a hardware cache that stores recent page table entries, reducing the need to access memory for address translation."
      },
      {
        "id": "fin-q20",
        "type": "multiple_choice",
        "prompt": "Multi-level page tables are used to:",
        "options": [
          "Increase address translation speed",
          "Reduce the memory required for page tables",
          "Increase page size",
          "Enable shared memory"
        ],
        "correctAnswer": 1,
        "explanation": "Multi-level page tables allow sparse address spaces to use less memory because entire page table sections can be absent if unused."
      },
      {
        "id": "fin-q21",
        "type": "coding",
        "prompt": "Write code to translate a logical address to physical address using a page table.",
        "starterCode": "def translate(logical_addr, page_table, page_size):\n    # Return physical address\n    # page_table maps page numbers to frame numbers\n    pass",
        "language": "python",
        "solution": "def translate(logical_addr, page_table, page_size):\n    page_number = logical_addr // page_size\n    page_offset = logical_addr % page_size\n    if page_number >= len(page_table):\n        return -1  # Invalid page\n    frame_number = page_table[page_number]\n    physical_addr = frame_number * page_size + page_offset\n    return physical_addr",
        "testCases": [
          {
            "input": "translate(5000, [2, 3, 0, 5], 4096)",
            "expectedOutput": "13192",
            "isHidden": false,
            "description": "Page 1, offset 904"
          },
          {
            "input": "translate(0, [5], 4096)",
            "expectedOutput": "20480",
            "isHidden": false,
            "description": "First byte"
          }
        ],
        "correctAnswer": true,
        "explanation": "Extract page number and offset from logical address, lookup frame number in page table, compute physical address."
      },
      {
        "id": "fin-q22",
        "type": "multiple_choice",
        "prompt": "In segmentation, a logical address consists of:",
        "options": [
          "Page number and offset",
          "Segment number and offset",
          "Frame number and offset",
          "Base and limit"
        ],
        "correctAnswer": 1,
        "explanation": "Segmentation uses segment number to index into the segment table, and offset to specify position within the segment."
      },
      {
        "id": "fin-q23",
        "type": "multiple_choice",
        "prompt": "Demand paging loads pages:",
        "options": [
          "All at once when a program starts",
          "Only when they are accessed (on demand)",
          "Based on predicted future access",
          "In round-robin order"
        ],
        "correctAnswer": 1,
        "explanation": "Demand paging loads pages into memory only when they are accessed, causing a page fault that triggers loading from disk."
      },
      {
        "id": "fin-q24",
        "type": "true_false",
        "prompt": "Belady's anomaly refers to a situation where increasing the number of page frames increases the number of page faults.",
        "correctAnswer": true,
        "explanation": "Belady's anomaly can occur with FIFO page replacement, where adding more frames paradoxically causes more page faults."
      },
      {
        "id": "fin-q25",
        "type": "multiple_choice",
        "prompt": "Using LRU page replacement with 3 frames, how many page faults occur for reference string: 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5?",
        "codeSnippet": "Reference: 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5\nFrames: 3\n\nTrack LRU order after each reference...",
        "options": [
          "7",
          "8",
          "10",
          "12"
        ],
        "correctAnswer": 2,
        "explanation": "LRU with 3 frames: 1(F), 2(F), 3(F), 4(F-evict 1), 1(F-evict 2), 2(F-evict 3), 5(F-evict 4), 1(H), 2(H), 3(F-evict 5), 4(F-evict 1), 5(F-evict 2) = 10 page faults."
      },
      {
        "id": "fin-q26",
        "type": "multiple_choice",
        "prompt": "Which page replacement algorithm is optimal but impractical to implement?",
        "options": [
          "FIFO",
          "LRU",
          "OPT (Optimal)",
          "Clock"
        ],
        "correctAnswer": 2,
        "explanation": "OPT replaces the page that will not be used for the longest time in the future. It's optimal but requires future knowledge, which is impossible in practice."
      },
      {
        "id": "fin-q27",
        "type": "fill_blank",
        "prompt": "Thrashing occurs when a system spends more time ______ than executing processes.",
        "correctAnswer": "paging",
        "explanation": "Thrashing happens when there are too many processes competing for too few frames, causing constant page faults and disk I/O."
      },
      {
        "id": "fin-q28",
        "type": "written",
        "prompt": "Explain the working set model and how it helps prevent thrashing.",
        "correctAnswer": "working set pages locality time window prevent thrashing",
        "explanation": "The working set is the set of pages a process is actively using, based on locality of reference.",
        "modelAnswer": "The working set model is based on the principle of locality. The working set W(t,Δ) is the set of pages referenced by a process during a time window Δ ending at time t. This represents the pages the process is actively using. How it prevents thrashing: 1) The OS estimates each process's working set size. 2) Only admit a process if there are enough free frames for its working set. 3) If total working set demand exceeds available frames, suspend some processes. 4) This ensures each active process has enough frames to avoid constant page faults. The key insight is that a process needs its working set in memory to make progress; otherwise, it thrashes."
      },
      {
        "id": "fin-q29",
        "type": "multiple_choice",
        "prompt": "Memory-mapped files allow file access through:",
        "options": [
          "read() and write() system calls only",
          "Memory load and store operations",
          "Network protocols",
          "Keyboard input"
        ],
        "correctAnswer": 1,
        "explanation": "Memory-mapped files map file contents to a region of virtual memory, allowing file access through normal memory operations (load/store) instead of read/write system calls."
      },
      {
        "id": "fin-q30",
        "type": "multiple_choice",
        "prompt": "An inode in Unix file systems contains:",
        "options": [
          "Only the file name",
          "File metadata and pointers to data blocks",
          "Only the file data",
          "Only the directory structure"
        ],
        "correctAnswer": 1,
        "explanation": "An inode contains file metadata (permissions, owner, size, timestamps) and pointers to data blocks (direct, indirect, double indirect)."
      },
      {
        "id": "fin-q31",
        "type": "true_false",
        "prompt": "Hard links in Unix share the same inode number.",
        "correctAnswer": true,
        "explanation": "Hard links are multiple directory entries pointing to the same inode. They share the same file data and metadata."
      },
      {
        "id": "fin-q32",
        "type": "multiple_choice",
        "prompt": "If a file system uses 4KB blocks and 32-bit block pointers, how much data can be addressed with one level of indirection?",
        "codeSnippet": "Block size = 4KB = 4096 bytes\nPointer size = 4 bytes\nPointers per block = 4096 / 4 = 1024\nSingle indirect block = 1024 × 4KB = ?",
        "options": [
          "1MB",
          "4MB",
          "16MB",
          "1GB"
        ],
        "correctAnswer": 1,
        "explanation": "1024 pointers × 4KB per block = 4096KB = 4MB can be addressed through a single indirect block."
      },
      {
        "id": "fin-q33",
        "type": "multiple_choice",
        "prompt": "Which file system feature helps recover from crashes?",
        "options": [
          "Compression",
          "Encryption",
          "Journaling",
          "Quotas"
        ],
        "correctAnswer": 2,
        "explanation": "Journaling logs operations before performing them, allowing the system to replay or undo incomplete operations after a crash."
      },
      {
        "id": "fin-q34",
        "type": "multiple_choice",
        "prompt": "Copy-on-write (COW) in file systems means:",
        "options": [
          "Data is copied when read",
          "Modifications create new copies instead of overwriting originals",
          "All writes are synchronous",
          "Files cannot be modified"
        ],
        "correctAnswer": 1,
        "explanation": "COW file systems like Btrfs and ZFS write modifications to new locations rather than overwriting original data, enabling features like snapshots."
      },
      {
        "id": "fin-q35",
        "type": "multiple_choice",
        "prompt": "The fork() system call:",
        "options": [
          "Creates a new thread",
          "Creates a new process as a copy of the parent",
          "Terminates the current process",
          "Allocates memory"
        ],
        "correctAnswer": 1,
        "explanation": "fork() creates a new process that is a copy of the parent process, including code, data, and resources. The child gets a copy of the parent's address space."
      },
      {
        "id": "fin-q36",
        "type": "written",
        "prompt": "Describe how virtual memory enables processes to have more memory than physically available.",
        "correctAnswer": "virtual address space swap disk demand paging",
        "explanation": "Virtual memory separates logical from physical memory, using disk as backing store.",
        "modelAnswer": "Virtual memory allows processes to use more memory than physically available through: 1) ADDRESS SPACE ABSTRACTION: Each process has its own virtual address space (e.g., 4GB on 32-bit), independent of physical RAM size. 2) DEMAND PAGING: Only actively used pages reside in physical memory; unused pages stay on disk. 3) SWAPPING: When physical memory is full, the OS swaps out less-used pages to disk to make room for needed pages. 4) PAGE TABLES: The MMU translates virtual addresses to physical addresses using page tables, with a present bit indicating if a page is in memory. Benefits: Processes can use large virtual address spaces, multiple processes can run simultaneously without interference, and the illusion of abundant memory is maintained even with limited RAM."
      },
      {
        "id": "fin-q37",
        "type": "multiple_choice",
        "prompt": "RAID-5 provides fault tolerance through:",
        "options": [
          "Mirroring all data",
          "Distributed parity across disks",
          "Striping without redundancy",
          "Triple replication"
        ],
        "correctAnswer": 1,
        "explanation": "RAID-5 stripes data and parity information across all disks. If one disk fails, data can be reconstructed from the parity information on the remaining disks."
      },
      {
        "id": "fin-q38",
        "type": "fill_blank",
        "prompt": "A ______ is a file that contains a path to another file, and can cross file system boundaries.",
        "correctAnswer": "symbolic link",
        "explanation": "Symbolic (soft) links store the path to the target file. Unlike hard links, they can cross file systems and link to directories."
      },
      {
        "id": "fin-q39",
        "type": "multiple_choice",
        "prompt": "The clock page replacement algorithm uses:",
        "options": [
          "A FIFO queue with a reference bit",
          "A LRU stack",
          "Random selection",
          "Optimal future knowledge"
        ],
        "correctAnswer": 0,
        "explanation": "The clock (second-chance) algorithm arranges pages in a circle and uses a reference bit. It gives each page a \"second chance\" before replacement."
      },
      {
        "id": "fin-q40",
        "type": "coding",
        "prompt": "Implement the FIFO page replacement algorithm.",
        "starterCode": "def fifo_page_replacement(reference_string, num_frames):\n    # Return number of page faults\n    pass",
        "language": "python",
        "solution": "def fifo_page_replacement(reference_string, num_frames):\n    frames = []\n    page_faults = 0\n\n    for page in reference_string:\n        if page not in frames:\n            page_faults += 1\n            if len(frames) < num_frames:\n                frames.append(page)\n            else:\n                frames.pop(0)  # Remove oldest\n                frames.append(page)\n\n    return page_faults",
        "testCases": [
          {
            "input": "[1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5], 3",
            "expectedOutput": "9",
            "isHidden": false,
            "description": "Standard reference string"
          },
          {
            "input": "[1, 2, 3, 1, 2, 3], 3",
            "expectedOutput": "3",
            "isHidden": false,
            "description": "No replacements needed"
          }
        ],
        "correctAnswer": true,
        "explanation": "FIFO replaces the oldest page in memory. Track pages in a queue and remove from the front when replacement is needed."
      },
      {
        "id": "fin-q41",
        "type": "true_false",
        "prompt": "The effective access time (EAT) with TLB is calculated as: EAT = hit_ratio × TLB_time + (1 - hit_ratio) × (TLB_time + memory_time + memory_time)",
        "correctAnswer": false,
        "explanation": "The correct formula is: EAT = hit_ratio × (TLB_time + memory_time) + (1 - hit_ratio) × (TLB_time + 2 × memory_time). On a TLB hit, you still need one memory access for data."
      },
      {
        "id": "fin-q42",
        "type": "multiple_choice",
        "prompt": "Which file allocation method provides the best sequential and random access performance?",
        "options": [
          "Contiguous allocation",
          "Linked allocation",
          "Indexed allocation",
          "None of the above"
        ],
        "correctAnswer": 0,
        "explanation": "Contiguous allocation provides excellent sequential access (blocks are adjacent) and random access (direct calculation of block address). However, it suffers from external fragmentation."
      }
    ]
  }
]