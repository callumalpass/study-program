[
  {
    "id": "math402-ex-1-1",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 1,
    "title": "Computing Machine Epsilon",
    "description": "Write a program to experimentally determine machine epsilon for your system. Write a function that computes machine epsilon by finding the smallest positive number ε such that 1.0 + ε > 1.0 in floating-point arithmetic. Test for both single and double precision.",
    "starterCode": "import numpy as np\n\ndef compute_machine_epsilon(dtype=np.float64):\n    \"\"\"\n    Compute machine epsilon experimentally.\n\n    Parameters:\n    - dtype: numpy data type (float32 or float64)\n\n    Returns:\n    - epsilon: machine epsilon\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test\neps_32 = compute_machine_epsilon(np.float32)\neps_64 = compute_machine_epsilon(np.float64)\n\nprint(f\"Computed epsilon (float32): {eps_32:.2e}\")\nprint(f\"NumPy epsilon (float32): {np.finfo(np.float32).eps:.2e}\")\nprint()\nprint(f\"Computed epsilon (float64): {eps_64:.2e}\")\nprint(f\"NumPy epsilon (float64): {np.finfo(np.float64).eps:.2e}\")",
    "hints": [
      "Start with ε = 1 and repeatedly halve it",
      "Stop when 1.0 + ε/2 == 1.0",
      "Compare with numpy.finfo values"
    ],
    "solution": "import numpy as np\n\ndef compute_machine_epsilon(dtype=np.float64):\n    \"\"\"\n    Compute machine epsilon experimentally.\n\n    Parameters:\n    - dtype: numpy data type (float32 or float64)\n\n    Returns:\n    - epsilon: machine epsilon\n    \"\"\"\n    eps = dtype(1.0)\n\n    while dtype(1.0) + dtype(eps / 2.0) != dtype(1.0):\n        eps = dtype(eps / 2.0)\n\n    return eps\n\n# Test\neps_32 = compute_machine_epsilon(np.float32)\neps_64 = compute_machine_epsilon(np.float64)\n\nprint(f\"Computed epsilon (float32): {eps_32:.2e}\")\nprint(f\"NumPy epsilon (float32): {np.finfo(np.float32).eps:.2e}\")\nprint()\nprint(f\"Computed epsilon (float64): {eps_64:.2e}\")\nprint(f\"NumPy epsilon (float64): {np.finfo(np.float64).eps:.2e}\")\n\n# Verification\nassert abs(eps_32 - np.finfo(np.float32).eps) < 1e-10\nassert abs(eps_64 - np.finfo(np.float64).eps) < 1e-15\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "dtype=np.float32",
        "expectedOutput": "≈ 1.19e-07",
        "isHidden": false,
        "description": "Test with single precision float"
      },
      {
        "input": "dtype=np.float64",
        "expectedOutput": "≈ 2.22e-16",
        "isHidden": false,
        "description": "Test with double precision float"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-2",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 2,
    "title": "Stable Quadratic Formula",
    "description": "Implement a numerically stable quadratic formula that avoids cancellation. Implement a function to solve ax² + bx + c = 0 that avoids catastrophic cancellation when b² >> 4ac. The standard formula suffers from cancellation when computing -b ± √(b² - 4ac) when the terms are nearly equal.",
    "starterCode": "import numpy as np\n\ndef quadratic_stable(a, b, c):\n    \"\"\"\n    Solve ax² + bx + c = 0 using numerically stable formula.\n\n    Parameters:\n    - a, b, c: coefficients\n\n    Returns:\n    - (x1, x2): roots (real or complex)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test cases\ntest_cases = [\n    (1, 5, 6, (-3.0, -2.0)),  # (x+2)(x+3) = 0\n    (1, -1e10, 1, (-1e10, -1e-10)),  # Large b\n    (1, 0, -4, (-2.0, 2.0)),  # x² - 4 = 0\n]\n\nprint(\"Testing stable quadratic formula:\\n\")\nfor a, b, c, expected in test_cases:\n    result = quadratic_stable(a, b, c)\n    print(f\"({a})x² + ({b})x + ({c}) = 0\")\n    print(f\"  Result: {result}\")\n    print(f\"  Expected: {expected}\")\n    print()",
    "hints": [
      "Compute the numerically stable root first",
      "Use Vieta's formula x₁x₂ = c/a for the second root",
      "Handle edge cases (a=0, discriminant < 0)"
    ],
    "solution": "import numpy as np\n\ndef quadratic_stable(a, b, c):\n    \"\"\"\n    Solve ax² + bx + c = 0 using numerically stable formula.\n\n    Parameters:\n    - a, b, c: coefficients\n\n    Returns:\n    - (x1, x2): roots (real or complex)\n    \"\"\"\n    # Handle degenerate case\n    if abs(a) < 1e-15:\n        if abs(b) < 1e-15:\n            return None if abs(c) > 1e-15 else float('inf')\n        return -c / b, None\n\n    # Compute discriminant\n    disc = b**2 - 4*a*c\n\n    # Complex roots\n    if disc < 0:\n        real = -b / (2*a)\n        imag = np.sqrt(-disc) / (2*a)\n        return complex(real, imag), complex(real, -imag)\n\n    # Real roots - stable formula\n    sqrt_disc = np.sqrt(disc)\n\n    # Compute one root avoiding cancellation\n    if b >= 0:\n        x1 = (-b - sqrt_disc) / (2*a)\n    else:\n        x1 = (-b + sqrt_disc) / (2*a)\n\n    # Second root using Vieta's formula\n    x2 = c / (a * x1) if abs(x1) > 1e-15 else (-b - x1)\n\n    # Return in sorted order\n    if isinstance(x1, complex):\n        return x1, x2\n    return tuple(sorted([x1, x2]))\n\n# Test cases\ntest_cases = [\n    (1, 5, 6, (-3.0, -2.0)),  # (x+2)(x+3) = 0\n    (1, -1e10, 1, (-1e10, -1e-10)),  # Large b\n    (1, 0, -4, (-2.0, 2.0)),  # x² - 4 = 0\n    (1, 0, 4, None),  # Complex roots\n    (1, -4, 4, (2.0, 2.0)),  # Double root\n]\n\nprint(\"Testing stable quadratic formula:\\n\")\nfor a, b, c, expected in test_cases:\n    result = quadratic_stable(a, b, c)\n    print(f\"({a})x² + ({b})x + ({c}) = 0\")\n    print(f\"  Result: {result}\")\n    if expected:\n        print(f\"  Expected: {expected}\")\n\n    # Verify\n    if result and not isinstance(result[0], complex):\n        x1, x2 = result\n        residual1 = abs(a*x1**2 + b*x1 + c)\n        residual2 = abs(a*x2**2 + b*x2 + c)\n        print(f\"  Residuals: {residual1:.2e}, {residual2:.2e}\")\n        assert residual1 < 1e-10 and residual2 < 1e-10\n    print()\n\nprint(\"All tests passed!\")",
    "testCases": [
      {
        "input": "a=1, b=1e10, c=1",
        "expectedOutput": "roots near -1e10 and -1e-10",
        "isHidden": false,
        "description": "Test with large coefficient b to check stability"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-3",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 3,
    "title": "Kahan Summation Algorithm",
    "description": "Implement compensated summation for improved accuracy when summing many numbers. Implement Kahan's compensated summation algorithm to minimize rounding errors when summing a large array of floating-point numbers. Compare accuracy with naive summation for challenging test cases.",
    "starterCode": "import numpy as np\n\ndef kahan_sum(data):\n    \"\"\"\n    Kahan compensated summation algorithm.\n\n    Reduces cumulative rounding error from O(nε) to O(ε).\n\n    Parameters:\n    - data: array-like of numbers to sum\n\n    Returns:\n    - sum: compensated sum\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case\nprint(\"Test: Large + many small\\n\")\ndata1 = [1.0] + [1e-10] * 1000\nresult = kahan_sum(data1)\nprint(f\"Kahan sum: {result:.15f}\")",
    "hints": [
      "Keep a running compensation variable to track lost precision",
      "For each element: y = x - c, t = s + y, c = (t - s) - y, s = t",
      "The compensation c accumulates the rounding errors"
    ],
    "solution": "import numpy as np\n\ndef kahan_sum(data):\n    \"\"\"\n    Kahan compensated summation algorithm.\n\n    Reduces cumulative rounding error from O(nε) to O(ε).\n\n    Parameters:\n    - data: array-like of numbers to sum\n\n    Returns:\n    - sum: compensated sum\n    \"\"\"\n    s = 0.0  # Running sum\n    c = 0.0  # Running compensation\n\n    for x in data:\n        y = x - c  # Subtract previous compensation\n        t = s + y  # New sum\n        c = (t - s) - y  # Compute new compensation\n        s = t  # Update sum\n\n    return s\n\ndef compare_summation_methods(data):\n    \"\"\"Compare different summation methods.\"\"\"\n    # Naive summation\n    naive = sum(data)\n\n    # Kahan summation\n    kahan = kahan_sum(data)\n\n    # NumPy (uses pairwise summation)\n    numpy_sum = np.sum(data)\n\n    # High precision reference\n    from decimal import Decimal, getcontext\n    getcontext().prec = 100\n    exact = float(sum(Decimal(str(float(x))) for x in data))\n\n    results = {\n        'Naive': (naive, abs(naive - exact)),\n        'Kahan': (kahan, abs(kahan - exact)),\n        'NumPy': (numpy_sum, abs(numpy_sum - exact)),\n    }\n\n    return results, exact\n\n# Test case: sum of many small numbers with large initial value\nprint(\"Test 1: Large + many small\\n\")\ndata1 = [1.0] + [1e-10] * 10000000\nresults1, exact1 = compare_summation_methods(data1)\n\nprint(f\"Exact value: {exact1:.15f}\\n\")\nfor method, (value, error) in results1.items():\n    print(f\"{method:10s}: {value:.15f}  (error: {error:.2e})\")\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# Test case: alternating positive and negative\nprint(\"Test 2: Alternating signs\\n\")\ndata2 = [1.0, -1.0, 1e-10] * 1000\nresults2, exact2 = compare_summation_methods(data2)\n\nprint(f\"Exact value: {exact2:.15e}\\n\")\nfor method, (value, error) in results2.items():\n    print(f\"{method:10s}: {value:.15e}  (error: {error:.2e})\")\n\nprint(\"\\nKahan summation successfully reduces cumulative error!\")",
    "testCases": [
      {
        "input": "data=[1.0] + [1e-10] * 1000",
        "expectedOutput": "More accurate sum than naive summation",
        "isHidden": false,
        "description": "Test with large value plus many small values"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-4",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 2,
    "title": "Relative and Absolute Error",
    "description": "Compute and analyze relative and absolute errors in numerical approximations. Given true values and approximations, calculate both types of errors and determine which approximation is better using relative error analysis.",
    "starterCode": "import numpy as np\n\ndef compute_errors(true_value, approx_value):\n    \"\"\"\n    Compute absolute and relative errors.\n\n    Parameters:\n    - true_value: true value\n    - approx_value: approximate value\n\n    Returns:\n    - (absolute_error, relative_error): tuple of errors\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test cases\ntest_cases = [\n    (np.pi, 3.14),\n    (1e-10, 1.1e-10),\n    (1000, 1001),\n    (np.e, 2.718),\n]\n\nprint(\"Error Analysis:\\n\")\nfor true_val, approx_val in test_cases:\n    abs_err, rel_err = compute_errors(true_val, approx_val)\n    print(f\"True: {true_val:.10f}, Approx: {approx_val:.10f}\")\n    print(f\"  Absolute error: {abs_err:.2e}\")\n    print(f\"  Relative error: {rel_err:.2e}\")\n    print()",
    "hints": [
      "Absolute error = |true - approx|",
      "Relative error = |true - approx| / |true|",
      "Handle case when true value is zero"
    ],
    "solution": "import numpy as np\n\ndef compute_errors(true_value, approx_value):\n    \"\"\"\n    Compute absolute and relative errors.\n\n    Parameters:\n    - true_value: true value\n    - approx_value: approximate value\n\n    Returns:\n    - (absolute_error, relative_error): tuple of errors\n    \"\"\"\n    absolute_error = abs(true_value - approx_value)\n\n    # Relative error (handle zero case)\n    if abs(true_value) < 1e-15:\n        relative_error = float('inf') if absolute_error > 1e-15 else 0.0\n    else:\n        relative_error = absolute_error / abs(true_value)\n\n    return absolute_error, relative_error\n\n# Test cases\ntest_cases = [\n    (np.pi, 3.14),\n    (1e-10, 1.1e-10),\n    (1000, 1001),\n    (np.e, 2.718),\n]\n\nprint(\"Error Analysis:\\n\")\nfor true_val, approx_val in test_cases:\n    abs_err, rel_err = compute_errors(true_val, approx_val)\n    print(f\"True: {true_val:.10f}, Approx: {approx_val:.10f}\")\n    print(f\"  Absolute error: {abs_err:.2e}\")\n    print(f\"  Relative error: {rel_err:.2e}\")\n    print()\n\n# Comparison example\nprint(\"=\"*60)\nprint(\"\\nComparison: Which approximation is better?\\n\")\n\n# Case 1: Small numbers\ntrue1, approx1a, approx1b = 1e-10, 1.1e-10, 2e-10\nabs_err_a, rel_err_a = compute_errors(true1, approx1a)\nabs_err_b, rel_err_b = compute_errors(true1, approx1b)\n\nprint(f\"True value: {true1:.2e}\")\nprint(f\"Approx A: {approx1a:.2e} (rel err: {rel_err_a:.2%})\")\nprint(f\"Approx B: {approx1b:.2e} (rel err: {rel_err_b:.2%})\")\nprint(f\"Better approximation: {'A' if rel_err_a < rel_err_b else 'B'}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "true_value=π, approx_value=3.14",
        "expectedOutput": "absolute error ≈ 1.59e-03, relative error ≈ 5.07e-04",
        "isHidden": false,
        "description": "Test error computation for π approximation"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-5",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 3,
    "title": "Condition Number Analysis",
    "description": "Compute and interpret condition numbers for various problems. Implement functions to calculate condition numbers for different mathematical operations and analyze problem sensitivity to input perturbations.",
    "starterCode": "import numpy as np\n\ndef condition_number_function(f, df, x):\n    \"\"\"\n    Compute condition number for evaluating f at x.\n\n    Condition number κ = |x·f'(x)/f(x)|\n\n    Parameters:\n    - f: function to evaluate\n    - df: derivative of f\n    - x: point of evaluation\n\n    Returns:\n    - condition_number: condition number at x\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef condition_number_matrix(A):\n    \"\"\"\n    Compute condition number of matrix A.\n\n    κ(A) = ||A|| · ||A^(-1)||\n\n    Parameters:\n    - A: square matrix\n\n    Returns:\n    - condition_number: condition number\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function evaluation\nprint(\"Function Evaluation Conditioning:\\n\")\nf = lambda x: np.sin(x)\ndf = lambda x: np.cos(x)\n\ntest_points = [np.pi/6, np.pi/4, np.pi/2]\nfor x in test_points:\n    kappa = condition_number_function(f, df, x)\n    print(f\"sin({x:.4f}): κ = {kappa:.4f}\")\n\n# Test matrix conditioning\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nMatrix Conditioning:\\n\")\n\nmatrices = {\n    \"Well-conditioned\": np.array([[4, 1], [1, 3]]),\n    \"Ill-conditioned\": np.array([[1, 1], [1, 1.0001]]),\n}\n\nfor name, A in matrices.items():\n    kappa = condition_number_matrix(A)\n    print(f\"{name}: κ = {kappa:.2e}\")",
    "hints": [
      "For functions: κ = |x·f'(x)/f(x)| measures relative output change per relative input change",
      "For matrices: use numpy.linalg.norm and numpy.linalg.inv",
      "Large condition number indicates ill-conditioned problem"
    ],
    "solution": "import numpy as np\n\ndef condition_number_function(f, df, x):\n    \"\"\"\n    Compute condition number for evaluating f at x.\n\n    Condition number κ = |x·f'(x)/f(x)|\n\n    Parameters:\n    - f: function to evaluate\n    - df: derivative of f\n    - x: point of evaluation\n\n    Returns:\n    - condition_number: condition number at x\n    \"\"\"\n    fx = f(x)\n    dfx = df(x)\n\n    if abs(fx) < 1e-15:\n        return float('inf')\n\n    kappa = abs(x * dfx / fx)\n    return kappa\n\ndef condition_number_matrix(A):\n    \"\"\"\n    Compute condition number of matrix A.\n\n    κ(A) = ||A|| · ||A^(-1)||\n\n    Parameters:\n    - A: square matrix\n\n    Returns:\n    - condition_number: condition number\n    \"\"\"\n    # Use numpy's built-in condition number (2-norm)\n    kappa = np.linalg.cond(A)\n\n    # Or compute manually:\n    # norm_A = np.linalg.norm(A, 2)\n    # norm_Ainv = np.linalg.norm(np.linalg.inv(A), 2)\n    # kappa = norm_A * norm_Ainv\n\n    return kappa\n\n# Test function evaluation\nprint(\"Function Evaluation Conditioning:\\n\")\nf = lambda x: np.sin(x)\ndf = lambda x: np.cos(x)\n\ntest_points = [np.pi/6, np.pi/4, np.pi/2]\nfor x in test_points:\n    kappa = condition_number_function(f, df, x)\n    print(f\"sin({x:.4f}): κ = {kappa:.4f}\")\n\nprint(\"\\nInterpretation: sin(x) becomes better conditioned as x approaches π/2\")\nprint(\"because sin(π/2) = 1 is far from zero.\\n\")\n\n# Test matrix conditioning\nprint(\"=\"*60)\nprint(\"\\nMatrix Conditioning:\\n\")\n\nmatrices = {\n    \"Well-conditioned\": np.array([[4, 1], [1, 3]]),\n    \"Ill-conditioned\": np.array([[1, 1], [1, 1.0001]]),\n    \"Very ill-conditioned\": np.array([[1, 1], [1, 1.00001]]),\n}\n\nfor name, A in matrices.items():\n    kappa = condition_number_matrix(A)\n    print(f\"{name}: κ = {kappa:.2e}\")\n\n# Demonstrate effect of ill-conditioning\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nEffect of Ill-conditioning:\\n\")\n\nA_ill = np.array([[1, 1], [1, 1.0001]])\nb = np.array([2, 2.0001])\nx_exact = np.linalg.solve(A_ill, b)\n\n# Perturb b slightly\nb_perturbed = b + np.array([0, 1e-8])\nx_perturbed = np.linalg.solve(A_ill, b_perturbed)\n\nrel_input_change = np.linalg.norm(b_perturbed - b) / np.linalg.norm(b)\nrel_output_change = np.linalg.norm(x_perturbed - x_exact) / np.linalg.norm(x_exact)\n\nprint(f\"Relative input change: {rel_input_change:.2e}\")\nprint(f\"Relative output change: {rel_output_change:.2e}\")\nprint(f\"Amplification factor: {rel_output_change/rel_input_change:.2e}\")\nprint(f\"Condition number: {condition_number_matrix(A_ill):.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "A = [[4, 1], [1, 3]]",
        "expectedOutput": "κ ≈ 5.83 (well-conditioned)",
        "isHidden": false,
        "description": "Test condition number of well-conditioned matrix"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-6",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 2,
    "title": "Forward and Backward Error Analysis",
    "description": "Analyze forward and backward errors in numerical computations. Implement functions to compute both types of errors and understand their relationship through the condition number.",
    "starterCode": "import numpy as np\n\ndef analyze_errors(A, b, x_computed):\n    \"\"\"\n    Analyze forward and backward errors for solving Ax = b.\n\n    Forward error: ||x_true - x_computed||\n    Backward error: ||b - A·x_computed||\n\n    Parameters:\n    - A: coefficient matrix\n    - b: right-hand side\n    - x_computed: computed solution\n\n    Returns:\n    - (forward_error, backward_error, condition_number)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case\nA = np.array([[10, 7, 8, 7],\n              [7, 5, 6, 5],\n              [8, 6, 10, 9],\n              [7, 5, 9, 10]], dtype=float)\n\nb = np.array([32, 23, 33, 31], dtype=float)\n\n# Compute solution\nx_true = np.linalg.solve(A, b)\n\n# Introduce small error\nx_computed = x_true + np.array([1e-6, -1e-6, 1e-6, -1e-6])\n\nfwd_err, bwd_err, kappa = analyze_errors(A, b, x_computed)\n\nprint(f\"Forward error: {fwd_err:.2e}\")\nprint(f\"Backward error: {bwd_err:.2e}\")\nprint(f\"Condition number: {kappa:.2e}\")\nprint(f\"Error amplification: {fwd_err/bwd_err:.2e}\")",
    "hints": [
      "Forward error measures how far the solution is from true answer",
      "Backward error measures residual ||b - Ax||",
      "Relationship: forward_error ≤ condition_number × backward_error"
    ],
    "solution": "import numpy as np\n\ndef analyze_errors(A, b, x_computed):\n    \"\"\"\n    Analyze forward and backward errors for solving Ax = b.\n\n    Forward error: ||x_true - x_computed||\n    Backward error: ||b - A·x_computed||\n\n    Parameters:\n    - A: coefficient matrix\n    - b: right-hand side\n    - x_computed: computed solution\n\n    Returns:\n    - (forward_error, backward_error, condition_number)\n    \"\"\"\n    # True solution\n    x_true = np.linalg.solve(A, b)\n\n    # Forward error (absolute)\n    forward_error = np.linalg.norm(x_true - x_computed)\n\n    # Backward error (residual)\n    residual = b - A @ x_computed\n    backward_error = np.linalg.norm(residual)\n\n    # Condition number\n    condition_number = np.linalg.cond(A)\n\n    return forward_error, backward_error, condition_number\n\n# Test case: Hilbert matrix (ill-conditioned)\nn = 4\nA = np.array([[1/(i+j+1) for j in range(n)] for i in range(n)])\nb = A @ np.ones(n)  # True solution is [1, 1, 1, 1]\n\nprint(\"Testing with Hilbert matrix (ill-conditioned):\\n\")\n\n# Compute solution\nx_true = np.ones(n)\nx_computed = np.linalg.solve(A, b)\n\n# Analyze errors\nfwd_err, bwd_err, kappa = analyze_errors(A, b, x_computed)\n\nprint(f\"True solution: {x_true}\")\nprint(f\"Computed solution: {x_computed}\")\nprint(f\"\\nForward error: {fwd_err:.2e}\")\nprint(f\"Backward error: {bwd_err:.2e}\")\nprint(f\"Condition number: {kappa:.2e}\")\nprint(f\"Error amplification: {fwd_err/bwd_err:.2e}\")\n\n# Verify relationship\nprint(f\"\\nVerifying: forward_error ≤ κ(A) × backward_error\")\nprint(f\"{fwd_err:.2e} ≤ {kappa * bwd_err:.2e}: {fwd_err <= kappa * bwd_err}\")\n\n# Test with well-conditioned matrix\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nTesting with well-conditioned matrix:\\n\")\n\nA_well = np.array([[4, 1], [1, 3]], dtype=float)\nb_well = np.array([1, 2], dtype=float)\nx_true_well = np.linalg.solve(A_well, b_well)\n\n# Add small error\nx_computed_well = x_true_well + np.array([1e-8, -1e-8])\n\nfwd_err_well, bwd_err_well, kappa_well = analyze_errors(A_well, b_well, x_computed_well)\n\nprint(f\"Forward error: {fwd_err_well:.2e}\")\nprint(f\"Backward error: {bwd_err_well:.2e}\")\nprint(f\"Condition number: {kappa_well:.2e}\")\nprint(f\"Error amplification: {fwd_err_well/bwd_err_well:.2e}\")\nprint(f\"\\nNote: Well-conditioned problem has smaller error amplification!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "Hilbert matrix n=4",
        "expectedOutput": "Large error amplification due to ill-conditioning",
        "isHidden": false,
        "description": "Test error analysis on ill-conditioned system"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-7",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 3,
    "title": "Loss of Significance Detection",
    "description": "Detect and avoid loss of significance in numerical computations. Implement functions that identify when catastrophic cancellation occurs and provide numerically stable alternatives.",
    "starterCode": "import numpy as np\n\ndef safe_subtract(a, b, threshold=1e-10):\n    \"\"\"\n    Detect potential loss of significance in subtraction.\n\n    Parameters:\n    - a, b: numbers to subtract\n    - threshold: relative difference threshold\n\n    Returns:\n    - (result, warning): result and warning flag\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef variance_stable(data):\n    \"\"\"\n    Compute variance using numerically stable algorithm.\n\n    Naive: var = E[X²] - E[X]²  (unstable)\n    Stable: var = E[(X - μ)²]  (stable)\n\n    Parameters:\n    - data: array of numbers\n\n    Returns:\n    - variance: computed variance\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test loss of significance\nprint(\"Loss of Significance Detection:\\n\")\n\ntest_pairs = [\n    (1.234567890123456, 1.234567890123457),\n    (1e10, 1e10 + 1),\n    (100, 50),\n]\n\nfor a, b in test_pairs:\n    result, warning = safe_subtract(a, b, threshold=0.01)\n    print(f\"{a} - {b} = {result}\")\n    if warning:\n        print(\"  ⚠ WARNING: Potential loss of significance!\")\n    print()\n\n# Test variance computation\nprint(\"=\"*60)\nprint(\"\\nStable Variance Computation:\\n\")\n\n# Data with large mean, small variance\ndata = np.array([1e10, 1e10 + 1, 1e10 + 2, 1e10 + 3, 1e10 + 4])\n\nvar_stable = variance_stable(data)\nvar_numpy = np.var(data)\n\nprint(f\"Stable variance: {var_stable:.6f}\")\nprint(f\"NumPy variance: {var_numpy:.6f}\")",
    "hints": [
      "Loss of significance occurs when subtracting nearly equal numbers",
      "Check relative difference: |a - b| / max(|a|, |b|)",
      "For variance: use two-pass algorithm or online algorithm"
    ],
    "solution": "import numpy as np\n\ndef safe_subtract(a, b, threshold=1e-10):\n    \"\"\"\n    Detect potential loss of significance in subtraction.\n\n    Parameters:\n    - a, b: numbers to subtract\n    - threshold: relative difference threshold\n\n    Returns:\n    - (result, warning): result and warning flag\n    \"\"\"\n    result = a - b\n\n    # Check for loss of significance\n    max_magnitude = max(abs(a), abs(b))\n    if max_magnitude > 0:\n        relative_diff = abs(result) / max_magnitude\n        warning = relative_diff < threshold\n    else:\n        warning = False\n\n    return result, warning\n\ndef variance_naive(data):\n    \"\"\"Naive variance computation (unstable).\"\"\"\n    n = len(data)\n    mean = sum(data) / n\n    sum_sq = sum(x**2 for x in data)\n    return sum_sq / n - mean**2\n\ndef variance_stable(data):\n    \"\"\"\n    Compute variance using numerically stable algorithm.\n\n    Naive: var = E[X²] - E[X]²  (unstable)\n    Stable: var = E[(X - μ)²]  (stable)\n\n    Parameters:\n    - data: array of numbers\n\n    Returns:\n    - variance: computed variance\n    \"\"\"\n    n = len(data)\n\n    # First pass: compute mean\n    mean = sum(data) / n\n\n    # Second pass: compute variance\n    variance = sum((x - mean)**2 for x in data) / n\n\n    return variance\n\ndef variance_online(data):\n    \"\"\"Online algorithm (Welford's method) - single pass.\"\"\"\n    n = 0\n    mean = 0.0\n    M2 = 0.0\n\n    for x in data:\n        n += 1\n        delta = x - mean\n        mean += delta / n\n        delta2 = x - mean\n        M2 += delta * delta2\n\n    if n < 2:\n        return 0.0\n    return M2 / n\n\n# Test loss of significance\nprint(\"Loss of Significance Detection:\\n\")\n\ntest_pairs = [\n    (1.234567890123456, 1.234567890123457, \"Nearly equal numbers\"),\n    (1e10, 1e10 + 1, \"Large numbers, small difference\"),\n    (100, 50, \"Well-separated numbers\"),\n]\n\nfor a, b, description in test_pairs:\n    result, warning = safe_subtract(a, b, threshold=0.01)\n    print(f\"{description}:\")\n    print(f\"  {a} - {b} = {result}\")\n    if warning:\n        print(\"  ⚠ WARNING: Potential loss of significance!\")\n    else:\n        print(\"  ✓ No significant cancellation detected\")\n    print()\n\n# Test variance computation\nprint(\"=\"*60)\nprint(\"\\nStable Variance Computation:\\n\")\n\n# Data with large mean, small variance (challenging case)\ndata_large = np.array([1e10, 1e10 + 1, 1e10 + 2, 1e10 + 3, 1e10 + 4])\n\nprint(\"Case 1: Large mean, small variance\")\nprint(f\"Data: [{data_large[0]:.0f}, {data_large[1]:.0f}, ..., {data_large[-1]:.0f}]\\n\")\n\nvar_naive = variance_naive(data_large)\nvar_stable = variance_stable(data_large)\nvar_online = variance_online(data_large)\nvar_numpy = np.var(data_large)\n\nprint(f\"Naive algorithm:  {var_naive:.6f}\")\nprint(f\"Stable algorithm: {var_stable:.6f}\")\nprint(f\"Online algorithm: {var_online:.6f}\")\nprint(f\"NumPy (stable):   {var_numpy:.6f}\")\n\nprint(\"\\nNote: Naive algorithm may suffer from catastrophic cancellation!\")\n\n# Normal case\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nCase 2: Normal data\\n\")\n\ndata_normal = np.array([1, 2, 3, 4, 5])\nvar_naive_norm = variance_naive(data_normal)\nvar_stable_norm = variance_stable(data_normal)\n\nprint(f\"Data: {data_normal}\")\nprint(f\"Naive:  {var_naive_norm:.6f}\")\nprint(f\"Stable: {var_stable_norm:.6f}\")\nprint(f\"NumPy:  {np.var(data_normal):.6f}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "data=[1e10, 1e10+1, 1e10+2, 1e10+3, 1e10+4]",
        "expectedOutput": "Stable algorithm gives correct variance ≈ 2.0",
        "isHidden": false,
        "description": "Test stable variance computation"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-8",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 4,
    "title": "Error Propagation Analysis",
    "description": "Analyze how errors propagate through arithmetic operations and function evaluations. Implement functions to estimate error bounds using differential calculus and compare with actual errors.",
    "starterCode": "import numpy as np\n\ndef estimate_error_propagation(f, grad_f, x, dx):\n    \"\"\"\n    Estimate error propagation using linear approximation.\n\n    For f(x + dx) ≈ f(x) + ∇f(x)·dx\n    Error: |f(x + dx) - f(x)| ≈ ||∇f(x)|| · ||dx||\n\n    Parameters:\n    - f: function (vector -> scalar)\n    - grad_f: gradient function\n    - x: point of evaluation\n    - dx: input perturbation\n\n    Returns:\n    - (estimated_error, actual_error)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef analyze_arithmetic_error(op, a, da, b, db):\n    \"\"\"\n    Analyze error propagation in arithmetic operations.\n\n    Parameters:\n    - op: operation ('+', '-', '*', '/')\n    - a, b: operands\n    - da, db: absolute errors in operands\n\n    Returns:\n    - (result, estimated_error)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function error propagation\nprint(\"Function Error Propagation:\\n\")\n\nf = lambda x: np.sin(x[0]) * np.cos(x[1])\ngrad_f = lambda x: np.array([np.cos(x[0])*np.cos(x[1]),\n                              -np.sin(x[0])*np.sin(x[1])])\n\nx = np.array([np.pi/4, np.pi/3])\ndx = np.array([1e-4, 1e-4])\n\nest_err, act_err = estimate_error_propagation(f, grad_f, x, dx)\nprint(f\"Estimated error: {est_err:.2e}\")\nprint(f\"Actual error: {act_err:.2e}\")\n\n# Test arithmetic operations\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nArithmetic Error Propagation:\\n\")\n\noperations = ['+', '-', '*', '/']\na, da = 100.0, 0.1\nb, db = 50.0, 0.05\n\nfor op in operations:\n    result, error = analyze_arithmetic_error(op, a, da, b, db)\n    print(f\"{a} {op} {b} = {result} ± {error}\")",
    "hints": [
      "Use first-order Taylor expansion for error estimation",
      "Addition/subtraction: errors add",
      "Multiplication: relative errors add",
      "Division: relative errors add"
    ],
    "solution": "import numpy as np\n\ndef estimate_error_propagation(f, grad_f, x, dx):\n    \"\"\"\n    Estimate error propagation using linear approximation.\n\n    For f(x + dx) ≈ f(x) + ∇f(x)·dx\n    Error: |f(x + dx) - f(x)| ≈ ||∇f(x)|| · ||dx||\n\n    Parameters:\n    - f: function (vector -> scalar)\n    - grad_f: gradient function\n    - x: point of evaluation\n    - dx: input perturbation\n\n    Returns:\n    - (estimated_error, actual_error)\n    \"\"\"\n    # Compute gradient\n    gradient = grad_f(x)\n\n    # Estimate error using linear approximation\n    # |Δf| ≈ |∇f · Δx| ≤ ||∇f|| · ||Δx||\n    estimated_error = np.linalg.norm(gradient) * np.linalg.norm(dx)\n\n    # Compute actual error\n    f_x = f(x)\n    f_x_plus_dx = f(x + dx)\n    actual_error = abs(f_x_plus_dx - f_x)\n\n    return estimated_error, actual_error\n\ndef analyze_arithmetic_error(op, a, da, b, db):\n    \"\"\"\n    Analyze error propagation in arithmetic operations.\n\n    Error propagation formulas:\n    - Addition: δ(a+b) ≈ δa + δb\n    - Subtraction: δ(a-b) ≈ δa + δb\n    - Multiplication: δ(a×b)/|a×b| ≈ δa/|a| + δb/|b|\n    - Division: δ(a/b)/|a/b| ≈ δa/|a| + δb/|b|\n\n    Parameters:\n    - op: operation ('+', '-', '*', '/')\n    - a, b: operands\n    - da, db: absolute errors in operands\n\n    Returns:\n    - (result, estimated_error)\n    \"\"\"\n    if op == '+':\n        result = a + b\n        error = da + db\n    elif op == '-':\n        result = a - b\n        error = da + db\n    elif op == '*':\n        result = a * b\n        # Convert to absolute error: δ(ab) ≈ |b|·δa + |a|·δb\n        error = abs(b) * da + abs(a) * db\n    elif op == '/':\n        if abs(b) < 1e-15:\n            return None, float('inf')\n        result = a / b\n        # Convert to absolute error: δ(a/b) ≈ (δa + |a/b|·δb) / |b|\n        error = (da + abs(result) * db) / abs(b)\n    else:\n        raise ValueError(f\"Unknown operation: {op}\")\n\n    return result, error\n\n# Test function error propagation\nprint(\"Function Error Propagation Analysis\\n\")\nprint(\"=\"*60)\n\nf = lambda x: np.sin(x[0]) * np.cos(x[1])\ngrad_f = lambda x: np.array([np.cos(x[0])*np.cos(x[1]),\n                              -np.sin(x[0])*np.sin(x[1])])\n\nx = np.array([np.pi/4, np.pi/3])\ndx = np.array([1e-4, 1e-4])\n\nprint(f\"Function: f(x,y) = sin(x)·cos(y)\")\nprint(f\"Point: x = {x}\")\nprint(f\"Perturbation: dx = {dx}\\n\")\n\nest_err, act_err = estimate_error_propagation(f, grad_f, x, dx)\nprint(f\"Estimated error: {est_err:.6e}\")\nprint(f\"Actual error:    {act_err:.6e}\")\nprint(f\"Ratio (actual/estimated): {act_err/est_err:.4f}\")\n\n# Test with different perturbations\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nError scaling with perturbation size:\\n\")\n\nfor scale in [1e-2, 1e-4, 1e-6, 1e-8]:\n    dx_scaled = np.array([scale, scale])\n    est, act = estimate_error_propagation(f, grad_f, x, dx_scaled)\n    print(f\"||dx|| = {scale:.0e}: estimated = {est:.2e}, actual = {act:.2e}\")\n\n# Test arithmetic operations\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nArithmetic Error Propagation:\\n\")\n\noperations = [('+', 'Addition'), ('-', 'Subtraction'),\n              ('*', 'Multiplication'), ('/', 'Division')]\na, da = 100.0, 0.1\nb, db = 50.0, 0.05\n\nprint(f\"a = {a} ± {da}\")\nprint(f\"b = {b} ± {db}\\n\")\n\nfor op, name in operations:\n    result, error = analyze_arithmetic_error(op, a, da, b, db)\n    rel_error = error / abs(result) * 100 if abs(result) > 1e-15 else float('inf')\n    print(f\"{name:15s}: {a} {op} {b} = {result:.2f} ± {error:.4f} ({rel_error:.3f}%)\")\n\n# Demonstrate accumulation of errors\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nError Accumulation Example:\\n\")\n\n# Compute (a + b) × (a - b) with error propagation\nsum_val, sum_err = analyze_arithmetic_error('+', a, da, b, db)\ndiff_val, diff_err = analyze_arithmetic_error('-', a, da, b, db)\nproduct_val, product_err = analyze_arithmetic_error('*', sum_val, sum_err, diff_val, diff_err)\n\nprint(f\"(a + b) = {sum_val:.2f} ± {sum_err:.4f}\")\nprint(f\"(a - b) = {diff_val:.2f} ± {diff_err:.4f}\")\nprint(f\"(a + b) × (a - b) = {product_val:.2f} ± {product_err:.4f}\")\n\n# Compare with direct computation\ndirect_val = a**2 - b**2\nprint(f\"\\nDirect: a² - b² = {direct_val:.2f}\")\nprint(f\"Error propagated through two paths!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x,y) = sin(x)·cos(y), dx = [1e-4, 1e-4]",
        "expectedOutput": "Estimated error closely matches actual error",
        "isHidden": false,
        "description": "Test error propagation in function evaluation"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-9",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 2,
    "title": "Floating-Point Representation",
    "description": "Understand floating-point representation and its limitations. Implement functions to decompose floating-point numbers into sign, exponent, and mantissa, and demonstrate representation issues.",
    "starterCode": "import numpy as np\nimport struct\n\ndef decompose_float(x):\n    \"\"\"\n    Decompose a float into sign, exponent, and mantissa.\n\n    IEEE 754 double precision:\n    - 1 sign bit\n    - 11 exponent bits (biased by 1023)\n    - 52 mantissa bits\n\n    Parameters:\n    - x: float to decompose\n\n    Returns:\n    - (sign, exponent, mantissa): components\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef test_associativity():\n    \"\"\"\n    Test whether floating-point addition is associative.\n\n    Check if (a + b) + c == a + (b + c)\n\n    Returns:\n    - list of test results\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test decomposition\nprint(\"Floating-Point Decomposition:\\n\")\n\ntest_values = [1.0, -1.0, 0.5, np.pi, 1e308]\nfor x in test_values:\n    sign, exp, mantissa = decompose_float(x)\n    print(f\"{x:.6e}: sign={sign}, exp={exp}, mantissa={mantissa:.16e}\")\n\n# Test associativity\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nAssociativity Test:\\n\")\n\nresults = test_associativity()\nfor result in results:\n    print(result)",
    "hints": [
      "Use struct.unpack to get binary representation",
      "Extract bits using bitwise operations",
      "Test associativity with numbers of very different magnitudes"
    ],
    "solution": "import numpy as np\nimport struct\n\ndef decompose_float(x):\n    \"\"\"\n    Decompose a float into sign, exponent, and mantissa.\n\n    IEEE 754 double precision:\n    - 1 sign bit\n    - 11 exponent bits (biased by 1023)\n    - 52 mantissa bits\n\n    Parameters:\n    - x: float to decompose\n\n    Returns:\n    - (sign, exponent, mantissa): components\n    \"\"\"\n    # Pack as double and unpack as unsigned long long\n    packed = struct.pack('d', x)\n    bits = struct.unpack('Q', packed)[0]\n\n    # Extract components\n    sign = (bits >> 63) & 1\n    exponent = (bits >> 52) & 0x7FF\n    mantissa_bits = bits & 0xFFFFFFFFFFFFF\n\n    # Convert mantissa to decimal (add implicit 1 for normalized numbers)\n    if exponent != 0:\n        mantissa = 1.0 + mantissa_bits / (2**52)\n    else:\n        mantissa = mantissa_bits / (2**52)\n\n    # Unbias exponent\n    exponent_unbiased = exponent - 1023\n\n    return sign, exponent_unbiased, mantissa\n\ndef test_associativity():\n    \"\"\"\n    Test whether floating-point addition is associative.\n\n    Check if (a + b) + c == a + (b + c)\n\n    Returns:\n    - list of test results\n    \"\"\"\n    results = []\n\n    # Test case 1: Large + small + small\n    a, b, c = 1.0, 1e-16, 1e-16\n    left = (a + b) + c\n    right = a + (b + c)\n    results.append(f\"Test 1: ({a} + {b}) + {c}\")\n    results.append(f\"  Left-to-right:  {left:.20f}\")\n    results.append(f\"  Right-to-left:  {right:.20f}\")\n    results.append(f\"  Associative: {left == right}\\n\")\n\n    # Test case 2: Large positive + large negative + small\n    a, b, c = 1e16, -1e16, 1.0\n    left = (a + b) + c\n    right = a + (b + c)\n    results.append(f\"Test 2: ({a:.0e} + {b:.0e}) + {c}\")\n    results.append(f\"  Left-to-right:  {left:.1f}\")\n    results.append(f\"  Right-to-left:  {right:.1f}\")\n    results.append(f\"  Associative: {left == right}\\n\")\n\n    # Test case 3: Normal case\n    a, b, c = 1.0, 2.0, 3.0\n    left = (a + b) + c\n    right = a + (b + c)\n    results.append(f\"Test 3: ({a} + {b}) + {c}\")\n    results.append(f\"  Left-to-right:  {left}\")\n    results.append(f\"  Right-to-left:  {right}\")\n    results.append(f\"  Associative: {left == right}\")\n\n    return results\n\n# Test decomposition\nprint(\"Floating-Point Decomposition\\n\")\nprint(\"=\"*60 + \"\\n\")\n\ntest_values = [1.0, -1.0, 0.5, 2.0, np.pi, 1e-308, 1e308]\nfor x in test_values:\n    sign, exp, mantissa = decompose_float(x)\n\n    # Reconstruct value\n    reconstructed = (-1)**sign * mantissa * (2**exp)\n\n    print(f\"Value: {x:.6e}\")\n    print(f\"  Sign: {'-' if sign else '+'}\")\n    print(f\"  Exponent: {exp}\")\n    print(f\"  Mantissa: {mantissa:.16f}\")\n    print(f\"  Reconstructed: {reconstructed:.6e}\")\n    print(f\"  Match: {abs(x - reconstructed) < 1e-15}\\n\")\n\n# Test associativity\nprint(\"=\"*60)\nprint(\"\\nFloating-Point Associativity Test\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nresults = test_associativity()\nfor result in results:\n    print(result)\n\n# Demonstrate representable numbers\nprint(\"=\"*60)\nprint(\"\\nRepresentable Numbers Around 1.0:\\n\")\n\neps = np.finfo(float).eps\nprint(f\"Machine epsilon: {eps:.2e}\\n\")\n\ntest_additions = [\n    (1.0, eps/2, \"1.0 + ε/2\"),\n    (1.0, eps, \"1.0 + ε\"),\n    (1.0, 2*eps, \"1.0 + 2ε\"),\n]\n\nfor base, delta, description in test_additions:\n    result = base + delta\n    print(f\"{description}:\")\n    print(f\"  Result: {result:.20f}\")\n    print(f\"  Equal to {base}: {result == base}\\n\")\n\nprint(\"All tests passed!\")",
    "testCases": [
      {
        "input": "x = 1.0",
        "expectedOutput": "sign=0, exponent=0, mantissa=1.0",
        "isHidden": false,
        "description": "Decompose 1.0 into IEEE 754 components"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-10",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 3,
    "title": "Interval Arithmetic",
    "description": "Implement interval arithmetic to bound rounding errors rigorously. Create an Interval class that performs arithmetic operations while tracking upper and lower bounds, providing guaranteed error bounds.",
    "starterCode": "import numpy as np\n\nclass Interval:\n    \"\"\"\n    Interval arithmetic for rigorous error bounds.\n\n    Represents a number as [lower, upper] bound.\n    \"\"\"\n\n    def __init__(self, lower, upper=None):\n        \"\"\"\n        Initialize interval.\n\n        Parameters:\n        - lower: lower bound (or exact value if upper is None)\n        - upper: upper bound\n        \"\"\"\n        # TODO: Implement this method\n        pass\n\n    def __add__(self, other):\n        \"\"\"Add two intervals.\"\"\"\n        # TODO: Implement this method\n        pass\n\n    def __sub__(self, other):\n        \"\"\"Subtract two intervals.\"\"\"\n        # TODO: Implement this method\n        pass\n\n    def __mul__(self, other):\n        \"\"\"Multiply two intervals.\"\"\"\n        # TODO: Implement this method\n        pass\n\n    def __truediv__(self, other):\n        \"\"\"Divide two intervals.\"\"\"\n        # TODO: Implement this method\n        pass\n\n    def width(self):\n        \"\"\"Return width of interval.\"\"\"\n        # TODO: Implement this method\n        pass\n\n    def __repr__(self):\n        return f\"[{self.lower}, {self.upper}]\"\n\n# Test interval arithmetic\nprint(\"Interval Arithmetic:\\n\")\n\nx = Interval(1.0, 1.1)\ny = Interval(2.0, 2.1)\n\nprint(f\"x = {x}\")\nprint(f\"y = {y}\")\nprint(f\"x + y = {x + y}\")\nprint(f\"x - y = {x - y}\")\nprint(f\"x × y = {x * y}\")\nprint(f\"x / y = {x / y}\")",
    "hints": [
      "For addition: [a,b] + [c,d] = [a+c, b+d]",
      "For multiplication: consider all four products",
      "Use numpy.nextafter for rigorous bounds",
      "Division requires special handling when 0 is in the interval"
    ],
    "solution": "import numpy as np\n\nclass Interval:\n    \"\"\"\n    Interval arithmetic for rigorous error bounds.\n\n    Represents a number as [lower, upper] bound.\n    \"\"\"\n\n    def __init__(self, lower, upper=None):\n        \"\"\"\n        Initialize interval.\n\n        Parameters:\n        - lower: lower bound (or exact value if upper is None)\n        - upper: upper bound\n        \"\"\"\n        if upper is None:\n            # Point interval\n            self.lower = float(lower)\n            self.upper = float(lower)\n        else:\n            self.lower = float(lower)\n            self.upper = float(upper)\n\n        if self.lower > self.upper:\n            raise ValueError(\"Lower bound must be <= upper bound\")\n\n    def __add__(self, other):\n        \"\"\"Add two intervals: [a,b] + [c,d] = [a+c, b+d]\"\"\"\n        if not isinstance(other, Interval):\n            other = Interval(other)\n\n        # Use directed rounding for rigor\n        lower = np.nextafter(self.lower + other.lower, -np.inf)\n        upper = np.nextafter(self.upper + other.upper, np.inf)\n\n        return Interval(lower, upper)\n\n    def __sub__(self, other):\n        \"\"\"Subtract intervals: [a,b] - [c,d] = [a-d, b-c]\"\"\"\n        if not isinstance(other, Interval):\n            other = Interval(other)\n\n        lower = np.nextafter(self.lower - other.upper, -np.inf)\n        upper = np.nextafter(self.upper - other.lower, np.inf)\n\n        return Interval(lower, upper)\n\n    def __mul__(self, other):\n        \"\"\"Multiply intervals: [a,b] × [c,d] = [min, max] of all products\"\"\"\n        if not isinstance(other, Interval):\n            other = Interval(other)\n\n        # Compute all four products\n        products = [\n            self.lower * other.lower,\n            self.lower * other.upper,\n            self.upper * other.lower,\n            self.upper * other.upper\n        ]\n\n        lower = np.nextafter(min(products), -np.inf)\n        upper = np.nextafter(max(products), np.inf)\n\n        return Interval(lower, upper)\n\n    def __truediv__(self, other):\n        \"\"\"Divide intervals: [a,b] / [c,d] = [a,b] × [1/d, 1/c]\"\"\"\n        if not isinstance(other, Interval):\n            other = Interval(other)\n\n        # Check for division by zero\n        if other.lower <= 0 <= other.upper:\n            raise ValueError(\"Division by interval containing zero\")\n\n        # Compute reciprocal\n        recip_lower = 1.0 / other.upper\n        recip_upper = 1.0 / other.lower\n\n        return self * Interval(recip_lower, recip_upper)\n\n    def width(self):\n        \"\"\"Return width of interval.\"\"\"\n        return self.upper - self.lower\n\n    def midpoint(self):\n        \"\"\"Return midpoint of interval.\"\"\"\n        return (self.lower + self.upper) / 2\n\n    def contains(self, value):\n        \"\"\"Check if value is in interval.\"\"\"\n        return self.lower <= value <= self.upper\n\n    def __repr__(self):\n        if self.lower == self.upper:\n            return f\"[{self.lower}]\"\n        return f\"[{self.lower:.10f}, {self.upper:.10f}]\"\n\n# Test interval arithmetic\nprint(\"Interval Arithmetic\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nx = Interval(1.0, 1.1)\ny = Interval(2.0, 2.1)\n\nprint(f\"x = {x}\")\nprint(f\"y = {y}\\n\")\n\noperations = [\n    (\"x + y\", x + y),\n    (\"x - y\", x - y),\n    (\"x × y\", x * y),\n    (\"x / y\", x / y),\n]\n\nfor name, result in operations:\n    print(f\"{name} = {result}\")\n    print(f\"  Width: {result.width():.10e}\")\n    print(f\"  Midpoint: {result.midpoint():.10f}\\n\")\n\n# Demonstrate error accumulation\nprint(\"=\"*60)\nprint(\"\\nError Accumulation Example\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Compute 1/3 + 1/3 + 1/3 with interval arithmetic\none_third = Interval(1.0) / Interval(3.0)\nprint(f\"1/3 ≈ {one_third}\")\nprint(f\"Width: {one_third.width():.2e}\\n\")\n\nsum_result = one_third + one_third + one_third\nprint(f\"1/3 + 1/3 + 1/3 = {sum_result}\")\nprint(f\"Contains 1.0: {sum_result.contains(1.0)}\")\nprint(f\"Width: {sum_result.width():.2e}\\n\")\n\n# Compare with exact computation\nexact = 1.0\nprint(f\"Exact value: {exact}\")\nprint(f\"Floating-point error: {abs(sum_result.midpoint() - exact):.2e}\")\nprint(f\"Interval guarantees: {sum_result.lower} ≤ 1.0 ≤ {sum_result.upper}\")\n\n# Demonstrate dependency problem\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nDependency Problem\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nx_interval = Interval(0.9, 1.1)\n\n# x - x should be 0, but interval arithmetic gives:\ndiff = x_interval - x_interval\nprint(f\"x = {x_interval}\")\nprint(f\"x - x = {diff}\")\nprint(f\"Contains 0: {diff.contains(0.0)}\")\nprint(\"\\nNote: x - x is not [0,0] due to interval dependency!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "x=[1.0,1.1], y=[2.0,2.1]",
        "expectedOutput": "x + y = [3.0, 3.2], x × y = [2.0, 2.31]",
        "isHidden": false,
        "description": "Test interval arithmetic operations"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-11",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 4,
    "title": "Wilkinson Polynomial Sensitivity",
    "description": "Analyze the extreme sensitivity of the Wilkinson polynomial to coefficient perturbations. Demonstrate how small changes in coefficients can drastically change the roots, illustrating ill-conditioning in polynomial root-finding.",
    "starterCode": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef wilkinson_polynomial(n=20):\n    \"\"\"\n    Generate Wilkinson polynomial coefficients.\n\n    W(x) = (x-1)(x-2)...(x-n)\n\n    Parameters:\n    - n: degree of polynomial\n\n    Returns:\n    - coefficients: polynomial coefficients\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef perturb_coefficient(coeffs, index, perturbation):\n    \"\"\"\n    Perturb a single coefficient.\n\n    Parameters:\n    - coeffs: polynomial coefficients\n    - index: index to perturb\n    - perturbation: amount to add\n\n    Returns:\n    - perturbed_coeffs: new coefficients\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef find_roots(coeffs):\n    \"\"\"\n    Find polynomial roots.\n\n    Parameters:\n    - coeffs: polynomial coefficients\n\n    Returns:\n    - roots: computed roots\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Generate Wilkinson polynomial\nn = 20\ncoeffs = wilkinson_polynomial(n)\n\n# Find roots of original polynomial\nroots_original = find_roots(coeffs)\n\n# Perturb one coefficient slightly\nperturbation = 1e-10\ncoeffs_perturbed = perturb_coefficient(coeffs, n//2, perturbation)\nroots_perturbed = find_roots(coeffs_perturbed)\n\nprint(f\"Wilkinson polynomial W_{n}(x)\\n\")\nprint(f\"Perturbation: {perturbation:.2e} added to coefficient {n//2}\\n\")\nprint(f\"True roots: {list(range(1, n+1))}\\n\")\nprint(f\"Max root error: {max(abs(roots_original - np.arange(1, n+1))):.2e}\")",
    "hints": [
      "Use numpy.poly to generate coefficients from roots",
      "Use numpy.roots to find roots",
      "Perturb the middle coefficient for dramatic effect",
      "Plot roots in complex plane to visualize"
    ],
    "solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef wilkinson_polynomial(n=20):\n    \"\"\"\n    Generate Wilkinson polynomial coefficients.\n\n    W(x) = (x-1)(x-2)...(x-n)\n\n    Parameters:\n    - n: degree of polynomial\n\n    Returns:\n    - coefficients: polynomial coefficients\n    \"\"\"\n    # Generate polynomial from roots 1, 2, ..., n\n    roots = np.arange(1, n + 1)\n    coeffs = np.poly(roots)\n    return coeffs\n\ndef perturb_coefficient(coeffs, index, perturbation):\n    \"\"\"\n    Perturb a single coefficient.\n\n    Parameters:\n    - coeffs: polynomial coefficients\n    - index: index to perturb\n    - perturbation: amount to add\n\n    Returns:\n    - perturbed_coeffs: new coefficients\n    \"\"\"\n    perturbed = coeffs.copy()\n    perturbed[index] += perturbation\n    return perturbed\n\ndef find_roots(coeffs):\n    \"\"\"\n    Find polynomial roots.\n\n    Parameters:\n    - coeffs: polynomial coefficients\n\n    Returns:\n    - roots: computed roots\n    \"\"\"\n    return np.roots(coeffs)\n\n# Generate Wilkinson polynomial\nn = 20\ncoeffs = wilkinson_polynomial(n)\n\nprint(f\"Wilkinson Polynomial Sensitivity Analysis\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(f\"W_{n}(x) = (x-1)(x-2)...(x-{n})\\n\")\nprint(f\"Polynomial degree: {n}\")\nprint(f\"Number of coefficients: {len(coeffs)}\\n\")\n\n# Find roots of original polynomial\ntrue_roots = np.arange(1, n + 1)\nroots_original = find_roots(coeffs)\n\n# Sort roots by real part\nroots_original_sorted = np.sort(roots_original)\n\nprint(\"Original polynomial roots:\\n\")\nprint(\"True roots:\", true_roots)\nprint(\"Computed roots (real parts):\", roots_original_sorted.real[:10], \"...\")\n\n# Compute error in original\nerrors_original = []\nfor i in range(n):\n    min_error = min(abs(roots_original_sorted[i] - j) for j in true_roots)\n    errors_original.append(min_error)\n\nprint(f\"\\nMax error in original: {max(errors_original):.2e}\")\nprint(f\"Mean error in original: {np.mean(errors_original):.2e}\\n\")\n\n# Perturb one coefficient\nprint(\"=\"*60)\nprint(\"\\nPerturbing single coefficient\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nperturbation = 2**(-23)  # Roughly single-precision epsilon\nperturb_index = n - 10  # Perturb x^10 coefficient\n\nprint(f\"Adding {perturbation:.2e} to coefficient of x^{perturb_index}\")\nprint(f\"Relative perturbation: {abs(perturbation/coeffs[perturb_index]):.2e}\\n\")\n\ncoeffs_perturbed = perturb_coefficient(coeffs, perturb_index, perturbation)\nroots_perturbed = find_roots(coeffs_perturbed)\nroots_perturbed_sorted = np.sort_complex(roots_perturbed)\n\n# Analyze perturbed roots\nprint(\"Perturbed polynomial roots:\\n\")\n\nreal_roots = []\ncomplex_roots = []\n\nfor root in roots_perturbed:\n    if abs(root.imag) < 1e-6:\n        real_roots.append(root.real)\n    else:\n        complex_roots.append(root)\n\nprint(f\"Real roots: {len(real_roots)}\")\nprint(f\"Complex roots: {len(complex_roots)}\\n\")\n\nif real_roots:\n    print(f\"Real roots: {sorted(real_roots)[:10]} ...\\n\")\n\nif complex_roots:\n    print(\"Sample complex roots:\")\n    for root in complex_roots[:5]:\n        print(f\"  {root.real:.4f} + {root.imag:.4f}i\")\n    print()\n\n# Compute condition number\nprint(\"=\"*60)\nprint(\"\\nCondition Number Analysis\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Estimate condition number of root finding\nmax_root_change = 0\nfor i in range(min(10, len(roots_original))):\n    root_change = min(abs(roots_perturbed[j] - roots_original[i])\n                     for j in range(len(roots_perturbed)))\n    max_root_change = max(max_root_change, root_change)\n\ncoeff_change = perturbation / np.linalg.norm(coeffs)\nroot_change_norm = max_root_change / np.linalg.norm(true_roots)\n\ncondition_estimate = root_change_norm / coeff_change\n\nprint(f\"Relative coefficient change: {coeff_change:.2e}\")\nprint(f\"Relative root change: {root_change_norm:.2e}\")\nprint(f\"Estimated condition number: {condition_estimate:.2e}\\n\")\n\nprint(\"Interpretation: Wilkinson polynomial is extremely ill-conditioned!\")\nprint(f\"Tiny perturbation ({perturbation:.2e}) causes roots to become complex.\\n\")\n\n# Visualize roots\nprint(\"=\"*60)\nprint(\"\\nVisualization: Roots in Complex Plane\\n\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Original roots\nax1.scatter(roots_original.real, roots_original.imag, c='blue', s=50, alpha=0.6)\nax1.scatter(true_roots, np.zeros_like(true_roots), c='red', s=100,\n           marker='x', linewidths=2, label='True roots')\nax1.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nax1.set_xlabel('Real')\nax1.set_ylabel('Imaginary')\nax1.set_title('Original Wilkinson Polynomial')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Perturbed roots\nax2.scatter(roots_perturbed.real, roots_perturbed.imag, c='green', s=50, alpha=0.6)\nax2.scatter(true_roots, np.zeros_like(true_roots), c='red', s=100,\n           marker='x', linewidths=2, label='True roots')\nax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nax2.set_xlabel('Real')\nax2.set_ylabel('Imaginary')\nax2.set_title(f'Perturbed (coeff {perturb_index} + {perturbation:.2e})')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('/tmp/wilkinson_roots.png', dpi=150, bbox_inches='tight')\nprint(\"Plot saved to /tmp/wilkinson_roots.png\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "n=20, perturbation=2^(-23)",
        "expectedOutput": "Small perturbation causes complex roots to appear",
        "isHidden": false,
        "description": "Demonstrate Wilkinson polynomial sensitivity"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-12",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 3,
    "title": "Significant Digits and Rounding",
    "description": "Implement functions to determine significant digits and perform proper rounding. Create utilities to analyze how many digits are trustworthy in numerical results and round appropriately.",
    "starterCode": "import numpy as np\n\ndef count_significant_digits(value, true_value):\n    \"\"\"\n    Count number of significant digits in approximation.\n\n    Parameters:\n    - value: approximate value\n    - true_value: true value\n\n    Returns:\n    - significant_digits: number of correct significant digits\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef round_to_significant_digits(value, n_digits):\n    \"\"\"\n    Round value to n significant digits.\n\n    Parameters:\n    - value: value to round\n    - n_digits: number of significant digits\n\n    Returns:\n    - rounded_value: value rounded to n digits\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef format_with_uncertainty(value, uncertainty):\n    \"\"\"\n    Format value with uncertainty using proper significant figures.\n\n    Example: 123.456 ± 0.078 -> \"123.46 ± 0.08\"\n\n    Parameters:\n    - value: measured value\n    - uncertainty: uncertainty\n\n    Returns:\n    - formatted_string: properly formatted result\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test significant digits\nprint(\"Significant Digits Analysis:\\n\")\n\ntest_cases = [\n    (3.14159, np.pi, \"π approximation\"),\n    (2.718, np.e, \"e approximation\"),\n    (1.414, np.sqrt(2), \"√2 approximation\"),\n]\n\nfor approx, true, description in test_cases:\n    sig_digits = count_significant_digits(approx, true)\n    print(f\"{description}: {sig_digits} significant digits\")\n\n# Test rounding\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nRounding to Significant Digits:\\n\")\n\nvalues = [123.456, 0.001234, 1234567.89]\nfor val in values:\n    for n in [3, 4, 5]:\n        rounded = round_to_significant_digits(val, n)\n        print(f\"{val} -> {n} digits: {rounded}\")\n\n# Test uncertainty formatting\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nFormatting with Uncertainty:\\n\")\n\nmeasurements = [\n    (123.456789, 0.078),\n    (0.001234567, 0.000045),\n    (1234.5, 123.4),\n]\n\nfor value, uncertainty in measurements:\n    formatted = format_with_uncertainty(value, uncertainty)\n    print(formatted)",
    "hints": [
      "Significant digits: -log10(relative_error)",
      "For rounding: use order of magnitude",
      "Uncertainty determines last significant digit"
    ],
    "solution": "import numpy as np\n\ndef count_significant_digits(value, true_value):\n    \"\"\"\n    Count number of significant digits in approximation.\n\n    Significant digits = -log₁₀(relative error)\n\n    Parameters:\n    - value: approximate value\n    - true_value: true value\n\n    Returns:\n    - significant_digits: number of correct significant digits\n    \"\"\"\n    if abs(true_value) < 1e-15:\n        return 0 if abs(value - true_value) > 1e-15 else float('inf')\n\n    relative_error = abs(value - true_value) / abs(true_value)\n\n    if relative_error < 1e-15:\n        return 15  # Maximum for double precision\n\n    significant_digits = -np.log10(relative_error)\n    return max(0, int(np.floor(significant_digits)))\n\ndef round_to_significant_digits(value, n_digits):\n    \"\"\"\n    Round value to n significant digits.\n\n    Parameters:\n    - value: value to round\n    - n_digits: number of significant digits\n\n    Returns:\n    - rounded_value: value rounded to n digits\n    \"\"\"\n    if value == 0:\n        return 0.0\n\n    # Determine order of magnitude\n    magnitude = np.floor(np.log10(abs(value)))\n\n    # Round to n significant digits\n    scale = 10 ** (magnitude - n_digits + 1)\n    rounded = np.round(value / scale) * scale\n\n    return rounded\n\ndef format_with_uncertainty(value, uncertainty):\n    \"\"\"\n    Format value with uncertainty using proper significant figures.\n\n    Rules:\n    1. Uncertainty determines last significant digit\n    2. Value should be rounded to same precision\n    3. Use scientific notation if needed\n\n    Parameters:\n    - value: measured value\n    - uncertainty: uncertainty\n\n    Returns:\n    - formatted_string: properly formatted result\n    \"\"\"\n    if uncertainty <= 0:\n        return f\"{value}\"\n\n    # Determine decimal places from uncertainty\n    # Round uncertainty to 1-2 significant digits\n    unc_magnitude = np.floor(np.log10(uncertainty))\n\n    # Use 2 sig figs for uncertainty if first digit is 1, else 1 sig fig\n    first_digit = int(uncertainty / (10 ** unc_magnitude))\n    n_unc_digits = 2 if first_digit == 1 else 1\n\n    unc_rounded = round_to_significant_digits(uncertainty, n_unc_digits)\n\n    # Determine decimal places\n    if unc_magnitude >= 0:\n        decimal_places = 0\n    else:\n        decimal_places = int(-unc_magnitude) + (n_unc_digits - 1)\n\n    # Format value and uncertainty\n    if abs(value) >= 1000 or abs(value) < 0.01:\n        # Use scientific notation\n        exp = int(np.floor(np.log10(abs(value))))\n        val_scaled = value / (10 ** exp)\n        unc_scaled = unc_rounded / (10 ** exp)\n        return f\"({val_scaled:.{decimal_places}f} ± {unc_scaled:.{decimal_places}f}) × 10^{exp}\"\n    else:\n        # Regular notation\n        return f\"{value:.{decimal_places}f} ± {unc_rounded:.{decimal_places}f}\"\n\n# Test significant digits\nprint(\"Significant Digits Analysis\\n\")\nprint(\"=\"*60 + \"\\n\")\n\ntest_cases = [\n    (3.14159, np.pi, \"π ≈ 3.14159\"),\n    (3.14, np.pi, \"π ≈ 3.14\"),\n    (2.718, np.e, \"e ≈ 2.718\"),\n    (2.7, np.e, \"e ≈ 2.7\"),\n    (1.414, np.sqrt(2), \"√2 ≈ 1.414\"),\n    (1.4, np.sqrt(2), \"√2 ≈ 1.4\"),\n]\n\nfor approx, true, description in test_cases:\n    sig_digits = count_significant_digits(approx, true)\n    rel_error = abs(approx - true) / abs(true)\n    print(f\"{description}:\")\n    print(f\"  Significant digits: {sig_digits}\")\n    print(f\"  Relative error: {rel_error:.2e}\\n\")\n\n# Test rounding\nprint(\"=\"*60)\nprint(\"\\nRounding to Significant Digits\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nvalues = [123.456, 0.001234, 1234567.89, np.pi, -0.0009876]\n\nfor val in values:\n    print(f\"Original: {val}\")\n    for n in [2, 3, 4, 5]:\n        rounded = round_to_significant_digits(val, n)\n        print(f\"  {n} sig. digits: {rounded}\")\n    print()\n\n# Test uncertainty formatting\nprint(\"=\"*60)\nprint(\"\\nFormatting with Uncertainty\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nmeasurements = [\n    (123.456789, 0.078, \"Laboratory measurement\"),\n    (0.001234567, 0.000045, \"Small value\"),\n    (1234.5, 123.4, \"Large uncertainty\"),\n    (299792458.0, 1.2, \"Speed of light (m/s)\"),\n    (6.62607015e-34, 1.2e-42, \"Planck constant\"),\n]\n\nfor value, uncertainty, description in measurements:\n    formatted = format_with_uncertainty(value, uncertainty)\n    print(f\"{description}:\")\n    print(f\"  Raw: {value} ± {uncertainty}\")\n    print(f\"  Formatted: {formatted}\\n\")\n\n# Demonstrate error in calculations\nprint(\"=\"*60)\nprint(\"\\nError Propagation Example\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Measure side of square\nside = 10.5  # cm\nside_uncertainty = 0.2  # cm\n\n# Calculate area\narea = side ** 2\narea_uncertainty = 2 * side * side_uncertainty  # Error propagation\n\nprint(f\"Side length: {format_with_uncertainty(side, side_uncertainty)} cm\")\nprint(f\"Area: {format_with_uncertainty(area, area_uncertainty)} cm²\")\n\nsig_digits_side = count_significant_digits(side, side)  # Perfect measurement\nsig_digits_area = count_significant_digits(area, area)\n\nprint(f\"\\nNote: Uncertainty limits significant digits in result!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "value=3.14159, true_value=π",
        "expectedOutput": "5 significant digits",
        "isHidden": false,
        "description": "Count significant digits in π approximation"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-13",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 2,
    "title": "Horner Method for Polynomial Evaluation",
    "description": "Implement Horner's method for numerically stable polynomial evaluation. Compare the number of operations and numerical stability with naive evaluation.",
    "starterCode": "import numpy as np\n\ndef poly_eval_naive(coeffs, x):\n    \"\"\"\n    Evaluate polynomial naively: a₀ + a₁x + a₂x² + ...\n\n    Parameters:\n    - coeffs: [a₀, a₁, a₂, ...] (increasing degree)\n    - x: evaluation point\n\n    Returns:\n    - result: polynomial value\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef poly_eval_horner(coeffs, x):\n    \"\"\"\n    Evaluate polynomial using Horner's method.\n\n    Horner: ((aₙx + aₙ₋₁)x + ... + a₁)x + a₀\n\n    Parameters:\n    - coeffs: [a₀, a₁, a₂, ...] (increasing degree)\n    - x: evaluation point\n\n    Returns:\n    - result: polynomial value\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef count_operations(method, coeffs):\n    \"\"\"\n    Count multiplications and additions for polynomial evaluation.\n\n    Parameters:\n    - method: 'naive' or 'horner'\n    - coeffs: polynomial coefficients\n\n    Returns:\n    - (mults, adds): operation counts\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test polynomial evaluation\ncoeffs = [1, -3, 2, 1]  # 1 - 3x + 2x² + x³\nx = 2.5\n\nprint(\"Polynomial: p(x) = 1 - 3x + 2x² + x³\\n\")\nprint(f\"Evaluation at x = {x}:\\n\")\n\nresult_naive = poly_eval_naive(coeffs, x)\nresult_horner = poly_eval_horner(coeffs, x)\n\nprint(f\"Naive method:  {result_naive}\")\nprint(f\"Horner method: {result_horner}\")\nprint(f\"NumPy:         {np.polyval(coeffs[::-1], x)}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nOperation Counts:\\n\")\n\nmults_naive, adds_naive = count_operations('naive', coeffs)\nmults_horner, adds_horner = count_operations('horner', coeffs)\n\nprint(f\"Naive:  {mults_naive} multiplications, {adds_naive} additions\")\nprint(f\"Horner: {mults_horner} multiplications, {adds_horner} additions\")",
    "hints": [
      "Horner's method: work from highest degree down",
      "Naive: n(n+1)/2 multiplications for degree n",
      "Horner: n multiplications for degree n"
    ],
    "solution": "import numpy as np\nimport time\n\ndef poly_eval_naive(coeffs, x):\n    \"\"\"\n    Evaluate polynomial naively: a₀ + a₁x + a₂x² + ...\n\n    Parameters:\n    - coeffs: [a₀, a₁, a₂, ...] (increasing degree)\n    - x: evaluation point\n\n    Returns:\n    - result: polynomial value\n    \"\"\"\n    result = 0.0\n    for i, a in enumerate(coeffs):\n        result += a * (x ** i)\n    return result\n\ndef poly_eval_horner(coeffs, x):\n    \"\"\"\n    Evaluate polynomial using Horner's method.\n\n    Horner: ((aₙx + aₙ₋₁)x + ... + a₁)x + a₀\n\n    Parameters:\n    - coeffs: [a₀, a₁, a₂, ...] (increasing degree)\n    - x: evaluation point\n\n    Returns:\n    - result: polynomial value\n    \"\"\"\n    # Start from highest degree coefficient\n    result = 0.0\n    for a in reversed(coeffs):\n        result = result * x + a\n    return result\n\ndef count_operations(method, coeffs):\n    \"\"\"\n    Count multiplications and additions for polynomial evaluation.\n\n    Parameters:\n    - method: 'naive' or 'horner'\n    - coeffs: polynomial coefficients\n\n    Returns:\n    - (mults, adds): operation counts\n    \"\"\"\n    n = len(coeffs) - 1  # degree\n\n    if method == 'naive':\n        # For each term aᵢx^i: i multiplications (for x^i) + 1 (for aᵢ·x^i)\n        # Total: Σᵢ₌₀ⁿ i + n = n(n+1)/2 + n multiplications\n        # n additions to sum terms\n        mults = n * (n + 1) // 2 + n\n        adds = n\n    elif method == 'horner':\n        # n multiplications (by x) and n additions\n        mults = n\n        adds = n\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n\n    return mults, adds\n\n# Test polynomial evaluation\nprint(\"Horner's Method for Polynomial Evaluation\\n\")\nprint(\"=\"*60 + \"\\n\")\n\ncoeffs = [1, -3, 2, 1]  # 1 - 3x + 2x² + x³\nx = 2.5\n\nprint(\"Polynomial: p(x) = 1 - 3x + 2x² + x³\")\nprint(f\"Evaluation at x = {x}\\n\")\n\nresult_naive = poly_eval_naive(coeffs, x)\nresult_horner = poly_eval_horner(coeffs, x)\nresult_numpy = np.polyval(coeffs[::-1], x)\n\nprint(f\"Naive method:  {result_naive:.10f}\")\nprint(f\"Horner method: {result_horner:.10f}\")\nprint(f\"NumPy:         {result_numpy:.10f}\")\n\n# Verify all methods agree\nassert abs(result_naive - result_horner) < 1e-10\nassert abs(result_naive - result_numpy) < 1e-10\nprint(\"\\n✓ All methods agree!\")\n\n# Operation counts\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nOperation Counts\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nmults_naive, adds_naive = count_operations('naive', coeffs)\nmults_horner, adds_horner = count_operations('horner', coeffs)\n\nn = len(coeffs) - 1\nprint(f\"Polynomial degree: {n}\\n\")\n\nprint(f\"Naive method:\")\nprint(f\"  Multiplications: {mults_naive}\")\nprint(f\"  Additions: {adds_naive}\")\nprint(f\"  Total: {mults_naive + adds_naive}\\n\")\n\nprint(f\"Horner's method:\")\nprint(f\"  Multiplications: {mults_horner}\")\nprint(f\"  Additions: {adds_horner}\")\nprint(f\"  Total: {mults_horner + adds_horner}\\n\")\n\nprint(f\"Speedup: {(mults_naive + adds_naive) / (mults_horner + adds_horner):.2f}×\")\n\n# Test with high-degree polynomial\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nHigh-Degree Polynomial Example\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nn_high = 20\ncoeffs_high = np.random.randn(n_high + 1)\nx_high = 1.5\n\nprint(f\"Degree: {n_high}\\n\")\n\n# Time comparisons\nn_trials = 10000\n\nstart = time.time()\nfor _ in range(n_trials):\n    _ = poly_eval_naive(coeffs_high, x_high)\ntime_naive = (time.time() - start) / n_trials\n\nstart = time.time()\nfor _ in range(n_trials):\n    _ = poly_eval_horner(coeffs_high, x_high)\ntime_horner = (time.time() - start) / n_trials\n\nstart = time.time()\nfor _ in range(n_trials):\n    _ = np.polyval(coeffs_high[::-1], x_high)\ntime_numpy = (time.time() - start) / n_trials\n\nprint(f\"Naive:  {time_naive*1e6:.2f} μs\")\nprint(f\"Horner: {time_horner*1e6:.2f} μs\")\nprint(f\"NumPy:  {time_numpy*1e6:.2f} μs\\n\")\n\nprint(f\"Horner speedup over naive: {time_naive/time_horner:.2f}×\")\n\n# Operation count comparison\nmults_naive_high, adds_naive_high = count_operations('naive', coeffs_high)\nmults_horner_high, adds_horner_high = count_operations('horner', coeffs_high)\n\nprint(f\"\\nOperation counts (degree {n_high}):\")\nprint(f\"  Naive:  {mults_naive_high + adds_naive_high} ops\")\nprint(f\"  Horner: {mults_horner_high + adds_horner_high} ops\")\nprint(f\"  Ratio:  {(mults_naive_high + adds_naive_high) / (mults_horner_high + adds_horner_high):.2f}×\")\n\n# Stability demonstration\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nNumerical Stability\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Wilkinson-like polynomial (challenging)\nroots = np.arange(1, 11)\ncoeffs_wilk = np.poly(roots)[::-1]  # Convert to increasing degree\n\nx_test = 5.5\nresult_naive_wilk = poly_eval_naive(coeffs_wilk, x_test)\nresult_horner_wilk = poly_eval_horner(coeffs_wilk, x_test)\n\n# True value\ntrue_value = np.prod(x_test - roots)\n\nprint(f\"Wilkinson-type polynomial at x = {x_test}:\")\nprint(f\"True value:    {true_value:.10e}\")\nprint(f\"Naive:         {result_naive_wilk:.10e}\")\nprint(f\"Horner:        {result_horner_wilk:.10e}\\n\")\n\nerror_naive = abs(result_naive_wilk - true_value)\nerror_horner = abs(result_horner_wilk - true_value)\n\nprint(f\"Naive error:   {error_naive:.2e}\")\nprint(f\"Horner error:  {error_horner:.2e}\")\n\nprint(\"\\nHorner's method: fewer operations AND better stability!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "p(x) = 1 - 3x + 2x² + x³, x = 2.5",
        "expectedOutput": "Horner uses fewer operations than naive",
        "isHidden": false,
        "description": "Compare Horner vs naive polynomial evaluation"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-14",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 4,
    "title": "Numerical Differentiation Error Analysis",
    "description": "Analyze truncation and rounding error tradeoffs in numerical differentiation. Implement various finite difference formulas and find the optimal step size that balances these competing errors.",
    "starterCode": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef forward_difference(f, x, h):\n    \"\"\"\n    Forward difference approximation: f'(x) ≈ (f(x+h) - f(x)) / h\n\n    Truncation error: O(h)\n    Rounding error: O(ε/h)\n\n    Parameters:\n    - f: function\n    - x: point\n    - h: step size\n\n    Returns:\n    - approximation: f'(x)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef central_difference(f, x, h):\n    \"\"\"\n    Central difference approximation: f'(x) ≈ (f(x+h) - f(x-h)) / (2h)\n\n    Truncation error: O(h²)\n    Rounding error: O(ε/h)\n\n    Parameters:\n    - f: function\n    - x: point\n    - h: step size\n\n    Returns:\n    - approximation: f'(x)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef find_optimal_step_size(f, df, x, method='central'):\n    \"\"\"\n    Find optimal step size that minimizes total error.\n\n    Parameters:\n    - f: function\n    - df: true derivative\n    - x: point\n    - method: 'forward' or 'central'\n\n    Returns:\n    - (h_optimal, min_error): optimal step size and minimum error\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function\nf = lambda x: np.sin(x)\ndf = lambda x: np.cos(x)\nx = 1.0\n\nprint(\"Numerical Differentiation Error Analysis\\n\")\nprint(f\"Function: f(x) = sin(x)\")\nprint(f\"Point: x = {x}\")\nprint(f\"True derivative: f'({x}) = {df(x):.10f}\\n\")\n\n# Test various step sizes\nh_values = 10.0 ** np.arange(-16, 0, 0.5)\n\nprint(\"Testing forward difference:\")\nfor h in [1e-2, 1e-4, 1e-8, 1e-12]:\n    approx = forward_difference(f, x, h)\n    error = abs(approx - df(x))\n    print(f\"h = {h:.0e}: approx = {approx:.10f}, error = {error:.2e}\")\n\n# Find optimal step size\nh_opt, min_err = find_optimal_step_size(f, df, x, 'central')\nprint(f\"\\nOptimal step size: {h_opt:.2e}\")\nprint(f\"Minimum error: {min_err:.2e}\")",
    "hints": [
      "Total error = truncation_error + rounding_error",
      "Forward: optimal h ≈ √ε where ε is machine epsilon",
      "Central: optimal h ≈ ε^(1/3)",
      "Plot error vs h to visualize"
    ],
    "solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef forward_difference(f, x, h):\n    \"\"\"\n    Forward difference approximation: f'(x) ≈ (f(x+h) - f(x)) / h\n\n    Truncation error: O(h)\n    Rounding error: O(ε/h)\n\n    Parameters:\n    - f: function\n    - x: point\n    - h: step size\n\n    Returns:\n    - approximation: f'(x)\n    \"\"\"\n    return (f(x + h) - f(x)) / h\n\ndef central_difference(f, x, h):\n    \"\"\"\n    Central difference approximation: f'(x) ≈ (f(x+h) - f(x-h)) / (2h)\n\n    Truncation error: O(h²)\n    Rounding error: O(ε/h)\n\n    Parameters:\n    - f: function\n    - x: point\n    - h: step size\n\n    Returns:\n    - approximation: f'(x)\n    \"\"\"\n    return (f(x + h) - f(x - h)) / (2 * h)\n\ndef find_optimal_step_size(f, df, x, method='central'):\n    \"\"\"\n    Find optimal step size that minimizes total error.\n\n    For forward difference: h_opt ≈ √ε\n    For central difference: h_opt ≈ ε^(1/3)\n\n    Parameters:\n    - f: function\n    - df: true derivative\n    - x: point\n    - method: 'forward' or 'central'\n\n    Returns:\n    - (h_optimal, min_error): optimal step size and minimum error\n    \"\"\"\n    # Test range of step sizes\n    h_values = 10.0 ** np.linspace(-16, -1, 150)\n    errors = []\n\n    for h in h_values:\n        if method == 'forward':\n            approx = forward_difference(f, x, h)\n        elif method == 'central':\n            approx = central_difference(f, x, h)\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n\n        error = abs(approx - df(x))\n        errors.append(error)\n\n    errors = np.array(errors)\n    min_idx = np.argmin(errors)\n\n    return h_values[min_idx], errors[min_idx]\n\n# Test function\nprint(\"Numerical Differentiation Error Analysis\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf = lambda x: np.sin(x)\ndf = lambda x: np.cos(x)\nx = 1.0\n\nprint(f\"Function: f(x) = sin(x)\")\nprint(f\"Point: x = {x}\")\nprint(f\"True derivative: f'({x}) = {df(x):.10f}\\n\")\n\n# Test various step sizes\nprint(\"=\"*60)\nprint(\"\\nForward Difference\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nh_values_test = [1e-1, 1e-2, 1e-4, 1e-8, 1e-12, 1e-15]\n\nfor h in h_values_test:\n    approx = forward_difference(f, x, h)\n    error = abs(approx - df(x))\n    print(f\"h = {h:.0e}: approx = {approx:.10f}, error = {error:.2e}\")\n\nprint(\"\\nNote: error decreases then increases (rounding error dominates)\")\n\n# Central difference\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nCentral Difference\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nfor h in h_values_test:\n    approx = central_difference(f, x, h)\n    error = abs(approx - df(x))\n    print(f\"h = {h:.0e}: approx = {approx:.10f}, error = {error:.2e}\")\n\n# Find optimal step sizes\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nOptimal Step Sizes\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nh_opt_fwd, min_err_fwd = find_optimal_step_size(f, df, x, 'forward')\nh_opt_ctr, min_err_ctr = find_optimal_step_size(f, df, x, 'central')\n\neps = np.finfo(float).eps\ntheoretical_h_fwd = np.sqrt(eps)\ntheoretical_h_ctr = eps ** (1/3)\n\nprint(\"Forward difference:\")\nprint(f\"  Optimal h (experimental): {h_opt_fwd:.2e}\")\nprint(f\"  Optimal h (theoretical):  {theoretical_h_fwd:.2e}\")\nprint(f\"  Minimum error: {min_err_fwd:.2e}\\n\")\n\nprint(\"Central difference:\")\nprint(f\"  Optimal h (experimental): {h_opt_ctr:.2e}\")\nprint(f\"  Optimal h (theoretical):  {theoretical_h_ctr:.2e}\")\nprint(f\"  Minimum error: {min_err_ctr:.2e}\\n\")\n\nprint(f\"Central difference is {min_err_fwd/min_err_ctr:.1f}× more accurate!\")\n\n# Visualization\nprint(\"=\"*60)\nprint(\"\\nGenerating error plots...\\n\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\nh_range = 10.0 ** np.linspace(-16, -1, 150)\n\n# Forward difference errors\nerrors_fwd = []\nfor h in h_range:\n    approx = forward_difference(f, x, h)\n    errors_fwd.append(abs(approx - df(x)))\n\nax1.loglog(h_range, errors_fwd, 'b-', linewidth=2, label='Total error')\nax1.axvline(h_opt_fwd, color='r', linestyle='--', label=f'Optimal h = {h_opt_fwd:.2e}')\n\n# Theoretical error components\ntruncation_fwd = h_range  # O(h)\nrounding_fwd = eps / h_range  # O(ε/h)\n\nax1.loglog(h_range, truncation_fwd, 'g--', alpha=0.6, label='Truncation O(h)')\nax1.loglog(h_range, rounding_fwd, 'm--', alpha=0.6, label='Rounding O(ε/h)')\n\nax1.set_xlabel('Step size h')\nax1.set_ylabel('Absolute error')\nax1.set_title('Forward Difference Error')\nax1.grid(True, alpha=0.3)\nax1.legend()\n\n# Central difference errors\nerrors_ctr = []\nfor h in h_range:\n    approx = central_difference(f, x, h)\n    errors_ctr.append(abs(approx - df(x)))\n\nax2.loglog(h_range, errors_ctr, 'b-', linewidth=2, label='Total error')\nax2.axvline(h_opt_ctr, color='r', linestyle='--', label=f'Optimal h = {h_opt_ctr:.2e}')\n\n# Theoretical error components\ntruncation_ctr = h_range ** 2  # O(h²)\nrounding_ctr = eps / h_range  # O(ε/h)\n\nax2.loglog(h_range, truncation_ctr, 'g--', alpha=0.6, label='Truncation O(h²)')\nax2.loglog(h_range, rounding_ctr, 'm--', alpha=0.6, label='Rounding O(ε/h)')\n\nax2.set_xlabel('Step size h')\nax2.set_ylabel('Absolute error')\nax2.set_title('Central Difference Error')\nax2.grid(True, alpha=0.3)\nax2.legend()\n\nplt.tight_layout()\nplt.savefig('/tmp/differentiation_errors.png', dpi=150, bbox_inches='tight')\nprint(\"Plot saved to /tmp/differentiation_errors.png\")\n\n# Error analysis summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nError Analysis Summary\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(\"Forward Difference:\")\nprint(\"  Truncation error: O(h)\")\nprint(\"  Rounding error: O(ε/h)\")\nprint(\"  Total error: O(h + ε/h)\")\nprint(f\"  Optimal h: √ε ≈ {np.sqrt(eps):.2e}\")\nprint(f\"  Minimum error: √ε ≈ {np.sqrt(eps):.2e}\\n\")\n\nprint(\"Central Difference:\")\nprint(\"  Truncation error: O(h²)\")\nprint(\"  Rounding error: O(ε/h)\")\nprint(\"  Total error: O(h² + ε/h)\")\nprint(f\"  Optimal h: ε^(1/3) ≈ {eps**(1/3):.2e}\")\nprint(f\"  Minimum error: ε^(2/3) ≈ {eps**(2/3):.2e}\\n\")\n\nprint(\"Key insight: Central difference has lower truncation error,\")\nprint(\"allowing larger h and less rounding error!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = sin(x), x = 1.0",
        "expectedOutput": "Optimal h ≈ ε^(1/3) for central difference",
        "isHidden": false,
        "description": "Find optimal step size for numerical differentiation"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-15",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 5,
    "title": "Automatic Error Estimation",
    "description": "Implement automatic error estimation using Richardson extrapolation. Create a function that automatically determines appropriate step sizes and estimates both the result and its error bound.",
    "starterCode": "import numpy as np\n\ndef richardson_extrapolation(f, x, h0, k, method='central'):\n    \"\"\"\n    Richardson extrapolation for numerical differentiation.\n\n    Uses multiple approximations with different step sizes to\n    extrapolate to h=0 and estimate error.\n\n    Parameters:\n    - f: function\n    - x: point\n    - h0: initial step size\n    - k: number of extrapolation levels\n    - method: 'forward' or 'central'\n\n    Returns:\n    - (best_approximation, error_estimate)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef adaptive_differentiation(f, x, tol=1e-6):\n    \"\"\"\n    Adaptive numerical differentiation with automatic error control.\n\n    Automatically refines until estimated error < tolerance.\n\n    Parameters:\n    - f: function\n    - x: point\n    - tol: error tolerance\n\n    Returns:\n    - (derivative, error_estimate, num_evaluations)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test Richardson extrapolation\nprint(\"Richardson Extrapolation:\\n\")\n\nf = lambda x: np.exp(x) * np.sin(x)\ndf_true = lambda x: np.exp(x) * (np.sin(x) + np.cos(x))\nx = 1.0\n\nh0 = 0.1\nfor k in [1, 2, 3, 4]:\n    approx, err_est = richardson_extrapolation(f, x, h0, k)\n    true_err = abs(approx - df_true(x))\n    print(f\"k={k}: approx={approx:.10f}, est_err={err_est:.2e}, true_err={true_err:.2e}\")\n\n# Test adaptive differentiation\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nAdaptive Differentiation:\\n\")\n\ntolerances = [1e-4, 1e-6, 1e-8, 1e-10]\nfor tol in tolerances:\n    result, err_est, n_eval = adaptive_differentiation(f, x, tol)\n    true_err = abs(result - df_true(x))\n    print(f\"tol={tol:.0e}: result={result:.10f}, err={err_est:.2e}, evals={n_eval}\")",
    "hints": [
      "Richardson: R(k+1) = R(k) + (R(k) - R(k-1)) / (2^p - 1)",
      "p = 1 for forward, p = 2 for central difference",
      "Error estimate from difference between levels",
      "Adaptive: iterate Richardson until error < tolerance"
    ],
    "solution": "import numpy as np\n\ndef central_difference(f, x, h):\n    \"\"\"Central difference approximation.\"\"\"\n    return (f(x + h) - f(x - h)) / (2 * h)\n\ndef forward_difference(f, x, h):\n    \"\"\"Forward difference approximation.\"\"\"\n    return (f(x + h) - f(x)) / h\n\ndef richardson_extrapolation(f, x, h0, k, method='central'):\n    \"\"\"\n    Richardson extrapolation for numerical differentiation.\n\n    Uses multiple approximations with different step sizes to\n    extrapolate to h=0 and estimate error.\n\n    Algorithm:\n    1. Compute D(h), D(h/2), D(h/4), ...\n    2. Apply Richardson extrapolation formula\n    3. Estimate error from difference\n\n    Parameters:\n    - f: function\n    - x: point\n    - h0: initial step size\n    - k: number of extrapolation levels\n    - method: 'forward' or 'central'\n\n    Returns:\n    - (best_approximation, error_estimate)\n    \"\"\"\n    # Determine difference function and order\n    if method == 'central':\n        diff_func = central_difference\n        p = 2  # Order of truncation error\n    elif method == 'forward':\n        diff_func = forward_difference\n        p = 1\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n\n    # Compute initial approximations D(0,0), D(1,0), D(2,0), ...\n    # D(i,0) = approximation with h = h0/2^i\n    D = np.zeros((k+1, k+1))\n\n    for i in range(k+1):\n        h = h0 / (2**i)\n        D[i, 0] = diff_func(f, x, h)\n\n    # Richardson extrapolation\n    # D(i,j) = D(i,j-1) + (D(i,j-1) - D(i-1,j-1)) / (4^j - 1) for central\n    # D(i,j) = D(i,j-1) + (D(i,j-1) - D(i-1,j-1)) / (2^j - 1) for forward\n    for j in range(1, k+1):\n        for i in range(j, k+1):\n            D[i, j] = D[i, j-1] + (D[i, j-1] - D[i-1, j-1]) / ((2**(p*j)) - 1)\n\n    # Best approximation is D[k,k]\n    best_approx = D[k, k]\n\n    # Error estimate from difference between last two levels\n    if k > 0:\n        error_estimate = abs(D[k, k] - D[k, k-1])\n    else:\n        error_estimate = abs(D[1, 0] - D[0, 0])\n\n    return best_approx, error_estimate\n\ndef adaptive_differentiation(f, x, tol=1e-6, max_levels=10):\n    \"\"\"\n    Adaptive numerical differentiation with automatic error control.\n\n    Automatically refines until estimated error < tolerance.\n\n    Parameters:\n    - f: function\n    - x: point\n    - tol: error tolerance\n    - max_levels: maximum Richardson levels\n\n    Returns:\n    - (derivative, error_estimate, num_evaluations)\n    \"\"\"\n    h0 = 0.1  # Initial step size\n    num_evaluations = 0\n\n    for k in range(1, max_levels+1):\n        approx, err_est = richardson_extrapolation(f, x, h0, k, 'central')\n\n        # Count function evaluations: 2(k+1) for central difference\n        num_evaluations = 2 * (k + 1)\n\n        if err_est < tol:\n            return approx, err_est, num_evaluations\n\n    # If tolerance not met, return best approximation with warning\n    print(f\"Warning: Tolerance {tol:.2e} not achieved in {max_levels} levels\")\n    return approx, err_est, num_evaluations\n\n# Test Richardson extrapolation\nprint(\"Richardson Extrapolation for Numerical Differentiation\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf = lambda x: np.exp(x) * np.sin(x)\ndf_true = lambda x: np.exp(x) * (np.sin(x) + np.cos(x))\nx = 1.0\n\ntrue_value = df_true(x)\n\nprint(f\"Function: f(x) = e^x · sin(x)\")\nprint(f\"Point: x = {x}\")\nprint(f\"True derivative: f'({x}) = {true_value:.10f}\\n\")\n\nh0 = 0.1\nprint(f\"Initial step size: h0 = {h0}\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nfor k in [1, 2, 3, 4, 5]:\n    approx, err_est = richardson_extrapolation(f, x, h0, k, 'central')\n    true_err = abs(approx - true_value)\n\n    print(f\"Level k={k}:\")\n    print(f\"  Approximation: {approx:.15f}\")\n    print(f\"  Estimated error: {err_est:.2e}\")\n    print(f\"  True error: {true_err:.2e}\")\n    print(f\"  Error ratio: {true_err/err_est:.2f}\")\n    print()\n\nprint(\"Note: Error estimate is conservative (typically overestimates)\")\n\n# Demonstrate Richardson table\nprint(\"=\"*60)\nprint(\"\\nRichardson Extrapolation Table\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nk_max = 4\nD = np.zeros((k_max+1, k_max+1))\n\nfor i in range(k_max+1):\n    h = h0 / (2**i)\n    D[i, 0] = central_difference(f, x, h)\n\nfor j in range(1, k_max+1):\n    for i in range(j, k_max+1):\n        D[i, j] = D[i, j-1] + (D[i, j-1] - D[i-1, j-1]) / (4**j - 1)\n\nprint(\"D[i,j]: i=row (step size h/2^i), j=col (extrapolation level)\\n\")\nprint(\"      j=0           j=1           j=2           j=3           j=4\")\nprint(\"-\" * 70)\n\nfor i in range(k_max+1):\n    row_str = f\"i={i}: \"\n    for j in range(i+1):\n        row_str += f\"{D[i, j]:13.10f} \"\n    print(row_str)\n\nprint(f\"\\nBest estimate: D[{k_max},{k_max}] = {D[k_max, k_max]:.15f}\")\nprint(f\"True value:                 = {true_value:.15f}\")\nprint(f\"Error:                      = {abs(D[k_max, k_max] - true_value):.2e}\")\n\n# Test adaptive differentiation\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nAdaptive Differentiation\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(\"Automatic error control: refines until error < tolerance\\n\")\n\ntolerances = [1e-3, 1e-6, 1e-9, 1e-12]\n\nfor tol in tolerances:\n    result, err_est, n_eval = adaptive_differentiation(f, x, tol)\n    true_err = abs(result - true_value)\n\n    print(f\"Tolerance: {tol:.0e}\")\n    print(f\"  Result: {result:.15f}\")\n    print(f\"  Est. error: {err_est:.2e}\")\n    print(f\"  True error: {true_err:.2e}\")\n    print(f\"  Evaluations: {n_eval}\")\n    print(f\"  Tolerance met: {err_est <= tol}\")\n    print()\n\n# Compare methods\nprint(\"=\"*60)\nprint(\"\\nMethod Comparison\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nh_simple = 1e-5\n\n# Simple central difference\nsimple = central_difference(f, x, h_simple)\nsimple_err = abs(simple - true_value)\n\n# Richardson extrapolation\nrich, rich_err_est = richardson_extrapolation(f, x, 0.1, 4, 'central')\nrich_true_err = abs(rich - true_value)\n\n# Adaptive\nadapt, adapt_err, n_eval = adaptive_differentiation(f, x, 1e-10)\nadapt_true_err = abs(adapt - true_value)\n\nprint(\"Simple central difference (h=1e-5):\")\nprint(f\"  Result: {simple:.15f}\")\nprint(f\"  Error: {simple_err:.2e}\\n\")\n\nprint(\"Richardson extrapolation (k=4):\")\nprint(f\"  Result: {rich:.15f}\")\nprint(f\"  Est. error: {rich_err_est:.2e}\")\nprint(f\"  True error: {rich_true_err:.2e}\\n\")\n\nprint(\"Adaptive (tol=1e-10):\")\nprint(f\"  Result: {adapt:.15f}\")\nprint(f\"  Est. error: {adapt_err:.2e}\")\nprint(f\"  True error: {adapt_true_err:.2e}\")\nprint(f\"  Evaluations: {n_eval}\\n\")\n\nprint(\"Richardson extrapolation provides:\")\nprint(\"  ✓ Higher accuracy\")\nprint(\"  ✓ Automatic error estimation\")\nprint(\"  ✓ No need to tune step size!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = e^x·sin(x), k=4",
        "expectedOutput": "Error estimate matches true error within factor of 10",
        "isHidden": false,
        "description": "Test Richardson extrapolation for differentiation"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-16",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 3,
    "title": "Summation Algorithm Comparison",
    "description": "Compare different summation algorithms (naive, pairwise, Kahan) for accuracy and performance. Implement and benchmark multiple summation strategies on challenging test cases.",
    "starterCode": "import numpy as np\nimport time\n\ndef naive_sum(data):\n    \"\"\"Simple left-to-right summation.\"\"\"\n    # TODO: Implement this function\n    pass\n\ndef pairwise_sum(data):\n    \"\"\"\n    Pairwise summation (recursive).\n\n    Sum pairs: (a+b) + (c+d) + ...\n    Reduces error from O(nε) to O(log n · ε)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef kahan_sum(data):\n    \"\"\"Kahan compensated summation.\"\"\"\n    # TODO: Implement this function\n    pass\n\ndef sorted_sum(data):\n    \"\"\"\n    Sum in increasing order of magnitude.\n    Helps reduce cancellation errors.\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef compare_summation_methods(data, true_sum=None):\n    \"\"\"\n    Compare all summation methods.\n\n    Parameters:\n    - data: array to sum\n    - true_sum: true sum (if known)\n\n    Returns:\n    - results: dictionary of method -> (sum, error, time)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case 1: Many small numbers\nprint(\"Test 1: Large + many small\\n\")\ndata1 = [1.0] + [1e-10] * 100000\n\nresults1 = compare_summation_methods(data1)\nfor method, (result, error, elapsed) in results1.items():\n    print(f\"{method:15s}: {result:.15f} ({elapsed*1000:.2f} ms)\")",
    "hints": [
      "Pairwise: recursively split array and sum pairs",
      "Sorted: sort by absolute value before summing",
      "Use high precision (Decimal) for true sum",
      "Benchmark with time.perf_counter()"
    ],
    "solution": "import numpy as np\nimport time\nfrom decimal import Decimal, getcontext\n\ndef naive_sum(data):\n    \"\"\"Simple left-to-right summation.\"\"\"\n    total = 0.0\n    for x in data:\n        total += x\n    return total\n\ndef pairwise_sum(data):\n    \"\"\"\n    Pairwise summation (recursive).\n\n    Sum pairs: (a+b) + (c+d) + ...\n    Reduces error from O(nε) to O(log n · ε)\n    \"\"\"\n    data = np.asarray(data)\n    n = len(data)\n\n    if n <= 128:\n        # Base case: use naive summation\n        return np.sum(data)\n\n    # Recursive case: split and sum\n    mid = n // 2\n    return pairwise_sum(data[:mid]) + pairwise_sum(data[mid:])\n\ndef kahan_sum(data):\n    \"\"\"Kahan compensated summation.\"\"\"\n    s = 0.0\n    c = 0.0\n\n    for x in data:\n        y = x - c\n        t = s + y\n        c = (t - s) - y\n        s = t\n\n    return s\n\ndef sorted_sum(data):\n    \"\"\"\n    Sum in increasing order of magnitude.\n    Helps reduce cancellation errors.\n    \"\"\"\n    # Sort by absolute value\n    sorted_data = sorted(data, key=abs)\n    return naive_sum(sorted_data)\n\ndef true_sum_high_precision(data):\n    \"\"\"Compute true sum using high precision arithmetic.\"\"\"\n    getcontext().prec = 100\n    total = Decimal(0)\n    for x in data:\n        total += Decimal(str(float(x)))\n    return float(total)\n\ndef compare_summation_methods(data, true_sum=None):\n    \"\"\"\n    Compare all summation methods.\n\n    Parameters:\n    - data: array to sum\n    - true_sum: true sum (if known)\n\n    Returns:\n    - results: dictionary of method -> (sum, error, time)\n    \"\"\"\n    if true_sum is None:\n        true_sum = true_sum_high_precision(data)\n\n    methods = {\n        'Naive': naive_sum,\n        'Pairwise': pairwise_sum,\n        'Kahan': kahan_sum,\n        'Sorted': sorted_sum,\n        'NumPy': lambda d: np.sum(d),\n    }\n\n    results = {}\n\n    for name, method in methods.items():\n        start = time.perf_counter()\n        result = method(data)\n        elapsed = time.perf_counter() - start\n\n        error = abs(result - true_sum)\n        results[name] = (result, error, elapsed)\n\n    return results, true_sum\n\n# Test cases\nprint(\"Summation Algorithm Comparison\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Test case 1: Large value + many small values\nprint(\"Test 1: Large + many small numbers\\n\")\nprint(\"Data: [1.0] + [1e-10] * 100000\\n\")\n\ndata1 = np.array([1.0] + [1e-10] * 100000)\nresults1, true1 = compare_summation_methods(data1)\n\nprint(f\"True sum (high precision): {true1:.15f}\\n\")\nprint(f\"{'Method':<15} {'Result':<20} {'Error':<12} {'Time (ms)':<10}\")\nprint(\"-\" * 60)\n\nfor method, (result, error, elapsed) in sorted(results1.items(), key=lambda x: x[1][1]):\n    print(f\"{method:<15} {result:<20.15f} {error:<12.2e} {elapsed*1000:<10.3f}\")\n\n# Test case 2: Alternating signs\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nTest 2: Alternating signs\\n\")\nprint(\"Data: [1.0, -1.0, 1e-10] * 10000\\n\")\n\ndata2 = np.array([1.0, -1.0, 1e-10] * 10000)\nresults2, true2 = compare_summation_methods(data2)\n\nprint(f\"True sum (high precision): {true2:.15e}\\n\")\nprint(f\"{'Method':<15} {'Result':<20} {'Error':<12} {'Time (ms)':<10}\")\nprint(\"-\" * 60)\n\nfor method, (result, error, elapsed) in sorted(results2.items(), key=lambda x: x[1][1]):\n    print(f\"{method:<15} {result:<20.15e} {error:<12.2e} {elapsed*1000:<10.3f}\")\n\n# Test case 3: Random numbers\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nTest 3: Random numbers (mixed magnitudes)\\n\")\n\nnp.random.seed(42)\ndata3 = np.concatenate([\n    np.random.randn(1000) * 1e10,\n    np.random.randn(1000) * 1e-10,\n    np.random.randn(1000),\n])\n\nprint(f\"Data: {len(data3)} random numbers with varying magnitudes\\n\")\n\nresults3, true3 = compare_summation_methods(data3)\n\nprint(f\"True sum (high precision): {true3:.10e}\\n\")\nprint(f\"{'Method':<15} {'Result':<20} {'Error':<12} {'Time (ms)':<10}\")\nprint(\"-\" * 60)\n\nfor method, (result, error, elapsed) in sorted(results3.items(), key=lambda x: x[1][1]):\n    print(f\"{method:<15} {result:<20.10e} {error:<12.2e} {elapsed*1000:<10.3f}\")\n\n# Performance benchmark with large array\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nPerformance Benchmark (1,000,000 elements)\\n\")\nprint(\"=\"*60 + \"\\n\")\n\ndata_large = np.random.randn(1000000)\n\nmethods_bench = {\n    'Naive': naive_sum,\n    'Pairwise': pairwise_sum,\n    'Kahan': kahan_sum,\n    'NumPy': lambda d: np.sum(d),\n}\n\nprint(f\"{'Method':<15} {'Time (ms)':<12} {'Speedup vs Naive':<15}\")\nprint(\"-\" * 45)\n\ntimes = {}\nfor name, method in methods_bench.items():\n    start = time.perf_counter()\n    _ = method(data_large)\n    elapsed = time.perf_counter() - start\n    times[name] = elapsed\n\nnaive_time = times['Naive']\nfor name, elapsed in sorted(times.items(), key=lambda x: x[1]):\n    speedup = naive_time / elapsed\n    print(f\"{name:<15} {elapsed*1000:<12.2f} {speedup:<15.2f}×\")\n\n# Summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nSummary\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(\"Accuracy (best to worst):\")\nprint(\"  1. Kahan: Best accuracy, handles challenging cases\")\nprint(\"  2. Sorted: Good for reducing cancellation\")\nprint(\"  3. Pairwise: Better than naive, used by NumPy\")\nprint(\"  4. Naive: Worst accuracy, O(nε) error\\n\")\n\nprint(\"Performance (fastest to slowest):\")\nprint(\"  1. NumPy: Optimized C implementation\")\nprint(\"  2. Naive: Simple loop, cache-friendly\")\nprint(\"  3. Pairwise: Recursive overhead\")\nprint(\"  4. Kahan: Extra operations per element\\n\")\n\nprint(\"Recommendations:\")\nprint(\"  • Default: Use NumPy (good accuracy + performance)\")\nprint(\"  • Need high accuracy: Use Kahan\")\nprint(\"  • Large arrays: NumPy pairwise\")\nprint(\"  • Mixed magnitudes: Sorted or Kahan\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "data=[1.0] + [1e-10]*100000",
        "expectedOutput": "Kahan sum has smallest error",
        "isHidden": false,
        "description": "Compare summation algorithms on challenging data"
      }
    ],
    "language": "python"
  }
]
