[
  {
    "id": "math402-quiz-6a",
    "subjectId": "math402",
    "topicId": "math402-topic-6",
    "title": "Iterative Methods for Linear Systems - Fundamentals",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "The Jacobi method updates each component of x^{(k+1)} using:",
        "options": [
          "The inverse of the matrix",
          "All components of x^{(k)} only",
          "The residual vector",
          "A mixture of x^{(k)} and x^{(k+1)} components"
        ],
        "correctAnswer": 1,
        "explanation": "Jacobi simultaneously updates all components using only old values: x_i^{(k+1)} = (b_i - Σ_{j≠i} a_{ij}x_j^{(k)})/a_{ii}. This allows parallel implementation."
      },
      {
        "id": "q2",
        "type": "multiple_choice",
        "prompt": "The Gauss-Seidel method differs from Jacobi by:",
        "options": [
          "Using updated components x^{(k+1)} as soon as they are computed",
          "Using partial pivoting",
          "Being applicable to more matrices",
          "Requiring fewer iterations"
        ],
        "correctAnswer": 0,
        "explanation": "Gauss-Seidel uses x_i^{(k+1)} values immediately in computing subsequent components: x_i^{(k+1)} = (b_i - Σ_{j<i} a_{ij}x_j^{(k+1)} - Σ_{j>i} a_{ij}x_j^{(k)})/a_{ii}."
      },
      {
        "id": "q3",
        "type": "code_output",
        "prompt": "What is the output after one Jacobi iteration?",
        "codeSnippet": "import numpy as np\nA = np.array([[4.0, 1.0], [1.0, 3.0]])\nb = np.array([5.0, 4.0])\nx_old = np.array([0.0, 0.0])\nx_new = np.zeros(2)\nx_new[0] = (b[0] - A[0,1]*x_old[1]) / A[0,0]\nx_new[1] = (b[1] - A[1,0]*x_old[0]) / A[1,1]\nprint(f'{x_new[0]:.2f}')",
        "correctAnswer": "1.25",
        "explanation": "Jacobi iteration: x_new[0] = (5 - 1·0)/4 = 1.25, x_new[1] = (4 - 1·0)/3 ≈ 1.33. Uses only old values."
      },
      {
        "id": "q4",
        "type": "true_false",
        "prompt": "Iterative methods are generally preferred over direct methods for small dense systems.",
        "correctAnswer": false,
        "explanation": "False. For small dense systems, direct methods (Gaussian elimination, LU) are typically faster and more reliable. Iterative methods excel for large sparse systems where storing/computing factors is expensive."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "Iterative methods converge when the spectral radius of the iteration matrix is ____ than 1.",
        "correctAnswer": "less",
        "explanation": "Convergence requires ρ(G) < 1, where ρ is the spectral radius (largest eigenvalue magnitude) of the iteration matrix G."
      }
    ]
  },
  {
    "id": "math402-quiz-6b",
    "subjectId": "math402",
    "topicId": "math402-topic-6",
    "title": "Iterative Methods for Linear Systems - Application",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "The Successive Over-Relaxation (SOR) method introduces a relaxation parameter ω where:",
        "options": [
          "ω must always equal 1",
          "0 < ω < 1 for under-relaxation, ω = 1 gives Gauss-Seidel, 1 < ω < 2 for over-relaxation",
          "ω > 2 for fastest convergence",
          "ω is the condition number"
        ],
        "correctAnswer": 1,
        "explanation": "SOR: x^{(k+1)} = ωx̃^{(k+1)} + (1-ω)x^{(k)}, where x̃ is the Gauss-Seidel update. Optimal ω ∈ (1,2) can significantly accelerate convergence; ω = 1 recovers Gauss-Seidel."
      },
      {
        "id": "q2",
        "type": "code_output",
        "prompt": "What does this Gauss-Seidel iteration output?",
        "codeSnippet": "import numpy as np\nA = np.array([[4.0, 1.0], [1.0, 3.0]])\nb = np.array([5.0, 4.0])\nx = np.array([0.0, 0.0])\n# One Gauss-Seidel iteration\nx[0] = (b[0] - A[0,1]*x[1]) / A[0,0]\nx[1] = (b[1] - A[1,0]*x[0]) / A[1,1]\nprint(f'{x[1]:.4f}')",
        "correctAnswer": "0.9167",
        "explanation": "Gauss-Seidel uses updated values immediately: x[0] = (5-1·0)/4 = 1.25, then x[1] = (4-1·1.25)/3 = 2.75/3 ≈ 0.9167."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "The Conjugate Gradient (CG) method is applicable when A is:",
        "options": [
          "Symmetric positive definite",
          "Any invertible matrix",
          "Diagonal",
          "Lower triangular"
        ],
        "correctAnswer": 0,
        "explanation": "CG requires A to be symmetric positive definite (SPD). It solves Ax = b in at most n iterations (in exact arithmetic) by minimizing the energy norm ||x - x*||_A."
      },
      {
        "id": "q4",
        "type": "true_false",
        "prompt": "Gauss-Seidel always converges faster than Jacobi when both converge.",
        "correctAnswer": false,
        "explanation": "False. While Gauss-Seidel typically converges faster, there exist matrices for which Jacobi converges faster. However, if both converge, Gauss-Seidel often requires fewer iterations."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "GMRES (Generalized Minimal Residual) minimizes the ____ of the residual over the Krylov subspace.",
        "correctAnswer": "2-norm",
        "explanation": "GMRES minimizes ||r^{(k)}||_2 = ||Ax^{(k)} - b||_2 over the Krylov subspace K_k. It works for any invertible matrix, not just SPD."
      }
    ]
  },
  {
    "id": "math402-quiz-6c",
    "subjectId": "math402",
    "topicId": "math402-topic-6",
    "title": "Iterative Methods for Linear Systems - Mastery",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "For symmetric positive definite A, the optimal SOR parameter ω can be computed from:",
        "options": [
          "The condition number of A",
          "The matrix norm",
          "The spectral radius of the Jacobi iteration matrix",
          "The largest eigenvalue of A"
        ],
        "correctAnswer": 2,
        "explanation": "For SPD tridiagonal matrices (like discretized PDEs), ω_opt = 2/(1 + √(1 - ρ(G_J)^2)), where ρ(G_J) is the spectral radius of the Jacobi matrix. This can dramatically accelerate convergence."
      },
      {
        "id": "q2",
        "type": "multiple_choice",
        "prompt": "Preconditioning a linear system Ax = b means:",
        "options": [
          "Solving M^{-1}Ax = M^{-1}b where M approximates A and M^{-1} is easy to apply",
          "Reordering rows and columns",
          "Using double precision arithmetic",
          "Scaling rows to have unit norm"
        ],
        "correctAnswer": 0,
        "explanation": "A preconditioner M approximates A such that κ(M^{-1}A) << κ(A). Good preconditioners are cheap to apply (M^{-1}z for any z) and cluster eigenvalues, accelerating iterative convergence."
      },
      {
        "id": "q3",
        "type": "code_output",
        "prompt": "What spectral radius does this iteration matrix have?",
        "codeSnippet": "import numpy as np\nD = np.array([[4, 0], [0, 3]])\nL_U = np.array([[0, 1], [1, 0]])\nG_jacobi = -np.linalg.inv(D) @ L_U\neig_vals = np.linalg.eigvals(G_jacobi)\nrho = np.max(np.abs(eig_vals))\nprint(f'{rho:.4f}')",
        "correctAnswer": "0.2887",
        "explanation": "G_jacobi = -D^{-1}(L+U) for A = [[4,1],[1,3]]. G = -[[1/4,0],[0,1/3]][[0,1],[1,0]] = [[0,-1/4],[-1/3,0]]. Eigenvalues: ±√(1/12) ≈ ±0.2887. ρ ≈ 0.2887."
      },
      {
        "id": "q4",
        "type": "true_false",
        "prompt": "In exact arithmetic, CG converges in at most n iterations for an n×n SPD matrix.",
        "correctAnswer": true,
        "explanation": "True. CG is a direct method in exact arithmetic, finding the exact solution in at most n steps. In practice, roundoff and preconditioning make it effective as an iterative method."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "CG search directions are A-orthogonal, also called ____.",
        "correctAnswer": "conjugate",
        "explanation": "CG search directions p^{(i)} satisfy (p^{(i)})^T A p^{(j)} = 0 for i ≠ j. This A-orthogonality (conjugacy) ensures each direction optimally contributes to minimizing ||x - x*||_A."
      }
    ]
  }
]
