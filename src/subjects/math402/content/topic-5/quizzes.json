[
  {
    "id": "math402-quiz-5a",
    "subjectId": "math402",
    "topicId": "math402-topic-5",
    "title": "Direct Methods for Linear Systems - Fundamentals",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "Gaussian elimination without pivoting can fail when:",
        "options": [
          "The matrix is not square",
          "A zero pivot is encountered",
          "The matrix is symmetric",
          "The right-hand side is zero"
        ],
        "correctAnswer": 1,
        "explanation": "If a diagonal element (pivot) becomes zero during elimination, division by zero occurs. Pivoting (row swapping) is needed to continue."
      },
      {
        "id": "q2",
        "type": "multiple_choice",
        "prompt": "The computational complexity of Gaussian elimination for an n×n system is:",
        "options": [
          "O(n)",
          "O(n^3)",
          "O(n^2)",
          "O(2^n)"
        ],
        "correctAnswer": 1,
        "explanation": "Gaussian elimination requires approximately (2/3)n^3 floating-point operations for the forward elimination phase, making it O(n^3)."
      },
      {
        "id": "q3",
        "type": "code_output",
        "prompt": "What is the output after one step of Gaussian elimination?",
        "codeSnippet": "import numpy as np\nA = np.array([[2.0, 1.0], [4.0, 3.0]])\nb = np.array([3.0, 7.0])\n# Eliminate A[1,0]\nmultiplier = A[1,0] / A[0,0]\nA[1] = A[1] - multiplier * A[0]\nb[1] = b[1] - multiplier * b[0]\nprint(f'{A[1,1]:.1f}')",
        "correctAnswer": "1.0",
        "explanation": "Multiplier = 4/2 = 2. A[1,1] = 3 - 2·1 = 1.0. This eliminates the (1,0) element, creating upper triangular form."
      },
      {
        "id": "q4",
        "type": "true_false",
        "prompt": "LU decomposition is only possible for invertible matrices.",
        "correctAnswer": false,
        "explanation": "False. LU decomposition (with partial pivoting as PA=LU) is possible for any matrix, though the L and U factors reveal rank deficiency for singular matrices."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "In LU decomposition A = LU, L is a ____ triangular matrix and U is an upper triangular matrix.",
        "correctAnswer": "lower",
        "explanation": "L (Lower) is lower triangular with ones on the diagonal, and U (Upper) is upper triangular."
      }
    ]
  },
  {
    "id": "math402-quiz-5b",
    "subjectId": "math402",
    "topicId": "math402-topic-5",
    "title": "Direct Methods for Linear Systems - Application",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "Partial pivoting in Gaussian elimination involves:",
        "options": [
          "Swapping both rows and columns",
          "Dividing by the largest element in the matrix",
          "Scaling each row by its maximum element",
          "Swapping rows to maximize the pivot element in the current column"
        ],
        "correctAnswer": 3,
        "explanation": "Partial pivoting selects the row with the largest absolute value in the current column as the pivot row, improving numerical stability by avoiding small pivots."
      },
      {
        "id": "q2",
        "type": "code_output",
        "prompt": "What is the output of this forward substitution?",
        "codeSnippet": "import numpy as np\nL = np.array([[1, 0, 0], [2, 1, 0], [1, 3, 1]])\nb = np.array([2, 6, 11])\nx = np.zeros(3)\nx[0] = b[0] / L[0,0]\nx[1] = (b[1] - L[1,0]*x[0]) / L[1,1]\nx[2] = (b[2] - L[2,0]*x[0] - L[2,1]*x[1]) / L[2,2]\nprint(f'{x[2]:.0f}')",
        "correctAnswer": "3",
        "explanation": "Forward substitution: x[0] = 2/1 = 2, x[1] = (6-2·2)/1 = 2, x[2] = (11-1·2-3·2)/1 = (11-2-6)/1 = 3."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "Cholesky decomposition A = LL^T can be applied when A is:",
        "options": [
          "Any invertible matrix",
          "Upper triangular",
          "Diagonal",
          "Symmetric positive definite"
        ],
        "correctAnswer": 3,
        "explanation": "Cholesky requires A to be symmetric positive definite (SPD). It computes L such that A = LL^T, using only (1/3)n^3 operations—half the cost of standard LU."
      },
      {
        "id": "q4",
        "type": "true_false",
        "prompt": "The condition number κ(A) = ||A|| ||A^{-1}|| measures the sensitivity of the solution to perturbations in the data.",
        "correctAnswer": true,
        "explanation": "True. A large condition number indicates ill-conditioning: small changes in A or b can cause large changes in the solution x. κ(A) ≥ 1 for all matrices."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "QR factorization decomposes A into an orthogonal matrix Q and an upper ____ matrix R.",
        "correctAnswer": "triangular",
        "explanation": "QR factorization gives A = QR where Q is orthogonal (Q^TQ = I) and R is upper triangular. It's particularly useful for least squares problems."
      }
    ]
  },
  {
    "id": "math402-quiz-5c",
    "subjectId": "math402",
    "topicId": "math402-topic-5",
    "title": "Direct Methods for Linear Systems - Mastery",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "The growth factor in Gaussian elimination with partial pivoting measures:",
        "options": [
          "How much the elements grow during elimination",
          "The increase in the condition number",
          "The number of row swaps performed",
          "The error in the computed solution"
        ],
        "correctAnswer": 0,
        "explanation": "Growth factor ρ = max_{i,j,k}|a_{ij}^{(k)}| / max_{i,j}|a_{ij}| measures element growth. For partial pivoting, ρ ≤ 2^{n-1}, though this worst case is rarely observed in practice."
      },
      {
        "id": "q2",
        "type": "multiple_choice",
        "prompt": "The Singular Value Decomposition (SVD) A = UΣV^T provides:",
        "options": [
          "Optimal low-rank approximations and condition number computation",
          "Lower computational cost than QR factorization",
          "A faster alternative to LU for solving linear systems",
          "A method only applicable to square matrices"
        ],
        "correctAnswer": 0,
        "explanation": "SVD reveals the rank, null space, condition number (κ = σ_max/σ_min), and optimal low-rank approximations of any matrix. It's computationally expensive but extremely informative."
      },
      {
        "id": "q3",
        "type": "code_output",
        "prompt": "What condition number does this code compute?",
        "codeSnippet": "import numpy as np\nA = np.array([[4, 0], [0, 1]])\neig_vals = np.linalg.eigvals(A)\ncond = np.max(eig_vals) / np.min(eig_vals)\nprint(f'{cond:.1f}')",
        "correctAnswer": "4.0",
        "explanation": "For diagonal matrix A = diag(4,1), eigenvalues are 4 and 1. For symmetric A, κ(A) = |λ_max|/|λ_min| = 4/1 = 4.0."
      },
      {
        "id": "q4",
        "type": "true_false",
        "prompt": "For a symmetric positive definite matrix, Cholesky decomposition is both faster and more stable than LU decomposition.",
        "correctAnswer": true,
        "explanation": "True. Cholesky requires half the operations of LU, produces a smaller growth factor, and doesn't require pivoting for SPD matrices, making it preferable when applicable."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "Iterative refinement improves accuracy by repeatedly solving for the ____ vector.",
        "correctAnswer": "residual",
        "explanation": "Iterative refinement solves A(x + δx) = b - Ax̃ = r (residual) repeatedly to improve accuracy. Computing r in higher precision can recover full accuracy even for ill-conditioned systems."
      }
    ]
  }
]
