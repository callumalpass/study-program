[
  {
    "id": "math402-ex-2-1",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 1,
    "title": "Bisection Method",
    "description": "Implement the bisection method for finding roots of continuous functions. The bisection method is guaranteed to converge by repeatedly halving an interval that contains a root.",
    "starterCode": "import numpy as np\n\ndef bisection(f, a, b, tol=1e-6, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using bisection method.\n\n    Parameters:\n    - f: function\n    - a, b: interval endpoints (f(a) and f(b) must have opposite signs)\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function: f(x) = x² - 2\nf = lambda x: x**2 - 2\na, b = 0, 2\n\nprint(\"Finding root of f(x) = x² - 2 on [0, 2]\\n\")\n\nroot, iters = bisection(f, a, b)\n\nprint(f\"Root: {root:.10f}\")\nprint(f\"√2:   {np.sqrt(2):.10f}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"f(root): {f(root):.2e}\")",
    "hints": [
      "Check that f(a) and f(b) have opposite signs",
      "Compute midpoint c = (a + b) / 2",
      "Update interval based on sign of f(c)",
      "Stop when |b - a| < tolerance"
    ],
    "solution": "import numpy as np\n\ndef bisection(f, a, b, tol=1e-6, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using bisection method.\n\n    Parameters:\n    - f: function\n    - a, b: interval endpoints (f(a) and f(b) must have opposite signs)\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    fa = f(a)\n    fb = f(b)\n\n    # Check that f(a) and f(b) have opposite signs\n    if fa * fb > 0:\n        raise ValueError(\"f(a) and f(b) must have opposite signs\")\n\n    for i in range(max_iter):\n        # Compute midpoint\n        c = (a + b) / 2\n        fc = f(c)\n\n        # Check convergence\n        if abs(b - a) < tol or abs(fc) < tol:\n            return c, i + 1\n\n        # Update interval\n        if fa * fc < 0:\n            b = c\n            fb = fc\n        else:\n            a = c\n            fa = fc\n\n    # Max iterations reached\n    return (a + b) / 2, max_iter\n\n# Test function: f(x) = x² - 2\nf = lambda x: x**2 - 2\na, b = 0, 2\n\nprint(\"Bisection Method: Finding root of f(x) = x² - 2\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nroot, iters = bisection(f, a, b)\n\nprint(f\"Root:       {root:.10f}\")\nprint(f\"True (√2):  {np.sqrt(2):.10f}\")\nprint(f\"Error:      {abs(root - np.sqrt(2)):.2e}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"f(root):    {f(root):.2e}\\n\")\n\n# Test more functions\nprint(\"=\"*60)\nprint(\"\\nAdditional Test Cases\\n\")\nprint(\"=\"*60 + \"\\n\")\n\ntest_cases = [\n    (lambda x: x**3 - x - 2, 1, 2, 1.5213797, \"x³ - x - 2\"),\n    (lambda x: np.cos(x) - x, 0, 1, 0.7390851, \"cos(x) - x\"),\n    (lambda x: np.exp(x) - 3, 0, 2, 1.0986123, \"e^x - 3\"),\n]\n\nfor f, a, b, true_root, desc in test_cases:\n    root, iters = bisection(f, a, b, tol=1e-10)\n    error = abs(root - true_root)\n    print(f\"{desc}:\")\n    print(f\"  Root: {root:.10f}\")\n    print(f\"  Error: {error:.2e}\")\n    print(f\"  Iterations: {iters}\\n\")\n\nprint(\"All tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = x² - 2, [0, 2]",
        "expectedOutput": "root ≈ 1.4142135624 (√2)",
        "isHidden": false,
        "description": "Find square root of 2 using bisection"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-2",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 2,
    "title": "Newton Method",
    "description": "Implement Newton's method for root finding. Newton's method uses the derivative to achieve quadratic convergence, but requires a good initial guess and derivative information.",
    "starterCode": "import numpy as np\n\ndef newton(f, df, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using Newton's method.\n\n    Newton iteration: x_{n+1} = x_n - f(x_n)/f'(x_n)\n\n    Parameters:\n    - f: function\n    - df: derivative of f\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function: f(x) = x² - 2\nf = lambda x: x**2 - 2\ndf = lambda x: 2*x\nx0 = 1.0\n\nprint(\"Finding root of f(x) = x² - 2\\n\")\nprint(f\"Initial guess: x0 = {x0}\\n\")\n\nroot, iters = newton(f, df, x0)\n\nprint(f\"Root: {root:.15f}\")\nprint(f\"√2:   {np.sqrt(2):.15f}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"Error: {abs(root - np.sqrt(2)):.2e}\")",
    "hints": [
      "Newton iteration: x_new = x - f(x)/f'(x)",
      "Check for division by zero (f'(x) = 0)",
      "Monitor |x_new - x| for convergence",
      "Newton converges quadratically near the root"
    ],
    "solution": "import numpy as np\n\ndef newton(f, df, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using Newton's method.\n\n    Newton iteration: x_{n+1} = x_n - f(x_n)/f'(x_n)\n\n    Parameters:\n    - f: function\n    - df: derivative of f\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    x = x0\n\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n\n        # Check for zero derivative\n        if abs(dfx) < 1e-15:\n            raise ValueError(\"Derivative is zero at x = {x}\")\n\n        # Newton iteration\n        x_new = x - fx / dfx\n\n        # Check convergence\n        if abs(x_new - x) < tol or abs(fx) < tol:\n            return x_new, i + 1\n\n        x = x_new\n\n    return x, max_iter\n\n# Test function: f(x) = x² - 2\nprint(\"Newton's Method: Finding root of f(x) = x² - 2\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf = lambda x: x**2 - 2\ndf = lambda x: 2*x\nx0 = 1.0\n\nprint(f\"Initial guess: x0 = {x0}\\n\")\n\nroot, iters = newton(f, df, x0)\ntrue_root = np.sqrt(2)\n\nprint(f\"Root:       {root:.15f}\")\nprint(f\"True (√2):  {true_root:.15f}\")\nprint(f\"Error:      {abs(root - true_root):.2e}\")\nprint(f\"Iterations: {iters}\\n\")\n\n# Compare convergence with bisection\nprint(\"=\"*60)\nprint(\"\\nConvergence Comparison\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Newton with iteration tracking\nx = x0\nprint(\"Newton's method convergence:\")\nfor i in range(5):\n    fx = f(x)\n    dfx = df(x)\n    x_new = x - fx / dfx\n    error = abs(x_new - true_root)\n    print(f\"  Iteration {i+1}: x = {x_new:.15f}, error = {error:.2e}\")\n    if error < 1e-15:\n        break\n    x = x_new\n\nprint(\"\\nNote: Newton's method achieves machine precision in few iterations!\")\n\n# Test quadratic convergence\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nQuadratic Convergence\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nx = x0\nerrors = []\nfor i in range(5):\n    error = abs(x - true_root)\n    errors.append(error)\n    if error < 1e-15:\n        break\n    fx = f(x)\n    dfx = df(x)\n    x = x - fx / dfx\n\nprint(\"Error sequence:\")\nfor i, err in enumerate(errors):\n    print(f\"  e_{i} = {err:.2e}\")\n    if i > 0:\n        ratio = errors[i] / errors[i-1]**2\n        print(f\"    e_{i}/e_{i-1}² ≈ {ratio:.2f} (should be roughly constant)\\n\")\n\n# Additional test cases\nprint(\"=\"*60)\nprint(\"\\nAdditional Test Cases\\n\")\nprint(\"=\"*60 + \"\\n\")\n\ntest_cases = [\n    (lambda x: x**3 - 2, lambda x: 3*x**2, 1.5, 2**(1/3), \"x³ = 2\"),\n    (lambda x: np.cos(x) - x, lambda x: -np.sin(x) - 1, 0.5, 0.7390851332, \"cos(x) = x\"),\n    (lambda x: np.exp(x) - 3, lambda x: np.exp(x), 1.0, np.log(3), \"e^x = 3\"),\n]\n\nfor f, df, x0, true_root, desc in test_cases:\n    root, iters = newton(f, df, x0)\n    error = abs(root - true_root)\n    print(f\"{desc}:\")\n    print(f\"  Root: {root:.12f}\")\n    print(f\"  Error: {error:.2e}\")\n    print(f\"  Iterations: {iters}\\n\")\n\nprint(\"All tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = x² - 2, df(x) = 2x, x0 = 1.0",
        "expectedOutput": "Converges to √2 in ~4 iterations",
        "isHidden": false,
        "description": "Test Newton method with quadratic convergence"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-3",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 2,
    "title": "Secant Method",
    "description": "Implement the secant method as a derivative-free alternative to Newton's method. The secant method approximates the derivative using finite differences.",
    "starterCode": "import numpy as np\n\ndef secant(f, x0, x1, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using secant method.\n\n    Secant iteration: x_{n+1} = x_n - f(x_n) * (x_n - x_{n-1}) / (f(x_n) - f(x_{n-1}))\n\n    Parameters:\n    - f: function\n    - x0, x1: initial guesses\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function\nf = lambda x: x**2 - 2\nx0, x1 = 1.0, 2.0\n\nprint(\"Finding root of f(x) = x² - 2\\n\")\nprint(f\"Initial guesses: x0 = {x0}, x1 = {x1}\\n\")\n\nroot, iters = secant(f, x0, x1)\n\nprint(f\"Root: {root:.15f}\")\nprint(f\"√2:   {np.sqrt(2):.15f}\")\nprint(f\"Iterations: {iters}\")",
    "hints": [
      "Secant approximates derivative: f'(x) ≈ (f(x_n) - f(x_{n-1})) / (x_n - x_{n-1})",
      "Update: x_new = x1 - f(x1) * (x1 - x0) / (f(x1) - f(x0))",
      "Check for division by zero",
      "Convergence order is φ ≈ 1.618 (golden ratio)"
    ],
    "solution": "import numpy as np\n\ndef secant(f, x0, x1, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using secant method.\n\n    Secant iteration: x_{n+1} = x_n - f(x_n) * (x_n - x_{n-1}) / (f(x_n) - f(x_{n-1}))\n\n    Parameters:\n    - f: function\n    - x0, x1: initial guesses\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    f0 = f(x0)\n    f1 = f(x1)\n\n    for i in range(max_iter):\n        # Check for division by zero\n        if abs(f1 - f0) < 1e-15:\n            raise ValueError(\"Division by zero in secant method\")\n\n        # Secant iteration\n        x_new = x1 - f1 * (x1 - x0) / (f1 - f0)\n\n        # Check convergence\n        if abs(x_new - x1) < tol:\n            return x_new, i + 1\n\n        # Update for next iteration\n        x0, f0 = x1, f1\n        x1, f1 = x_new, f(x_new)\n\n    return x1, max_iter\n\n# Test function\nprint(\"Secant Method: Finding root of f(x) = x² - 2\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf = lambda x: x**2 - 2\nx0, x1 = 1.0, 2.0\ntrue_root = np.sqrt(2)\n\nprint(f\"Initial guesses: x0 = {x0}, x1 = {x1}\\n\")\n\nroot, iters = secant(f, x0, x1)\n\nprint(f\"Root:       {root:.15f}\")\nprint(f\"True (√2):  {true_root:.15f}\")\nprint(f\"Error:      {abs(root - true_root):.2e}\")\nprint(f\"Iterations: {iters}\\n\")\n\n# Convergence analysis\nprint(\"=\"*60)\nprint(\"\\nConvergence Analysis\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nx0, x1 = 1.0, 2.0\nerrors = []\n\nf0 = f(x0)\nf1 = f(x1)\n\nprint(\"Iteration history:\")\nfor i in range(10):\n    error = abs(x1 - true_root)\n    errors.append(error)\n    print(f\"  Iteration {i}: x = {x1:.15f}, error = {error:.2e}\")\n\n    if error < 1e-15:\n        break\n\n    # Secant iteration\n    if abs(f1 - f0) < 1e-15:\n        break\n    x_new = x1 - f1 * (x1 - x0) / (f1 - f0)\n    x0, f0 = x1, f1\n    x1, f1 = x_new, f(x_new)\n\n# Estimate convergence order\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nConvergence Order Estimation\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nif len(errors) >= 4:\n    # Estimate α from e_{n+1} ≈ C * e_n^α\n    # Taking logs: log(e_{n+1}) ≈ log(C) + α * log(e_n)\n    # Slope gives α\n\n    print(\"Error sequence and estimated convergence order:\\n\")\n    for i in range(min(5, len(errors)-1)):\n        if errors[i] > 0 and errors[i+1] > 0:\n            if i > 0 and errors[i-1] > 0:\n                alpha = np.log(errors[i+1]/errors[i]) / np.log(errors[i]/errors[i-1])\n                print(f\"  e_{i} = {errors[i]:.2e}\")\n                print(f\"    Estimated order α ≈ {alpha:.3f}\\n\")\n\n    print(\"Note: Secant method has convergence order φ ≈ 1.618 (golden ratio)\")\n\n# Method comparison\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nMethod Comparison\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Secant (no derivative)\nf_test = lambda x: x**3 - 2\nx0, x1 = 1.0, 2.0\nroot_secant, iters_secant = secant(f_test, x0, x1)\n\n# Newton (requires derivative)\ndf_test = lambda x: 3*x**2\ndef newton_simple(f, df, x0, tol=1e-10, max_iter=100):\n    x = x0\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n        if abs(dfx) < 1e-15:\n            break\n        x_new = x - fx / dfx\n        if abs(x_new - x) < tol:\n            return x_new, i + 1\n        x = x_new\n    return x, max_iter\n\nroot_newton, iters_newton = newton_simple(f_test, df_test, 1.5)\n\ntrue_root_test = 2**(1/3)\n\nprint(f\"Finding ³√2:\\n\")\nprint(f\"Secant method:\")\nprint(f\"  Root: {root_secant:.15f}\")\nprint(f\"  Error: {abs(root_secant - true_root_test):.2e}\")\nprint(f\"  Iterations: {iters_secant}\\n\")\n\nprint(f\"Newton's method:\")\nprint(f\"  Root: {root_newton:.15f}\")\nprint(f\"  Error: {abs(root_newton - true_root_test):.2e}\")\nprint(f\"  Iterations: {iters_newton}\\n\")\n\nprint(\"Secant: No derivative needed, slightly more iterations\")\nprint(\"Newton: Requires derivative, faster convergence\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = x² - 2, x0 = 1.0, x1 = 2.0",
        "expectedOutput": "Converges to √2 with superlinear convergence",
        "isHidden": false,
        "description": "Test secant method convergence"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-4",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 3,
    "title": "Fixed-Point Iteration",
    "description": "Implement fixed-point iteration and analyze convergence conditions. A fixed point of g(x) satisfies x = g(x), and the iteration x_{n+1} = g(x_n) converges when |g'(x)| < 1 near the fixed point.",
    "starterCode": "import numpy as np\n\ndef fixed_point(g, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find fixed point of g(x) using iteration x_{n+1} = g(x_n).\n\n    Converges when |g'(x*)| < 1 at the fixed point x*.\n\n    Parameters:\n    - g: iteration function\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (fixed_point, iterations, converged): result, iterations, success flag\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef analyze_convergence(g, dg, x_star):\n    \"\"\"\n    Analyze convergence of fixed-point iteration.\n\n    Parameters:\n    - g: iteration function\n    - dg: derivative of g\n    - x_star: fixed point\n\n    Returns:\n    - convergence_info: dictionary with analysis\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Example: Find √2 by solving x = 2/x, rearranged as x = (x + 2/x)/2\ng = lambda x: (x + 2/x) / 2  # Babylonian method\nx0 = 1.0\n\nprint(\"Fixed-Point Iteration: Finding √2\\n\")\n\nroot, iters, converged = fixed_point(g, x0)\n\nprint(f\"Fixed point: {root:.15f}\")\nprint(f\"√2:          {np.sqrt(2):.15f}\")\nprint(f\"Iterations:  {iters}\")\nprint(f\"Converged:   {converged}\")",
    "hints": [
      "Iterate: x = g(x) until |x_new - x| < tolerance",
      "Check |g'(x*)| < 1 for convergence",
      "Linear convergence when 0 < |g'(x*)| < 1",
      "Quadratic convergence when g'(x*) = 0"
    ],
    "solution": "import numpy as np\n\ndef fixed_point(g, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find fixed point of g(x) using iteration x_{n+1} = g(x_n).\n\n    Converges when |g'(x*)| < 1 at the fixed point x*.\n\n    Parameters:\n    - g: iteration function\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (fixed_point, iterations, converged): result, iterations, success flag\n    \"\"\"\n    x = x0\n\n    for i in range(max_iter):\n        x_new = g(x)\n\n        # Check convergence\n        if abs(x_new - x) < tol:\n            return x_new, i + 1, True\n\n        x = x_new\n\n    # Did not converge\n    return x, max_iter, False\n\ndef analyze_convergence(g, dg, x_star):\n    \"\"\"\n    Analyze convergence of fixed-point iteration.\n\n    Parameters:\n    - g: iteration function\n    - dg: derivative of g\n    - x_star: fixed point\n\n    Returns:\n    - convergence_info: dictionary with analysis\n    \"\"\"\n    dg_star = dg(x_star)\n\n    info = {\n        'derivative': dg_star,\n        'will_converge': abs(dg_star) < 1,\n        'convergence_type': None,\n        'asymptotic_error_constant': abs(dg_star)\n    }\n\n    if abs(dg_star) < 1e-10:\n        info['convergence_type'] = 'At least quadratic'\n    elif abs(dg_star) < 1:\n        info['convergence_type'] = 'Linear'\n    else:\n        info['convergence_type'] = 'Divergent'\n\n    return info\n\n# Example 1: Babylonian method for √2\nprint(\"Fixed-Point Iteration Methods\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(\"Example 1: Babylonian method for √2\\n\")\nprint(\"Fixed point equation: x = (x + 2/x)/2\\n\")\n\ng1 = lambda x: (x + 2/x) / 2\ndg1 = lambda x: 0.5 * (1 - 2/x**2)\nx0 = 1.0\ntrue_root = np.sqrt(2)\n\nroot1, iters1, conv1 = fixed_point(g1, x0)\n\nprint(f\"Fixed point: {root1:.15f}\")\nprint(f\"True (√2):   {true_root:.15f}\")\nprint(f\"Error:       {abs(root1 - true_root):.2e}\")\nprint(f\"Iterations:  {iters1}\")\nprint(f\"Converged:   {conv1}\\n\")\n\ninfo1 = analyze_convergence(g1, dg1, true_root)\nprint(\"Convergence analysis:\")\nprint(f\"  g'(x*) = {info1['derivative']:.6f}\")\nprint(f\"  Will converge: {info1['will_converge']}\")\nprint(f\"  Type: {info1['convergence_type']}\\n\")\n\n# Example 2: Bad iteration for √2\nprint(\"=\"*60)\nprint(\"\\nExample 2: Poor choice g(x) = 2/x\\n\")\n\ng2 = lambda x: 2 / x\ndg2 = lambda x: -2 / x**2\n\ntry:\n    root2, iters2, conv2 = fixed_point(g2, x0, max_iter=20)\n    print(f\"Result: {root2:.10f}\")\n    print(f\"Iterations: {iters2}\")\n    print(f\"Converged: {conv2}\\n\")\nexcept:\n    print(\"Failed to converge (oscillates)\\n\")\n\ninfo2 = analyze_convergence(g2, dg2, true_root)\nprint(\"Convergence analysis:\")\nprint(f\"  g'(x*) = {info2['derivative']:.6f}\")\nprint(f\"  |g'(x*)| = {abs(info2['derivative']):.6f}\")\nprint(f\"  Will converge: {info2['will_converge']}\")\nprint(f\"  Type: {info2['convergence_type']}\\n\")\nprint(\"Note: |g'(x*)| = 1, so convergence is not guaranteed!\")\n\n# Example 3: Newton as fixed-point\nprint(\"=\"*60)\nprint(\"\\nExample 3: Newton's method as fixed-point iteration\\n\")\nprint(\"For f(x) = x² - 2, Newton is: x = x - (x² - 2)/(2x) = (x + 2/x)/2\\n\")\nprint(\"Same as Babylonian method!\\n\")\n\n# Show quadratic convergence\nx = x0\nprint(\"Iteration history (quadratic convergence):\")\nfor i in range(5):\n    error = abs(x - true_root)\n    print(f\"  Iteration {i}: x = {x:.15f}, error = {error:.2e}\")\n    if error < 1e-15:\n        break\n    x = g1(x)\n\n# Example 4: Find root of cos(x) = x\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nExample 4: Finding root of cos(x) = x\\n\")\n\ng3 = lambda x: np.cos(x)\ndg3 = lambda x: -np.sin(x)\nx0_cos = 0.5\n\nroot3, iters3, conv3 = fixed_point(g3, x0_cos)\nprint(f\"Fixed point: {root3:.15f}\")\nprint(f\"Iterations:  {iters3}\")\nprint(f\"cos(x*):     {np.cos(root3):.15f}\")\nprint(f\"Verification: {abs(root3 - np.cos(root3)):.2e}\\n\")\n\ninfo3 = analyze_convergence(g3, dg3, root3)\nprint(\"Convergence analysis:\")\nprint(f\"  g'(x*) = {info3['derivative']:.6f}\")\nprint(f\"  |g'(x*)| = {abs(info3['derivative']):.6f} < 1\")\nprint(f\"  Type: {info3['convergence_type']}\")\n\n# Convergence comparison\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nConvergence Rate Comparison\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Fast convergence (Babylonian)\nx_fast = 1.0\nerrors_fast = []\nfor i in range(8):\n    errors_fast.append(abs(x_fast - true_root))\n    x_fast = g1(x_fast)\n\n# Slow convergence (cos)\nx_slow = 0.5\nerrors_slow = []\ntrue_cos = root3\nfor i in range(8):\n    errors_slow.append(abs(x_slow - true_cos))\n    x_slow = g3(x_slow)\n\nprint(\"Babylonian method (g'(x*) ≈ 0):\")\nfor i, err in enumerate(errors_fast):\n    print(f\"  Iteration {i}: error = {err:.2e}\")\n\nprint(\"\\ncos(x) iteration (g'(x*) ≈ 0.67):\")\nfor i, err in enumerate(errors_slow):\n    print(f\"  Iteration {i}: error = {err:.2e}\")\n\nprint(\"\\nNote: Smaller |g'(x*)| leads to faster convergence!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "g(x) = (x + 2/x)/2, x0 = 1.0",
        "expectedOutput": "Converges to √2 with quadratic convergence",
        "isHidden": false,
        "description": "Test Babylonian method for square root"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-5",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 3,
    "title": "Method Convergence Comparison",
    "description": "Compare convergence rates of bisection, Newton, and secant methods. Implement all three methods and analyze their performance on various test functions.",
    "starterCode": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef compare_methods(f, df, a, b, x0, true_root):\n    \"\"\"\n    Compare bisection, Newton, and secant methods.\n\n    Parameters:\n    - f: function\n    - df: derivative\n    - a, b: interval for bisection\n    - x0: initial guess for Newton/secant\n    - true_root: true root for error computation\n\n    Returns:\n    - results: dictionary with convergence history\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function: f(x) = x² - 2\nf = lambda x: x**2 - 2\ndf = lambda x: 2*x\na, b = 0, 2\nx0 = 1.0\ntrue_root = np.sqrt(2)\n\nprint(\"Convergence Comparison: f(x) = x² - 2\\n\")\n\nresults = compare_methods(f, df, a, b, x0, true_root)\n\nfor method, history in results.items():\n    print(f\"{method}:\")\n    print(f\"  Iterations: {len(history)}\")\n    print(f\"  Final error: {history[-1]:.2e}\\n\")",
    "hints": [
      "Implement bisection, Newton, secant with error tracking",
      "Store error at each iteration",
      "Plot log(error) vs iteration to see convergence rates",
      "Bisection: linear, Newton: quadratic, Secant: superlinear"
    ],
    "solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef bisection_with_history(f, a, b, tol=1e-12, max_iter=100):\n    \"\"\"Bisection with error history.\"\"\"\n    history = []\n    fa, fb = f(a), f(b)\n\n    for i in range(max_iter):\n        c = (a + b) / 2\n        fc = f(c)\n        history.append(c)\n\n        if abs(b - a) < tol or abs(fc) < tol:\n            break\n\n        if fa * fc < 0:\n            b, fb = c, fc\n        else:\n            a, fa = c, fc\n\n    return history\n\ndef newton_with_history(f, df, x0, tol=1e-12, max_iter=100):\n    \"\"\"Newton's method with error history.\"\"\"\n    history = [x0]\n    x = x0\n\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n\n        if abs(dfx) < 1e-15:\n            break\n\n        x_new = x - fx / dfx\n        history.append(x_new)\n\n        if abs(x_new - x) < tol:\n            break\n\n        x = x_new\n\n    return history\n\ndef secant_with_history(f, x0, x1, tol=1e-12, max_iter=100):\n    \"\"\"Secant method with error history.\"\"\"\n    history = [x0, x1]\n    f0, f1 = f(x0), f(x1)\n\n    for i in range(max_iter):\n        if abs(f1 - f0) < 1e-15:\n            break\n\n        x_new = x1 - f1 * (x1 - x0) / (f1 - f0)\n        history.append(x_new)\n\n        if abs(x_new - x1) < tol:\n            break\n\n        x0, f0 = x1, f1\n        x1, f1 = x_new, f(x_new)\n\n    return history\n\ndef compare_methods(f, df, a, b, x0, true_root):\n    \"\"\"\n    Compare bisection, Newton, and secant methods.\n\n    Parameters:\n    - f: function\n    - df: derivative\n    - a, b: interval for bisection\n    - x0: initial guess for Newton/secant\n    - true_root: true root for error computation\n\n    Returns:\n    - results: dictionary with convergence history\n    \"\"\"\n    # Run methods\n    bisect_hist = bisection_with_history(f, a, b)\n    newton_hist = newton_with_history(f, df, x0)\n    secant_hist = secant_with_history(f, x0, (a+b)/2)\n\n    # Compute errors\n    results = {\n        'Bisection': [abs(x - true_root) for x in bisect_hist],\n        'Newton': [abs(x - true_root) for x in newton_hist],\n        'Secant': [abs(x - true_root) for x in secant_hist],\n    }\n\n    return results\n\n# Test function: f(x) = x² - 2\nprint(\"Method Convergence Comparison\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf = lambda x: x**2 - 2\ndf = lambda x: 2*x\na, b = 0, 2\nx0 = 1.0\ntrue_root = np.sqrt(2)\n\nprint(\"Finding root of f(x) = x² - 2\\n\")\nprint(f\"True root: √2 = {true_root:.15f}\\n\")\n\nresults = compare_methods(f, df, a, b, x0, true_root)\n\n# Print summary\nprint(\"=\"*60)\nprint(\"\\nConvergence Summary\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nfor method, errors in results.items():\n    print(f\"{method}:\")\n    print(f\"  Iterations to converge: {len(errors)}\")\n    print(f\"  Initial error: {errors[0]:.2e}\")\n    print(f\"  Final error: {errors[-1]:.2e}\")\n\n    # Estimate convergence order\n    if len(errors) >= 4:\n        # α ≈ log(e_{n+1}/e_n) / log(e_n/e_{n-1})\n        i = len(errors) - 3\n        if errors[i-1] > 0 and errors[i] > 0 and errors[i+1] > 0:\n            alpha = np.log(errors[i+1]/errors[i]) / np.log(errors[i]/errors[i-1])\n            print(f\"  Estimated convergence order: α ≈ {alpha:.2f}\")\n    print()\n\n# Detailed convergence history\nprint(\"=\"*60)\nprint(\"\\nDetailed Convergence History\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nmax_iters = max(len(errors) for errors in results.values())\nprint(f\"{'Iteration':<12} {'Bisection':<15} {'Newton':<15} {'Secant':<15}\")\nprint(\"-\" * 60)\n\nfor i in range(max_iters):\n    row = f\"{i:<12}\"\n    for method in ['Bisection', 'Newton', 'Secant']:\n        errors = results[method]\n        if i < len(errors):\n            row += f\"{errors[i]:<15.2e}\"\n        else:\n            row += f\"{'—':<15}\"\n    print(row)\n\n# Convergence rate analysis\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nConvergence Rate Analysis\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(\"Expected convergence orders:\")\nprint(\"  Bisection: Linear (α = 1)\")\nprint(\"  Newton: Quadratic (α = 2)\")\nprint(\"  Secant: Superlinear (α ≈ 1.618)\\n\")\n\nfor method, errors in results.items():\n    print(f\"{method}:\")\n    if len(errors) >= 4:\n        print(\"  Error ratios e_{n+1}/e_n:\")\n        for i in range(min(5, len(errors)-1)):\n            if errors[i] > 1e-15:\n                ratio = errors[i+1] / errors[i]\n                print(f\"    Iteration {i}: {ratio:.4f}\")\n    print()\n\n# Visualization\nprint(\"=\"*60)\nprint(\"\\nGenerating convergence plot...\\n\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Linear scale\nfor method, errors in results.items():\n    ax1.plot(range(len(errors)), errors, 'o-', label=method, linewidth=2, markersize=6)\n\nax1.set_xlabel('Iteration')\nax1.set_ylabel('Absolute Error')\nax1.set_title('Convergence Comparison (Linear Scale)')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Log scale\nfor method, errors in results.items():\n    # Filter out zeros for log plot\n    nonzero_errors = [(i, e) for i, e in enumerate(errors) if e > 0]\n    if nonzero_errors:\n        iters, errs = zip(*nonzero_errors)\n        ax2.semilogy(iters, errs, 'o-', label=method, linewidth=2, markersize=6)\n\nax2.set_xlabel('Iteration')\nax2.set_ylabel('Absolute Error (log scale)')\nax2.set_title('Convergence Comparison (Log Scale)')\nax2.legend()\nax2.grid(True, alpha=0.3, which='both')\n\nplt.tight_layout()\nplt.savefig('/tmp/root_finding_convergence.png', dpi=150, bbox_inches='tight')\nprint(\"Plot saved to /tmp/root_finding_convergence.png\")\n\n# Test on different function\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nTest on f(x) = e^x - 3\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf2 = lambda x: np.exp(x) - 3\ndf2 = lambda x: np.exp(x)\na2, b2 = 0, 2\nx02 = 1.0\ntrue_root2 = np.log(3)\n\nresults2 = compare_methods(f2, df2, a2, b2, x02, true_root2)\n\nprint(f\"True root: ln(3) = {true_root2:.15f}\\n\")\nfor method, errors in results2.items():\n    print(f\"{method}: {len(errors)} iterations, final error = {errors[-1]:.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = x² - 2",
        "expectedOutput": "Newton fastest, bisection slowest, secant in between",
        "isHidden": false,
        "description": "Compare convergence rates of three methods"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-6",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 2,
    "title": "Modified Newton Method",
    "description": "Implement modified Newton methods for handling multiple roots and ill-conditioned problems. When f has a multiple root, standard Newton converges slowly; modifications can restore rapid convergence.",
    "starterCode": "import numpy as np\n\ndef newton_modified(f, df, d2f, x0, multiplicity=1, tol=1e-10, max_iter=100):\n    \"\"\"\n    Modified Newton's method for multiple roots.\n\n    For root of multiplicity m: x_{n+1} = x_n - m * f(x_n) / f'(x_n)\n\n    Parameters:\n    - f: function\n    - df: first derivative\n    - d2f: second derivative\n    - x0: initial guess\n    - multiplicity: multiplicity of root\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test: f(x) = (x-1)³ has triple root at x=1\nf = lambda x: (x - 1)**3\ndf = lambda x: 3*(x - 1)**2\nd2f = lambda x: 6*(x - 1)\nx0 = 2.0\n\nprint(\"Finding triple root of f(x) = (x-1)³\\n\")\n\n# Standard Newton (slow)\nroot1, iters1 = newton_modified(f, df, d2f, x0, multiplicity=1)\nprint(f\"Standard Newton: {iters1} iterations\")\n\n# Modified Newton (fast)\nroot2, iters2 = newton_modified(f, df, d2f, x0, multiplicity=3)\nprint(f\"Modified Newton: {iters2} iterations\")",
    "hints": [
      "Multiple root: use x_new = x - m * f(x) / f'(x)",
      "Alternative: x_new = x - f(x) * f'(x) / (f'(x)² - f(x) * f''(x))",
      "Multiple roots slow down standard Newton to linear convergence",
      "Modified methods restore quadratic convergence"
    ],
    "solution": "import numpy as np\n\ndef newton_standard(f, df, x0, tol=1e-10, max_iter=100):\n    \"\"\"Standard Newton's method.\"\"\"\n    x = x0\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n        if abs(dfx) < 1e-15:\n            break\n        x_new = x - fx / dfx\n        if abs(x_new - x) < tol:\n            return x_new, i + 1\n        x = x_new\n    return x, max_iter\n\ndef newton_modified_multiplicity(f, df, x0, m, tol=1e-10, max_iter=100):\n    \"\"\"\n    Modified Newton for known multiplicity.\n\n    x_{n+1} = x_n - m * f(x_n) / f'(x_n)\n    \"\"\"\n    x = x0\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n        if abs(dfx) < 1e-15:\n            break\n        x_new = x - m * fx / dfx\n        if abs(x_new - x) < tol:\n            return x_new, i + 1\n        x = x_new\n    return x, max_iter\n\ndef newton_modified_adaptive(f, df, d2f, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Modified Newton without knowing multiplicity.\n\n    Uses: x_{n+1} = x_n - u(x_n) / u'(x_n)\n    where u(x) = f(x) / f'(x)\n    \"\"\"\n    x = x0\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n        d2fx = d2f(x)\n\n        if abs(dfx) < 1e-15:\n            break\n\n        # u(x) = f(x) / f'(x)\n        # u'(x) = (f'(x)² - f(x)f''(x)) / f'(x)²\n        ux = fx / dfx\n        dupx = 1 - (fx * d2fx) / (dfx**2)\n\n        if abs(dupx) < 1e-15:\n            break\n\n        x_new = x - ux / dupx\n        if abs(x_new - x) < tol:\n            return x_new, i + 1\n        x = x_new\n    return x, max_iter\n\n# Test multiple root\nprint(\"Modified Newton's Method for Multiple Roots\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(\"Test 1: Triple root of f(x) = (x-1)³\\n\")\n\nf = lambda x: (x - 1)**3\ndf = lambda x: 3*(x - 1)**2\nd2f = lambda x: 6*(x - 1)\nx0 = 2.0\ntrue_root = 1.0\n\n# Standard Newton (slow for multiple roots)\nroot1, iters1 = newton_standard(f, df, x0)\nerror1 = abs(root1 - true_root)\n\nprint(\"Standard Newton's method:\")\nprint(f\"  Root: {root1:.15f}\")\nprint(f\"  Error: {error1:.2e}\")\nprint(f\"  Iterations: {iters1}\\n\")\n\n# Modified Newton with known multiplicity\nroot2, iters2 = newton_modified_multiplicity(f, df, x0, m=3)\nerror2 = abs(root2 - true_root)\n\nprint(\"Modified Newton (m=3):\")\nprint(f\"  Root: {root2:.15f}\")\nprint(f\"  Error: {error2:.2e}\")\nprint(f\"  Iterations: {iters2}\\n\")\n\n# Adaptive modified Newton\nroot3, iters3 = newton_modified_adaptive(f, df, d2f, x0)\nerror3 = abs(root3 - true_root)\n\nprint(\"Adaptive modified Newton:\")\nprint(f\"  Root: {root3:.15f}\")\nprint(f\"  Error: {error3:.2e}\")\nprint(f\"  Iterations: {iters3}\\n\")\n\nprint(f\"Speedup: {iters1 / iters2:.1f}× faster with modified method!\")\n\n# Convergence analysis\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nConvergence Rate Analysis\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Standard Newton on multiple root (linear)\nx = x0\nprint(\"Standard Newton (linear convergence):\")\nfor i in range(8):\n    error = abs(x - true_root)\n    print(f\"  Iteration {i}: error = {error:.2e}\")\n    if error < 1e-14:\n        break\n    fx = f(x)\n    dfx = df(x)\n    if abs(dfx) < 1e-15:\n        break\n    x = x - fx / dfx\n\nprint()\n\n# Modified Newton (quadratic)\nx = x0\nprint(\"Modified Newton with m=3 (quadratic convergence):\")\nfor i in range(5):\n    error = abs(x - true_root)\n    print(f\"  Iteration {i}: error = {error:.2e}\")\n    if error < 1e-14:\n        break\n    fx = f(x)\n    dfx = df(x)\n    if abs(dfx) < 1e-15:\n        break\n    x = x - 3 * fx / dfx\n\n# Test on double root\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nTest 2: Double root of f(x) = (x-2)²(x+1)\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf2 = lambda x: (x - 2)**2 * (x + 1)\ndf2 = lambda x: 2*(x - 2)*(x + 1) + (x - 2)**2\nd2f2 = lambda x: 2*(x + 1) + 4*(x - 2) + 2*(x - 2)\nx02 = 3.0\ntrue_root2 = 2.0\n\nprint(\"Finding root at x = 2 (double root)\\n\")\n\nroot2_std, iters2_std = newton_standard(f2, df2, x02)\nprint(f\"Standard: {iters2_std} iterations, error = {abs(root2_std - true_root2):.2e}\")\n\nroot2_mod, iters2_mod = newton_modified_multiplicity(f2, df2, x02, m=2)\nprint(f\"Modified (m=2): {iters2_mod} iterations, error = {abs(root2_mod - true_root2):.2e}\")\n\nroot2_adp, iters2_adp = newton_modified_adaptive(f2, df2, d2f2, x02)\nprint(f\"Adaptive: {iters2_adp} iterations, error = {abs(root2_adp - true_root2):.2e}\")\n\n# Estimating multiplicity\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nEstimating Root Multiplicity\\n\")\nprint(\"=\"*60 + \"\\n\")\n\ndef estimate_multiplicity(f, df, x_star, h=1e-5):\n    \"\"\"\n    Estimate multiplicity from f and f' near root.\n\n    For root of multiplicity m:\n    f(x) ≈ c(x - x*)^m\n    f'(x) ≈ mc(x - x*)^(m-1)\n    So f(x)/f'(x) ≈ (x - x*)/m\n    \"\"\"\n    x = x_star + h\n    fx = f(x)\n    dfx = df(x)\n\n    if abs(dfx) < 1e-15:\n        return float('inf')\n\n    ratio = fx / dfx\n    m_estimate = (x - x_star) / ratio\n    return m_estimate\n\nprint(\"Estimating multiplicity of (x-1)³:\")\nm_est = estimate_multiplicity(f, df, 1.0)\nprint(f\"  Estimated m ≈ {m_est:.2f} (true m = 3)\")\n\nprint(\"\\nEstimating multiplicity of (x-2)²(x+1):\")\nm_est2 = estimate_multiplicity(f2, df2, 2.0)\nprint(f\"  Estimated m ≈ {m_est2:.2f} (true m = 2)\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = (x-1)³, x0 = 2.0",
        "expectedOutput": "Modified method ~3× faster than standard Newton",
        "isHidden": false,
        "description": "Test modified Newton on multiple root"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-7",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 1,
    "title": "Regula Falsi (False Position)",
    "description": "Implement the regula falsi (false position) method, which combines the reliability of bisection with faster convergence by using linear interpolation. Unlike bisection which always chooses the midpoint, regula falsi uses the x-intercept of the secant line connecting f(a) and f(b). The method keeps a bracket but converges faster than bisection for many functions.",
    "starterCode": "import numpy as np\n\ndef regula_falsi(f, a, b, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root using regula falsi (false position) method.\n    \n    Uses x-intercept of secant line: c = (a*f(b) - b*f(a)) / (f(b) - f(a))\n    \n    Parameters:\n    - f: function\n    - a, b: interval endpoints (f(a) and f(b) must have opposite signs)\n    - tol: tolerance\n    - max_iter: maximum iterations\n    \n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    # TODO: Implement regula falsi\n    pass\n\n# Test\nf = lambda x: x**3 - x - 2\na, b = 1, 2\n\nroot, iters = regula_falsi(f, a, b)\nprint(f\"Root: {root:.10f}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"f(root): {f(root):.2e}\")",
    "hints": [
      "Compute c = (a*f(b) - b*f(a)) / (f(b) - f(a))",
      "Update interval same as bisection: keep the bracket",
      "Regula falsi can get stuck if the function is very curved",
      "Consider implementing Illinois algorithm to prevent stagnation"
    ],
    "solution": "import numpy as np\n\ndef regula_falsi(f, a, b, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root using regula falsi (false position) method.\n    \n    Uses x-intercept of secant line: c = (a*f(b) - b*f(a)) / (f(b) - f(a))\n    \n    Parameters:\n    - f: function\n    - a, b: interval endpoints (f(a) and f(b) must have opposite signs)\n    - tol: tolerance\n    - max_iter: maximum iterations\n    \n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    fa = f(a)\n    fb = f(b)\n    \n    if fa * fb > 0:\n        raise ValueError(\"f(a) and f(b) must have opposite signs\")\n    \n    for i in range(max_iter):\n        # Compute x-intercept of secant line\n        c = (a * fb - b * fa) / (fb - fa)\n        fc = f(c)\n        \n        if abs(fc) < tol or abs(b - a) < tol:\n            return c, i + 1\n        \n        # Update interval\n        if fa * fc < 0:\n            b = c\n            fb = fc\n        else:\n            a = c\n            fa = fc\n    \n    return c, max_iter\n\n# Test\nf = lambda x: x**3 - x - 2\na, b = 1, 2\n\nroot, iters = regula_falsi(f, a, b)\nprint(f\"Root: {root:.10f}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"f(root): {f(root):.2e}\")\n\n# Compare with bisection\nprint(\"\\nComparison with bisection (same function):\")\nprint(f\"Regula Falsi: {iters} iterations\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = x³ - x - 2, [1, 2]",
        "expectedOutput": "root ≈ 1.5213797",
        "isHidden": false,
        "description": "Find cubic root using regula falsi"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-8",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 2,
    "title": "Convergence Order Estimation",
    "description": "Implement a function to experimentally estimate the convergence order of root-finding methods. For a method with convergence order p, errors satisfy e_{n+1} ≈ C * e_n^p. By tracking errors at consecutive iterations, we can estimate p = log(e_{n+1}/e_n) / log(e_n/e_{n-1}). Compare estimated orders for bisection (p=1), secant (p≈1.618), and Newton (p=2).",
    "starterCode": "import numpy as np\n\ndef estimate_convergence_order(errors):\n    \"\"\"\n    Estimate convergence order from error sequence.\n    \n    For convergence order p: e_{n+1} ≈ C * e_n^p\n    Estimate: p ≈ log(e_{n+1}/e_n) / log(e_n/e_{n-1})\n    \n    Parameters:\n    - errors: list of absolute errors at each iteration\n    \n    Returns:\n    - orders: list of estimated orders\n    \"\"\"\n    # TODO: Implement convergence order estimation\n    pass\n\n# Test with Newton's method on f(x) = x² - 2\nf = lambda x: x**2 - 2\ndf = lambda x: 2*x\ntrue_root = np.sqrt(2)\n\n# Run Newton's method tracking errors\nx = 1.5\nerrors = []\nfor i in range(8):\n    errors.append(abs(x - true_root))\n    if errors[-1] < 1e-15:\n        break\n    x = x - f(x) / df(x)\n\norders = estimate_convergence_order(errors)\nprint(\"Newton's method convergence order estimation:\")\nfor i, order in enumerate(orders):\n    print(f\"  Estimated p_{i} ≈ {order:.3f}\")\nprint(f\"\\nExpected: p = 2 (quadratic convergence)\")",
    "hints": [
      "Use the formula p ≈ log(e_{n+1}/e_n) / log(e_n/e_{n-1})",
      "Handle cases where errors are zero or very small",
      "The estimate becomes more accurate as errors decrease",
      "Need at least 3 consecutive errors to estimate one order"
    ],
    "solution": "import numpy as np\n\ndef estimate_convergence_order(errors):\n    \"\"\"\n    Estimate convergence order from error sequence.\n    \n    For convergence order p: e_{n+1} ≈ C * e_n^p\n    Estimate: p ≈ log(e_{n+1}/e_n) / log(e_n/e_{n-1})\n    \n    Parameters:\n    - errors: list of absolute errors at each iteration\n    \n    Returns:\n    - orders: list of estimated orders\n    \"\"\"\n    orders = []\n    for i in range(1, len(errors) - 1):\n        if errors[i-1] > 1e-15 and errors[i] > 1e-15 and errors[i+1] > 1e-15:\n            ratio1 = errors[i] / errors[i-1]\n            ratio2 = errors[i+1] / errors[i]\n            if ratio1 > 1e-15:\n                p = np.log(ratio2) / np.log(ratio1)\n                orders.append(p)\n    return orders\n\n# Test with Newton's method on f(x) = x² - 2\nprint(\"Convergence Order Estimation\\n\")\nprint(\"=\"*60)\n\nf = lambda x: x**2 - 2\ndf = lambda x: 2*x\ntrue_root = np.sqrt(2)\n\n# Run Newton's method tracking errors\nx = 1.5\nerrors_newton = []\nfor i in range(8):\n    errors_newton.append(abs(x - true_root))\n    if errors_newton[-1] < 1e-15:\n        break\n    x = x - f(x) / df(x)\n\norders_newton = estimate_convergence_order(errors_newton)\nprint(\"\\nNewton's method:\")\nfor i, order in enumerate(orders_newton):\n    print(f\"  Estimated p_{i} ≈ {order:.3f}\")\nprint(f\"Expected: p = 2\")\n\n# Test with Secant method\nx0, x1 = 1.0, 2.0\nerrors_secant = []\nf0, f1 = f(x0), f(x1)\nfor i in range(10):\n    errors_secant.append(abs(x1 - true_root))\n    if errors_secant[-1] < 1e-15:\n        break\n    x_new = x1 - f1 * (x1 - x0) / (f1 - f0)\n    x0, f0 = x1, f1\n    x1, f1 = x_new, f(x_new)\n\norders_secant = estimate_convergence_order(errors_secant)\nprint(\"\\nSecant method:\")\nfor i, order in enumerate(orders_secant):\n    print(f\"  Estimated p_{i} ≈ {order:.3f}\")\nprint(f\"Expected: p ≈ 1.618 (golden ratio)\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "Newton's method errors",
        "expectedOutput": "Estimated order ≈ 2.0",
        "isHidden": false,
        "description": "Verify quadratic convergence detection"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-9",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 3,
    "title": "Brent's Method",
    "description": "Implement Brent's method, a hybrid root-finding algorithm that combines bisection, secant, and inverse quadratic interpolation. Brent's method guarantees convergence like bisection while achieving superlinear convergence when possible. It is one of the most robust and efficient root-finding methods available.",
    "starterCode": "import numpy as np\n\ndef brent(f, a, b, tol=1e-12, max_iter=100):\n    \"\"\"\n    Find root using Brent's method.\n    \n    Combines bisection, secant, and inverse quadratic interpolation.\n    \n    Parameters:\n    - f: function\n    - a, b: interval endpoints (f(a) and f(b) must have opposite signs)\n    - tol: tolerance\n    - max_iter: maximum iterations\n    \n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    # TODO: Implement Brent's method\n    pass\n\n# Test\nf = lambda x: x**3 - 2*x - 5\na, b = 2, 3\n\nroot, iters = brent(f, a, b)\nprint(f\"Root: {root:.15f}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"f(root): {f(root):.2e}\")",
    "hints": [
      "Track a, b, c where c is the previous best guess",
      "Use inverse quadratic interpolation when three distinct function values exist",
      "Fall back to secant or bisection when interpolation would fail",
      "Always accept the step if it improves the bracket"
    ],
    "solution": "import numpy as np\n\ndef brent(f, a, b, tol=1e-12, max_iter=100):\n    \"\"\"\n    Find root using Brent's method.\n    \n    Combines bisection, secant, and inverse quadratic interpolation.\n    \n    Parameters:\n    - f: function\n    - a, b: interval endpoints (f(a) and f(b) must have opposite signs)\n    - tol: tolerance\n    - max_iter: maximum iterations\n    \n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    fa = f(a)\n    fb = f(b)\n    \n    if fa * fb > 0:\n        raise ValueError(\"f(a) and f(b) must have opposite signs\")\n    \n    if abs(fa) < abs(fb):\n        a, b = b, a\n        fa, fb = fb, fa\n    \n    c = a\n    fc = fa\n    mflag = True\n    d = 0\n    \n    for i in range(max_iter):\n        if abs(fb) < tol:\n            return b, i + 1\n        \n        if abs(b - a) < tol:\n            return b, i + 1\n        \n        # Try inverse quadratic interpolation\n        if fa != fc and fb != fc:\n            s = (a * fb * fc) / ((fa - fb) * (fa - fc)) + \\\n                (b * fa * fc) / ((fb - fa) * (fb - fc)) + \\\n                (c * fa * fb) / ((fc - fa) * (fc - fb))\n        else:\n            # Secant method\n            s = b - fb * (b - a) / (fb - fa)\n        \n        # Conditions to reject interpolation and use bisection\n        cond1 = not ((3*a + b) / 4 < s < b or b < s < (3*a + b) / 4)\n        cond2 = mflag and abs(s - b) >= abs(b - c) / 2\n        cond3 = not mflag and abs(s - b) >= abs(c - d) / 2\n        cond4 = mflag and abs(b - c) < tol\n        cond5 = not mflag and abs(c - d) < tol\n        \n        if cond1 or cond2 or cond3 or cond4 or cond5:\n            s = (a + b) / 2\n            mflag = True\n        else:\n            mflag = False\n        \n        fs = f(s)\n        d = c\n        c = b\n        fc = fb\n        \n        if fa * fs < 0:\n            b = s\n            fb = fs\n        else:\n            a = s\n            fa = fs\n        \n        if abs(fa) < abs(fb):\n            a, b = b, a\n            fa, fb = fb, fa\n    \n    return b, max_iter\n\n# Test\nf = lambda x: x**3 - 2*x - 5\na, b = 2, 3\n\nroot, iters = brent(f, a, b)\nprint(f\"Root: {root:.15f}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"f(root): {f(root):.2e}\")\n\n# Compare with scipy\nfrom scipy.optimize import brentq\nscipy_root = brentq(f, a, b)\nprint(f\"\\nSciPy brentq: {scipy_root:.15f}\")\nprint(f\"Difference: {abs(root - scipy_root):.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = x³ - 2x - 5, [2, 3]",
        "expectedOutput": "root ≈ 2.0945514815",
        "isHidden": false,
        "description": "Find root using Brent's method"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-10",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 3,
    "title": "Steffensen's Method",
    "description": "Implement Steffensen's method, which achieves quadratic convergence like Newton's method but without requiring the derivative. It uses Aitken's Δ² acceleration applied to fixed-point iteration. For g(x) = x - f(x), the acceleration formula eliminates the need for f'(x) while maintaining quadratic convergence.",
    "starterCode": "import numpy as np\n\ndef steffensen(f, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root using Steffensen's method.\n    \n    Uses Aitken's Δ² acceleration on fixed-point iteration.\n    x_{n+1} = x_n - f(x_n)² / (f(x_n + f(x_n)) - f(x_n))\n    \n    Parameters:\n    - f: function\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n    \n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    # TODO: Implement Steffensen's method\n    pass\n\n# Test\nf = lambda x: x**2 - 2\nx0 = 1.0\n\nroot, iters = steffensen(f, x0)\nprint(f\"Root: {root:.15f}\")\nprint(f\"√2:   {np.sqrt(2):.15f}\")\nprint(f\"Iterations: {iters}\")",
    "hints": [
      "Compute g(x) = f(x + f(x)) and use the formula x_new = x - f(x)² / (g(x) - f(x))",
      "This is equivalent to applying the secant method with x and x + f(x)",
      "Achieves quadratic convergence without computing derivatives",
      "Be careful of division by small numbers"
    ],
    "solution": "import numpy as np\n\ndef steffensen(f, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root using Steffensen's method.\n    \n    Uses Aitken's Δ² acceleration on fixed-point iteration.\n    x_{n+1} = x_n - f(x_n)² / (f(x_n + f(x_n)) - f(x_n))\n    \n    Parameters:\n    - f: function\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n    \n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    x = x0\n    \n    for i in range(max_iter):\n        fx = f(x)\n        \n        if abs(fx) < tol:\n            return x, i + 1\n        \n        # Compute f(x + f(x))\n        gx = f(x + fx)\n        \n        # Avoid division by zero\n        denom = gx - fx\n        if abs(denom) < 1e-15:\n            return x, i + 1\n        \n        # Steffensen update\n        x_new = x - fx * fx / denom\n        \n        if abs(x_new - x) < tol:\n            return x_new, i + 1\n        \n        x = x_new\n    \n    return x, max_iter\n\n# Test\nf = lambda x: x**2 - 2\nx0 = 1.0\ntrue_root = np.sqrt(2)\n\nroot, iters = steffensen(f, x0)\nprint(f\"Root: {root:.15f}\")\nprint(f\"√2:   {true_root:.15f}\")\nprint(f\"Error: {abs(root - true_root):.2e}\")\nprint(f\"Iterations: {iters}\")\n\n# Compare with Newton\nprint(\"\\nComparison: Steffensen achieves quadratic convergence without derivatives!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = x² - 2, x0 = 1.0",
        "expectedOutput": "Converges to √2 with quadratic convergence",
        "isHidden": false,
        "description": "Test Steffensen's derivative-free quadratic convergence"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-11",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 4,
    "title": "Halley's Method",
    "description": "Implement Halley's method, which achieves cubic convergence using both first and second derivatives. The iteration is x_{n+1} = x_n - 2f(x_n)f'(x_n) / (2f'(x_n)² - f(x_n)f''(x_n)). While requiring more computation per step, Halley's method converges faster than Newton's method.",
    "starterCode": "import numpy as np\n\ndef halley(f, df, d2f, x0, tol=1e-12, max_iter=100):\n    \"\"\"\n    Find root using Halley's method (cubic convergence).\n    \n    x_{n+1} = x_n - 2f(x_n)f'(x_n) / (2f'(x_n)² - f(x_n)f''(x_n))\n    \n    Parameters:\n    - f: function\n    - df: first derivative\n    - d2f: second derivative\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n    \n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    # TODO: Implement Halley's method\n    pass\n\n# Test\nf = lambda x: x**3 - 2\ndf = lambda x: 3*x**2\nd2f = lambda x: 6*x\nx0 = 1.5\n\nroot, iters = halley(f, df, d2f, x0)\nprint(f\"Root: {root:.15f}\")\nprint(f\"2^(1/3): {2**(1/3):.15f}\")\nprint(f\"Iterations: {iters}\")",
    "hints": [
      "Use the formula: x_new = x - 2*f*df / (2*df² - f*d2f)",
      "This is derived from the second-order Taylor expansion",
      "Cubic convergence means errors satisfy e_{n+1} ≈ C * e_n³",
      "Requires fewer iterations than Newton but more work per iteration"
    ],
    "solution": "import numpy as np\n\ndef halley(f, df, d2f, x0, tol=1e-12, max_iter=100):\n    \"\"\"\n    Find root using Halley's method (cubic convergence).\n    \n    x_{n+1} = x_n - 2f(x_n)f'(x_n) / (2f'(x_n)² - f(x_n)f''(x_n))\n    \n    Parameters:\n    - f: function\n    - df: first derivative\n    - d2f: second derivative\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n    \n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    x = x0\n    \n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n        d2fx = d2f(x)\n        \n        if abs(fx) < tol:\n            return x, i + 1\n        \n        # Halley's formula\n        numer = 2 * fx * dfx\n        denom = 2 * dfx**2 - fx * d2fx\n        \n        if abs(denom) < 1e-15:\n            return x, i + 1\n        \n        x_new = x - numer / denom\n        \n        if abs(x_new - x) < tol:\n            return x_new, i + 1\n        \n        x = x_new\n    \n    return x, max_iter\n\n# Test\nf = lambda x: x**3 - 2\ndf = lambda x: 3*x**2\nd2f = lambda x: 6*x\nx0 = 1.5\ntrue_root = 2**(1/3)\n\nroot, iters = halley(f, df, d2f, x0)\nprint(f\"Root: {root:.15f}\")\nprint(f\"2^(1/3): {true_root:.15f}\")\nprint(f\"Error: {abs(root - true_root):.2e}\")\nprint(f\"Iterations: {iters}\")\n\n# Convergence analysis\nprint(\"\\nCubic convergence demonstration:\")\nx = x0\nfor i in range(4):\n    error = abs(x - true_root)\n    print(f\"  Iteration {i}: error = {error:.2e}\")\n    if error < 1e-15:\n        break\n    fx = f(x)\n    dfx = df(x)\n    d2fx = d2f(x)\n    x = x - 2*fx*dfx / (2*dfx**2 - fx*d2fx)\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = x³ - 2, x0 = 1.5",
        "expectedOutput": "Converges to 2^(1/3) with cubic convergence",
        "isHidden": false,
        "description": "Test Halley's cubic convergence"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-12",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 4,
    "title": "Müller's Method",
    "description": "Implement Müller's method, which uses quadratic interpolation through three points to find roots. Unlike the secant method (linear), Müller's method can find complex roots even when starting from real values. The method approximates f(x) with a parabola and uses the quadratic formula to find the next estimate.",
    "starterCode": "import numpy as np\n\ndef muller(f, x0, x1, x2, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root using Müller's method (quadratic interpolation).\n    \n    Uses three points to fit a parabola, then finds its root.\n    \n    Parameters:\n    - f: function\n    - x0, x1, x2: three initial guesses\n    - tol: tolerance\n    - max_iter: maximum iterations\n    \n    Returns:\n    - (root, iterations): root (may be complex) and number of iterations\n    \"\"\"\n    # TODO: Implement Müller's method\n    pass\n\n# Test on polynomial with real root\nf = lambda x: x**3 - x - 2\nx0, x1, x2 = 0.5, 1.0, 1.5\n\nroot, iters = muller(f, x0, x1, x2)\nprint(f\"Root: {root:.10f}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"f(root): {f(root):.2e}\")",
    "hints": [
      "Compute divided differences to get the quadratic coefficients",
      "Use the quadratic formula with the sign that gives the largest denominator",
      "Handle complex arithmetic - the root may be complex",
      "Convergence order is approximately 1.84"
    ],
    "solution": "import numpy as np\n\ndef muller(f, x0, x1, x2, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root using Müller's method (quadratic interpolation).\n    \n    Uses three points to fit a parabola, then finds its root.\n    \n    Parameters:\n    - f: function\n    - x0, x1, x2: three initial guesses\n    - tol: tolerance\n    - max_iter: maximum iterations\n    \n    Returns:\n    - (root, iterations): root (may be complex) and number of iterations\n    \"\"\"\n    x0, x1, x2 = complex(x0), complex(x1), complex(x2)\n    \n    for i in range(max_iter):\n        f0, f1, f2 = f(x0), f(x1), f(x2)\n        \n        # Compute divided differences\n        h0 = x1 - x0\n        h1 = x2 - x1\n        d0 = (f1 - f0) / h0\n        d1 = (f2 - f1) / h1\n        \n        # Quadratic coefficients\n        a = (d1 - d0) / (h1 + h0)\n        b = a * h1 + d1\n        c = f2\n        \n        # Discriminant\n        disc = np.sqrt(b**2 - 4*a*c)\n        \n        # Choose sign to maximize denominator\n        if abs(b + disc) > abs(b - disc):\n            denom = b + disc\n        else:\n            denom = b - disc\n        \n        if abs(denom) < 1e-15:\n            return x2, i + 1\n        \n        # Next estimate\n        x3 = x2 - 2*c / denom\n        \n        if abs(x3 - x2) < tol:\n            return x3, i + 1\n        \n        # Shift points\n        x0, x1, x2 = x1, x2, x3\n    \n    return x2, max_iter\n\n# Test on polynomial with real root\nf = lambda x: x**3 - x - 2\nx0, x1, x2 = 0.5, 1.0, 1.5\n\nroot, iters = muller(f, x0, x1, x2)\nprint(f\"Root: {root:.10f}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"f(root): {abs(f(root)):.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = x³ - x - 2, initial points 0.5, 1.0, 1.5",
        "expectedOutput": "root ≈ 1.5213797",
        "isHidden": false,
        "description": "Find real root using Müller's method"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-13",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 3,
    "title": "Polynomial Deflation",
    "description": "Implement polynomial deflation to find all roots of a polynomial. After finding one root r, divide the polynomial by (x - r) to get a lower-degree polynomial and repeat. Use Newton's method or Müller's method to find individual roots. Handle both real and complex roots.",
    "starterCode": "import numpy as np\n\ndef poly_eval(coeffs, x):\n    \"\"\"Evaluate polynomial with coefficients [a_n, ..., a_1, a_0] at x.\"\"\"\n    result = 0\n    for c in coeffs:\n        result = result * x + c\n    return result\n\ndef poly_deflate(coeffs, root):\n    \"\"\"\n    Divide polynomial by (x - root) using synthetic division.\n    \n    Parameters:\n    - coeffs: polynomial coefficients [a_n, ..., a_1, a_0]\n    - root: root to divide out\n    \n    Returns:\n    - new_coeffs: coefficients of quotient polynomial\n    \"\"\"\n    # TODO: Implement synthetic division\n    pass\n\ndef find_all_roots(coeffs, tol=1e-10):\n    \"\"\"\n    Find all roots of polynomial using deflation.\n    \n    Parameters:\n    - coeffs: polynomial coefficients [a_n, ..., a_1, a_0]\n    - tol: tolerance\n    \n    Returns:\n    - roots: list of all roots\n    \"\"\"\n    # TODO: Implement deflation-based root finding\n    pass\n\n# Test: x³ - 6x² + 11x - 6 = (x-1)(x-2)(x-3)\ncoeffs = [1, -6, 11, -6]\nroots = find_all_roots(coeffs)\nprint(\"Roots of x³ - 6x² + 11x - 6:\")\nfor r in roots:\n    print(f\"  x = {r:.10f}\")\nprint(\"\\nExpected: 1, 2, 3\")",
    "hints": [
      "Synthetic division: start with leading coeff, multiply by root, add to next coeff",
      "Use Newton or Müller to find each root before deflating",
      "Complex roots come in conjugate pairs for real polynomials",
      "Polish roots at the end using the original polynomial"
    ],
    "solution": "import numpy as np\n\ndef poly_eval(coeffs, x):\n    \"\"\"Evaluate polynomial with coefficients [a_n, ..., a_1, a_0] at x.\"\"\"\n    result = 0\n    for c in coeffs:\n        result = result * x + c\n    return result\n\ndef poly_deriv(coeffs):\n    \"\"\"Return coefficients of derivative.\"\"\"\n    n = len(coeffs) - 1\n    return [(n - i) * coeffs[i] for i in range(n)]\n\ndef poly_deflate(coeffs, root):\n    \"\"\"\n    Divide polynomial by (x - root) using synthetic division.\n    \n    Parameters:\n    - coeffs: polynomial coefficients [a_n, ..., a_1, a_0]\n    - root: root to divide out\n    \n    Returns:\n    - new_coeffs: coefficients of quotient polynomial\n    \"\"\"\n    n = len(coeffs) - 1\n    new_coeffs = [coeffs[0]]\n    for i in range(1, n):\n        new_coeffs.append(new_coeffs[-1] * root + coeffs[i])\n    return new_coeffs\n\ndef newton_poly(coeffs, x0, tol=1e-10, max_iter=100):\n    \"\"\"Newton's method for polynomials.\"\"\"\n    dcoeffs = poly_deriv(coeffs)\n    x = complex(x0)\n    for i in range(max_iter):\n        fx = poly_eval(coeffs, x)\n        dfx = poly_eval(dcoeffs, x)\n        if abs(dfx) < 1e-15:\n            break\n        x_new = x - fx / dfx\n        if abs(x_new - x) < tol:\n            return x_new\n        x = x_new\n    return x\n\ndef find_all_roots(coeffs, tol=1e-10):\n    \"\"\"\n    Find all roots of polynomial using deflation.\n    \n    Parameters:\n    - coeffs: polynomial coefficients [a_n, ..., a_1, a_0]\n    - tol: tolerance\n    \n    Returns:\n    - roots: list of all roots\n    \"\"\"\n    roots = []\n    current_coeffs = list(coeffs)\n    \n    while len(current_coeffs) > 1:\n        # Find a root of current polynomial\n        x0 = 0.5 + 0.5j  # Complex starting point\n        root = newton_poly(current_coeffs, x0, tol)\n        \n        # Check if essentially real\n        if abs(root.imag) < tol:\n            root = root.real\n        \n        roots.append(root)\n        \n        # Deflate\n        current_coeffs = poly_deflate(current_coeffs, root)\n    \n    # Polish roots using original polynomial\n    polished = []\n    for r in roots:\n        polished.append(newton_poly(coeffs, r, tol))\n    \n    return polished\n\n# Test: x³ - 6x² + 11x - 6 = (x-1)(x-2)(x-3)\ncoeffs = [1, -6, 11, -6]\nroots = find_all_roots(coeffs)\nprint(\"Roots of x³ - 6x² + 11x - 6:\")\nfor r in sorted([r.real if isinstance(r, complex) else r for r in roots]):\n    print(f\"  x = {r:.10f}\")\nprint(\"\\nExpected: 1, 2, 3\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "x³ - 6x² + 11x - 6",
        "expectedOutput": "roots: 1, 2, 3",
        "isHidden": false,
        "description": "Find all roots of cubic polynomial"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-14",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 3,
    "title": "Newton-Horner Method for Polynomials",
    "description": "Implement the Newton-Horner method, an efficient combination of Newton's method with Horner's scheme for polynomial evaluation. Horner's method evaluates both f(x) and f'(x) in a single pass through the coefficients, making polynomial root finding very efficient.",
    "starterCode": "import numpy as np\n\ndef horner_eval(coeffs, x):\n    \"\"\"\n    Evaluate polynomial and its derivative using Horner's scheme.\n    \n    Coefficients are [a_n, a_{n-1}, ..., a_1, a_0].\n    \n    Parameters:\n    - coeffs: polynomial coefficients\n    - x: point to evaluate\n    \n    Returns:\n    - (f(x), f'(x)): function value and derivative\n    \"\"\"\n    # TODO: Implement Horner's scheme for f and f'\n    pass\n\ndef newton_horner(coeffs, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root of polynomial using Newton-Horner method.\n    \n    Parameters:\n    - coeffs: polynomial coefficients [a_n, ..., a_0]\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n    \n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    # TODO: Implement Newton-Horner\n    pass\n\n# Test: x³ - x - 2\ncoeffs = [1, 0, -1, -2]\nx0 = 1.5\n\nroot, iters = newton_horner(coeffs, x0)\nprint(f\"Root: {root:.15f}\")\nprint(f\"Iterations: {iters}\")",
    "hints": [
      "Horner's scheme: p = a_n, then p = p*x + a_i for i = n-1 to 0",
      "Track q = p' = n*a_n*x^{n-1} + ... during the same loop",
      "This gives both f(x) and f'(x) in O(n) operations",
      "Newton update: x_new = x - f(x)/f'(x)"
    ],
    "solution": "import numpy as np\n\ndef horner_eval(coeffs, x):\n    \"\"\"\n    Evaluate polynomial and its derivative using Horner's scheme.\n    \n    Coefficients are [a_n, a_{n-1}, ..., a_1, a_0].\n    \n    Parameters:\n    - coeffs: polynomial coefficients\n    - x: point to evaluate\n    \n    Returns:\n    - (f(x), f'(x)): function value and derivative\n    \"\"\"\n    n = len(coeffs) - 1\n    p = coeffs[0]  # f(x)\n    q = 0  # f'(x)\n    \n    for i in range(1, len(coeffs)):\n        q = q * x + p\n        p = p * x + coeffs[i]\n    \n    return p, q\n\ndef newton_horner(coeffs, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root of polynomial using Newton-Horner method.\n    \n    Parameters:\n    - coeffs: polynomial coefficients [a_n, ..., a_0]\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n    \n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    x = x0\n    \n    for i in range(max_iter):\n        fx, dfx = horner_eval(coeffs, x)\n        \n        if abs(fx) < tol:\n            return x, i + 1\n        \n        if abs(dfx) < 1e-15:\n            break\n        \n        x_new = x - fx / dfx\n        \n        if abs(x_new - x) < tol:\n            return x_new, i + 1\n        \n        x = x_new\n    \n    return x, max_iter\n\n# Test: x³ - x - 2\ncoeffs = [1, 0, -1, -2]\nx0 = 1.5\n\nroot, iters = newton_horner(coeffs, x0)\nprint(f\"Root: {root:.15f}\")\nprint(f\"Iterations: {iters}\")\n\n# Verify\nprint(f\"p(root) = {horner_eval(coeffs, root)[0]:.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "x³ - x - 2, x0 = 1.5",
        "expectedOutput": "root ≈ 1.5213797",
        "isHidden": false,
        "description": "Efficient polynomial root finding"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-15",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 5,
    "title": "Complex Roots via Müller's Method",
    "description": "Extend Müller's method to reliably find all complex roots of polynomials with real coefficients. Implement the full algorithm including proper handling of complex arithmetic, conjugate pairs, and root polishing. Find all roots of x⁴ + 1 = 0 (which has only complex roots).",
    "starterCode": "import numpy as np\n\ndef muller_complex(f, x0, x1, x2, tol=1e-10, max_iter=100):\n    \"\"\"\n    Müller's method with full complex arithmetic.\n    \n    Parameters:\n    - f: function (should handle complex input)\n    - x0, x1, x2: three initial guesses (complex)\n    - tol: tolerance\n    - max_iter: maximum iterations\n    \n    Returns:\n    - (root, iterations): complex root and iterations\n    \"\"\"\n    # TODO: Implement Müller's method for complex roots\n    pass\n\ndef find_all_complex_roots(coeffs, tol=1e-10):\n    \"\"\"\n    Find all roots of polynomial including complex ones.\n    \n    Parameters:\n    - coeffs: polynomial coefficients [a_n, ..., a_0]\n    - tol: tolerance\n    \n    Returns:\n    - roots: list of all roots (may be complex)\n    \"\"\"\n    # TODO: Implement using Müller + deflation\n    pass\n\n# Test: x⁴ + 1 = 0 has roots e^{iπ/4}, e^{i3π/4}, e^{i5π/4}, e^{i7π/4}\ncoeffs = [1, 0, 0, 0, 1]\nroots = find_all_complex_roots(coeffs)\n\nprint(\"Roots of x⁴ + 1:\")\nfor r in roots:\n    print(f\"  {r:.10f}\")\nprint(\"\\nExpected: ±(1±i)/√2\")",
    "hints": [
      "All arithmetic should use complex numbers from the start",
      "Use numpy's complex square root for the discriminant",
      "After finding a complex root, also deflate by its conjugate",
      "Polish all roots against the original polynomial at the end"
    ],
    "solution": "import numpy as np\n\ndef poly_eval_complex(coeffs, x):\n    \"\"\"Evaluate polynomial at complex x.\"\"\"\n    result = complex(0)\n    for c in coeffs:\n        result = result * x + c\n    return result\n\ndef poly_deflate_complex(coeffs, root):\n    \"\"\"Synthetic division with complex root.\"\"\"\n    n = len(coeffs) - 1\n    new_coeffs = [complex(coeffs[0])]\n    for i in range(1, n):\n        new_coeffs.append(new_coeffs[-1] * root + coeffs[i])\n    return new_coeffs\n\ndef muller_complex(f, x0, x1, x2, tol=1e-10, max_iter=100):\n    \"\"\"\n    Müller's method with full complex arithmetic.\n    \n    Parameters:\n    - f: function (should handle complex input)\n    - x0, x1, x2: three initial guesses (complex)\n    - tol: tolerance\n    - max_iter: maximum iterations\n    \n    Returns:\n    - (root, iterations): complex root and iterations\n    \"\"\"\n    x0, x1, x2 = complex(x0), complex(x1), complex(x2)\n    \n    for i in range(max_iter):\n        f0, f1, f2 = f(x0), f(x1), f(x2)\n        \n        h0 = x1 - x0\n        h1 = x2 - x1\n        d0 = (f1 - f0) / h0\n        d1 = (f2 - f1) / h1\n        \n        a = (d1 - d0) / (h1 + h0)\n        b = a * h1 + d1\n        c = f2\n        \n        disc = np.sqrt(b**2 - 4*a*c)\n        \n        if abs(b + disc) > abs(b - disc):\n            denom = b + disc\n        else:\n            denom = b - disc\n        \n        if abs(denom) < 1e-15:\n            return x2, i + 1\n        \n        x3 = x2 - 2*c / denom\n        \n        if abs(x3 - x2) < tol:\n            return x3, i + 1\n        \n        x0, x1, x2 = x1, x2, x3\n    \n    return x2, max_iter\n\ndef find_all_complex_roots(coeffs, tol=1e-10):\n    \"\"\"\n    Find all roots of polynomial including complex ones.\n    \n    Parameters:\n    - coeffs: polynomial coefficients [a_n, ..., a_0]\n    - tol: tolerance\n    \n    Returns:\n    - roots: list of all roots (may be complex)\n    \"\"\"\n    roots = []\n    current_coeffs = [complex(c) for c in coeffs]\n    \n    while len(current_coeffs) > 1:\n        f = lambda x: poly_eval_complex(current_coeffs, x)\n        \n        # Try different starting points\n        for x0 in [0.5+0.5j, -0.5+0.5j, 1.0, -1.0+1.0j]:\n            try:\n                root, _ = muller_complex(f, x0, x0+0.1, x0+0.2, tol)\n                if abs(f(root)) < 1e-6:\n                    break\n            except:\n                continue\n        \n        roots.append(root)\n        current_coeffs = poly_deflate_complex(current_coeffs, root)\n    \n    # Polish roots\n    f_orig = lambda x: poly_eval_complex(coeffs, x)\n    polished = []\n    for r in roots:\n        pr, _ = muller_complex(f_orig, r, r+0.01, r+0.02, tol)\n        polished.append(pr)\n    \n    return polished\n\n# Test: x⁴ + 1 = 0 has roots e^{iπ/4}, e^{i3π/4}, e^{i5π/4}, e^{i7π/4}\ncoeffs = [1, 0, 0, 0, 1]\nroots = find_all_complex_roots(coeffs)\n\nprint(\"Roots of x⁴ + 1:\")\nfor r in roots:\n    print(f\"  {r:.6f}\")\n\n# Expected roots\nprint(\"\\nExpected (e^{ikπ/4} for k=1,3,5,7):\")\nfor k in [1, 3, 5, 7]:\n    expected = np.exp(1j * k * np.pi / 4)\n    print(f\"  {expected:.6f}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "x⁴ + 1 = 0",
        "expectedOutput": "Four complex roots on unit circle",
        "isHidden": false,
        "description": "Find all complex roots of polynomial"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-16",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 5,
    "title": "Robust Root Finder",
    "description": "Implement a robust root-finding function that automatically selects and switches between methods based on convergence behavior. Start with a fast method (Newton/Secant), detect if it's not converging, and fall back to a safer method (bisection/Brent). Include safeguards against divergence and cycling.",
    "starterCode": "import numpy as np\n\ndef robust_root(f, a, b, df=None, tol=1e-12, max_iter=100):\n    \"\"\"\n    Robust root finder with automatic method selection.\n    \n    Tries fast methods first, falls back to safer methods if needed.\n    \n    Parameters:\n    - f: function\n    - a, b: interval (assumes root is bracketed)\n    - df: derivative (optional, for Newton's method)\n    - tol: tolerance\n    - max_iter: maximum total iterations\n    \n    Returns:\n    - (root, iterations, method_used): result\n    \"\"\"\n    # TODO: Implement robust root finder\n    pass\n\n# Test on various functions\ntest_cases = [\n    (lambda x: x**3 - x - 2, 1, 2, \"smooth cubic\"),\n    (lambda x: np.sign(x - 1.5), 0, 3, \"step function\"),\n    (lambda x: (x - 2)**20, 1, 3, \"flat near root\"),\n    (lambda x: np.exp(x) - 100, 0, 10, \"exponential\"),\n]\n\nfor f, a, b, name in test_cases:\n    root, iters, method = robust_root(f, a, b)\n    print(f\"{name}: root={root:.10f}, iters={iters}, method={method}\")",
    "hints": [
      "Start with Newton/Secant for speed if derivative available",
      "Monitor convergence rate - switch if it stalls",
      "Keep a bracket [a, b] maintained at all times",
      "Use bisection as ultimate fallback",
      "Count iterations across all method attempts"
    ],
    "solution": "import numpy as np\n\ndef robust_root(f, a, b, df=None, tol=1e-12, max_iter=100):\n    \"\"\"\n    Robust root finder with automatic method selection.\n    \n    Tries fast methods first, falls back to safer methods if needed.\n    \n    Parameters:\n    - f: function\n    - a, b: interval (assumes root is bracketed)\n    - df: derivative (optional, for Newton's method)\n    - tol: tolerance\n    - max_iter: maximum total iterations\n    \n    Returns:\n    - (root, iterations, method_used): result\n    \"\"\"\n    fa, fb = f(a), f(b)\n    \n    if fa * fb > 0:\n        # Not bracketed - try anyway\n        pass\n    \n    # Ensure a < b and fa and fb have correct signs\n    if abs(fa) < abs(fb):\n        a, b = b, a\n        fa, fb = fb, fa\n    \n    total_iters = 0\n    method_used = \"none\"\n    \n    # Try Newton if derivative available\n    if df is not None:\n        x = (a + b) / 2\n        prev_x = x\n        newton_worked = True\n        \n        for i in range(min(10, max_iter)):\n            total_iters += 1\n            fx = f(x)\n            dfx = df(x)\n            \n            if abs(fx) < tol:\n                return x, total_iters, \"Newton\"\n            \n            if abs(dfx) < 1e-15:\n                newton_worked = False\n                break\n            \n            x_new = x - fx / dfx\n            \n            # Check if still in bracket\n            if x_new < min(a, b) or x_new > max(a, b):\n                newton_worked = False\n                break\n            \n            # Check for convergence\n            if abs(x_new - x) < tol:\n                return x_new, total_iters, \"Newton\"\n            \n            # Check if making progress\n            if abs(x_new - x) > 0.5 * abs(x - prev_x) and i > 2:\n                newton_worked = False\n                break\n            \n            prev_x = x\n            x = x_new\n        \n        if newton_worked and abs(f(x)) < tol * 100:\n            return x, total_iters, \"Newton\"\n    \n    # Fall back to Brent-like method\n    c = a\n    fc = fa\n    d = b - a\n    e = d\n    \n    for i in range(max_iter - total_iters):\n        total_iters += 1\n        \n        if abs(fc) < abs(fb):\n            a, b, c = b, c, a\n            fa, fb, fc = fb, fc, fa\n        \n        tol1 = 2 * np.finfo(float).eps * abs(b) + tol / 2\n        m = (c - b) / 2\n        \n        if abs(m) <= tol1 or fb == 0:\n            return b, total_iters, \"Brent\"\n        \n        # Try interpolation\n        if abs(e) >= tol1 and abs(fa) > abs(fb):\n            s = fb / fa\n            if a == c:\n                p = 2 * m * s\n                q = 1 - s\n            else:\n                q = fa / fc\n                r = fb / fc\n                p = s * (2 * m * q * (q - r) - (b - a) * (r - 1))\n                q = (q - 1) * (r - 1) * (s - 1)\n            \n            if p > 0:\n                q = -q\n            p = abs(p)\n            \n            if 2 * p < min(3 * m * q - abs(tol1 * q), abs(e * q)):\n                e = d\n                d = p / q\n            else:\n                d = m\n                e = m\n        else:\n            d = m\n            e = m\n        \n        a = b\n        fa = fb\n        \n        if abs(d) > tol1:\n            b = b + d\n        elif m > 0:\n            b = b + tol1\n        else:\n            b = b - tol1\n        \n        fb = f(b)\n        \n        if (fb > 0) == (fc > 0):\n            c = a\n            fc = fa\n            d = b - a\n            e = d\n    \n    return b, total_iters, \"Brent\"\n\n# Test on various functions\ntest_cases = [\n    (lambda x: x**3 - x - 2, 1, 2, \"smooth cubic\"),\n    (lambda x: np.exp(x) - 100, 0, 10, \"exponential\"),\n    (lambda x: x**2 - 2, 0, 2, \"quadratic\"),\n]\n\nfor f, a, b, name in test_cases:\n    root, iters, method = robust_root(f, a, b)\n    print(f\"{name}: root={root:.10f}, iters={iters}, method={method}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "Various test functions",
        "expectedOutput": "Finds roots robustly with minimal iterations",
        "isHidden": false,
        "description": "Test robust method selection"
      }
    ],
    "language": "python"
  }
]
