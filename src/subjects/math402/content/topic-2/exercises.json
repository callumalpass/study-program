[
  {
    "id": "math402-ex-2-1",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 1,
    "title": "Bisection Method",
    "description": "Implement the bisection method for finding roots of continuous functions. The bisection method is guaranteed to converge by repeatedly halving an interval that contains a root.",
    "starterCode": "import numpy as np\n\ndef bisection(f, a, b, tol=1e-6, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using bisection method.\n\n    Parameters:\n    - f: function\n    - a, b: interval endpoints (f(a) and f(b) must have opposite signs)\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function: f(x) = x² - 2\nf = lambda x: x**2 - 2\na, b = 0, 2\n\nprint(\"Finding root of f(x) = x² - 2 on [0, 2]\\n\")\n\nroot, iters = bisection(f, a, b)\n\nprint(f\"Root: {root:.10f}\")\nprint(f\"√2:   {np.sqrt(2):.10f}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"f(root): {f(root):.2e}\")",
    "hints": [
      "Check that f(a) and f(b) have opposite signs",
      "Compute midpoint c = (a + b) / 2",
      "Update interval based on sign of f(c)",
      "Stop when |b - a| < tolerance"
    ],
    "solution": "import numpy as np\n\ndef bisection(f, a, b, tol=1e-6, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using bisection method.\n\n    Parameters:\n    - f: function\n    - a, b: interval endpoints (f(a) and f(b) must have opposite signs)\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    fa = f(a)\n    fb = f(b)\n\n    # Check that f(a) and f(b) have opposite signs\n    if fa * fb > 0:\n        raise ValueError(\"f(a) and f(b) must have opposite signs\")\n\n    for i in range(max_iter):\n        # Compute midpoint\n        c = (a + b) / 2\n        fc = f(c)\n\n        # Check convergence\n        if abs(b - a) < tol or abs(fc) < tol:\n            return c, i + 1\n\n        # Update interval\n        if fa * fc < 0:\n            b = c\n            fb = fc\n        else:\n            a = c\n            fa = fc\n\n    # Max iterations reached\n    return (a + b) / 2, max_iter\n\n# Test function: f(x) = x² - 2\nf = lambda x: x**2 - 2\na, b = 0, 2\n\nprint(\"Bisection Method: Finding root of f(x) = x² - 2\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nroot, iters = bisection(f, a, b)\n\nprint(f\"Root:       {root:.10f}\")\nprint(f\"True (√2):  {np.sqrt(2):.10f}\")\nprint(f\"Error:      {abs(root - np.sqrt(2)):.2e}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"f(root):    {f(root):.2e}\\n\")\n\n# Test more functions\nprint(\"=\"*60)\nprint(\"\\nAdditional Test Cases\\n\")\nprint(\"=\"*60 + \"\\n\")\n\ntest_cases = [\n    (lambda x: x**3 - x - 2, 1, 2, 1.5213797, \"x³ - x - 2\"),\n    (lambda x: np.cos(x) - x, 0, 1, 0.7390851, \"cos(x) - x\"),\n    (lambda x: np.exp(x) - 3, 0, 2, 1.0986123, \"e^x - 3\"),\n]\n\nfor f, a, b, true_root, desc in test_cases:\n    root, iters = bisection(f, a, b, tol=1e-10)\n    error = abs(root - true_root)\n    print(f\"{desc}:\")\n    print(f\"  Root: {root:.10f}\")\n    print(f\"  Error: {error:.2e}\")\n    print(f\"  Iterations: {iters}\\n\")\n\nprint(\"All tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = x² - 2, [0, 2]",
        "expectedOutput": "root ≈ 1.4142135624 (√2)",
        "isHidden": false,
        "description": "Find square root of 2 using bisection"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-2",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 2,
    "title": "Newton Method",
    "description": "Implement Newton's method for root finding. Newton's method uses the derivative to achieve quadratic convergence, but requires a good initial guess and derivative information.",
    "starterCode": "import numpy as np\n\ndef newton(f, df, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using Newton's method.\n\n    Newton iteration: x_{n+1} = x_n - f(x_n)/f'(x_n)\n\n    Parameters:\n    - f: function\n    - df: derivative of f\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function: f(x) = x² - 2\nf = lambda x: x**2 - 2\ndf = lambda x: 2*x\nx0 = 1.0\n\nprint(\"Finding root of f(x) = x² - 2\\n\")\nprint(f\"Initial guess: x0 = {x0}\\n\")\n\nroot, iters = newton(f, df, x0)\n\nprint(f\"Root: {root:.15f}\")\nprint(f\"√2:   {np.sqrt(2):.15f}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"Error: {abs(root - np.sqrt(2)):.2e}\")",
    "hints": [
      "Newton iteration: x_new = x - f(x)/f'(x)",
      "Check for division by zero (f'(x) = 0)",
      "Monitor |x_new - x| for convergence",
      "Newton converges quadratically near the root"
    ],
    "solution": "import numpy as np\n\ndef newton(f, df, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using Newton's method.\n\n    Newton iteration: x_{n+1} = x_n - f(x_n)/f'(x_n)\n\n    Parameters:\n    - f: function\n    - df: derivative of f\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    x = x0\n\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n\n        # Check for zero derivative\n        if abs(dfx) < 1e-15:\n            raise ValueError(\"Derivative is zero at x = {x}\")\n\n        # Newton iteration\n        x_new = x - fx / dfx\n\n        # Check convergence\n        if abs(x_new - x) < tol or abs(fx) < tol:\n            return x_new, i + 1\n\n        x = x_new\n\n    return x, max_iter\n\n# Test function: f(x) = x² - 2\nprint(\"Newton's Method: Finding root of f(x) = x² - 2\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf = lambda x: x**2 - 2\ndf = lambda x: 2*x\nx0 = 1.0\n\nprint(f\"Initial guess: x0 = {x0}\\n\")\n\nroot, iters = newton(f, df, x0)\ntrue_root = np.sqrt(2)\n\nprint(f\"Root:       {root:.15f}\")\nprint(f\"True (√2):  {true_root:.15f}\")\nprint(f\"Error:      {abs(root - true_root):.2e}\")\nprint(f\"Iterations: {iters}\\n\")\n\n# Compare convergence with bisection\nprint(\"=\"*60)\nprint(\"\\nConvergence Comparison\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Newton with iteration tracking\nx = x0\nprint(\"Newton's method convergence:\")\nfor i in range(5):\n    fx = f(x)\n    dfx = df(x)\n    x_new = x - fx / dfx\n    error = abs(x_new - true_root)\n    print(f\"  Iteration {i+1}: x = {x_new:.15f}, error = {error:.2e}\")\n    if error < 1e-15:\n        break\n    x = x_new\n\nprint(\"\\nNote: Newton's method achieves machine precision in few iterations!\")\n\n# Test quadratic convergence\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nQuadratic Convergence\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nx = x0\nerrors = []\nfor i in range(5):\n    error = abs(x - true_root)\n    errors.append(error)\n    if error < 1e-15:\n        break\n    fx = f(x)\n    dfx = df(x)\n    x = x - fx / dfx\n\nprint(\"Error sequence:\")\nfor i, err in enumerate(errors):\n    print(f\"  e_{i} = {err:.2e}\")\n    if i > 0:\n        ratio = errors[i] / errors[i-1]**2\n        print(f\"    e_{i}/e_{i-1}² ≈ {ratio:.2f} (should be roughly constant)\\n\")\n\n# Additional test cases\nprint(\"=\"*60)\nprint(\"\\nAdditional Test Cases\\n\")\nprint(\"=\"*60 + \"\\n\")\n\ntest_cases = [\n    (lambda x: x**3 - 2, lambda x: 3*x**2, 1.5, 2**(1/3), \"x³ = 2\"),\n    (lambda x: np.cos(x) - x, lambda x: -np.sin(x) - 1, 0.5, 0.7390851332, \"cos(x) = x\"),\n    (lambda x: np.exp(x) - 3, lambda x: np.exp(x), 1.0, np.log(3), \"e^x = 3\"),\n]\n\nfor f, df, x0, true_root, desc in test_cases:\n    root, iters = newton(f, df, x0)\n    error = abs(root - true_root)\n    print(f\"{desc}:\")\n    print(f\"  Root: {root:.12f}\")\n    print(f\"  Error: {error:.2e}\")\n    print(f\"  Iterations: {iters}\\n\")\n\nprint(\"All tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = x² - 2, df(x) = 2x, x0 = 1.0",
        "expectedOutput": "Converges to √2 in ~4 iterations",
        "isHidden": false,
        "description": "Test Newton method with quadratic convergence"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-3",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 2,
    "title": "Secant Method",
    "description": "Implement the secant method as a derivative-free alternative to Newton's method. The secant method approximates the derivative using finite differences.",
    "starterCode": "import numpy as np\n\ndef secant(f, x0, x1, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using secant method.\n\n    Secant iteration: x_{n+1} = x_n - f(x_n) * (x_n - x_{n-1}) / (f(x_n) - f(x_{n-1}))\n\n    Parameters:\n    - f: function\n    - x0, x1: initial guesses\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function\nf = lambda x: x**2 - 2\nx0, x1 = 1.0, 2.0\n\nprint(\"Finding root of f(x) = x² - 2\\n\")\nprint(f\"Initial guesses: x0 = {x0}, x1 = {x1}\\n\")\n\nroot, iters = secant(f, x0, x1)\n\nprint(f\"Root: {root:.15f}\")\nprint(f\"√2:   {np.sqrt(2):.15f}\")\nprint(f\"Iterations: {iters}\")",
    "hints": [
      "Secant approximates derivative: f'(x) ≈ (f(x_n) - f(x_{n-1})) / (x_n - x_{n-1})",
      "Update: x_new = x1 - f(x1) * (x1 - x0) / (f(x1) - f(x0))",
      "Check for division by zero",
      "Convergence order is φ ≈ 1.618 (golden ratio)"
    ],
    "solution": "import numpy as np\n\ndef secant(f, x0, x1, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using secant method.\n\n    Secant iteration: x_{n+1} = x_n - f(x_n) * (x_n - x_{n-1}) / (f(x_n) - f(x_{n-1}))\n\n    Parameters:\n    - f: function\n    - x0, x1: initial guesses\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    f0 = f(x0)\n    f1 = f(x1)\n\n    for i in range(max_iter):\n        # Check for division by zero\n        if abs(f1 - f0) < 1e-15:\n            raise ValueError(\"Division by zero in secant method\")\n\n        # Secant iteration\n        x_new = x1 - f1 * (x1 - x0) / (f1 - f0)\n\n        # Check convergence\n        if abs(x_new - x1) < tol:\n            return x_new, i + 1\n\n        # Update for next iteration\n        x0, f0 = x1, f1\n        x1, f1 = x_new, f(x_new)\n\n    return x1, max_iter\n\n# Test function\nprint(\"Secant Method: Finding root of f(x) = x² - 2\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf = lambda x: x**2 - 2\nx0, x1 = 1.0, 2.0\ntrue_root = np.sqrt(2)\n\nprint(f\"Initial guesses: x0 = {x0}, x1 = {x1}\\n\")\n\nroot, iters = secant(f, x0, x1)\n\nprint(f\"Root:       {root:.15f}\")\nprint(f\"True (√2):  {true_root:.15f}\")\nprint(f\"Error:      {abs(root - true_root):.2e}\")\nprint(f\"Iterations: {iters}\\n\")\n\n# Convergence analysis\nprint(\"=\"*60)\nprint(\"\\nConvergence Analysis\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nx0, x1 = 1.0, 2.0\nerrors = []\n\nf0 = f(x0)\nf1 = f(x1)\n\nprint(\"Iteration history:\")\nfor i in range(10):\n    error = abs(x1 - true_root)\n    errors.append(error)\n    print(f\"  Iteration {i}: x = {x1:.15f}, error = {error:.2e}\")\n\n    if error < 1e-15:\n        break\n\n    # Secant iteration\n    if abs(f1 - f0) < 1e-15:\n        break\n    x_new = x1 - f1 * (x1 - x0) / (f1 - f0)\n    x0, f0 = x1, f1\n    x1, f1 = x_new, f(x_new)\n\n# Estimate convergence order\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nConvergence Order Estimation\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nif len(errors) >= 4:\n    # Estimate α from e_{n+1} ≈ C * e_n^α\n    # Taking logs: log(e_{n+1}) ≈ log(C) + α * log(e_n)\n    # Slope gives α\n\n    print(\"Error sequence and estimated convergence order:\\n\")\n    for i in range(min(5, len(errors)-1)):\n        if errors[i] > 0 and errors[i+1] > 0:\n            if i > 0 and errors[i-1] > 0:\n                alpha = np.log(errors[i+1]/errors[i]) / np.log(errors[i]/errors[i-1])\n                print(f\"  e_{i} = {errors[i]:.2e}\")\n                print(f\"    Estimated order α ≈ {alpha:.3f}\\n\")\n\n    print(\"Note: Secant method has convergence order φ ≈ 1.618 (golden ratio)\")\n\n# Method comparison\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nMethod Comparison\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Secant (no derivative)\nf_test = lambda x: x**3 - 2\nx0, x1 = 1.0, 2.0\nroot_secant, iters_secant = secant(f_test, x0, x1)\n\n# Newton (requires derivative)\ndf_test = lambda x: 3*x**2\ndef newton_simple(f, df, x0, tol=1e-10, max_iter=100):\n    x = x0\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n        if abs(dfx) < 1e-15:\n            break\n        x_new = x - fx / dfx\n        if abs(x_new - x) < tol:\n            return x_new, i + 1\n        x = x_new\n    return x, max_iter\n\nroot_newton, iters_newton = newton_simple(f_test, df_test, 1.5)\n\ntrue_root_test = 2**(1/3)\n\nprint(f\"Finding ³√2:\\n\")\nprint(f\"Secant method:\")\nprint(f\"  Root: {root_secant:.15f}\")\nprint(f\"  Error: {abs(root_secant - true_root_test):.2e}\")\nprint(f\"  Iterations: {iters_secant}\\n\")\n\nprint(f\"Newton's method:\")\nprint(f\"  Root: {root_newton:.15f}\")\nprint(f\"  Error: {abs(root_newton - true_root_test):.2e}\")\nprint(f\"  Iterations: {iters_newton}\\n\")\n\nprint(\"Secant: No derivative needed, slightly more iterations\")\nprint(\"Newton: Requires derivative, faster convergence\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = x² - 2, x0 = 1.0, x1 = 2.0",
        "expectedOutput": "Converges to √2 with superlinear convergence",
        "isHidden": false,
        "description": "Test secant method convergence"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-4",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 3,
    "title": "Fixed-Point Iteration",
    "description": "Implement fixed-point iteration and analyze convergence conditions. A fixed point of g(x) satisfies x = g(x), and the iteration x_{n+1} = g(x_n) converges when |g'(x)| < 1 near the fixed point.",
    "starterCode": "import numpy as np\n\ndef fixed_point(g, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find fixed point of g(x) using iteration x_{n+1} = g(x_n).\n\n    Converges when |g'(x*)| < 1 at the fixed point x*.\n\n    Parameters:\n    - g: iteration function\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (fixed_point, iterations, converged): result, iterations, success flag\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef analyze_convergence(g, dg, x_star):\n    \"\"\"\n    Analyze convergence of fixed-point iteration.\n\n    Parameters:\n    - g: iteration function\n    - dg: derivative of g\n    - x_star: fixed point\n\n    Returns:\n    - convergence_info: dictionary with analysis\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Example: Find √2 by solving x = 2/x, rearranged as x = (x + 2/x)/2\ng = lambda x: (x + 2/x) / 2  # Babylonian method\nx0 = 1.0\n\nprint(\"Fixed-Point Iteration: Finding √2\\n\")\n\nroot, iters, converged = fixed_point(g, x0)\n\nprint(f\"Fixed point: {root:.15f}\")\nprint(f\"√2:          {np.sqrt(2):.15f}\")\nprint(f\"Iterations:  {iters}\")\nprint(f\"Converged:   {converged}\")",
    "hints": [
      "Iterate: x = g(x) until |x_new - x| < tolerance",
      "Check |g'(x*)| < 1 for convergence",
      "Linear convergence when 0 < |g'(x*)| < 1",
      "Quadratic convergence when g'(x*) = 0"
    ],
    "solution": "import numpy as np\n\ndef fixed_point(g, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find fixed point of g(x) using iteration x_{n+1} = g(x_n).\n\n    Converges when |g'(x*)| < 1 at the fixed point x*.\n\n    Parameters:\n    - g: iteration function\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (fixed_point, iterations, converged): result, iterations, success flag\n    \"\"\"\n    x = x0\n\n    for i in range(max_iter):\n        x_new = g(x)\n\n        # Check convergence\n        if abs(x_new - x) < tol:\n            return x_new, i + 1, True\n\n        x = x_new\n\n    # Did not converge\n    return x, max_iter, False\n\ndef analyze_convergence(g, dg, x_star):\n    \"\"\"\n    Analyze convergence of fixed-point iteration.\n\n    Parameters:\n    - g: iteration function\n    - dg: derivative of g\n    - x_star: fixed point\n\n    Returns:\n    - convergence_info: dictionary with analysis\n    \"\"\"\n    dg_star = dg(x_star)\n\n    info = {\n        'derivative': dg_star,\n        'will_converge': abs(dg_star) < 1,\n        'convergence_type': None,\n        'asymptotic_error_constant': abs(dg_star)\n    }\n\n    if abs(dg_star) < 1e-10:\n        info['convergence_type'] = 'At least quadratic'\n    elif abs(dg_star) < 1:\n        info['convergence_type'] = 'Linear'\n    else:\n        info['convergence_type'] = 'Divergent'\n\n    return info\n\n# Example 1: Babylonian method for √2\nprint(\"Fixed-Point Iteration Methods\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(\"Example 1: Babylonian method for √2\\n\")\nprint(\"Fixed point equation: x = (x + 2/x)/2\\n\")\n\ng1 = lambda x: (x + 2/x) / 2\ndg1 = lambda x: 0.5 * (1 - 2/x**2)\nx0 = 1.0\ntrue_root = np.sqrt(2)\n\nroot1, iters1, conv1 = fixed_point(g1, x0)\n\nprint(f\"Fixed point: {root1:.15f}\")\nprint(f\"True (√2):   {true_root:.15f}\")\nprint(f\"Error:       {abs(root1 - true_root):.2e}\")\nprint(f\"Iterations:  {iters1}\")\nprint(f\"Converged:   {conv1}\\n\")\n\ninfo1 = analyze_convergence(g1, dg1, true_root)\nprint(\"Convergence analysis:\")\nprint(f\"  g'(x*) = {info1['derivative']:.6f}\")\nprint(f\"  Will converge: {info1['will_converge']}\")\nprint(f\"  Type: {info1['convergence_type']}\\n\")\n\n# Example 2: Bad iteration for √2\nprint(\"=\"*60)\nprint(\"\\nExample 2: Poor choice g(x) = 2/x\\n\")\n\ng2 = lambda x: 2 / x\ndg2 = lambda x: -2 / x**2\n\ntry:\n    root2, iters2, conv2 = fixed_point(g2, x0, max_iter=20)\n    print(f\"Result: {root2:.10f}\")\n    print(f\"Iterations: {iters2}\")\n    print(f\"Converged: {conv2}\\n\")\nexcept:\n    print(\"Failed to converge (oscillates)\\n\")\n\ninfo2 = analyze_convergence(g2, dg2, true_root)\nprint(\"Convergence analysis:\")\nprint(f\"  g'(x*) = {info2['derivative']:.6f}\")\nprint(f\"  |g'(x*)| = {abs(info2['derivative']):.6f}\")\nprint(f\"  Will converge: {info2['will_converge']}\")\nprint(f\"  Type: {info2['convergence_type']}\\n\")\nprint(\"Note: |g'(x*)| = 1, so convergence is not guaranteed!\")\n\n# Example 3: Newton as fixed-point\nprint(\"=\"*60)\nprint(\"\\nExample 3: Newton's method as fixed-point iteration\\n\")\nprint(\"For f(x) = x² - 2, Newton is: x = x - (x² - 2)/(2x) = (x + 2/x)/2\\n\")\nprint(\"Same as Babylonian method!\\n\")\n\n# Show quadratic convergence\nx = x0\nprint(\"Iteration history (quadratic convergence):\")\nfor i in range(5):\n    error = abs(x - true_root)\n    print(f\"  Iteration {i}: x = {x:.15f}, error = {error:.2e}\")\n    if error < 1e-15:\n        break\n    x = g1(x)\n\n# Example 4: Find root of cos(x) = x\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nExample 4: Finding root of cos(x) = x\\n\")\n\ng3 = lambda x: np.cos(x)\ndg3 = lambda x: -np.sin(x)\nx0_cos = 0.5\n\nroot3, iters3, conv3 = fixed_point(g3, x0_cos)\nprint(f\"Fixed point: {root3:.15f}\")\nprint(f\"Iterations:  {iters3}\")\nprint(f\"cos(x*):     {np.cos(root3):.15f}\")\nprint(f\"Verification: {abs(root3 - np.cos(root3)):.2e}\\n\")\n\ninfo3 = analyze_convergence(g3, dg3, root3)\nprint(\"Convergence analysis:\")\nprint(f\"  g'(x*) = {info3['derivative']:.6f}\")\nprint(f\"  |g'(x*)| = {abs(info3['derivative']):.6f} < 1\")\nprint(f\"  Type: {info3['convergence_type']}\")\n\n# Convergence comparison\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nConvergence Rate Comparison\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Fast convergence (Babylonian)\nx_fast = 1.0\nerrors_fast = []\nfor i in range(8):\n    errors_fast.append(abs(x_fast - true_root))\n    x_fast = g1(x_fast)\n\n# Slow convergence (cos)\nx_slow = 0.5\nerrors_slow = []\ntrue_cos = root3\nfor i in range(8):\n    errors_slow.append(abs(x_slow - true_cos))\n    x_slow = g3(x_slow)\n\nprint(\"Babylonian method (g'(x*) ≈ 0):\")\nfor i, err in enumerate(errors_fast):\n    print(f\"  Iteration {i}: error = {err:.2e}\")\n\nprint(\"\\ncos(x) iteration (g'(x*) ≈ 0.67):\")\nfor i, err in enumerate(errors_slow):\n    print(f\"  Iteration {i}: error = {err:.2e}\")\n\nprint(\"\\nNote: Smaller |g'(x*)| leads to faster convergence!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "g(x) = (x + 2/x)/2, x0 = 1.0",
        "expectedOutput": "Converges to √2 with quadratic convergence",
        "isHidden": false,
        "description": "Test Babylonian method for square root"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-5",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 3,
    "title": "Method Convergence Comparison",
    "description": "Compare convergence rates of bisection, Newton, and secant methods. Implement all three methods and analyze their performance on various test functions.",
    "starterCode": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef compare_methods(f, df, a, b, x0, true_root):\n    \"\"\"\n    Compare bisection, Newton, and secant methods.\n\n    Parameters:\n    - f: function\n    - df: derivative\n    - a, b: interval for bisection\n    - x0: initial guess for Newton/secant\n    - true_root: true root for error computation\n\n    Returns:\n    - results: dictionary with convergence history\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function: f(x) = x² - 2\nf = lambda x: x**2 - 2\ndf = lambda x: 2*x\na, b = 0, 2\nx0 = 1.0\ntrue_root = np.sqrt(2)\n\nprint(\"Convergence Comparison: f(x) = x² - 2\\n\")\n\nresults = compare_methods(f, df, a, b, x0, true_root)\n\nfor method, history in results.items():\n    print(f\"{method}:\")\n    print(f\"  Iterations: {len(history)}\")\n    print(f\"  Final error: {history[-1]:.2e}\\n\")",
    "hints": [
      "Implement bisection, Newton, secant with error tracking",
      "Store error at each iteration",
      "Plot log(error) vs iteration to see convergence rates",
      "Bisection: linear, Newton: quadratic, Secant: superlinear"
    ],
    "solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef bisection_with_history(f, a, b, tol=1e-12, max_iter=100):\n    \"\"\"Bisection with error history.\"\"\"\n    history = []\n    fa, fb = f(a), f(b)\n\n    for i in range(max_iter):\n        c = (a + b) / 2\n        fc = f(c)\n        history.append(c)\n\n        if abs(b - a) < tol or abs(fc) < tol:\n            break\n\n        if fa * fc < 0:\n            b, fb = c, fc\n        else:\n            a, fa = c, fc\n\n    return history\n\ndef newton_with_history(f, df, x0, tol=1e-12, max_iter=100):\n    \"\"\"Newton's method with error history.\"\"\"\n    history = [x0]\n    x = x0\n\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n\n        if abs(dfx) < 1e-15:\n            break\n\n        x_new = x - fx / dfx\n        history.append(x_new)\n\n        if abs(x_new - x) < tol:\n            break\n\n        x = x_new\n\n    return history\n\ndef secant_with_history(f, x0, x1, tol=1e-12, max_iter=100):\n    \"\"\"Secant method with error history.\"\"\"\n    history = [x0, x1]\n    f0, f1 = f(x0), f(x1)\n\n    for i in range(max_iter):\n        if abs(f1 - f0) < 1e-15:\n            break\n\n        x_new = x1 - f1 * (x1 - x0) / (f1 - f0)\n        history.append(x_new)\n\n        if abs(x_new - x1) < tol:\n            break\n\n        x0, f0 = x1, f1\n        x1, f1 = x_new, f(x_new)\n\n    return history\n\ndef compare_methods(f, df, a, b, x0, true_root):\n    \"\"\"\n    Compare bisection, Newton, and secant methods.\n\n    Parameters:\n    - f: function\n    - df: derivative\n    - a, b: interval for bisection\n    - x0: initial guess for Newton/secant\n    - true_root: true root for error computation\n\n    Returns:\n    - results: dictionary with convergence history\n    \"\"\"\n    # Run methods\n    bisect_hist = bisection_with_history(f, a, b)\n    newton_hist = newton_with_history(f, df, x0)\n    secant_hist = secant_with_history(f, x0, (a+b)/2)\n\n    # Compute errors\n    results = {\n        'Bisection': [abs(x - true_root) for x in bisect_hist],\n        'Newton': [abs(x - true_root) for x in newton_hist],\n        'Secant': [abs(x - true_root) for x in secant_hist],\n    }\n\n    return results\n\n# Test function: f(x) = x² - 2\nprint(\"Method Convergence Comparison\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf = lambda x: x**2 - 2\ndf = lambda x: 2*x\na, b = 0, 2\nx0 = 1.0\ntrue_root = np.sqrt(2)\n\nprint(\"Finding root of f(x) = x² - 2\\n\")\nprint(f\"True root: √2 = {true_root:.15f}\\n\")\n\nresults = compare_methods(f, df, a, b, x0, true_root)\n\n# Print summary\nprint(\"=\"*60)\nprint(\"\\nConvergence Summary\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nfor method, errors in results.items():\n    print(f\"{method}:\")\n    print(f\"  Iterations to converge: {len(errors)}\")\n    print(f\"  Initial error: {errors[0]:.2e}\")\n    print(f\"  Final error: {errors[-1]:.2e}\")\n\n    # Estimate convergence order\n    if len(errors) >= 4:\n        # α ≈ log(e_{n+1}/e_n) / log(e_n/e_{n-1})\n        i = len(errors) - 3\n        if errors[i-1] > 0 and errors[i] > 0 and errors[i+1] > 0:\n            alpha = np.log(errors[i+1]/errors[i]) / np.log(errors[i]/errors[i-1])\n            print(f\"  Estimated convergence order: α ≈ {alpha:.2f}\")\n    print()\n\n# Detailed convergence history\nprint(\"=\"*60)\nprint(\"\\nDetailed Convergence History\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nmax_iters = max(len(errors) for errors in results.values())\nprint(f\"{'Iteration':<12} {'Bisection':<15} {'Newton':<15} {'Secant':<15}\")\nprint(\"-\" * 60)\n\nfor i in range(max_iters):\n    row = f\"{i:<12}\"\n    for method in ['Bisection', 'Newton', 'Secant']:\n        errors = results[method]\n        if i < len(errors):\n            row += f\"{errors[i]:<15.2e}\"\n        else:\n            row += f\"{'—':<15}\"\n    print(row)\n\n# Convergence rate analysis\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nConvergence Rate Analysis\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(\"Expected convergence orders:\")\nprint(\"  Bisection: Linear (α = 1)\")\nprint(\"  Newton: Quadratic (α = 2)\")\nprint(\"  Secant: Superlinear (α ≈ 1.618)\\n\")\n\nfor method, errors in results.items():\n    print(f\"{method}:\")\n    if len(errors) >= 4:\n        print(\"  Error ratios e_{n+1}/e_n:\")\n        for i in range(min(5, len(errors)-1)):\n            if errors[i] > 1e-15:\n                ratio = errors[i+1] / errors[i]\n                print(f\"    Iteration {i}: {ratio:.4f}\")\n    print()\n\n# Visualization\nprint(\"=\"*60)\nprint(\"\\nGenerating convergence plot...\\n\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Linear scale\nfor method, errors in results.items():\n    ax1.plot(range(len(errors)), errors, 'o-', label=method, linewidth=2, markersize=6)\n\nax1.set_xlabel('Iteration')\nax1.set_ylabel('Absolute Error')\nax1.set_title('Convergence Comparison (Linear Scale)')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Log scale\nfor method, errors in results.items():\n    # Filter out zeros for log plot\n    nonzero_errors = [(i, e) for i, e in enumerate(errors) if e > 0]\n    if nonzero_errors:\n        iters, errs = zip(*nonzero_errors)\n        ax2.semilogy(iters, errs, 'o-', label=method, linewidth=2, markersize=6)\n\nax2.set_xlabel('Iteration')\nax2.set_ylabel('Absolute Error (log scale)')\nax2.set_title('Convergence Comparison (Log Scale)')\nax2.legend()\nax2.grid(True, alpha=0.3, which='both')\n\nplt.tight_layout()\nplt.savefig('/tmp/root_finding_convergence.png', dpi=150, bbox_inches='tight')\nprint(\"Plot saved to /tmp/root_finding_convergence.png\")\n\n# Test on different function\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nTest on f(x) = e^x - 3\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf2 = lambda x: np.exp(x) - 3\ndf2 = lambda x: np.exp(x)\na2, b2 = 0, 2\nx02 = 1.0\ntrue_root2 = np.log(3)\n\nresults2 = compare_methods(f2, df2, a2, b2, x02, true_root2)\n\nprint(f\"True root: ln(3) = {true_root2:.15f}\\n\")\nfor method, errors in results2.items():\n    print(f\"{method}: {len(errors)} iterations, final error = {errors[-1]:.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = x² - 2",
        "expectedOutput": "Newton fastest, bisection slowest, secant in between",
        "isHidden": false,
        "description": "Compare convergence rates of three methods"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-6",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 2,
    "title": "Modified Newton Method",
    "description": "Implement modified Newton methods for handling multiple roots and ill-conditioned problems. When f has a multiple root, standard Newton converges slowly; modifications can restore rapid convergence.",
    "starterCode": "import numpy as np\n\ndef newton_modified(f, df, d2f, x0, multiplicity=1, tol=1e-10, max_iter=100):\n    \"\"\"\n    Modified Newton's method for multiple roots.\n\n    For root of multiplicity m: x_{n+1} = x_n - m * f(x_n) / f'(x_n)\n\n    Parameters:\n    - f: function\n    - df: first derivative\n    - d2f: second derivative\n    - x0: initial guess\n    - multiplicity: multiplicity of root\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test: f(x) = (x-1)³ has triple root at x=1\nf = lambda x: (x - 1)**3\ndf = lambda x: 3*(x - 1)**2\nd2f = lambda x: 6*(x - 1)\nx0 = 2.0\n\nprint(\"Finding triple root of f(x) = (x-1)³\\n\")\n\n# Standard Newton (slow)\nroot1, iters1 = newton_modified(f, df, d2f, x0, multiplicity=1)\nprint(f\"Standard Newton: {iters1} iterations\")\n\n# Modified Newton (fast)\nroot2, iters2 = newton_modified(f, df, d2f, x0, multiplicity=3)\nprint(f\"Modified Newton: {iters2} iterations\")",
    "hints": [
      "Multiple root: use x_new = x - m * f(x) / f'(x)",
      "Alternative: x_new = x - f(x) * f'(x) / (f'(x)² - f(x) * f''(x))",
      "Multiple roots slow down standard Newton to linear convergence",
      "Modified methods restore quadratic convergence"
    ],
    "solution": "import numpy as np\n\ndef newton_standard(f, df, x0, tol=1e-10, max_iter=100):\n    \"\"\"Standard Newton's method.\"\"\"\n    x = x0\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n        if abs(dfx) < 1e-15:\n            break\n        x_new = x - fx / dfx\n        if abs(x_new - x) < tol:\n            return x_new, i + 1\n        x = x_new\n    return x, max_iter\n\ndef newton_modified_multiplicity(f, df, x0, m, tol=1e-10, max_iter=100):\n    \"\"\"\n    Modified Newton for known multiplicity.\n\n    x_{n+1} = x_n - m * f(x_n) / f'(x_n)\n    \"\"\"\n    x = x0\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n        if abs(dfx) < 1e-15:\n            break\n        x_new = x - m * fx / dfx\n        if abs(x_new - x) < tol:\n            return x_new, i + 1\n        x = x_new\n    return x, max_iter\n\ndef newton_modified_adaptive(f, df, d2f, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Modified Newton without knowing multiplicity.\n\n    Uses: x_{n+1} = x_n - u(x_n) / u'(x_n)\n    where u(x) = f(x) / f'(x)\n    \"\"\"\n    x = x0\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n        d2fx = d2f(x)\n\n        if abs(dfx) < 1e-15:\n            break\n\n        # u(x) = f(x) / f'(x)\n        # u'(x) = (f'(x)² - f(x)f''(x)) / f'(x)²\n        ux = fx / dfx\n        dupx = 1 - (fx * d2fx) / (dfx**2)\n\n        if abs(dupx) < 1e-15:\n            break\n\n        x_new = x - ux / dupx\n        if abs(x_new - x) < tol:\n            return x_new, i + 1\n        x = x_new\n    return x, max_iter\n\n# Test multiple root\nprint(\"Modified Newton's Method for Multiple Roots\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(\"Test 1: Triple root of f(x) = (x-1)³\\n\")\n\nf = lambda x: (x - 1)**3\ndf = lambda x: 3*(x - 1)**2\nd2f = lambda x: 6*(x - 1)\nx0 = 2.0\ntrue_root = 1.0\n\n# Standard Newton (slow for multiple roots)\nroot1, iters1 = newton_standard(f, df, x0)\nerror1 = abs(root1 - true_root)\n\nprint(\"Standard Newton's method:\")\nprint(f\"  Root: {root1:.15f}\")\nprint(f\"  Error: {error1:.2e}\")\nprint(f\"  Iterations: {iters1}\\n\")\n\n# Modified Newton with known multiplicity\nroot2, iters2 = newton_modified_multiplicity(f, df, x0, m=3)\nerror2 = abs(root2 - true_root)\n\nprint(\"Modified Newton (m=3):\")\nprint(f\"  Root: {root2:.15f}\")\nprint(f\"  Error: {error2:.2e}\")\nprint(f\"  Iterations: {iters2}\\n\")\n\n# Adaptive modified Newton\nroot3, iters3 = newton_modified_adaptive(f, df, d2f, x0)\nerror3 = abs(root3 - true_root)\n\nprint(\"Adaptive modified Newton:\")\nprint(f\"  Root: {root3:.15f}\")\nprint(f\"  Error: {error3:.2e}\")\nprint(f\"  Iterations: {iters3}\\n\")\n\nprint(f\"Speedup: {iters1 / iters2:.1f}× faster with modified method!\")\n\n# Convergence analysis\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nConvergence Rate Analysis\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Standard Newton on multiple root (linear)\nx = x0\nprint(\"Standard Newton (linear convergence):\")\nfor i in range(8):\n    error = abs(x - true_root)\n    print(f\"  Iteration {i}: error = {error:.2e}\")\n    if error < 1e-14:\n        break\n    fx = f(x)\n    dfx = df(x)\n    if abs(dfx) < 1e-15:\n        break\n    x = x - fx / dfx\n\nprint()\n\n# Modified Newton (quadratic)\nx = x0\nprint(\"Modified Newton with m=3 (quadratic convergence):\")\nfor i in range(5):\n    error = abs(x - true_root)\n    print(f\"  Iteration {i}: error = {error:.2e}\")\n    if error < 1e-14:\n        break\n    fx = f(x)\n    dfx = df(x)\n    if abs(dfx) < 1e-15:\n        break\n    x = x - 3 * fx / dfx\n\n# Test on double root\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nTest 2: Double root of f(x) = (x-2)²(x+1)\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf2 = lambda x: (x - 2)**2 * (x + 1)\ndf2 = lambda x: 2*(x - 2)*(x + 1) + (x - 2)**2\nd2f2 = lambda x: 2*(x + 1) + 4*(x - 2) + 2*(x - 2)\nx02 = 3.0\ntrue_root2 = 2.0\n\nprint(\"Finding root at x = 2 (double root)\\n\")\n\nroot2_std, iters2_std = newton_standard(f2, df2, x02)\nprint(f\"Standard: {iters2_std} iterations, error = {abs(root2_std - true_root2):.2e}\")\n\nroot2_mod, iters2_mod = newton_modified_multiplicity(f2, df2, x02, m=2)\nprint(f\"Modified (m=2): {iters2_mod} iterations, error = {abs(root2_mod - true_root2):.2e}\")\n\nroot2_adp, iters2_adp = newton_modified_adaptive(f2, df2, d2f2, x02)\nprint(f\"Adaptive: {iters2_adp} iterations, error = {abs(root2_adp - true_root2):.2e}\")\n\n# Estimating multiplicity\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nEstimating Root Multiplicity\\n\")\nprint(\"=\"*60 + \"\\n\")\n\ndef estimate_multiplicity(f, df, x_star, h=1e-5):\n    \"\"\"\n    Estimate multiplicity from f and f' near root.\n\n    For root of multiplicity m:\n    f(x) ≈ c(x - x*)^m\n    f'(x) ≈ mc(x - x*)^(m-1)\n    So f(x)/f'(x) ≈ (x - x*)/m\n    \"\"\"\n    x = x_star + h\n    fx = f(x)\n    dfx = df(x)\n\n    if abs(dfx) < 1e-15:\n        return float('inf')\n\n    ratio = fx / dfx\n    m_estimate = (x - x_star) / ratio\n    return m_estimate\n\nprint(\"Estimating multiplicity of (x-1)³:\")\nm_est = estimate_multiplicity(f, df, 1.0)\nprint(f\"  Estimated m ≈ {m_est:.2f} (true m = 3)\")\n\nprint(\"\\nEstimating multiplicity of (x-2)²(x+1):\")\nm_est2 = estimate_multiplicity(f2, df2, 2.0)\nprint(f\"  Estimated m ≈ {m_est2:.2f} (true m = 2)\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = (x-1)³, x0 = 2.0",
        "expectedOutput": "Modified method ~3× faster than standard Newton",
        "isHidden": false,
        "description": "Test modified Newton on multiple root"
      }
    ],
    "language": "python"
  }
]
