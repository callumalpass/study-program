[
  {
    "id": "math402-ex-1-1",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 1,
    "title": "Computing Machine Epsilon",
    "description": "Write a program to experimentally determine machine epsilon for your system. Write a function that computes machine epsilon by finding the smallest positive number ε such that 1.0 + ε > 1.0 in floating-point arithmetic. Test for both single and double precision.",
    "starterCode": "import numpy as np\n\ndef compute_machine_epsilon(dtype=np.float64):\n    \"\"\"\n    Compute machine epsilon experimentally.\n\n    Parameters:\n    - dtype: numpy data type (float32 or float64)\n\n    Returns:\n    - epsilon: machine epsilon\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test\neps_32 = compute_machine_epsilon(np.float32)\neps_64 = compute_machine_epsilon(np.float64)\n\nprint(f\"Computed epsilon (float32): {eps_32:.2e}\")\nprint(f\"NumPy epsilon (float32): {np.finfo(np.float32).eps:.2e}\")\nprint()\nprint(f\"Computed epsilon (float64): {eps_64:.2e}\")\nprint(f\"NumPy epsilon (float64): {np.finfo(np.float64).eps:.2e}\")",
    "hints": [
      "Start with ε = 1 and repeatedly halve it",
      "Stop when 1.0 + ε/2 == 1.0",
      "Compare with numpy.finfo values"
    ],
    "solution": "import numpy as np\n\ndef compute_machine_epsilon(dtype=np.float64):\n    \"\"\"\n    Compute machine epsilon experimentally.\n\n    Parameters:\n    - dtype: numpy data type (float32 or float64)\n\n    Returns:\n    - epsilon: machine epsilon\n    \"\"\"\n    eps = dtype(1.0)\n\n    while dtype(1.0) + dtype(eps / 2.0) != dtype(1.0):\n        eps = dtype(eps / 2.0)\n\n    return eps\n\n# Test\neps_32 = compute_machine_epsilon(np.float32)\neps_64 = compute_machine_epsilon(np.float64)\n\nprint(f\"Computed epsilon (float32): {eps_32:.2e}\")\nprint(f\"NumPy epsilon (float32): {np.finfo(np.float32).eps:.2e}\")\nprint()\nprint(f\"Computed epsilon (float64): {eps_64:.2e}\")\nprint(f\"NumPy epsilon (float64): {np.finfo(np.float64).eps:.2e}\")\n\n# Verification\nassert abs(eps_32 - np.finfo(np.float32).eps) < 1e-10\nassert abs(eps_64 - np.finfo(np.float64).eps) < 1e-15\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "dtype=np.float32",
        "expectedOutput": "≈ 1.19e-07",
        "isHidden": false,
        "description": "Test with single precision float"
      },
      {
        "input": "dtype=np.float64",
        "expectedOutput": "≈ 2.22e-16",
        "isHidden": false,
        "description": "Test with double precision float"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-2",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 2,
    "title": "Stable Quadratic Formula",
    "description": "Implement a numerically stable quadratic formula that avoids cancellation. Implement a function to solve ax² + bx + c = 0 that avoids catastrophic cancellation when b² >> 4ac. The standard formula suffers from cancellation when computing -b ± √(b² - 4ac) when the terms are nearly equal.",
    "starterCode": "import numpy as np\n\ndef quadratic_stable(a, b, c):\n    \"\"\"\n    Solve ax² + bx + c = 0 using numerically stable formula.\n\n    Parameters:\n    - a, b, c: coefficients\n\n    Returns:\n    - (x1, x2): roots (real or complex)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test cases\ntest_cases = [\n    (1, 5, 6, (-3.0, -2.0)),  # (x+2)(x+3) = 0\n    (1, -1e10, 1, (-1e10, -1e-10)),  # Large b\n    (1, 0, -4, (-2.0, 2.0)),  # x² - 4 = 0\n]\n\nprint(\"Testing stable quadratic formula:\\n\")\nfor a, b, c, expected in test_cases:\n    result = quadratic_stable(a, b, c)\n    print(f\"({a})x² + ({b})x + ({c}) = 0\")\n    print(f\"  Result: {result}\")\n    print(f\"  Expected: {expected}\")\n    print()",
    "hints": [
      "Compute the numerically stable root first",
      "Use Vieta's formula x₁x₂ = c/a for the second root",
      "Handle edge cases (a=0, discriminant < 0)"
    ],
    "solution": "import numpy as np\n\ndef quadratic_stable(a, b, c):\n    \"\"\"\n    Solve ax² + bx + c = 0 using numerically stable formula.\n\n    Parameters:\n    - a, b, c: coefficients\n\n    Returns:\n    - (x1, x2): roots (real or complex)\n    \"\"\"\n    # Handle degenerate case\n    if abs(a) < 1e-15:\n        if abs(b) < 1e-15:\n            return None if abs(c) > 1e-15 else float('inf')\n        return -c / b, None\n\n    # Compute discriminant\n    disc = b**2 - 4*a*c\n\n    # Complex roots\n    if disc < 0:\n        real = -b / (2*a)\n        imag = np.sqrt(-disc) / (2*a)\n        return complex(real, imag), complex(real, -imag)\n\n    # Real roots - stable formula\n    sqrt_disc = np.sqrt(disc)\n\n    # Compute one root avoiding cancellation\n    if b >= 0:\n        x1 = (-b - sqrt_disc) / (2*a)\n    else:\n        x1 = (-b + sqrt_disc) / (2*a)\n\n    # Second root using Vieta's formula\n    x2 = c / (a * x1) if abs(x1) > 1e-15 else (-b - x1)\n\n    # Return in sorted order\n    if isinstance(x1, complex):\n        return x1, x2\n    return tuple(sorted([x1, x2]))\n\n# Test cases\ntest_cases = [\n    (1, 5, 6, (-3.0, -2.0)),  # (x+2)(x+3) = 0\n    (1, -1e10, 1, (-1e10, -1e-10)),  # Large b\n    (1, 0, -4, (-2.0, 2.0)),  # x² - 4 = 0\n    (1, 0, 4, None),  # Complex roots\n    (1, -4, 4, (2.0, 2.0)),  # Double root\n]\n\nprint(\"Testing stable quadratic formula:\\n\")\nfor a, b, c, expected in test_cases:\n    result = quadratic_stable(a, b, c)\n    print(f\"({a})x² + ({b})x + ({c}) = 0\")\n    print(f\"  Result: {result}\")\n    if expected:\n        print(f\"  Expected: {expected}\")\n\n    # Verify\n    if result and not isinstance(result[0], complex):\n        x1, x2 = result\n        residual1 = abs(a*x1**2 + b*x1 + c)\n        residual2 = abs(a*x2**2 + b*x2 + c)\n        print(f\"  Residuals: {residual1:.2e}, {residual2:.2e}\")\n        assert residual1 < 1e-10 and residual2 < 1e-10\n    print()\n\nprint(\"All tests passed!\")",
    "testCases": [
      {
        "input": "a=1, b=1e10, c=1",
        "expectedOutput": "roots near -1e10 and -1e-10",
        "isHidden": false,
        "description": "Test with large coefficient b to check stability"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-3",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 3,
    "title": "Kahan Summation Algorithm",
    "description": "Implement compensated summation for improved accuracy when summing many numbers. Implement Kahan's compensated summation algorithm to minimize rounding errors when summing a large array of floating-point numbers. Compare accuracy with naive summation for challenging test cases.",
    "starterCode": "import numpy as np\n\ndef kahan_sum(data):\n    \"\"\"\n    Kahan compensated summation algorithm.\n\n    Reduces cumulative rounding error from O(nε) to O(ε).\n\n    Parameters:\n    - data: array-like of numbers to sum\n\n    Returns:\n    - sum: compensated sum\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case\nprint(\"Test: Large + many small\\n\")\ndata1 = [1.0] + [1e-10] * 1000\nresult = kahan_sum(data1)\nprint(f\"Kahan sum: {result:.15f}\")",
    "hints": [
      "Keep a running compensation variable to track lost precision",
      "For each element: y = x - c, t = s + y, c = (t - s) - y, s = t",
      "The compensation c accumulates the rounding errors"
    ],
    "solution": "import numpy as np\n\ndef kahan_sum(data):\n    \"\"\"\n    Kahan compensated summation algorithm.\n\n    Reduces cumulative rounding error from O(nε) to O(ε).\n\n    Parameters:\n    - data: array-like of numbers to sum\n\n    Returns:\n    - sum: compensated sum\n    \"\"\"\n    s = 0.0  # Running sum\n    c = 0.0  # Running compensation\n\n    for x in data:\n        y = x - c  # Subtract previous compensation\n        t = s + y  # New sum\n        c = (t - s) - y  # Compute new compensation\n        s = t  # Update sum\n\n    return s\n\ndef compare_summation_methods(data):\n    \"\"\"Compare different summation methods.\"\"\"\n    # Naive summation\n    naive = sum(data)\n\n    # Kahan summation\n    kahan = kahan_sum(data)\n\n    # NumPy (uses pairwise summation)\n    numpy_sum = np.sum(data)\n\n    # High precision reference\n    from decimal import Decimal, getcontext\n    getcontext().prec = 100\n    exact = float(sum(Decimal(str(float(x))) for x in data))\n\n    results = {\n        'Naive': (naive, abs(naive - exact)),\n        'Kahan': (kahan, abs(kahan - exact)),\n        'NumPy': (numpy_sum, abs(numpy_sum - exact)),\n    }\n\n    return results, exact\n\n# Test case: sum of many small numbers with large initial value\nprint(\"Test 1: Large + many small\\n\")\ndata1 = [1.0] + [1e-10] * 10000000\nresults1, exact1 = compare_summation_methods(data1)\n\nprint(f\"Exact value: {exact1:.15f}\\n\")\nfor method, (value, error) in results1.items():\n    print(f\"{method:10s}: {value:.15f}  (error: {error:.2e})\")\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# Test case: alternating positive and negative\nprint(\"Test 2: Alternating signs\\n\")\ndata2 = [1.0, -1.0, 1e-10] * 1000\nresults2, exact2 = compare_summation_methods(data2)\n\nprint(f\"Exact value: {exact2:.15e}\\n\")\nfor method, (value, error) in results2.items():\n    print(f\"{method:10s}: {value:.15e}  (error: {error:.2e})\")\n\nprint(\"\\nKahan summation successfully reduces cumulative error!\")",
    "testCases": [
      {
        "input": "data=[1.0] + [1e-10] * 1000",
        "expectedOutput": "More accurate sum than naive summation",
        "isHidden": false,
        "description": "Test with large value plus many small values"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-4",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 2,
    "title": "Relative and Absolute Error",
    "description": "Compute and analyze relative and absolute errors in numerical approximations. Given true values and approximations, calculate both types of errors and determine which approximation is better using relative error analysis.",
    "starterCode": "import numpy as np\n\ndef compute_errors(true_value, approx_value):\n    \"\"\"\n    Compute absolute and relative errors.\n\n    Parameters:\n    - true_value: true value\n    - approx_value: approximate value\n\n    Returns:\n    - (absolute_error, relative_error): tuple of errors\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test cases\ntest_cases = [\n    (np.pi, 3.14),\n    (1e-10, 1.1e-10),\n    (1000, 1001),\n    (np.e, 2.718),\n]\n\nprint(\"Error Analysis:\\n\")\nfor true_val, approx_val in test_cases:\n    abs_err, rel_err = compute_errors(true_val, approx_val)\n    print(f\"True: {true_val:.10f}, Approx: {approx_val:.10f}\")\n    print(f\"  Absolute error: {abs_err:.2e}\")\n    print(f\"  Relative error: {rel_err:.2e}\")\n    print()",
    "hints": [
      "Absolute error = |true - approx|",
      "Relative error = |true - approx| / |true|",
      "Handle case when true value is zero"
    ],
    "solution": "import numpy as np\n\ndef compute_errors(true_value, approx_value):\n    \"\"\"\n    Compute absolute and relative errors.\n\n    Parameters:\n    - true_value: true value\n    - approx_value: approximate value\n\n    Returns:\n    - (absolute_error, relative_error): tuple of errors\n    \"\"\"\n    absolute_error = abs(true_value - approx_value)\n\n    # Relative error (handle zero case)\n    if abs(true_value) < 1e-15:\n        relative_error = float('inf') if absolute_error > 1e-15 else 0.0\n    else:\n        relative_error = absolute_error / abs(true_value)\n\n    return absolute_error, relative_error\n\n# Test cases\ntest_cases = [\n    (np.pi, 3.14),\n    (1e-10, 1.1e-10),\n    (1000, 1001),\n    (np.e, 2.718),\n]\n\nprint(\"Error Analysis:\\n\")\nfor true_val, approx_val in test_cases:\n    abs_err, rel_err = compute_errors(true_val, approx_val)\n    print(f\"True: {true_val:.10f}, Approx: {approx_val:.10f}\")\n    print(f\"  Absolute error: {abs_err:.2e}\")\n    print(f\"  Relative error: {rel_err:.2e}\")\n    print()\n\n# Comparison example\nprint(\"=\"*60)\nprint(\"\\nComparison: Which approximation is better?\\n\")\n\n# Case 1: Small numbers\ntrue1, approx1a, approx1b = 1e-10, 1.1e-10, 2e-10\nabs_err_a, rel_err_a = compute_errors(true1, approx1a)\nabs_err_b, rel_err_b = compute_errors(true1, approx1b)\n\nprint(f\"True value: {true1:.2e}\")\nprint(f\"Approx A: {approx1a:.2e} (rel err: {rel_err_a:.2%})\")\nprint(f\"Approx B: {approx1b:.2e} (rel err: {rel_err_b:.2%})\")\nprint(f\"Better approximation: {'A' if rel_err_a < rel_err_b else 'B'}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "true_value=π, approx_value=3.14",
        "expectedOutput": "absolute error ≈ 1.59e-03, relative error ≈ 5.07e-04",
        "isHidden": false,
        "description": "Test error computation for π approximation"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-5",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 3,
    "title": "Condition Number Analysis",
    "description": "Compute and interpret condition numbers for various problems. Implement functions to calculate condition numbers for different mathematical operations and analyze problem sensitivity to input perturbations.",
    "starterCode": "import numpy as np\n\ndef condition_number_function(f, df, x):\n    \"\"\"\n    Compute condition number for evaluating f at x.\n\n    Condition number κ = |x·f'(x)/f(x)|\n\n    Parameters:\n    - f: function to evaluate\n    - df: derivative of f\n    - x: point of evaluation\n\n    Returns:\n    - condition_number: condition number at x\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef condition_number_matrix(A):\n    \"\"\"\n    Compute condition number of matrix A.\n\n    κ(A) = ||A|| · ||A^(-1)||\n\n    Parameters:\n    - A: square matrix\n\n    Returns:\n    - condition_number: condition number\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function evaluation\nprint(\"Function Evaluation Conditioning:\\n\")\nf = lambda x: np.sin(x)\ndf = lambda x: np.cos(x)\n\ntest_points = [np.pi/6, np.pi/4, np.pi/2]\nfor x in test_points:\n    kappa = condition_number_function(f, df, x)\n    print(f\"sin({x:.4f}): κ = {kappa:.4f}\")\n\n# Test matrix conditioning\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nMatrix Conditioning:\\n\")\n\nmatrices = {\n    \"Well-conditioned\": np.array([[4, 1], [1, 3]]),\n    \"Ill-conditioned\": np.array([[1, 1], [1, 1.0001]]),\n}\n\nfor name, A in matrices.items():\n    kappa = condition_number_matrix(A)\n    print(f\"{name}: κ = {kappa:.2e}\")",
    "hints": [
      "For functions: κ = |x·f'(x)/f(x)| measures relative output change per relative input change",
      "For matrices: use numpy.linalg.norm and numpy.linalg.inv",
      "Large condition number indicates ill-conditioned problem"
    ],
    "solution": "import numpy as np\n\ndef condition_number_function(f, df, x):\n    \"\"\"\n    Compute condition number for evaluating f at x.\n\n    Condition number κ = |x·f'(x)/f(x)|\n\n    Parameters:\n    - f: function to evaluate\n    - df: derivative of f\n    - x: point of evaluation\n\n    Returns:\n    - condition_number: condition number at x\n    \"\"\"\n    fx = f(x)\n    dfx = df(x)\n\n    if abs(fx) < 1e-15:\n        return float('inf')\n\n    kappa = abs(x * dfx / fx)\n    return kappa\n\ndef condition_number_matrix(A):\n    \"\"\"\n    Compute condition number of matrix A.\n\n    κ(A) = ||A|| · ||A^(-1)||\n\n    Parameters:\n    - A: square matrix\n\n    Returns:\n    - condition_number: condition number\n    \"\"\"\n    # Use numpy's built-in condition number (2-norm)\n    kappa = np.linalg.cond(A)\n\n    # Or compute manually:\n    # norm_A = np.linalg.norm(A, 2)\n    # norm_Ainv = np.linalg.norm(np.linalg.inv(A), 2)\n    # kappa = norm_A * norm_Ainv\n\n    return kappa\n\n# Test function evaluation\nprint(\"Function Evaluation Conditioning:\\n\")\nf = lambda x: np.sin(x)\ndf = lambda x: np.cos(x)\n\ntest_points = [np.pi/6, np.pi/4, np.pi/2]\nfor x in test_points:\n    kappa = condition_number_function(f, df, x)\n    print(f\"sin({x:.4f}): κ = {kappa:.4f}\")\n\nprint(\"\\nInterpretation: sin(x) becomes better conditioned as x approaches π/2\")\nprint(\"because sin(π/2) = 1 is far from zero.\\n\")\n\n# Test matrix conditioning\nprint(\"=\"*60)\nprint(\"\\nMatrix Conditioning:\\n\")\n\nmatrices = {\n    \"Well-conditioned\": np.array([[4, 1], [1, 3]]),\n    \"Ill-conditioned\": np.array([[1, 1], [1, 1.0001]]),\n    \"Very ill-conditioned\": np.array([[1, 1], [1, 1.00001]]),\n}\n\nfor name, A in matrices.items():\n    kappa = condition_number_matrix(A)\n    print(f\"{name}: κ = {kappa:.2e}\")\n\n# Demonstrate effect of ill-conditioning\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nEffect of Ill-conditioning:\\n\")\n\nA_ill = np.array([[1, 1], [1, 1.0001]])\nb = np.array([2, 2.0001])\nx_exact = np.linalg.solve(A_ill, b)\n\n# Perturb b slightly\nb_perturbed = b + np.array([0, 1e-8])\nx_perturbed = np.linalg.solve(A_ill, b_perturbed)\n\nrel_input_change = np.linalg.norm(b_perturbed - b) / np.linalg.norm(b)\nrel_output_change = np.linalg.norm(x_perturbed - x_exact) / np.linalg.norm(x_exact)\n\nprint(f\"Relative input change: {rel_input_change:.2e}\")\nprint(f\"Relative output change: {rel_output_change:.2e}\")\nprint(f\"Amplification factor: {rel_output_change/rel_input_change:.2e}\")\nprint(f\"Condition number: {condition_number_matrix(A_ill):.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "A = [[4, 1], [1, 3]]",
        "expectedOutput": "κ ≈ 5.83 (well-conditioned)",
        "isHidden": false,
        "description": "Test condition number of well-conditioned matrix"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-6",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 2,
    "title": "Forward and Backward Error Analysis",
    "description": "Analyze forward and backward errors in numerical computations. Implement functions to compute both types of errors and understand their relationship through the condition number.",
    "starterCode": "import numpy as np\n\ndef analyze_errors(A, b, x_computed):\n    \"\"\"\n    Analyze forward and backward errors for solving Ax = b.\n\n    Forward error: ||x_true - x_computed||\n    Backward error: ||b - A·x_computed||\n\n    Parameters:\n    - A: coefficient matrix\n    - b: right-hand side\n    - x_computed: computed solution\n\n    Returns:\n    - (forward_error, backward_error, condition_number)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case\nA = np.array([[10, 7, 8, 7],\n              [7, 5, 6, 5],\n              [8, 6, 10, 9],\n              [7, 5, 9, 10]], dtype=float)\n\nb = np.array([32, 23, 33, 31], dtype=float)\n\n# Compute solution\nx_true = np.linalg.solve(A, b)\n\n# Introduce small error\nx_computed = x_true + np.array([1e-6, -1e-6, 1e-6, -1e-6])\n\nfwd_err, bwd_err, kappa = analyze_errors(A, b, x_computed)\n\nprint(f\"Forward error: {fwd_err:.2e}\")\nprint(f\"Backward error: {bwd_err:.2e}\")\nprint(f\"Condition number: {kappa:.2e}\")\nprint(f\"Error amplification: {fwd_err/bwd_err:.2e}\")",
    "hints": [
      "Forward error measures how far the solution is from true answer",
      "Backward error measures residual ||b - Ax||",
      "Relationship: forward_error ≤ condition_number × backward_error"
    ],
    "solution": "import numpy as np\n\ndef analyze_errors(A, b, x_computed):\n    \"\"\"\n    Analyze forward and backward errors for solving Ax = b.\n\n    Forward error: ||x_true - x_computed||\n    Backward error: ||b - A·x_computed||\n\n    Parameters:\n    - A: coefficient matrix\n    - b: right-hand side\n    - x_computed: computed solution\n\n    Returns:\n    - (forward_error, backward_error, condition_number)\n    \"\"\"\n    # True solution\n    x_true = np.linalg.solve(A, b)\n\n    # Forward error (absolute)\n    forward_error = np.linalg.norm(x_true - x_computed)\n\n    # Backward error (residual)\n    residual = b - A @ x_computed\n    backward_error = np.linalg.norm(residual)\n\n    # Condition number\n    condition_number = np.linalg.cond(A)\n\n    return forward_error, backward_error, condition_number\n\n# Test case: Hilbert matrix (ill-conditioned)\nn = 4\nA = np.array([[1/(i+j+1) for j in range(n)] for i in range(n)])\nb = A @ np.ones(n)  # True solution is [1, 1, 1, 1]\n\nprint(\"Testing with Hilbert matrix (ill-conditioned):\\n\")\n\n# Compute solution\nx_true = np.ones(n)\nx_computed = np.linalg.solve(A, b)\n\n# Analyze errors\nfwd_err, bwd_err, kappa = analyze_errors(A, b, x_computed)\n\nprint(f\"True solution: {x_true}\")\nprint(f\"Computed solution: {x_computed}\")\nprint(f\"\\nForward error: {fwd_err:.2e}\")\nprint(f\"Backward error: {bwd_err:.2e}\")\nprint(f\"Condition number: {kappa:.2e}\")\nprint(f\"Error amplification: {fwd_err/bwd_err:.2e}\")\n\n# Verify relationship\nprint(f\"\\nVerifying: forward_error ≤ κ(A) × backward_error\")\nprint(f\"{fwd_err:.2e} ≤ {kappa * bwd_err:.2e}: {fwd_err <= kappa * bwd_err}\")\n\n# Test with well-conditioned matrix\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nTesting with well-conditioned matrix:\\n\")\n\nA_well = np.array([[4, 1], [1, 3]], dtype=float)\nb_well = np.array([1, 2], dtype=float)\nx_true_well = np.linalg.solve(A_well, b_well)\n\n# Add small error\nx_computed_well = x_true_well + np.array([1e-8, -1e-8])\n\nfwd_err_well, bwd_err_well, kappa_well = analyze_errors(A_well, b_well, x_computed_well)\n\nprint(f\"Forward error: {fwd_err_well:.2e}\")\nprint(f\"Backward error: {bwd_err_well:.2e}\")\nprint(f\"Condition number: {kappa_well:.2e}\")\nprint(f\"Error amplification: {fwd_err_well/bwd_err_well:.2e}\")\nprint(f\"\\nNote: Well-conditioned problem has smaller error amplification!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "Hilbert matrix n=4",
        "expectedOutput": "Large error amplification due to ill-conditioning",
        "isHidden": false,
        "description": "Test error analysis on ill-conditioned system"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-7",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 3,
    "title": "Loss of Significance Detection",
    "description": "Detect and avoid loss of significance in numerical computations. Implement functions that identify when catastrophic cancellation occurs and provide numerically stable alternatives.",
    "starterCode": "import numpy as np\n\ndef safe_subtract(a, b, threshold=1e-10):\n    \"\"\"\n    Detect potential loss of significance in subtraction.\n\n    Parameters:\n    - a, b: numbers to subtract\n    - threshold: relative difference threshold\n\n    Returns:\n    - (result, warning): result and warning flag\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef variance_stable(data):\n    \"\"\"\n    Compute variance using numerically stable algorithm.\n\n    Naive: var = E[X²] - E[X]²  (unstable)\n    Stable: var = E[(X - μ)²]  (stable)\n\n    Parameters:\n    - data: array of numbers\n\n    Returns:\n    - variance: computed variance\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test loss of significance\nprint(\"Loss of Significance Detection:\\n\")\n\ntest_pairs = [\n    (1.234567890123456, 1.234567890123457),\n    (1e10, 1e10 + 1),\n    (100, 50),\n]\n\nfor a, b in test_pairs:\n    result, warning = safe_subtract(a, b, threshold=0.01)\n    print(f\"{a} - {b} = {result}\")\n    if warning:\n        print(\"  ⚠ WARNING: Potential loss of significance!\")\n    print()\n\n# Test variance computation\nprint(\"=\"*60)\nprint(\"\\nStable Variance Computation:\\n\")\n\n# Data with large mean, small variance\ndata = np.array([1e10, 1e10 + 1, 1e10 + 2, 1e10 + 3, 1e10 + 4])\n\nvar_stable = variance_stable(data)\nvar_numpy = np.var(data)\n\nprint(f\"Stable variance: {var_stable:.6f}\")\nprint(f\"NumPy variance: {var_numpy:.6f}\")",
    "hints": [
      "Loss of significance occurs when subtracting nearly equal numbers",
      "Check relative difference: |a - b| / max(|a|, |b|)",
      "For variance: use two-pass algorithm or online algorithm"
    ],
    "solution": "import numpy as np\n\ndef safe_subtract(a, b, threshold=1e-10):\n    \"\"\"\n    Detect potential loss of significance in subtraction.\n\n    Parameters:\n    - a, b: numbers to subtract\n    - threshold: relative difference threshold\n\n    Returns:\n    - (result, warning): result and warning flag\n    \"\"\"\n    result = a - b\n\n    # Check for loss of significance\n    max_magnitude = max(abs(a), abs(b))\n    if max_magnitude > 0:\n        relative_diff = abs(result) / max_magnitude\n        warning = relative_diff < threshold\n    else:\n        warning = False\n\n    return result, warning\n\ndef variance_naive(data):\n    \"\"\"Naive variance computation (unstable).\"\"\"\n    n = len(data)\n    mean = sum(data) / n\n    sum_sq = sum(x**2 for x in data)\n    return sum_sq / n - mean**2\n\ndef variance_stable(data):\n    \"\"\"\n    Compute variance using numerically stable algorithm.\n\n    Naive: var = E[X²] - E[X]²  (unstable)\n    Stable: var = E[(X - μ)²]  (stable)\n\n    Parameters:\n    - data: array of numbers\n\n    Returns:\n    - variance: computed variance\n    \"\"\"\n    n = len(data)\n\n    # First pass: compute mean\n    mean = sum(data) / n\n\n    # Second pass: compute variance\n    variance = sum((x - mean)**2 for x in data) / n\n\n    return variance\n\ndef variance_online(data):\n    \"\"\"Online algorithm (Welford's method) - single pass.\"\"\"\n    n = 0\n    mean = 0.0\n    M2 = 0.0\n\n    for x in data:\n        n += 1\n        delta = x - mean\n        mean += delta / n\n        delta2 = x - mean\n        M2 += delta * delta2\n\n    if n < 2:\n        return 0.0\n    return M2 / n\n\n# Test loss of significance\nprint(\"Loss of Significance Detection:\\n\")\n\ntest_pairs = [\n    (1.234567890123456, 1.234567890123457, \"Nearly equal numbers\"),\n    (1e10, 1e10 + 1, \"Large numbers, small difference\"),\n    (100, 50, \"Well-separated numbers\"),\n]\n\nfor a, b, description in test_pairs:\n    result, warning = safe_subtract(a, b, threshold=0.01)\n    print(f\"{description}:\")\n    print(f\"  {a} - {b} = {result}\")\n    if warning:\n        print(\"  ⚠ WARNING: Potential loss of significance!\")\n    else:\n        print(\"  ✓ No significant cancellation detected\")\n    print()\n\n# Test variance computation\nprint(\"=\"*60)\nprint(\"\\nStable Variance Computation:\\n\")\n\n# Data with large mean, small variance (challenging case)\ndata_large = np.array([1e10, 1e10 + 1, 1e10 + 2, 1e10 + 3, 1e10 + 4])\n\nprint(\"Case 1: Large mean, small variance\")\nprint(f\"Data: [{data_large[0]:.0f}, {data_large[1]:.0f}, ..., {data_large[-1]:.0f}]\\n\")\n\nvar_naive = variance_naive(data_large)\nvar_stable = variance_stable(data_large)\nvar_online = variance_online(data_large)\nvar_numpy = np.var(data_large)\n\nprint(f\"Naive algorithm:  {var_naive:.6f}\")\nprint(f\"Stable algorithm: {var_stable:.6f}\")\nprint(f\"Online algorithm: {var_online:.6f}\")\nprint(f\"NumPy (stable):   {var_numpy:.6f}\")\n\nprint(\"\\nNote: Naive algorithm may suffer from catastrophic cancellation!\")\n\n# Normal case\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nCase 2: Normal data\\n\")\n\ndata_normal = np.array([1, 2, 3, 4, 5])\nvar_naive_norm = variance_naive(data_normal)\nvar_stable_norm = variance_stable(data_normal)\n\nprint(f\"Data: {data_normal}\")\nprint(f\"Naive:  {var_naive_norm:.6f}\")\nprint(f\"Stable: {var_stable_norm:.6f}\")\nprint(f\"NumPy:  {np.var(data_normal):.6f}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "data=[1e10, 1e10+1, 1e10+2, 1e10+3, 1e10+4]",
        "expectedOutput": "Stable algorithm gives correct variance ≈ 2.0",
        "isHidden": false,
        "description": "Test stable variance computation"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-8",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 4,
    "title": "Error Propagation Analysis",
    "description": "Analyze how errors propagate through arithmetic operations and function evaluations. Implement functions to estimate error bounds using differential calculus and compare with actual errors.",
    "starterCode": "import numpy as np\n\ndef estimate_error_propagation(f, grad_f, x, dx):\n    \"\"\"\n    Estimate error propagation using linear approximation.\n\n    For f(x + dx) ≈ f(x) + ∇f(x)·dx\n    Error: |f(x + dx) - f(x)| ≈ ||∇f(x)|| · ||dx||\n\n    Parameters:\n    - f: function (vector -> scalar)\n    - grad_f: gradient function\n    - x: point of evaluation\n    - dx: input perturbation\n\n    Returns:\n    - (estimated_error, actual_error)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef analyze_arithmetic_error(op, a, da, b, db):\n    \"\"\"\n    Analyze error propagation in arithmetic operations.\n\n    Parameters:\n    - op: operation ('+', '-', '*', '/')\n    - a, b: operands\n    - da, db: absolute errors in operands\n\n    Returns:\n    - (result, estimated_error)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function error propagation\nprint(\"Function Error Propagation:\\n\")\n\nf = lambda x: np.sin(x[0]) * np.cos(x[1])\ngrad_f = lambda x: np.array([np.cos(x[0])*np.cos(x[1]),\n                              -np.sin(x[0])*np.sin(x[1])])\n\nx = np.array([np.pi/4, np.pi/3])\ndx = np.array([1e-4, 1e-4])\n\nest_err, act_err = estimate_error_propagation(f, grad_f, x, dx)\nprint(f\"Estimated error: {est_err:.2e}\")\nprint(f\"Actual error: {act_err:.2e}\")\n\n# Test arithmetic operations\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nArithmetic Error Propagation:\\n\")\n\noperations = ['+', '-', '*', '/']\na, da = 100.0, 0.1\nb, db = 50.0, 0.05\n\nfor op in operations:\n    result, error = analyze_arithmetic_error(op, a, da, b, db)\n    print(f\"{a} {op} {b} = {result} ± {error}\")",
    "hints": [
      "Use first-order Taylor expansion for error estimation",
      "Addition/subtraction: errors add",
      "Multiplication: relative errors add",
      "Division: relative errors add"
    ],
    "solution": "import numpy as np\n\ndef estimate_error_propagation(f, grad_f, x, dx):\n    \"\"\"\n    Estimate error propagation using linear approximation.\n\n    For f(x + dx) ≈ f(x) + ∇f(x)·dx\n    Error: |f(x + dx) - f(x)| ≈ ||∇f(x)|| · ||dx||\n\n    Parameters:\n    - f: function (vector -> scalar)\n    - grad_f: gradient function\n    - x: point of evaluation\n    - dx: input perturbation\n\n    Returns:\n    - (estimated_error, actual_error)\n    \"\"\"\n    # Compute gradient\n    gradient = grad_f(x)\n\n    # Estimate error using linear approximation\n    # |Δf| ≈ |∇f · Δx| ≤ ||∇f|| · ||Δx||\n    estimated_error = np.linalg.norm(gradient) * np.linalg.norm(dx)\n\n    # Compute actual error\n    f_x = f(x)\n    f_x_plus_dx = f(x + dx)\n    actual_error = abs(f_x_plus_dx - f_x)\n\n    return estimated_error, actual_error\n\ndef analyze_arithmetic_error(op, a, da, b, db):\n    \"\"\"\n    Analyze error propagation in arithmetic operations.\n\n    Error propagation formulas:\n    - Addition: δ(a+b) ≈ δa + δb\n    - Subtraction: δ(a-b) ≈ δa + δb\n    - Multiplication: δ(a×b)/|a×b| ≈ δa/|a| + δb/|b|\n    - Division: δ(a/b)/|a/b| ≈ δa/|a| + δb/|b|\n\n    Parameters:\n    - op: operation ('+', '-', '*', '/')\n    - a, b: operands\n    - da, db: absolute errors in operands\n\n    Returns:\n    - (result, estimated_error)\n    \"\"\"\n    if op == '+':\n        result = a + b\n        error = da + db\n    elif op == '-':\n        result = a - b\n        error = da + db\n    elif op == '*':\n        result = a * b\n        # Convert to absolute error: δ(ab) ≈ |b|·δa + |a|·δb\n        error = abs(b) * da + abs(a) * db\n    elif op == '/':\n        if abs(b) < 1e-15:\n            return None, float('inf')\n        result = a / b\n        # Convert to absolute error: δ(a/b) ≈ (δa + |a/b|·δb) / |b|\n        error = (da + abs(result) * db) / abs(b)\n    else:\n        raise ValueError(f\"Unknown operation: {op}\")\n\n    return result, error\n\n# Test function error propagation\nprint(\"Function Error Propagation Analysis\\n\")\nprint(\"=\"*60)\n\nf = lambda x: np.sin(x[0]) * np.cos(x[1])\ngrad_f = lambda x: np.array([np.cos(x[0])*np.cos(x[1]),\n                              -np.sin(x[0])*np.sin(x[1])])\n\nx = np.array([np.pi/4, np.pi/3])\ndx = np.array([1e-4, 1e-4])\n\nprint(f\"Function: f(x,y) = sin(x)·cos(y)\")\nprint(f\"Point: x = {x}\")\nprint(f\"Perturbation: dx = {dx}\\n\")\n\nest_err, act_err = estimate_error_propagation(f, grad_f, x, dx)\nprint(f\"Estimated error: {est_err:.6e}\")\nprint(f\"Actual error:    {act_err:.6e}\")\nprint(f\"Ratio (actual/estimated): {act_err/est_err:.4f}\")\n\n# Test with different perturbations\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nError scaling with perturbation size:\\n\")\n\nfor scale in [1e-2, 1e-4, 1e-6, 1e-8]:\n    dx_scaled = np.array([scale, scale])\n    est, act = estimate_error_propagation(f, grad_f, x, dx_scaled)\n    print(f\"||dx|| = {scale:.0e}: estimated = {est:.2e}, actual = {act:.2e}\")\n\n# Test arithmetic operations\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nArithmetic Error Propagation:\\n\")\n\noperations = [('+', 'Addition'), ('-', 'Subtraction'),\n              ('*', 'Multiplication'), ('/', 'Division')]\na, da = 100.0, 0.1\nb, db = 50.0, 0.05\n\nprint(f\"a = {a} ± {da}\")\nprint(f\"b = {b} ± {db}\\n\")\n\nfor op, name in operations:\n    result, error = analyze_arithmetic_error(op, a, da, b, db)\n    rel_error = error / abs(result) * 100 if abs(result) > 1e-15 else float('inf')\n    print(f\"{name:15s}: {a} {op} {b} = {result:.2f} ± {error:.4f} ({rel_error:.3f}%)\")\n\n# Demonstrate accumulation of errors\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nError Accumulation Example:\\n\")\n\n# Compute (a + b) × (a - b) with error propagation\nsum_val, sum_err = analyze_arithmetic_error('+', a, da, b, db)\ndiff_val, diff_err = analyze_arithmetic_error('-', a, da, b, db)\nproduct_val, product_err = analyze_arithmetic_error('*', sum_val, sum_err, diff_val, diff_err)\n\nprint(f\"(a + b) = {sum_val:.2f} ± {sum_err:.4f}\")\nprint(f\"(a - b) = {diff_val:.2f} ± {diff_err:.4f}\")\nprint(f\"(a + b) × (a - b) = {product_val:.2f} ± {product_err:.4f}\")\n\n# Compare with direct computation\ndirect_val = a**2 - b**2\nprint(f\"\\nDirect: a² - b² = {direct_val:.2f}\")\nprint(f\"Error propagated through two paths!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x,y) = sin(x)·cos(y), dx = [1e-4, 1e-4]",
        "expectedOutput": "Estimated error closely matches actual error",
        "isHidden": false,
        "description": "Test error propagation in function evaluation"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-9",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 2,
    "title": "Floating-Point Representation",
    "description": "Understand floating-point representation and its limitations. Implement functions to decompose floating-point numbers into sign, exponent, and mantissa, and demonstrate representation issues.",
    "starterCode": "import numpy as np\nimport struct\n\ndef decompose_float(x):\n    \"\"\"\n    Decompose a float into sign, exponent, and mantissa.\n\n    IEEE 754 double precision:\n    - 1 sign bit\n    - 11 exponent bits (biased by 1023)\n    - 52 mantissa bits\n\n    Parameters:\n    - x: float to decompose\n\n    Returns:\n    - (sign, exponent, mantissa): components\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef test_associativity():\n    \"\"\"\n    Test whether floating-point addition is associative.\n\n    Check if (a + b) + c == a + (b + c)\n\n    Returns:\n    - list of test results\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test decomposition\nprint(\"Floating-Point Decomposition:\\n\")\n\ntest_values = [1.0, -1.0, 0.5, np.pi, 1e308]\nfor x in test_values:\n    sign, exp, mantissa = decompose_float(x)\n    print(f\"{x:.6e}: sign={sign}, exp={exp}, mantissa={mantissa:.16e}\")\n\n# Test associativity\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nAssociativity Test:\\n\")\n\nresults = test_associativity()\nfor result in results:\n    print(result)",
    "hints": [
      "Use struct.unpack to get binary representation",
      "Extract bits using bitwise operations",
      "Test associativity with numbers of very different magnitudes"
    ],
    "solution": "import numpy as np\nimport struct\n\ndef decompose_float(x):\n    \"\"\"\n    Decompose a float into sign, exponent, and mantissa.\n\n    IEEE 754 double precision:\n    - 1 sign bit\n    - 11 exponent bits (biased by 1023)\n    - 52 mantissa bits\n\n    Parameters:\n    - x: float to decompose\n\n    Returns:\n    - (sign, exponent, mantissa): components\n    \"\"\"\n    # Pack as double and unpack as unsigned long long\n    packed = struct.pack('d', x)\n    bits = struct.unpack('Q', packed)[0]\n\n    # Extract components\n    sign = (bits >> 63) & 1\n    exponent = (bits >> 52) & 0x7FF\n    mantissa_bits = bits & 0xFFFFFFFFFFFFF\n\n    # Convert mantissa to decimal (add implicit 1 for normalized numbers)\n    if exponent != 0:\n        mantissa = 1.0 + mantissa_bits / (2**52)\n    else:\n        mantissa = mantissa_bits / (2**52)\n\n    # Unbias exponent\n    exponent_unbiased = exponent - 1023\n\n    return sign, exponent_unbiased, mantissa\n\ndef test_associativity():\n    \"\"\"\n    Test whether floating-point addition is associative.\n\n    Check if (a + b) + c == a + (b + c)\n\n    Returns:\n    - list of test results\n    \"\"\"\n    results = []\n\n    # Test case 1: Large + small + small\n    a, b, c = 1.0, 1e-16, 1e-16\n    left = (a + b) + c\n    right = a + (b + c)\n    results.append(f\"Test 1: ({a} + {b}) + {c}\")\n    results.append(f\"  Left-to-right:  {left:.20f}\")\n    results.append(f\"  Right-to-left:  {right:.20f}\")\n    results.append(f\"  Associative: {left == right}\\n\")\n\n    # Test case 2: Large positive + large negative + small\n    a, b, c = 1e16, -1e16, 1.0\n    left = (a + b) + c\n    right = a + (b + c)\n    results.append(f\"Test 2: ({a:.0e} + {b:.0e}) + {c}\")\n    results.append(f\"  Left-to-right:  {left:.1f}\")\n    results.append(f\"  Right-to-left:  {right:.1f}\")\n    results.append(f\"  Associative: {left == right}\\n\")\n\n    # Test case 3: Normal case\n    a, b, c = 1.0, 2.0, 3.0\n    left = (a + b) + c\n    right = a + (b + c)\n    results.append(f\"Test 3: ({a} + {b}) + {c}\")\n    results.append(f\"  Left-to-right:  {left}\")\n    results.append(f\"  Right-to-left:  {right}\")\n    results.append(f\"  Associative: {left == right}\")\n\n    return results\n\n# Test decomposition\nprint(\"Floating-Point Decomposition\\n\")\nprint(\"=\"*60 + \"\\n\")\n\ntest_values = [1.0, -1.0, 0.5, 2.0, np.pi, 1e-308, 1e308]\nfor x in test_values:\n    sign, exp, mantissa = decompose_float(x)\n\n    # Reconstruct value\n    reconstructed = (-1)**sign * mantissa * (2**exp)\n\n    print(f\"Value: {x:.6e}\")\n    print(f\"  Sign: {'-' if sign else '+'}\")\n    print(f\"  Exponent: {exp}\")\n    print(f\"  Mantissa: {mantissa:.16f}\")\n    print(f\"  Reconstructed: {reconstructed:.6e}\")\n    print(f\"  Match: {abs(x - reconstructed) < 1e-15}\\n\")\n\n# Test associativity\nprint(\"=\"*60)\nprint(\"\\nFloating-Point Associativity Test\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nresults = test_associativity()\nfor result in results:\n    print(result)\n\n# Demonstrate representable numbers\nprint(\"=\"*60)\nprint(\"\\nRepresentable Numbers Around 1.0:\\n\")\n\neps = np.finfo(float).eps\nprint(f\"Machine epsilon: {eps:.2e}\\n\")\n\ntest_additions = [\n    (1.0, eps/2, \"1.0 + ε/2\"),\n    (1.0, eps, \"1.0 + ε\"),\n    (1.0, 2*eps, \"1.0 + 2ε\"),\n]\n\nfor base, delta, description in test_additions:\n    result = base + delta\n    print(f\"{description}:\")\n    print(f\"  Result: {result:.20f}\")\n    print(f\"  Equal to {base}: {result == base}\\n\")\n\nprint(\"All tests passed!\")",
    "testCases": [
      {
        "input": "x = 1.0",
        "expectedOutput": "sign=0, exponent=0, mantissa=1.0",
        "isHidden": false,
        "description": "Decompose 1.0 into IEEE 754 components"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-10",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 3,
    "title": "Interval Arithmetic",
    "description": "Implement interval arithmetic to bound rounding errors rigorously. Create an Interval class that performs arithmetic operations while tracking upper and lower bounds, providing guaranteed error bounds.",
    "starterCode": "import numpy as np\n\nclass Interval:\n    \"\"\"\n    Interval arithmetic for rigorous error bounds.\n\n    Represents a number as [lower, upper] bound.\n    \"\"\"\n\n    def __init__(self, lower, upper=None):\n        \"\"\"\n        Initialize interval.\n\n        Parameters:\n        - lower: lower bound (or exact value if upper is None)\n        - upper: upper bound\n        \"\"\"\n        # TODO: Implement this method\n        pass\n\n    def __add__(self, other):\n        \"\"\"Add two intervals.\"\"\"\n        # TODO: Implement this method\n        pass\n\n    def __sub__(self, other):\n        \"\"\"Subtract two intervals.\"\"\"\n        # TODO: Implement this method\n        pass\n\n    def __mul__(self, other):\n        \"\"\"Multiply two intervals.\"\"\"\n        # TODO: Implement this method\n        pass\n\n    def __truediv__(self, other):\n        \"\"\"Divide two intervals.\"\"\"\n        # TODO: Implement this method\n        pass\n\n    def width(self):\n        \"\"\"Return width of interval.\"\"\"\n        # TODO: Implement this method\n        pass\n\n    def __repr__(self):\n        return f\"[{self.lower}, {self.upper}]\"\n\n# Test interval arithmetic\nprint(\"Interval Arithmetic:\\n\")\n\nx = Interval(1.0, 1.1)\ny = Interval(2.0, 2.1)\n\nprint(f\"x = {x}\")\nprint(f\"y = {y}\")\nprint(f\"x + y = {x + y}\")\nprint(f\"x - y = {x - y}\")\nprint(f\"x × y = {x * y}\")\nprint(f\"x / y = {x / y}\")",
    "hints": [
      "For addition: [a,b] + [c,d] = [a+c, b+d]",
      "For multiplication: consider all four products",
      "Use numpy.nextafter for rigorous bounds",
      "Division requires special handling when 0 is in the interval"
    ],
    "solution": "import numpy as np\n\nclass Interval:\n    \"\"\"\n    Interval arithmetic for rigorous error bounds.\n\n    Represents a number as [lower, upper] bound.\n    \"\"\"\n\n    def __init__(self, lower, upper=None):\n        \"\"\"\n        Initialize interval.\n\n        Parameters:\n        - lower: lower bound (or exact value if upper is None)\n        - upper: upper bound\n        \"\"\"\n        if upper is None:\n            # Point interval\n            self.lower = float(lower)\n            self.upper = float(lower)\n        else:\n            self.lower = float(lower)\n            self.upper = float(upper)\n\n        if self.lower > self.upper:\n            raise ValueError(\"Lower bound must be <= upper bound\")\n\n    def __add__(self, other):\n        \"\"\"Add two intervals: [a,b] + [c,d] = [a+c, b+d]\"\"\"\n        if not isinstance(other, Interval):\n            other = Interval(other)\n\n        # Use directed rounding for rigor\n        lower = np.nextafter(self.lower + other.lower, -np.inf)\n        upper = np.nextafter(self.upper + other.upper, np.inf)\n\n        return Interval(lower, upper)\n\n    def __sub__(self, other):\n        \"\"\"Subtract intervals: [a,b] - [c,d] = [a-d, b-c]\"\"\"\n        if not isinstance(other, Interval):\n            other = Interval(other)\n\n        lower = np.nextafter(self.lower - other.upper, -np.inf)\n        upper = np.nextafter(self.upper - other.lower, np.inf)\n\n        return Interval(lower, upper)\n\n    def __mul__(self, other):\n        \"\"\"Multiply intervals: [a,b] × [c,d] = [min, max] of all products\"\"\"\n        if not isinstance(other, Interval):\n            other = Interval(other)\n\n        # Compute all four products\n        products = [\n            self.lower * other.lower,\n            self.lower * other.upper,\n            self.upper * other.lower,\n            self.upper * other.upper\n        ]\n\n        lower = np.nextafter(min(products), -np.inf)\n        upper = np.nextafter(max(products), np.inf)\n\n        return Interval(lower, upper)\n\n    def __truediv__(self, other):\n        \"\"\"Divide intervals: [a,b] / [c,d] = [a,b] × [1/d, 1/c]\"\"\"\n        if not isinstance(other, Interval):\n            other = Interval(other)\n\n        # Check for division by zero\n        if other.lower <= 0 <= other.upper:\n            raise ValueError(\"Division by interval containing zero\")\n\n        # Compute reciprocal\n        recip_lower = 1.0 / other.upper\n        recip_upper = 1.0 / other.lower\n\n        return self * Interval(recip_lower, recip_upper)\n\n    def width(self):\n        \"\"\"Return width of interval.\"\"\"\n        return self.upper - self.lower\n\n    def midpoint(self):\n        \"\"\"Return midpoint of interval.\"\"\"\n        return (self.lower + self.upper) / 2\n\n    def contains(self, value):\n        \"\"\"Check if value is in interval.\"\"\"\n        return self.lower <= value <= self.upper\n\n    def __repr__(self):\n        if self.lower == self.upper:\n            return f\"[{self.lower}]\"\n        return f\"[{self.lower:.10f}, {self.upper:.10f}]\"\n\n# Test interval arithmetic\nprint(\"Interval Arithmetic\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nx = Interval(1.0, 1.1)\ny = Interval(2.0, 2.1)\n\nprint(f\"x = {x}\")\nprint(f\"y = {y}\\n\")\n\noperations = [\n    (\"x + y\", x + y),\n    (\"x - y\", x - y),\n    (\"x × y\", x * y),\n    (\"x / y\", x / y),\n]\n\nfor name, result in operations:\n    print(f\"{name} = {result}\")\n    print(f\"  Width: {result.width():.10e}\")\n    print(f\"  Midpoint: {result.midpoint():.10f}\\n\")\n\n# Demonstrate error accumulation\nprint(\"=\"*60)\nprint(\"\\nError Accumulation Example\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Compute 1/3 + 1/3 + 1/3 with interval arithmetic\none_third = Interval(1.0) / Interval(3.0)\nprint(f\"1/3 ≈ {one_third}\")\nprint(f\"Width: {one_third.width():.2e}\\n\")\n\nsum_result = one_third + one_third + one_third\nprint(f\"1/3 + 1/3 + 1/3 = {sum_result}\")\nprint(f\"Contains 1.0: {sum_result.contains(1.0)}\")\nprint(f\"Width: {sum_result.width():.2e}\\n\")\n\n# Compare with exact computation\nexact = 1.0\nprint(f\"Exact value: {exact}\")\nprint(f\"Floating-point error: {abs(sum_result.midpoint() - exact):.2e}\")\nprint(f\"Interval guarantees: {sum_result.lower} ≤ 1.0 ≤ {sum_result.upper}\")\n\n# Demonstrate dependency problem\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nDependency Problem\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nx_interval = Interval(0.9, 1.1)\n\n# x - x should be 0, but interval arithmetic gives:\ndiff = x_interval - x_interval\nprint(f\"x = {x_interval}\")\nprint(f\"x - x = {diff}\")\nprint(f\"Contains 0: {diff.contains(0.0)}\")\nprint(\"\\nNote: x - x is not [0,0] due to interval dependency!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "x=[1.0,1.1], y=[2.0,2.1]",
        "expectedOutput": "x + y = [3.0, 3.2], x × y = [2.0, 2.31]",
        "isHidden": false,
        "description": "Test interval arithmetic operations"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-11",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 4,
    "title": "Wilkinson Polynomial Sensitivity",
    "description": "Analyze the extreme sensitivity of the Wilkinson polynomial to coefficient perturbations. Demonstrate how small changes in coefficients can drastically change the roots, illustrating ill-conditioning in polynomial root-finding.",
    "starterCode": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef wilkinson_polynomial(n=20):\n    \"\"\"\n    Generate Wilkinson polynomial coefficients.\n\n    W(x) = (x-1)(x-2)...(x-n)\n\n    Parameters:\n    - n: degree of polynomial\n\n    Returns:\n    - coefficients: polynomial coefficients\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef perturb_coefficient(coeffs, index, perturbation):\n    \"\"\"\n    Perturb a single coefficient.\n\n    Parameters:\n    - coeffs: polynomial coefficients\n    - index: index to perturb\n    - perturbation: amount to add\n\n    Returns:\n    - perturbed_coeffs: new coefficients\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef find_roots(coeffs):\n    \"\"\"\n    Find polynomial roots.\n\n    Parameters:\n    - coeffs: polynomial coefficients\n\n    Returns:\n    - roots: computed roots\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Generate Wilkinson polynomial\nn = 20\ncoeffs = wilkinson_polynomial(n)\n\n# Find roots of original polynomial\nroots_original = find_roots(coeffs)\n\n# Perturb one coefficient slightly\nperturbation = 1e-10\ncoeffs_perturbed = perturb_coefficient(coeffs, n//2, perturbation)\nroots_perturbed = find_roots(coeffs_perturbed)\n\nprint(f\"Wilkinson polynomial W_{n}(x)\\n\")\nprint(f\"Perturbation: {perturbation:.2e} added to coefficient {n//2}\\n\")\nprint(f\"True roots: {list(range(1, n+1))}\\n\")\nprint(f\"Max root error: {max(abs(roots_original - np.arange(1, n+1))):.2e}\")",
    "hints": [
      "Use numpy.poly to generate coefficients from roots",
      "Use numpy.roots to find roots",
      "Perturb the middle coefficient for dramatic effect",
      "Plot roots in complex plane to visualize"
    ],
    "solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef wilkinson_polynomial(n=20):\n    \"\"\"\n    Generate Wilkinson polynomial coefficients.\n\n    W(x) = (x-1)(x-2)...(x-n)\n\n    Parameters:\n    - n: degree of polynomial\n\n    Returns:\n    - coefficients: polynomial coefficients\n    \"\"\"\n    # Generate polynomial from roots 1, 2, ..., n\n    roots = np.arange(1, n + 1)\n    coeffs = np.poly(roots)\n    return coeffs\n\ndef perturb_coefficient(coeffs, index, perturbation):\n    \"\"\"\n    Perturb a single coefficient.\n\n    Parameters:\n    - coeffs: polynomial coefficients\n    - index: index to perturb\n    - perturbation: amount to add\n\n    Returns:\n    - perturbed_coeffs: new coefficients\n    \"\"\"\n    perturbed = coeffs.copy()\n    perturbed[index] += perturbation\n    return perturbed\n\ndef find_roots(coeffs):\n    \"\"\"\n    Find polynomial roots.\n\n    Parameters:\n    - coeffs: polynomial coefficients\n\n    Returns:\n    - roots: computed roots\n    \"\"\"\n    return np.roots(coeffs)\n\n# Generate Wilkinson polynomial\nn = 20\ncoeffs = wilkinson_polynomial(n)\n\nprint(f\"Wilkinson Polynomial Sensitivity Analysis\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(f\"W_{n}(x) = (x-1)(x-2)...(x-{n})\\n\")\nprint(f\"Polynomial degree: {n}\")\nprint(f\"Number of coefficients: {len(coeffs)}\\n\")\n\n# Find roots of original polynomial\ntrue_roots = np.arange(1, n + 1)\nroots_original = find_roots(coeffs)\n\n# Sort roots by real part\nroots_original_sorted = np.sort(roots_original)\n\nprint(\"Original polynomial roots:\\n\")\nprint(\"True roots:\", true_roots)\nprint(\"Computed roots (real parts):\", roots_original_sorted.real[:10], \"...\")\n\n# Compute error in original\nerrors_original = []\nfor i in range(n):\n    min_error = min(abs(roots_original_sorted[i] - j) for j in true_roots)\n    errors_original.append(min_error)\n\nprint(f\"\\nMax error in original: {max(errors_original):.2e}\")\nprint(f\"Mean error in original: {np.mean(errors_original):.2e}\\n\")\n\n# Perturb one coefficient\nprint(\"=\"*60)\nprint(\"\\nPerturbing single coefficient\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nperturbation = 2**(-23)  # Roughly single-precision epsilon\nperturb_index = n - 10  # Perturb x^10 coefficient\n\nprint(f\"Adding {perturbation:.2e} to coefficient of x^{perturb_index}\")\nprint(f\"Relative perturbation: {abs(perturbation/coeffs[perturb_index]):.2e}\\n\")\n\ncoeffs_perturbed = perturb_coefficient(coeffs, perturb_index, perturbation)\nroots_perturbed = find_roots(coeffs_perturbed)\nroots_perturbed_sorted = np.sort_complex(roots_perturbed)\n\n# Analyze perturbed roots\nprint(\"Perturbed polynomial roots:\\n\")\n\nreal_roots = []\ncomplex_roots = []\n\nfor root in roots_perturbed:\n    if abs(root.imag) < 1e-6:\n        real_roots.append(root.real)\n    else:\n        complex_roots.append(root)\n\nprint(f\"Real roots: {len(real_roots)}\")\nprint(f\"Complex roots: {len(complex_roots)}\\n\")\n\nif real_roots:\n    print(f\"Real roots: {sorted(real_roots)[:10]} ...\\n\")\n\nif complex_roots:\n    print(\"Sample complex roots:\")\n    for root in complex_roots[:5]:\n        print(f\"  {root.real:.4f} + {root.imag:.4f}i\")\n    print()\n\n# Compute condition number\nprint(\"=\"*60)\nprint(\"\\nCondition Number Analysis\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Estimate condition number of root finding\nmax_root_change = 0\nfor i in range(min(10, len(roots_original))):\n    root_change = min(abs(roots_perturbed[j] - roots_original[i])\n                     for j in range(len(roots_perturbed)))\n    max_root_change = max(max_root_change, root_change)\n\ncoeff_change = perturbation / np.linalg.norm(coeffs)\nroot_change_norm = max_root_change / np.linalg.norm(true_roots)\n\ncondition_estimate = root_change_norm / coeff_change\n\nprint(f\"Relative coefficient change: {coeff_change:.2e}\")\nprint(f\"Relative root change: {root_change_norm:.2e}\")\nprint(f\"Estimated condition number: {condition_estimate:.2e}\\n\")\n\nprint(\"Interpretation: Wilkinson polynomial is extremely ill-conditioned!\")\nprint(f\"Tiny perturbation ({perturbation:.2e}) causes roots to become complex.\\n\")\n\n# Visualize roots\nprint(\"=\"*60)\nprint(\"\\nVisualization: Roots in Complex Plane\\n\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Original roots\nax1.scatter(roots_original.real, roots_original.imag, c='blue', s=50, alpha=0.6)\nax1.scatter(true_roots, np.zeros_like(true_roots), c='red', s=100,\n           marker='x', linewidths=2, label='True roots')\nax1.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nax1.set_xlabel('Real')\nax1.set_ylabel('Imaginary')\nax1.set_title('Original Wilkinson Polynomial')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Perturbed roots\nax2.scatter(roots_perturbed.real, roots_perturbed.imag, c='green', s=50, alpha=0.6)\nax2.scatter(true_roots, np.zeros_like(true_roots), c='red', s=100,\n           marker='x', linewidths=2, label='True roots')\nax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nax2.set_xlabel('Real')\nax2.set_ylabel('Imaginary')\nax2.set_title(f'Perturbed (coeff {perturb_index} + {perturbation:.2e})')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('/tmp/wilkinson_roots.png', dpi=150, bbox_inches='tight')\nprint(\"Plot saved to /tmp/wilkinson_roots.png\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "n=20, perturbation=2^(-23)",
        "expectedOutput": "Small perturbation causes complex roots to appear",
        "isHidden": false,
        "description": "Demonstrate Wilkinson polynomial sensitivity"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-12",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 3,
    "title": "Significant Digits and Rounding",
    "description": "Implement functions to determine significant digits and perform proper rounding. Create utilities to analyze how many digits are trustworthy in numerical results and round appropriately.",
    "starterCode": "import numpy as np\n\ndef count_significant_digits(value, true_value):\n    \"\"\"\n    Count number of significant digits in approximation.\n\n    Parameters:\n    - value: approximate value\n    - true_value: true value\n\n    Returns:\n    - significant_digits: number of correct significant digits\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef round_to_significant_digits(value, n_digits):\n    \"\"\"\n    Round value to n significant digits.\n\n    Parameters:\n    - value: value to round\n    - n_digits: number of significant digits\n\n    Returns:\n    - rounded_value: value rounded to n digits\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef format_with_uncertainty(value, uncertainty):\n    \"\"\"\n    Format value with uncertainty using proper significant figures.\n\n    Example: 123.456 ± 0.078 -> \"123.46 ± 0.08\"\n\n    Parameters:\n    - value: measured value\n    - uncertainty: uncertainty\n\n    Returns:\n    - formatted_string: properly formatted result\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test significant digits\nprint(\"Significant Digits Analysis:\\n\")\n\ntest_cases = [\n    (3.14159, np.pi, \"π approximation\"),\n    (2.718, np.e, \"e approximation\"),\n    (1.414, np.sqrt(2), \"√2 approximation\"),\n]\n\nfor approx, true, description in test_cases:\n    sig_digits = count_significant_digits(approx, true)\n    print(f\"{description}: {sig_digits} significant digits\")\n\n# Test rounding\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nRounding to Significant Digits:\\n\")\n\nvalues = [123.456, 0.001234, 1234567.89]\nfor val in values:\n    for n in [3, 4, 5]:\n        rounded = round_to_significant_digits(val, n)\n        print(f\"{val} -> {n} digits: {rounded}\")\n\n# Test uncertainty formatting\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nFormatting with Uncertainty:\\n\")\n\nmeasurements = [\n    (123.456789, 0.078),\n    (0.001234567, 0.000045),\n    (1234.5, 123.4),\n]\n\nfor value, uncertainty in measurements:\n    formatted = format_with_uncertainty(value, uncertainty)\n    print(formatted)",
    "hints": [
      "Significant digits: -log10(relative_error)",
      "For rounding: use order of magnitude",
      "Uncertainty determines last significant digit"
    ],
    "solution": "import numpy as np\n\ndef count_significant_digits(value, true_value):\n    \"\"\"\n    Count number of significant digits in approximation.\n\n    Significant digits = -log₁₀(relative error)\n\n    Parameters:\n    - value: approximate value\n    - true_value: true value\n\n    Returns:\n    - significant_digits: number of correct significant digits\n    \"\"\"\n    if abs(true_value) < 1e-15:\n        return 0 if abs(value - true_value) > 1e-15 else float('inf')\n\n    relative_error = abs(value - true_value) / abs(true_value)\n\n    if relative_error < 1e-15:\n        return 15  # Maximum for double precision\n\n    significant_digits = -np.log10(relative_error)\n    return max(0, int(np.floor(significant_digits)))\n\ndef round_to_significant_digits(value, n_digits):\n    \"\"\"\n    Round value to n significant digits.\n\n    Parameters:\n    - value: value to round\n    - n_digits: number of significant digits\n\n    Returns:\n    - rounded_value: value rounded to n digits\n    \"\"\"\n    if value == 0:\n        return 0.0\n\n    # Determine order of magnitude\n    magnitude = np.floor(np.log10(abs(value)))\n\n    # Round to n significant digits\n    scale = 10 ** (magnitude - n_digits + 1)\n    rounded = np.round(value / scale) * scale\n\n    return rounded\n\ndef format_with_uncertainty(value, uncertainty):\n    \"\"\"\n    Format value with uncertainty using proper significant figures.\n\n    Rules:\n    1. Uncertainty determines last significant digit\n    2. Value should be rounded to same precision\n    3. Use scientific notation if needed\n\n    Parameters:\n    - value: measured value\n    - uncertainty: uncertainty\n\n    Returns:\n    - formatted_string: properly formatted result\n    \"\"\"\n    if uncertainty <= 0:\n        return f\"{value}\"\n\n    # Determine decimal places from uncertainty\n    # Round uncertainty to 1-2 significant digits\n    unc_magnitude = np.floor(np.log10(uncertainty))\n\n    # Use 2 sig figs for uncertainty if first digit is 1, else 1 sig fig\n    first_digit = int(uncertainty / (10 ** unc_magnitude))\n    n_unc_digits = 2 if first_digit == 1 else 1\n\n    unc_rounded = round_to_significant_digits(uncertainty, n_unc_digits)\n\n    # Determine decimal places\n    if unc_magnitude >= 0:\n        decimal_places = 0\n    else:\n        decimal_places = int(-unc_magnitude) + (n_unc_digits - 1)\n\n    # Format value and uncertainty\n    if abs(value) >= 1000 or abs(value) < 0.01:\n        # Use scientific notation\n        exp = int(np.floor(np.log10(abs(value))))\n        val_scaled = value / (10 ** exp)\n        unc_scaled = unc_rounded / (10 ** exp)\n        return f\"({val_scaled:.{decimal_places}f} ± {unc_scaled:.{decimal_places}f}) × 10^{exp}\"\n    else:\n        # Regular notation\n        return f\"{value:.{decimal_places}f} ± {unc_rounded:.{decimal_places}f}\"\n\n# Test significant digits\nprint(\"Significant Digits Analysis\\n\")\nprint(\"=\"*60 + \"\\n\")\n\ntest_cases = [\n    (3.14159, np.pi, \"π ≈ 3.14159\"),\n    (3.14, np.pi, \"π ≈ 3.14\"),\n    (2.718, np.e, \"e ≈ 2.718\"),\n    (2.7, np.e, \"e ≈ 2.7\"),\n    (1.414, np.sqrt(2), \"√2 ≈ 1.414\"),\n    (1.4, np.sqrt(2), \"√2 ≈ 1.4\"),\n]\n\nfor approx, true, description in test_cases:\n    sig_digits = count_significant_digits(approx, true)\n    rel_error = abs(approx - true) / abs(true)\n    print(f\"{description}:\")\n    print(f\"  Significant digits: {sig_digits}\")\n    print(f\"  Relative error: {rel_error:.2e}\\n\")\n\n# Test rounding\nprint(\"=\"*60)\nprint(\"\\nRounding to Significant Digits\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nvalues = [123.456, 0.001234, 1234567.89, np.pi, -0.0009876]\n\nfor val in values:\n    print(f\"Original: {val}\")\n    for n in [2, 3, 4, 5]:\n        rounded = round_to_significant_digits(val, n)\n        print(f\"  {n} sig. digits: {rounded}\")\n    print()\n\n# Test uncertainty formatting\nprint(\"=\"*60)\nprint(\"\\nFormatting with Uncertainty\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nmeasurements = [\n    (123.456789, 0.078, \"Laboratory measurement\"),\n    (0.001234567, 0.000045, \"Small value\"),\n    (1234.5, 123.4, \"Large uncertainty\"),\n    (299792458.0, 1.2, \"Speed of light (m/s)\"),\n    (6.62607015e-34, 1.2e-42, \"Planck constant\"),\n]\n\nfor value, uncertainty, description in measurements:\n    formatted = format_with_uncertainty(value, uncertainty)\n    print(f\"{description}:\")\n    print(f\"  Raw: {value} ± {uncertainty}\")\n    print(f\"  Formatted: {formatted}\\n\")\n\n# Demonstrate error in calculations\nprint(\"=\"*60)\nprint(\"\\nError Propagation Example\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Measure side of square\nside = 10.5  # cm\nside_uncertainty = 0.2  # cm\n\n# Calculate area\narea = side ** 2\narea_uncertainty = 2 * side * side_uncertainty  # Error propagation\n\nprint(f\"Side length: {format_with_uncertainty(side, side_uncertainty)} cm\")\nprint(f\"Area: {format_with_uncertainty(area, area_uncertainty)} cm²\")\n\nsig_digits_side = count_significant_digits(side, side)  # Perfect measurement\nsig_digits_area = count_significant_digits(area, area)\n\nprint(f\"\\nNote: Uncertainty limits significant digits in result!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "value=3.14159, true_value=π",
        "expectedOutput": "5 significant digits",
        "isHidden": false,
        "description": "Count significant digits in π approximation"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-13",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 2,
    "title": "Horner Method for Polynomial Evaluation",
    "description": "Implement Horner's method for numerically stable polynomial evaluation. Compare the number of operations and numerical stability with naive evaluation.",
    "starterCode": "import numpy as np\n\ndef poly_eval_naive(coeffs, x):\n    \"\"\"\n    Evaluate polynomial naively: a₀ + a₁x + a₂x² + ...\n\n    Parameters:\n    - coeffs: [a₀, a₁, a₂, ...] (increasing degree)\n    - x: evaluation point\n\n    Returns:\n    - result: polynomial value\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef poly_eval_horner(coeffs, x):\n    \"\"\"\n    Evaluate polynomial using Horner's method.\n\n    Horner: ((aₙx + aₙ₋₁)x + ... + a₁)x + a₀\n\n    Parameters:\n    - coeffs: [a₀, a₁, a₂, ...] (increasing degree)\n    - x: evaluation point\n\n    Returns:\n    - result: polynomial value\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef count_operations(method, coeffs):\n    \"\"\"\n    Count multiplications and additions for polynomial evaluation.\n\n    Parameters:\n    - method: 'naive' or 'horner'\n    - coeffs: polynomial coefficients\n\n    Returns:\n    - (mults, adds): operation counts\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test polynomial evaluation\ncoeffs = [1, -3, 2, 1]  # 1 - 3x + 2x² + x³\nx = 2.5\n\nprint(\"Polynomial: p(x) = 1 - 3x + 2x² + x³\\n\")\nprint(f\"Evaluation at x = {x}:\\n\")\n\nresult_naive = poly_eval_naive(coeffs, x)\nresult_horner = poly_eval_horner(coeffs, x)\n\nprint(f\"Naive method:  {result_naive}\")\nprint(f\"Horner method: {result_horner}\")\nprint(f\"NumPy:         {np.polyval(coeffs[::-1], x)}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nOperation Counts:\\n\")\n\nmults_naive, adds_naive = count_operations('naive', coeffs)\nmults_horner, adds_horner = count_operations('horner', coeffs)\n\nprint(f\"Naive:  {mults_naive} multiplications, {adds_naive} additions\")\nprint(f\"Horner: {mults_horner} multiplications, {adds_horner} additions\")",
    "hints": [
      "Horner's method: work from highest degree down",
      "Naive: n(n+1)/2 multiplications for degree n",
      "Horner: n multiplications for degree n"
    ],
    "solution": "import numpy as np\nimport time\n\ndef poly_eval_naive(coeffs, x):\n    \"\"\"\n    Evaluate polynomial naively: a₀ + a₁x + a₂x² + ...\n\n    Parameters:\n    - coeffs: [a₀, a₁, a₂, ...] (increasing degree)\n    - x: evaluation point\n\n    Returns:\n    - result: polynomial value\n    \"\"\"\n    result = 0.0\n    for i, a in enumerate(coeffs):\n        result += a * (x ** i)\n    return result\n\ndef poly_eval_horner(coeffs, x):\n    \"\"\"\n    Evaluate polynomial using Horner's method.\n\n    Horner: ((aₙx + aₙ₋₁)x + ... + a₁)x + a₀\n\n    Parameters:\n    - coeffs: [a₀, a₁, a₂, ...] (increasing degree)\n    - x: evaluation point\n\n    Returns:\n    - result: polynomial value\n    \"\"\"\n    # Start from highest degree coefficient\n    result = 0.0\n    for a in reversed(coeffs):\n        result = result * x + a\n    return result\n\ndef count_operations(method, coeffs):\n    \"\"\"\n    Count multiplications and additions for polynomial evaluation.\n\n    Parameters:\n    - method: 'naive' or 'horner'\n    - coeffs: polynomial coefficients\n\n    Returns:\n    - (mults, adds): operation counts\n    \"\"\"\n    n = len(coeffs) - 1  # degree\n\n    if method == 'naive':\n        # For each term aᵢx^i: i multiplications (for x^i) + 1 (for aᵢ·x^i)\n        # Total: Σᵢ₌₀ⁿ i + n = n(n+1)/2 + n multiplications\n        # n additions to sum terms\n        mults = n * (n + 1) // 2 + n\n        adds = n\n    elif method == 'horner':\n        # n multiplications (by x) and n additions\n        mults = n\n        adds = n\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n\n    return mults, adds\n\n# Test polynomial evaluation\nprint(\"Horner's Method for Polynomial Evaluation\\n\")\nprint(\"=\"*60 + \"\\n\")\n\ncoeffs = [1, -3, 2, 1]  # 1 - 3x + 2x² + x³\nx = 2.5\n\nprint(\"Polynomial: p(x) = 1 - 3x + 2x² + x³\")\nprint(f\"Evaluation at x = {x}\\n\")\n\nresult_naive = poly_eval_naive(coeffs, x)\nresult_horner = poly_eval_horner(coeffs, x)\nresult_numpy = np.polyval(coeffs[::-1], x)\n\nprint(f\"Naive method:  {result_naive:.10f}\")\nprint(f\"Horner method: {result_horner:.10f}\")\nprint(f\"NumPy:         {result_numpy:.10f}\")\n\n# Verify all methods agree\nassert abs(result_naive - result_horner) < 1e-10\nassert abs(result_naive - result_numpy) < 1e-10\nprint(\"\\n✓ All methods agree!\")\n\n# Operation counts\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nOperation Counts\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nmults_naive, adds_naive = count_operations('naive', coeffs)\nmults_horner, adds_horner = count_operations('horner', coeffs)\n\nn = len(coeffs) - 1\nprint(f\"Polynomial degree: {n}\\n\")\n\nprint(f\"Naive method:\")\nprint(f\"  Multiplications: {mults_naive}\")\nprint(f\"  Additions: {adds_naive}\")\nprint(f\"  Total: {mults_naive + adds_naive}\\n\")\n\nprint(f\"Horner's method:\")\nprint(f\"  Multiplications: {mults_horner}\")\nprint(f\"  Additions: {adds_horner}\")\nprint(f\"  Total: {mults_horner + adds_horner}\\n\")\n\nprint(f\"Speedup: {(mults_naive + adds_naive) / (mults_horner + adds_horner):.2f}×\")\n\n# Test with high-degree polynomial\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nHigh-Degree Polynomial Example\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nn_high = 20\ncoeffs_high = np.random.randn(n_high + 1)\nx_high = 1.5\n\nprint(f\"Degree: {n_high}\\n\")\n\n# Time comparisons\nn_trials = 10000\n\nstart = time.time()\nfor _ in range(n_trials):\n    _ = poly_eval_naive(coeffs_high, x_high)\ntime_naive = (time.time() - start) / n_trials\n\nstart = time.time()\nfor _ in range(n_trials):\n    _ = poly_eval_horner(coeffs_high, x_high)\ntime_horner = (time.time() - start) / n_trials\n\nstart = time.time()\nfor _ in range(n_trials):\n    _ = np.polyval(coeffs_high[::-1], x_high)\ntime_numpy = (time.time() - start) / n_trials\n\nprint(f\"Naive:  {time_naive*1e6:.2f} μs\")\nprint(f\"Horner: {time_horner*1e6:.2f} μs\")\nprint(f\"NumPy:  {time_numpy*1e6:.2f} μs\\n\")\n\nprint(f\"Horner speedup over naive: {time_naive/time_horner:.2f}×\")\n\n# Operation count comparison\nmults_naive_high, adds_naive_high = count_operations('naive', coeffs_high)\nmults_horner_high, adds_horner_high = count_operations('horner', coeffs_high)\n\nprint(f\"\\nOperation counts (degree {n_high}):\")\nprint(f\"  Naive:  {mults_naive_high + adds_naive_high} ops\")\nprint(f\"  Horner: {mults_horner_high + adds_horner_high} ops\")\nprint(f\"  Ratio:  {(mults_naive_high + adds_naive_high) / (mults_horner_high + adds_horner_high):.2f}×\")\n\n# Stability demonstration\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nNumerical Stability\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Wilkinson-like polynomial (challenging)\nroots = np.arange(1, 11)\ncoeffs_wilk = np.poly(roots)[::-1]  # Convert to increasing degree\n\nx_test = 5.5\nresult_naive_wilk = poly_eval_naive(coeffs_wilk, x_test)\nresult_horner_wilk = poly_eval_horner(coeffs_wilk, x_test)\n\n# True value\ntrue_value = np.prod(x_test - roots)\n\nprint(f\"Wilkinson-type polynomial at x = {x_test}:\")\nprint(f\"True value:    {true_value:.10e}\")\nprint(f\"Naive:         {result_naive_wilk:.10e}\")\nprint(f\"Horner:        {result_horner_wilk:.10e}\\n\")\n\nerror_naive = abs(result_naive_wilk - true_value)\nerror_horner = abs(result_horner_wilk - true_value)\n\nprint(f\"Naive error:   {error_naive:.2e}\")\nprint(f\"Horner error:  {error_horner:.2e}\")\n\nprint(\"\\nHorner's method: fewer operations AND better stability!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "p(x) = 1 - 3x + 2x² + x³, x = 2.5",
        "expectedOutput": "Horner uses fewer operations than naive",
        "isHidden": false,
        "description": "Compare Horner vs naive polynomial evaluation"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-14",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 4,
    "title": "Numerical Differentiation Error Analysis",
    "description": "Analyze truncation and rounding error tradeoffs in numerical differentiation. Implement various finite difference formulas and find the optimal step size that balances these competing errors.",
    "starterCode": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef forward_difference(f, x, h):\n    \"\"\"\n    Forward difference approximation: f'(x) ≈ (f(x+h) - f(x)) / h\n\n    Truncation error: O(h)\n    Rounding error: O(ε/h)\n\n    Parameters:\n    - f: function\n    - x: point\n    - h: step size\n\n    Returns:\n    - approximation: f'(x)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef central_difference(f, x, h):\n    \"\"\"\n    Central difference approximation: f'(x) ≈ (f(x+h) - f(x-h)) / (2h)\n\n    Truncation error: O(h²)\n    Rounding error: O(ε/h)\n\n    Parameters:\n    - f: function\n    - x: point\n    - h: step size\n\n    Returns:\n    - approximation: f'(x)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef find_optimal_step_size(f, df, x, method='central'):\n    \"\"\"\n    Find optimal step size that minimizes total error.\n\n    Parameters:\n    - f: function\n    - df: true derivative\n    - x: point\n    - method: 'forward' or 'central'\n\n    Returns:\n    - (h_optimal, min_error): optimal step size and minimum error\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function\nf = lambda x: np.sin(x)\ndf = lambda x: np.cos(x)\nx = 1.0\n\nprint(\"Numerical Differentiation Error Analysis\\n\")\nprint(f\"Function: f(x) = sin(x)\")\nprint(f\"Point: x = {x}\")\nprint(f\"True derivative: f'({x}) = {df(x):.10f}\\n\")\n\n# Test various step sizes\nh_values = 10.0 ** np.arange(-16, 0, 0.5)\n\nprint(\"Testing forward difference:\")\nfor h in [1e-2, 1e-4, 1e-8, 1e-12]:\n    approx = forward_difference(f, x, h)\n    error = abs(approx - df(x))\n    print(f\"h = {h:.0e}: approx = {approx:.10f}, error = {error:.2e}\")\n\n# Find optimal step size\nh_opt, min_err = find_optimal_step_size(f, df, x, 'central')\nprint(f\"\\nOptimal step size: {h_opt:.2e}\")\nprint(f\"Minimum error: {min_err:.2e}\")",
    "hints": [
      "Total error = truncation_error + rounding_error",
      "Forward: optimal h ≈ √ε where ε is machine epsilon",
      "Central: optimal h ≈ ε^(1/3)",
      "Plot error vs h to visualize"
    ],
    "solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef forward_difference(f, x, h):\n    \"\"\"\n    Forward difference approximation: f'(x) ≈ (f(x+h) - f(x)) / h\n\n    Truncation error: O(h)\n    Rounding error: O(ε/h)\n\n    Parameters:\n    - f: function\n    - x: point\n    - h: step size\n\n    Returns:\n    - approximation: f'(x)\n    \"\"\"\n    return (f(x + h) - f(x)) / h\n\ndef central_difference(f, x, h):\n    \"\"\"\n    Central difference approximation: f'(x) ≈ (f(x+h) - f(x-h)) / (2h)\n\n    Truncation error: O(h²)\n    Rounding error: O(ε/h)\n\n    Parameters:\n    - f: function\n    - x: point\n    - h: step size\n\n    Returns:\n    - approximation: f'(x)\n    \"\"\"\n    return (f(x + h) - f(x - h)) / (2 * h)\n\ndef find_optimal_step_size(f, df, x, method='central'):\n    \"\"\"\n    Find optimal step size that minimizes total error.\n\n    For forward difference: h_opt ≈ √ε\n    For central difference: h_opt ≈ ε^(1/3)\n\n    Parameters:\n    - f: function\n    - df: true derivative\n    - x: point\n    - method: 'forward' or 'central'\n\n    Returns:\n    - (h_optimal, min_error): optimal step size and minimum error\n    \"\"\"\n    # Test range of step sizes\n    h_values = 10.0 ** np.linspace(-16, -1, 150)\n    errors = []\n\n    for h in h_values:\n        if method == 'forward':\n            approx = forward_difference(f, x, h)\n        elif method == 'central':\n            approx = central_difference(f, x, h)\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n\n        error = abs(approx - df(x))\n        errors.append(error)\n\n    errors = np.array(errors)\n    min_idx = np.argmin(errors)\n\n    return h_values[min_idx], errors[min_idx]\n\n# Test function\nprint(\"Numerical Differentiation Error Analysis\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf = lambda x: np.sin(x)\ndf = lambda x: np.cos(x)\nx = 1.0\n\nprint(f\"Function: f(x) = sin(x)\")\nprint(f\"Point: x = {x}\")\nprint(f\"True derivative: f'({x}) = {df(x):.10f}\\n\")\n\n# Test various step sizes\nprint(\"=\"*60)\nprint(\"\\nForward Difference\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nh_values_test = [1e-1, 1e-2, 1e-4, 1e-8, 1e-12, 1e-15]\n\nfor h in h_values_test:\n    approx = forward_difference(f, x, h)\n    error = abs(approx - df(x))\n    print(f\"h = {h:.0e}: approx = {approx:.10f}, error = {error:.2e}\")\n\nprint(\"\\nNote: error decreases then increases (rounding error dominates)\")\n\n# Central difference\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nCentral Difference\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nfor h in h_values_test:\n    approx = central_difference(f, x, h)\n    error = abs(approx - df(x))\n    print(f\"h = {h:.0e}: approx = {approx:.10f}, error = {error:.2e}\")\n\n# Find optimal step sizes\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nOptimal Step Sizes\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nh_opt_fwd, min_err_fwd = find_optimal_step_size(f, df, x, 'forward')\nh_opt_ctr, min_err_ctr = find_optimal_step_size(f, df, x, 'central')\n\neps = np.finfo(float).eps\ntheoretical_h_fwd = np.sqrt(eps)\ntheoretical_h_ctr = eps ** (1/3)\n\nprint(\"Forward difference:\")\nprint(f\"  Optimal h (experimental): {h_opt_fwd:.2e}\")\nprint(f\"  Optimal h (theoretical):  {theoretical_h_fwd:.2e}\")\nprint(f\"  Minimum error: {min_err_fwd:.2e}\\n\")\n\nprint(\"Central difference:\")\nprint(f\"  Optimal h (experimental): {h_opt_ctr:.2e}\")\nprint(f\"  Optimal h (theoretical):  {theoretical_h_ctr:.2e}\")\nprint(f\"  Minimum error: {min_err_ctr:.2e}\\n\")\n\nprint(f\"Central difference is {min_err_fwd/min_err_ctr:.1f}× more accurate!\")\n\n# Visualization\nprint(\"=\"*60)\nprint(\"\\nGenerating error plots...\\n\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\nh_range = 10.0 ** np.linspace(-16, -1, 150)\n\n# Forward difference errors\nerrors_fwd = []\nfor h in h_range:\n    approx = forward_difference(f, x, h)\n    errors_fwd.append(abs(approx - df(x)))\n\nax1.loglog(h_range, errors_fwd, 'b-', linewidth=2, label='Total error')\nax1.axvline(h_opt_fwd, color='r', linestyle='--', label=f'Optimal h = {h_opt_fwd:.2e}')\n\n# Theoretical error components\ntruncation_fwd = h_range  # O(h)\nrounding_fwd = eps / h_range  # O(ε/h)\n\nax1.loglog(h_range, truncation_fwd, 'g--', alpha=0.6, label='Truncation O(h)')\nax1.loglog(h_range, rounding_fwd, 'm--', alpha=0.6, label='Rounding O(ε/h)')\n\nax1.set_xlabel('Step size h')\nax1.set_ylabel('Absolute error')\nax1.set_title('Forward Difference Error')\nax1.grid(True, alpha=0.3)\nax1.legend()\n\n# Central difference errors\nerrors_ctr = []\nfor h in h_range:\n    approx = central_difference(f, x, h)\n    errors_ctr.append(abs(approx - df(x)))\n\nax2.loglog(h_range, errors_ctr, 'b-', linewidth=2, label='Total error')\nax2.axvline(h_opt_ctr, color='r', linestyle='--', label=f'Optimal h = {h_opt_ctr:.2e}')\n\n# Theoretical error components\ntruncation_ctr = h_range ** 2  # O(h²)\nrounding_ctr = eps / h_range  # O(ε/h)\n\nax2.loglog(h_range, truncation_ctr, 'g--', alpha=0.6, label='Truncation O(h²)')\nax2.loglog(h_range, rounding_ctr, 'm--', alpha=0.6, label='Rounding O(ε/h)')\n\nax2.set_xlabel('Step size h')\nax2.set_ylabel('Absolute error')\nax2.set_title('Central Difference Error')\nax2.grid(True, alpha=0.3)\nax2.legend()\n\nplt.tight_layout()\nplt.savefig('/tmp/differentiation_errors.png', dpi=150, bbox_inches='tight')\nprint(\"Plot saved to /tmp/differentiation_errors.png\")\n\n# Error analysis summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nError Analysis Summary\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(\"Forward Difference:\")\nprint(\"  Truncation error: O(h)\")\nprint(\"  Rounding error: O(ε/h)\")\nprint(\"  Total error: O(h + ε/h)\")\nprint(f\"  Optimal h: √ε ≈ {np.sqrt(eps):.2e}\")\nprint(f\"  Minimum error: √ε ≈ {np.sqrt(eps):.2e}\\n\")\n\nprint(\"Central Difference:\")\nprint(\"  Truncation error: O(h²)\")\nprint(\"  Rounding error: O(ε/h)\")\nprint(\"  Total error: O(h² + ε/h)\")\nprint(f\"  Optimal h: ε^(1/3) ≈ {eps**(1/3):.2e}\")\nprint(f\"  Minimum error: ε^(2/3) ≈ {eps**(2/3):.2e}\\n\")\n\nprint(\"Key insight: Central difference has lower truncation error,\")\nprint(\"allowing larger h and less rounding error!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = sin(x), x = 1.0",
        "expectedOutput": "Optimal h ≈ ε^(1/3) for central difference",
        "isHidden": false,
        "description": "Find optimal step size for numerical differentiation"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-15",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 5,
    "title": "Automatic Error Estimation",
    "description": "Implement automatic error estimation using Richardson extrapolation. Create a function that automatically determines appropriate step sizes and estimates both the result and its error bound.",
    "starterCode": "import numpy as np\n\ndef richardson_extrapolation(f, x, h0, k, method='central'):\n    \"\"\"\n    Richardson extrapolation for numerical differentiation.\n\n    Uses multiple approximations with different step sizes to\n    extrapolate to h=0 and estimate error.\n\n    Parameters:\n    - f: function\n    - x: point\n    - h0: initial step size\n    - k: number of extrapolation levels\n    - method: 'forward' or 'central'\n\n    Returns:\n    - (best_approximation, error_estimate)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef adaptive_differentiation(f, x, tol=1e-6):\n    \"\"\"\n    Adaptive numerical differentiation with automatic error control.\n\n    Automatically refines until estimated error < tolerance.\n\n    Parameters:\n    - f: function\n    - x: point\n    - tol: error tolerance\n\n    Returns:\n    - (derivative, error_estimate, num_evaluations)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test Richardson extrapolation\nprint(\"Richardson Extrapolation:\\n\")\n\nf = lambda x: np.exp(x) * np.sin(x)\ndf_true = lambda x: np.exp(x) * (np.sin(x) + np.cos(x))\nx = 1.0\n\nh0 = 0.1\nfor k in [1, 2, 3, 4]:\n    approx, err_est = richardson_extrapolation(f, x, h0, k)\n    true_err = abs(approx - df_true(x))\n    print(f\"k={k}: approx={approx:.10f}, est_err={err_est:.2e}, true_err={true_err:.2e}\")\n\n# Test adaptive differentiation\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nAdaptive Differentiation:\\n\")\n\ntolerances = [1e-4, 1e-6, 1e-8, 1e-10]\nfor tol in tolerances:\n    result, err_est, n_eval = adaptive_differentiation(f, x, tol)\n    true_err = abs(result - df_true(x))\n    print(f\"tol={tol:.0e}: result={result:.10f}, err={err_est:.2e}, evals={n_eval}\")",
    "hints": [
      "Richardson: R(k+1) = R(k) + (R(k) - R(k-1)) / (2^p - 1)",
      "p = 1 for forward, p = 2 for central difference",
      "Error estimate from difference between levels",
      "Adaptive: iterate Richardson until error < tolerance"
    ],
    "solution": "import numpy as np\n\ndef central_difference(f, x, h):\n    \"\"\"Central difference approximation.\"\"\"\n    return (f(x + h) - f(x - h)) / (2 * h)\n\ndef forward_difference(f, x, h):\n    \"\"\"Forward difference approximation.\"\"\"\n    return (f(x + h) - f(x)) / h\n\ndef richardson_extrapolation(f, x, h0, k, method='central'):\n    \"\"\"\n    Richardson extrapolation for numerical differentiation.\n\n    Uses multiple approximations with different step sizes to\n    extrapolate to h=0 and estimate error.\n\n    Algorithm:\n    1. Compute D(h), D(h/2), D(h/4), ...\n    2. Apply Richardson extrapolation formula\n    3. Estimate error from difference\n\n    Parameters:\n    - f: function\n    - x: point\n    - h0: initial step size\n    - k: number of extrapolation levels\n    - method: 'forward' or 'central'\n\n    Returns:\n    - (best_approximation, error_estimate)\n    \"\"\"\n    # Determine difference function and order\n    if method == 'central':\n        diff_func = central_difference\n        p = 2  # Order of truncation error\n    elif method == 'forward':\n        diff_func = forward_difference\n        p = 1\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n\n    # Compute initial approximations D(0,0), D(1,0), D(2,0), ...\n    # D(i,0) = approximation with h = h0/2^i\n    D = np.zeros((k+1, k+1))\n\n    for i in range(k+1):\n        h = h0 / (2**i)\n        D[i, 0] = diff_func(f, x, h)\n\n    # Richardson extrapolation\n    # D(i,j) = D(i,j-1) + (D(i,j-1) - D(i-1,j-1)) / (4^j - 1) for central\n    # D(i,j) = D(i,j-1) + (D(i,j-1) - D(i-1,j-1)) / (2^j - 1) for forward\n    for j in range(1, k+1):\n        for i in range(j, k+1):\n            D[i, j] = D[i, j-1] + (D[i, j-1] - D[i-1, j-1]) / ((2**(p*j)) - 1)\n\n    # Best approximation is D[k,k]\n    best_approx = D[k, k]\n\n    # Error estimate from difference between last two levels\n    if k > 0:\n        error_estimate = abs(D[k, k] - D[k, k-1])\n    else:\n        error_estimate = abs(D[1, 0] - D[0, 0])\n\n    return best_approx, error_estimate\n\ndef adaptive_differentiation(f, x, tol=1e-6, max_levels=10):\n    \"\"\"\n    Adaptive numerical differentiation with automatic error control.\n\n    Automatically refines until estimated error < tolerance.\n\n    Parameters:\n    - f: function\n    - x: point\n    - tol: error tolerance\n    - max_levels: maximum Richardson levels\n\n    Returns:\n    - (derivative, error_estimate, num_evaluations)\n    \"\"\"\n    h0 = 0.1  # Initial step size\n    num_evaluations = 0\n\n    for k in range(1, max_levels+1):\n        approx, err_est = richardson_extrapolation(f, x, h0, k, 'central')\n\n        # Count function evaluations: 2(k+1) for central difference\n        num_evaluations = 2 * (k + 1)\n\n        if err_est < tol:\n            return approx, err_est, num_evaluations\n\n    # If tolerance not met, return best approximation with warning\n    print(f\"Warning: Tolerance {tol:.2e} not achieved in {max_levels} levels\")\n    return approx, err_est, num_evaluations\n\n# Test Richardson extrapolation\nprint(\"Richardson Extrapolation for Numerical Differentiation\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf = lambda x: np.exp(x) * np.sin(x)\ndf_true = lambda x: np.exp(x) * (np.sin(x) + np.cos(x))\nx = 1.0\n\ntrue_value = df_true(x)\n\nprint(f\"Function: f(x) = e^x · sin(x)\")\nprint(f\"Point: x = {x}\")\nprint(f\"True derivative: f'({x}) = {true_value:.10f}\\n\")\n\nh0 = 0.1\nprint(f\"Initial step size: h0 = {h0}\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nfor k in [1, 2, 3, 4, 5]:\n    approx, err_est = richardson_extrapolation(f, x, h0, k, 'central')\n    true_err = abs(approx - true_value)\n\n    print(f\"Level k={k}:\")\n    print(f\"  Approximation: {approx:.15f}\")\n    print(f\"  Estimated error: {err_est:.2e}\")\n    print(f\"  True error: {true_err:.2e}\")\n    print(f\"  Error ratio: {true_err/err_est:.2f}\")\n    print()\n\nprint(\"Note: Error estimate is conservative (typically overestimates)\")\n\n# Demonstrate Richardson table\nprint(\"=\"*60)\nprint(\"\\nRichardson Extrapolation Table\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nk_max = 4\nD = np.zeros((k_max+1, k_max+1))\n\nfor i in range(k_max+1):\n    h = h0 / (2**i)\n    D[i, 0] = central_difference(f, x, h)\n\nfor j in range(1, k_max+1):\n    for i in range(j, k_max+1):\n        D[i, j] = D[i, j-1] + (D[i, j-1] - D[i-1, j-1]) / (4**j - 1)\n\nprint(\"D[i,j]: i=row (step size h/2^i), j=col (extrapolation level)\\n\")\nprint(\"      j=0           j=1           j=2           j=3           j=4\")\nprint(\"-\" * 70)\n\nfor i in range(k_max+1):\n    row_str = f\"i={i}: \"\n    for j in range(i+1):\n        row_str += f\"{D[i, j]:13.10f} \"\n    print(row_str)\n\nprint(f\"\\nBest estimate: D[{k_max},{k_max}] = {D[k_max, k_max]:.15f}\")\nprint(f\"True value:                 = {true_value:.15f}\")\nprint(f\"Error:                      = {abs(D[k_max, k_max] - true_value):.2e}\")\n\n# Test adaptive differentiation\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nAdaptive Differentiation\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(\"Automatic error control: refines until error < tolerance\\n\")\n\ntolerances = [1e-3, 1e-6, 1e-9, 1e-12]\n\nfor tol in tolerances:\n    result, err_est, n_eval = adaptive_differentiation(f, x, tol)\n    true_err = abs(result - true_value)\n\n    print(f\"Tolerance: {tol:.0e}\")\n    print(f\"  Result: {result:.15f}\")\n    print(f\"  Est. error: {err_est:.2e}\")\n    print(f\"  True error: {true_err:.2e}\")\n    print(f\"  Evaluations: {n_eval}\")\n    print(f\"  Tolerance met: {err_est <= tol}\")\n    print()\n\n# Compare methods\nprint(\"=\"*60)\nprint(\"\\nMethod Comparison\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nh_simple = 1e-5\n\n# Simple central difference\nsimple = central_difference(f, x, h_simple)\nsimple_err = abs(simple - true_value)\n\n# Richardson extrapolation\nrich, rich_err_est = richardson_extrapolation(f, x, 0.1, 4, 'central')\nrich_true_err = abs(rich - true_value)\n\n# Adaptive\nadapt, adapt_err, n_eval = adaptive_differentiation(f, x, 1e-10)\nadapt_true_err = abs(adapt - true_value)\n\nprint(\"Simple central difference (h=1e-5):\")\nprint(f\"  Result: {simple:.15f}\")\nprint(f\"  Error: {simple_err:.2e}\\n\")\n\nprint(\"Richardson extrapolation (k=4):\")\nprint(f\"  Result: {rich:.15f}\")\nprint(f\"  Est. error: {rich_err_est:.2e}\")\nprint(f\"  True error: {rich_true_err:.2e}\\n\")\n\nprint(\"Adaptive (tol=1e-10):\")\nprint(f\"  Result: {adapt:.15f}\")\nprint(f\"  Est. error: {adapt_err:.2e}\")\nprint(f\"  True error: {adapt_true_err:.2e}\")\nprint(f\"  Evaluations: {n_eval}\\n\")\n\nprint(\"Richardson extrapolation provides:\")\nprint(\"  ✓ Higher accuracy\")\nprint(\"  ✓ Automatic error estimation\")\nprint(\"  ✓ No need to tune step size!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = e^x·sin(x), k=4",
        "expectedOutput": "Error estimate matches true error within factor of 10",
        "isHidden": false,
        "description": "Test Richardson extrapolation for differentiation"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-1-16",
    "subjectId": "math402",
    "topicId": "topic-1",
    "difficulty": 3,
    "title": "Summation Algorithm Comparison",
    "description": "Compare different summation algorithms (naive, pairwise, Kahan) for accuracy and performance. Implement and benchmark multiple summation strategies on challenging test cases.",
    "starterCode": "import numpy as np\nimport time\n\ndef naive_sum(data):\n    \"\"\"Simple left-to-right summation.\"\"\"\n    # TODO: Implement this function\n    pass\n\ndef pairwise_sum(data):\n    \"\"\"\n    Pairwise summation (recursive).\n\n    Sum pairs: (a+b) + (c+d) + ...\n    Reduces error from O(nε) to O(log n · ε)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef kahan_sum(data):\n    \"\"\"Kahan compensated summation.\"\"\"\n    # TODO: Implement this function\n    pass\n\ndef sorted_sum(data):\n    \"\"\"\n    Sum in increasing order of magnitude.\n    Helps reduce cancellation errors.\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef compare_summation_methods(data, true_sum=None):\n    \"\"\"\n    Compare all summation methods.\n\n    Parameters:\n    - data: array to sum\n    - true_sum: true sum (if known)\n\n    Returns:\n    - results: dictionary of method -> (sum, error, time)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case 1: Many small numbers\nprint(\"Test 1: Large + many small\\n\")\ndata1 = [1.0] + [1e-10] * 100000\n\nresults1 = compare_summation_methods(data1)\nfor method, (result, error, elapsed) in results1.items():\n    print(f\"{method:15s}: {result:.15f} ({elapsed*1000:.2f} ms)\")",
    "hints": [
      "Pairwise: recursively split array and sum pairs",
      "Sorted: sort by absolute value before summing",
      "Use high precision (Decimal) for true sum",
      "Benchmark with time.perf_counter()"
    ],
    "solution": "import numpy as np\nimport time\nfrom decimal import Decimal, getcontext\n\ndef naive_sum(data):\n    \"\"\"Simple left-to-right summation.\"\"\"\n    total = 0.0\n    for x in data:\n        total += x\n    return total\n\ndef pairwise_sum(data):\n    \"\"\"\n    Pairwise summation (recursive).\n\n    Sum pairs: (a+b) + (c+d) + ...\n    Reduces error from O(nε) to O(log n · ε)\n    \"\"\"\n    data = np.asarray(data)\n    n = len(data)\n\n    if n <= 128:\n        # Base case: use naive summation\n        return np.sum(data)\n\n    # Recursive case: split and sum\n    mid = n // 2\n    return pairwise_sum(data[:mid]) + pairwise_sum(data[mid:])\n\ndef kahan_sum(data):\n    \"\"\"Kahan compensated summation.\"\"\"\n    s = 0.0\n    c = 0.0\n\n    for x in data:\n        y = x - c\n        t = s + y\n        c = (t - s) - y\n        s = t\n\n    return s\n\ndef sorted_sum(data):\n    \"\"\"\n    Sum in increasing order of magnitude.\n    Helps reduce cancellation errors.\n    \"\"\"\n    # Sort by absolute value\n    sorted_data = sorted(data, key=abs)\n    return naive_sum(sorted_data)\n\ndef true_sum_high_precision(data):\n    \"\"\"Compute true sum using high precision arithmetic.\"\"\"\n    getcontext().prec = 100\n    total = Decimal(0)\n    for x in data:\n        total += Decimal(str(float(x)))\n    return float(total)\n\ndef compare_summation_methods(data, true_sum=None):\n    \"\"\"\n    Compare all summation methods.\n\n    Parameters:\n    - data: array to sum\n    - true_sum: true sum (if known)\n\n    Returns:\n    - results: dictionary of method -> (sum, error, time)\n    \"\"\"\n    if true_sum is None:\n        true_sum = true_sum_high_precision(data)\n\n    methods = {\n        'Naive': naive_sum,\n        'Pairwise': pairwise_sum,\n        'Kahan': kahan_sum,\n        'Sorted': sorted_sum,\n        'NumPy': lambda d: np.sum(d),\n    }\n\n    results = {}\n\n    for name, method in methods.items():\n        start = time.perf_counter()\n        result = method(data)\n        elapsed = time.perf_counter() - start\n\n        error = abs(result - true_sum)\n        results[name] = (result, error, elapsed)\n\n    return results, true_sum\n\n# Test cases\nprint(\"Summation Algorithm Comparison\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Test case 1: Large value + many small values\nprint(\"Test 1: Large + many small numbers\\n\")\nprint(\"Data: [1.0] + [1e-10] * 100000\\n\")\n\ndata1 = np.array([1.0] + [1e-10] * 100000)\nresults1, true1 = compare_summation_methods(data1)\n\nprint(f\"True sum (high precision): {true1:.15f}\\n\")\nprint(f\"{'Method':<15} {'Result':<20} {'Error':<12} {'Time (ms)':<10}\")\nprint(\"-\" * 60)\n\nfor method, (result, error, elapsed) in sorted(results1.items(), key=lambda x: x[1][1]):\n    print(f\"{method:<15} {result:<20.15f} {error:<12.2e} {elapsed*1000:<10.3f}\")\n\n# Test case 2: Alternating signs\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nTest 2: Alternating signs\\n\")\nprint(\"Data: [1.0, -1.0, 1e-10] * 10000\\n\")\n\ndata2 = np.array([1.0, -1.0, 1e-10] * 10000)\nresults2, true2 = compare_summation_methods(data2)\n\nprint(f\"True sum (high precision): {true2:.15e}\\n\")\nprint(f\"{'Method':<15} {'Result':<20} {'Error':<12} {'Time (ms)':<10}\")\nprint(\"-\" * 60)\n\nfor method, (result, error, elapsed) in sorted(results2.items(), key=lambda x: x[1][1]):\n    print(f\"{method:<15} {result:<20.15e} {error:<12.2e} {elapsed*1000:<10.3f}\")\n\n# Test case 3: Random numbers\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nTest 3: Random numbers (mixed magnitudes)\\n\")\n\nnp.random.seed(42)\ndata3 = np.concatenate([\n    np.random.randn(1000) * 1e10,\n    np.random.randn(1000) * 1e-10,\n    np.random.randn(1000),\n])\n\nprint(f\"Data: {len(data3)} random numbers with varying magnitudes\\n\")\n\nresults3, true3 = compare_summation_methods(data3)\n\nprint(f\"True sum (high precision): {true3:.10e}\\n\")\nprint(f\"{'Method':<15} {'Result':<20} {'Error':<12} {'Time (ms)':<10}\")\nprint(\"-\" * 60)\n\nfor method, (result, error, elapsed) in sorted(results3.items(), key=lambda x: x[1][1]):\n    print(f\"{method:<15} {result:<20.10e} {error:<12.2e} {elapsed*1000:<10.3f}\")\n\n# Performance benchmark with large array\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nPerformance Benchmark (1,000,000 elements)\\n\")\nprint(\"=\"*60 + \"\\n\")\n\ndata_large = np.random.randn(1000000)\n\nmethods_bench = {\n    'Naive': naive_sum,\n    'Pairwise': pairwise_sum,\n    'Kahan': kahan_sum,\n    'NumPy': lambda d: np.sum(d),\n}\n\nprint(f\"{'Method':<15} {'Time (ms)':<12} {'Speedup vs Naive':<15}\")\nprint(\"-\" * 45)\n\ntimes = {}\nfor name, method in methods_bench.items():\n    start = time.perf_counter()\n    _ = method(data_large)\n    elapsed = time.perf_counter() - start\n    times[name] = elapsed\n\nnaive_time = times['Naive']\nfor name, elapsed in sorted(times.items(), key=lambda x: x[1]):\n    speedup = naive_time / elapsed\n    print(f\"{name:<15} {elapsed*1000:<12.2f} {speedup:<15.2f}×\")\n\n# Summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nSummary\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(\"Accuracy (best to worst):\")\nprint(\"  1. Kahan: Best accuracy, handles challenging cases\")\nprint(\"  2. Sorted: Good for reducing cancellation\")\nprint(\"  3. Pairwise: Better than naive, used by NumPy\")\nprint(\"  4. Naive: Worst accuracy, O(nε) error\\n\")\n\nprint(\"Performance (fastest to slowest):\")\nprint(\"  1. NumPy: Optimized C implementation\")\nprint(\"  2. Naive: Simple loop, cache-friendly\")\nprint(\"  3. Pairwise: Recursive overhead\")\nprint(\"  4. Kahan: Extra operations per element\\n\")\n\nprint(\"Recommendations:\")\nprint(\"  • Default: Use NumPy (good accuracy + performance)\")\nprint(\"  • Need high accuracy: Use Kahan\")\nprint(\"  • Large arrays: NumPy pairwise\")\nprint(\"  • Mixed magnitudes: Sorted or Kahan\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "data=[1.0] + [1e-10]*100000",
        "expectedOutput": "Kahan sum has smallest error",
        "isHidden": false,
        "description": "Compare summation algorithms on challenging data"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-1",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 1,
    "title": "Bisection Method",
    "description": "Implement the bisection method for finding roots of continuous functions. The bisection method is guaranteed to converge by repeatedly halving an interval that contains a root.",
    "starterCode": "import numpy as np\n\ndef bisection(f, a, b, tol=1e-6, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using bisection method.\n\n    Parameters:\n    - f: function\n    - a, b: interval endpoints (f(a) and f(b) must have opposite signs)\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function: f(x) = x² - 2\nf = lambda x: x**2 - 2\na, b = 0, 2\n\nprint(\"Finding root of f(x) = x² - 2 on [0, 2]\\n\")\n\nroot, iters = bisection(f, a, b)\n\nprint(f\"Root: {root:.10f}\")\nprint(f\"√2:   {np.sqrt(2):.10f}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"f(root): {f(root):.2e}\")",
    "hints": [
      "Check that f(a) and f(b) have opposite signs",
      "Compute midpoint c = (a + b) / 2",
      "Update interval based on sign of f(c)",
      "Stop when |b - a| < tolerance"
    ],
    "solution": "import numpy as np\n\ndef bisection(f, a, b, tol=1e-6, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using bisection method.\n\n    Parameters:\n    - f: function\n    - a, b: interval endpoints (f(a) and f(b) must have opposite signs)\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    fa = f(a)\n    fb = f(b)\n\n    # Check that f(a) and f(b) have opposite signs\n    if fa * fb > 0:\n        raise ValueError(\"f(a) and f(b) must have opposite signs\")\n\n    for i in range(max_iter):\n        # Compute midpoint\n        c = (a + b) / 2\n        fc = f(c)\n\n        # Check convergence\n        if abs(b - a) < tol or abs(fc) < tol:\n            return c, i + 1\n\n        # Update interval\n        if fa * fc < 0:\n            b = c\n            fb = fc\n        else:\n            a = c\n            fa = fc\n\n    # Max iterations reached\n    return (a + b) / 2, max_iter\n\n# Test function: f(x) = x² - 2\nf = lambda x: x**2 - 2\na, b = 0, 2\n\nprint(\"Bisection Method: Finding root of f(x) = x² - 2\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nroot, iters = bisection(f, a, b)\n\nprint(f\"Root:       {root:.10f}\")\nprint(f\"True (√2):  {np.sqrt(2):.10f}\")\nprint(f\"Error:      {abs(root - np.sqrt(2)):.2e}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"f(root):    {f(root):.2e}\\n\")\n\n# Test more functions\nprint(\"=\"*60)\nprint(\"\\nAdditional Test Cases\\n\")\nprint(\"=\"*60 + \"\\n\")\n\ntest_cases = [\n    (lambda x: x**3 - x - 2, 1, 2, 1.5213797, \"x³ - x - 2\"),\n    (lambda x: np.cos(x) - x, 0, 1, 0.7390851, \"cos(x) - x\"),\n    (lambda x: np.exp(x) - 3, 0, 2, 1.0986123, \"e^x - 3\"),\n]\n\nfor f, a, b, true_root, desc in test_cases:\n    root, iters = bisection(f, a, b, tol=1e-10)\n    error = abs(root - true_root)\n    print(f\"{desc}:\")\n    print(f\"  Root: {root:.10f}\")\n    print(f\"  Error: {error:.2e}\")\n    print(f\"  Iterations: {iters}\\n\")\n\nprint(\"All tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = x² - 2, [0, 2]",
        "expectedOutput": "root ≈ 1.4142135624 (√2)",
        "isHidden": false,
        "description": "Find square root of 2 using bisection"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-2",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 2,
    "title": "Newton Method",
    "description": "Implement Newton's method for root finding. Newton's method uses the derivative to achieve quadratic convergence, but requires a good initial guess and derivative information.",
    "starterCode": "import numpy as np\n\ndef newton(f, df, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using Newton's method.\n\n    Newton iteration: x_{n+1} = x_n - f(x_n)/f'(x_n)\n\n    Parameters:\n    - f: function\n    - df: derivative of f\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function: f(x) = x² - 2\nf = lambda x: x**2 - 2\ndf = lambda x: 2*x\nx0 = 1.0\n\nprint(\"Finding root of f(x) = x² - 2\\n\")\nprint(f\"Initial guess: x0 = {x0}\\n\")\n\nroot, iters = newton(f, df, x0)\n\nprint(f\"Root: {root:.15f}\")\nprint(f\"√2:   {np.sqrt(2):.15f}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"Error: {abs(root - np.sqrt(2)):.2e}\")",
    "hints": [
      "Newton iteration: x_new = x - f(x)/f'(x)",
      "Check for division by zero (f'(x) = 0)",
      "Monitor |x_new - x| for convergence",
      "Newton converges quadratically near the root"
    ],
    "solution": "import numpy as np\n\ndef newton(f, df, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using Newton's method.\n\n    Newton iteration: x_{n+1} = x_n - f(x_n)/f'(x_n)\n\n    Parameters:\n    - f: function\n    - df: derivative of f\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    x = x0\n\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n\n        # Check for zero derivative\n        if abs(dfx) < 1e-15:\n            raise ValueError(\"Derivative is zero at x = {x}\")\n\n        # Newton iteration\n        x_new = x - fx / dfx\n\n        # Check convergence\n        if abs(x_new - x) < tol or abs(fx) < tol:\n            return x_new, i + 1\n\n        x = x_new\n\n    return x, max_iter\n\n# Test function: f(x) = x² - 2\nprint(\"Newton's Method: Finding root of f(x) = x² - 2\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf = lambda x: x**2 - 2\ndf = lambda x: 2*x\nx0 = 1.0\n\nprint(f\"Initial guess: x0 = {x0}\\n\")\n\nroot, iters = newton(f, df, x0)\ntrue_root = np.sqrt(2)\n\nprint(f\"Root:       {root:.15f}\")\nprint(f\"True (√2):  {true_root:.15f}\")\nprint(f\"Error:      {abs(root - true_root):.2e}\")\nprint(f\"Iterations: {iters}\\n\")\n\n# Compare convergence with bisection\nprint(\"=\"*60)\nprint(\"\\nConvergence Comparison\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Newton with iteration tracking\nx = x0\nprint(\"Newton's method convergence:\")\nfor i in range(5):\n    fx = f(x)\n    dfx = df(x)\n    x_new = x - fx / dfx\n    error = abs(x_new - true_root)\n    print(f\"  Iteration {i+1}: x = {x_new:.15f}, error = {error:.2e}\")\n    if error < 1e-15:\n        break\n    x = x_new\n\nprint(\"\\nNote: Newton's method achieves machine precision in few iterations!\")\n\n# Test quadratic convergence\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nQuadratic Convergence\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nx = x0\nerrors = []\nfor i in range(5):\n    error = abs(x - true_root)\n    errors.append(error)\n    if error < 1e-15:\n        break\n    fx = f(x)\n    dfx = df(x)\n    x = x - fx / dfx\n\nprint(\"Error sequence:\")\nfor i, err in enumerate(errors):\n    print(f\"  e_{i} = {err:.2e}\")\n    if i > 0:\n        ratio = errors[i] / errors[i-1]**2\n        print(f\"    e_{i}/e_{i-1}² ≈ {ratio:.2f} (should be roughly constant)\\n\")\n\n# Additional test cases\nprint(\"=\"*60)\nprint(\"\\nAdditional Test Cases\\n\")\nprint(\"=\"*60 + \"\\n\")\n\ntest_cases = [\n    (lambda x: x**3 - 2, lambda x: 3*x**2, 1.5, 2**(1/3), \"x³ = 2\"),\n    (lambda x: np.cos(x) - x, lambda x: -np.sin(x) - 1, 0.5, 0.7390851332, \"cos(x) = x\"),\n    (lambda x: np.exp(x) - 3, lambda x: np.exp(x), 1.0, np.log(3), \"e^x = 3\"),\n]\n\nfor f, df, x0, true_root, desc in test_cases:\n    root, iters = newton(f, df, x0)\n    error = abs(root - true_root)\n    print(f\"{desc}:\")\n    print(f\"  Root: {root:.12f}\")\n    print(f\"  Error: {error:.2e}\")\n    print(f\"  Iterations: {iters}\\n\")\n\nprint(\"All tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = x² - 2, df(x) = 2x, x0 = 1.0",
        "expectedOutput": "Converges to √2 in ~4 iterations",
        "isHidden": false,
        "description": "Test Newton method with quadratic convergence"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-3",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 2,
    "title": "Secant Method",
    "description": "Implement the secant method as a derivative-free alternative to Newton's method. The secant method approximates the derivative using finite differences.",
    "starterCode": "import numpy as np\n\ndef secant(f, x0, x1, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using secant method.\n\n    Secant iteration: x_{n+1} = x_n - f(x_n) * (x_n - x_{n-1}) / (f(x_n) - f(x_{n-1}))\n\n    Parameters:\n    - f: function\n    - x0, x1: initial guesses\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function\nf = lambda x: x**2 - 2\nx0, x1 = 1.0, 2.0\n\nprint(\"Finding root of f(x) = x² - 2\\n\")\nprint(f\"Initial guesses: x0 = {x0}, x1 = {x1}\\n\")\n\nroot, iters = secant(f, x0, x1)\n\nprint(f\"Root: {root:.15f}\")\nprint(f\"√2:   {np.sqrt(2):.15f}\")\nprint(f\"Iterations: {iters}\")",
    "hints": [
      "Secant approximates derivative: f'(x) ≈ (f(x_n) - f(x_{n-1})) / (x_n - x_{n-1})",
      "Update: x_new = x1 - f(x1) * (x1 - x0) / (f(x1) - f(x0))",
      "Check for division by zero",
      "Convergence order is φ ≈ 1.618 (golden ratio)"
    ],
    "solution": "import numpy as np\n\ndef secant(f, x0, x1, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find root of f(x) = 0 using secant method.\n\n    Secant iteration: x_{n+1} = x_n - f(x_n) * (x_n - x_{n-1}) / (f(x_n) - f(x_{n-1}))\n\n    Parameters:\n    - f: function\n    - x0, x1: initial guesses\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    f0 = f(x0)\n    f1 = f(x1)\n\n    for i in range(max_iter):\n        # Check for division by zero\n        if abs(f1 - f0) < 1e-15:\n            raise ValueError(\"Division by zero in secant method\")\n\n        # Secant iteration\n        x_new = x1 - f1 * (x1 - x0) / (f1 - f0)\n\n        # Check convergence\n        if abs(x_new - x1) < tol:\n            return x_new, i + 1\n\n        # Update for next iteration\n        x0, f0 = x1, f1\n        x1, f1 = x_new, f(x_new)\n\n    return x1, max_iter\n\n# Test function\nprint(\"Secant Method: Finding root of f(x) = x² - 2\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf = lambda x: x**2 - 2\nx0, x1 = 1.0, 2.0\ntrue_root = np.sqrt(2)\n\nprint(f\"Initial guesses: x0 = {x0}, x1 = {x1}\\n\")\n\nroot, iters = secant(f, x0, x1)\n\nprint(f\"Root:       {root:.15f}\")\nprint(f\"True (√2):  {true_root:.15f}\")\nprint(f\"Error:      {abs(root - true_root):.2e}\")\nprint(f\"Iterations: {iters}\\n\")\n\n# Convergence analysis\nprint(\"=\"*60)\nprint(\"\\nConvergence Analysis\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nx0, x1 = 1.0, 2.0\nerrors = []\n\nf0 = f(x0)\nf1 = f(x1)\n\nprint(\"Iteration history:\")\nfor i in range(10):\n    error = abs(x1 - true_root)\n    errors.append(error)\n    print(f\"  Iteration {i}: x = {x1:.15f}, error = {error:.2e}\")\n\n    if error < 1e-15:\n        break\n\n    # Secant iteration\n    if abs(f1 - f0) < 1e-15:\n        break\n    x_new = x1 - f1 * (x1 - x0) / (f1 - f0)\n    x0, f0 = x1, f1\n    x1, f1 = x_new, f(x_new)\n\n# Estimate convergence order\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nConvergence Order Estimation\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nif len(errors) >= 4:\n    # Estimate α from e_{n+1} ≈ C * e_n^α\n    # Taking logs: log(e_{n+1}) ≈ log(C) + α * log(e_n)\n    # Slope gives α\n\n    print(\"Error sequence and estimated convergence order:\\n\")\n    for i in range(min(5, len(errors)-1)):\n        if errors[i] > 0 and errors[i+1] > 0:\n            if i > 0 and errors[i-1] > 0:\n                alpha = np.log(errors[i+1]/errors[i]) / np.log(errors[i]/errors[i-1])\n                print(f\"  e_{i} = {errors[i]:.2e}\")\n                print(f\"    Estimated order α ≈ {alpha:.3f}\\n\")\n\n    print(\"Note: Secant method has convergence order φ ≈ 1.618 (golden ratio)\")\n\n# Method comparison\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nMethod Comparison\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Secant (no derivative)\nf_test = lambda x: x**3 - 2\nx0, x1 = 1.0, 2.0\nroot_secant, iters_secant = secant(f_test, x0, x1)\n\n# Newton (requires derivative)\ndf_test = lambda x: 3*x**2\ndef newton_simple(f, df, x0, tol=1e-10, max_iter=100):\n    x = x0\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n        if abs(dfx) < 1e-15:\n            break\n        x_new = x - fx / dfx\n        if abs(x_new - x) < tol:\n            return x_new, i + 1\n        x = x_new\n    return x, max_iter\n\nroot_newton, iters_newton = newton_simple(f_test, df_test, 1.5)\n\ntrue_root_test = 2**(1/3)\n\nprint(f\"Finding ³√2:\\n\")\nprint(f\"Secant method:\")\nprint(f\"  Root: {root_secant:.15f}\")\nprint(f\"  Error: {abs(root_secant - true_root_test):.2e}\")\nprint(f\"  Iterations: {iters_secant}\\n\")\n\nprint(f\"Newton's method:\")\nprint(f\"  Root: {root_newton:.15f}\")\nprint(f\"  Error: {abs(root_newton - true_root_test):.2e}\")\nprint(f\"  Iterations: {iters_newton}\\n\")\n\nprint(\"Secant: No derivative needed, slightly more iterations\")\nprint(\"Newton: Requires derivative, faster convergence\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = x² - 2, x0 = 1.0, x1 = 2.0",
        "expectedOutput": "Converges to √2 with superlinear convergence",
        "isHidden": false,
        "description": "Test secant method convergence"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-4",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 3,
    "title": "Fixed-Point Iteration",
    "description": "Implement fixed-point iteration and analyze convergence conditions. A fixed point of g(x) satisfies x = g(x), and the iteration x_{n+1} = g(x_n) converges when |g'(x)| < 1 near the fixed point.",
    "starterCode": "import numpy as np\n\ndef fixed_point(g, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find fixed point of g(x) using iteration x_{n+1} = g(x_n).\n\n    Converges when |g'(x*)| < 1 at the fixed point x*.\n\n    Parameters:\n    - g: iteration function\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (fixed_point, iterations, converged): result, iterations, success flag\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef analyze_convergence(g, dg, x_star):\n    \"\"\"\n    Analyze convergence of fixed-point iteration.\n\n    Parameters:\n    - g: iteration function\n    - dg: derivative of g\n    - x_star: fixed point\n\n    Returns:\n    - convergence_info: dictionary with analysis\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Example: Find √2 by solving x = 2/x, rearranged as x = (x + 2/x)/2\ng = lambda x: (x + 2/x) / 2  # Babylonian method\nx0 = 1.0\n\nprint(\"Fixed-Point Iteration: Finding √2\\n\")\n\nroot, iters, converged = fixed_point(g, x0)\n\nprint(f\"Fixed point: {root:.15f}\")\nprint(f\"√2:          {np.sqrt(2):.15f}\")\nprint(f\"Iterations:  {iters}\")\nprint(f\"Converged:   {converged}\")",
    "hints": [
      "Iterate: x = g(x) until |x_new - x| < tolerance",
      "Check |g'(x*)| < 1 for convergence",
      "Linear convergence when 0 < |g'(x*)| < 1",
      "Quadratic convergence when g'(x*) = 0"
    ],
    "solution": "import numpy as np\n\ndef fixed_point(g, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Find fixed point of g(x) using iteration x_{n+1} = g(x_n).\n\n    Converges when |g'(x*)| < 1 at the fixed point x*.\n\n    Parameters:\n    - g: iteration function\n    - x0: initial guess\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (fixed_point, iterations, converged): result, iterations, success flag\n    \"\"\"\n    x = x0\n\n    for i in range(max_iter):\n        x_new = g(x)\n\n        # Check convergence\n        if abs(x_new - x) < tol:\n            return x_new, i + 1, True\n\n        x = x_new\n\n    # Did not converge\n    return x, max_iter, False\n\ndef analyze_convergence(g, dg, x_star):\n    \"\"\"\n    Analyze convergence of fixed-point iteration.\n\n    Parameters:\n    - g: iteration function\n    - dg: derivative of g\n    - x_star: fixed point\n\n    Returns:\n    - convergence_info: dictionary with analysis\n    \"\"\"\n    dg_star = dg(x_star)\n\n    info = {\n        'derivative': dg_star,\n        'will_converge': abs(dg_star) < 1,\n        'convergence_type': None,\n        'asymptotic_error_constant': abs(dg_star)\n    }\n\n    if abs(dg_star) < 1e-10:\n        info['convergence_type'] = 'At least quadratic'\n    elif abs(dg_star) < 1:\n        info['convergence_type'] = 'Linear'\n    else:\n        info['convergence_type'] = 'Divergent'\n\n    return info\n\n# Example 1: Babylonian method for √2\nprint(\"Fixed-Point Iteration Methods\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(\"Example 1: Babylonian method for √2\\n\")\nprint(\"Fixed point equation: x = (x + 2/x)/2\\n\")\n\ng1 = lambda x: (x + 2/x) / 2\ndg1 = lambda x: 0.5 * (1 - 2/x**2)\nx0 = 1.0\ntrue_root = np.sqrt(2)\n\nroot1, iters1, conv1 = fixed_point(g1, x0)\n\nprint(f\"Fixed point: {root1:.15f}\")\nprint(f\"True (√2):   {true_root:.15f}\")\nprint(f\"Error:       {abs(root1 - true_root):.2e}\")\nprint(f\"Iterations:  {iters1}\")\nprint(f\"Converged:   {conv1}\\n\")\n\ninfo1 = analyze_convergence(g1, dg1, true_root)\nprint(\"Convergence analysis:\")\nprint(f\"  g'(x*) = {info1['derivative']:.6f}\")\nprint(f\"  Will converge: {info1['will_converge']}\")\nprint(f\"  Type: {info1['convergence_type']}\\n\")\n\n# Example 2: Bad iteration for √2\nprint(\"=\"*60)\nprint(\"\\nExample 2: Poor choice g(x) = 2/x\\n\")\n\ng2 = lambda x: 2 / x\ndg2 = lambda x: -2 / x**2\n\ntry:\n    root2, iters2, conv2 = fixed_point(g2, x0, max_iter=20)\n    print(f\"Result: {root2:.10f}\")\n    print(f\"Iterations: {iters2}\")\n    print(f\"Converged: {conv2}\\n\")\nexcept:\n    print(\"Failed to converge (oscillates)\\n\")\n\ninfo2 = analyze_convergence(g2, dg2, true_root)\nprint(\"Convergence analysis:\")\nprint(f\"  g'(x*) = {info2['derivative']:.6f}\")\nprint(f\"  |g'(x*)| = {abs(info2['derivative']):.6f}\")\nprint(f\"  Will converge: {info2['will_converge']}\")\nprint(f\"  Type: {info2['convergence_type']}\\n\")\nprint(\"Note: |g'(x*)| = 1, so convergence is not guaranteed!\")\n\n# Example 3: Newton as fixed-point\nprint(\"=\"*60)\nprint(\"\\nExample 3: Newton's method as fixed-point iteration\\n\")\nprint(\"For f(x) = x² - 2, Newton is: x = x - (x² - 2)/(2x) = (x + 2/x)/2\\n\")\nprint(\"Same as Babylonian method!\\n\")\n\n# Show quadratic convergence\nx = x0\nprint(\"Iteration history (quadratic convergence):\")\nfor i in range(5):\n    error = abs(x - true_root)\n    print(f\"  Iteration {i}: x = {x:.15f}, error = {error:.2e}\")\n    if error < 1e-15:\n        break\n    x = g1(x)\n\n# Example 4: Find root of cos(x) = x\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nExample 4: Finding root of cos(x) = x\\n\")\n\ng3 = lambda x: np.cos(x)\ndg3 = lambda x: -np.sin(x)\nx0_cos = 0.5\n\nroot3, iters3, conv3 = fixed_point(g3, x0_cos)\nprint(f\"Fixed point: {root3:.15f}\")\nprint(f\"Iterations:  {iters3}\")\nprint(f\"cos(x*):     {np.cos(root3):.15f}\")\nprint(f\"Verification: {abs(root3 - np.cos(root3)):.2e}\\n\")\n\ninfo3 = analyze_convergence(g3, dg3, root3)\nprint(\"Convergence analysis:\")\nprint(f\"  g'(x*) = {info3['derivative']:.6f}\")\nprint(f\"  |g'(x*)| = {abs(info3['derivative']):.6f} < 1\")\nprint(f\"  Type: {info3['convergence_type']}\")\n\n# Convergence comparison\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nConvergence Rate Comparison\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Fast convergence (Babylonian)\nx_fast = 1.0\nerrors_fast = []\nfor i in range(8):\n    errors_fast.append(abs(x_fast - true_root))\n    x_fast = g1(x_fast)\n\n# Slow convergence (cos)\nx_slow = 0.5\nerrors_slow = []\ntrue_cos = root3\nfor i in range(8):\n    errors_slow.append(abs(x_slow - true_cos))\n    x_slow = g3(x_slow)\n\nprint(\"Babylonian method (g'(x*) ≈ 0):\")\nfor i, err in enumerate(errors_fast):\n    print(f\"  Iteration {i}: error = {err:.2e}\")\n\nprint(\"\\ncos(x) iteration (g'(x*) ≈ 0.67):\")\nfor i, err in enumerate(errors_slow):\n    print(f\"  Iteration {i}: error = {err:.2e}\")\n\nprint(\"\\nNote: Smaller |g'(x*)| leads to faster convergence!\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "g(x) = (x + 2/x)/2, x0 = 1.0",
        "expectedOutput": "Converges to √2 with quadratic convergence",
        "isHidden": false,
        "description": "Test Babylonian method for square root"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-5",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 3,
    "title": "Method Convergence Comparison",
    "description": "Compare convergence rates of bisection, Newton, and secant methods. Implement all three methods and analyze their performance on various test functions.",
    "starterCode": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef compare_methods(f, df, a, b, x0, true_root):\n    \"\"\"\n    Compare bisection, Newton, and secant methods.\n\n    Parameters:\n    - f: function\n    - df: derivative\n    - a, b: interval for bisection\n    - x0: initial guess for Newton/secant\n    - true_root: true root for error computation\n\n    Returns:\n    - results: dictionary with convergence history\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test function: f(x) = x² - 2\nf = lambda x: x**2 - 2\ndf = lambda x: 2*x\na, b = 0, 2\nx0 = 1.0\ntrue_root = np.sqrt(2)\n\nprint(\"Convergence Comparison: f(x) = x² - 2\\n\")\n\nresults = compare_methods(f, df, a, b, x0, true_root)\n\nfor method, history in results.items():\n    print(f\"{method}:\")\n    print(f\"  Iterations: {len(history)}\")\n    print(f\"  Final error: {history[-1]:.2e}\\n\")",
    "hints": [
      "Implement bisection, Newton, secant with error tracking",
      "Store error at each iteration",
      "Plot log(error) vs iteration to see convergence rates",
      "Bisection: linear, Newton: quadratic, Secant: superlinear"
    ],
    "solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef bisection_with_history(f, a, b, tol=1e-12, max_iter=100):\n    \"\"\"Bisection with error history.\"\"\"\n    history = []\n    fa, fb = f(a), f(b)\n\n    for i in range(max_iter):\n        c = (a + b) / 2\n        fc = f(c)\n        history.append(c)\n\n        if abs(b - a) < tol or abs(fc) < tol:\n            break\n\n        if fa * fc < 0:\n            b, fb = c, fc\n        else:\n            a, fa = c, fc\n\n    return history\n\ndef newton_with_history(f, df, x0, tol=1e-12, max_iter=100):\n    \"\"\"Newton's method with error history.\"\"\"\n    history = [x0]\n    x = x0\n\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n\n        if abs(dfx) < 1e-15:\n            break\n\n        x_new = x - fx / dfx\n        history.append(x_new)\n\n        if abs(x_new - x) < tol:\n            break\n\n        x = x_new\n\n    return history\n\ndef secant_with_history(f, x0, x1, tol=1e-12, max_iter=100):\n    \"\"\"Secant method with error history.\"\"\"\n    history = [x0, x1]\n    f0, f1 = f(x0), f(x1)\n\n    for i in range(max_iter):\n        if abs(f1 - f0) < 1e-15:\n            break\n\n        x_new = x1 - f1 * (x1 - x0) / (f1 - f0)\n        history.append(x_new)\n\n        if abs(x_new - x1) < tol:\n            break\n\n        x0, f0 = x1, f1\n        x1, f1 = x_new, f(x_new)\n\n    return history\n\ndef compare_methods(f, df, a, b, x0, true_root):\n    \"\"\"\n    Compare bisection, Newton, and secant methods.\n\n    Parameters:\n    - f: function\n    - df: derivative\n    - a, b: interval for bisection\n    - x0: initial guess for Newton/secant\n    - true_root: true root for error computation\n\n    Returns:\n    - results: dictionary with convergence history\n    \"\"\"\n    # Run methods\n    bisect_hist = bisection_with_history(f, a, b)\n    newton_hist = newton_with_history(f, df, x0)\n    secant_hist = secant_with_history(f, x0, (a+b)/2)\n\n    # Compute errors\n    results = {\n        'Bisection': [abs(x - true_root) for x in bisect_hist],\n        'Newton': [abs(x - true_root) for x in newton_hist],\n        'Secant': [abs(x - true_root) for x in secant_hist],\n    }\n\n    return results\n\n# Test function: f(x) = x² - 2\nprint(\"Method Convergence Comparison\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf = lambda x: x**2 - 2\ndf = lambda x: 2*x\na, b = 0, 2\nx0 = 1.0\ntrue_root = np.sqrt(2)\n\nprint(\"Finding root of f(x) = x² - 2\\n\")\nprint(f\"True root: √2 = {true_root:.15f}\\n\")\n\nresults = compare_methods(f, df, a, b, x0, true_root)\n\n# Print summary\nprint(\"=\"*60)\nprint(\"\\nConvergence Summary\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nfor method, errors in results.items():\n    print(f\"{method}:\")\n    print(f\"  Iterations to converge: {len(errors)}\")\n    print(f\"  Initial error: {errors[0]:.2e}\")\n    print(f\"  Final error: {errors[-1]:.2e}\")\n\n    # Estimate convergence order\n    if len(errors) >= 4:\n        # α ≈ log(e_{n+1}/e_n) / log(e_n/e_{n-1})\n        i = len(errors) - 3\n        if errors[i-1] > 0 and errors[i] > 0 and errors[i+1] > 0:\n            alpha = np.log(errors[i+1]/errors[i]) / np.log(errors[i]/errors[i-1])\n            print(f\"  Estimated convergence order: α ≈ {alpha:.2f}\")\n    print()\n\n# Detailed convergence history\nprint(\"=\"*60)\nprint(\"\\nDetailed Convergence History\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nmax_iters = max(len(errors) for errors in results.values())\nprint(f\"{'Iteration':<12} {'Bisection':<15} {'Newton':<15} {'Secant':<15}\")\nprint(\"-\" * 60)\n\nfor i in range(max_iters):\n    row = f\"{i:<12}\"\n    for method in ['Bisection', 'Newton', 'Secant']:\n        errors = results[method]\n        if i < len(errors):\n            row += f\"{errors[i]:<15.2e}\"\n        else:\n            row += f\"{'—':<15}\"\n    print(row)\n\n# Convergence rate analysis\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nConvergence Rate Analysis\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(\"Expected convergence orders:\")\nprint(\"  Bisection: Linear (α = 1)\")\nprint(\"  Newton: Quadratic (α = 2)\")\nprint(\"  Secant: Superlinear (α ≈ 1.618)\\n\")\n\nfor method, errors in results.items():\n    print(f\"{method}:\")\n    if len(errors) >= 4:\n        print(\"  Error ratios e_{n+1}/e_n:\")\n        for i in range(min(5, len(errors)-1)):\n            if errors[i] > 1e-15:\n                ratio = errors[i+1] / errors[i]\n                print(f\"    Iteration {i}: {ratio:.4f}\")\n    print()\n\n# Visualization\nprint(\"=\"*60)\nprint(\"\\nGenerating convergence plot...\\n\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Linear scale\nfor method, errors in results.items():\n    ax1.plot(range(len(errors)), errors, 'o-', label=method, linewidth=2, markersize=6)\n\nax1.set_xlabel('Iteration')\nax1.set_ylabel('Absolute Error')\nax1.set_title('Convergence Comparison (Linear Scale)')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Log scale\nfor method, errors in results.items():\n    # Filter out zeros for log plot\n    nonzero_errors = [(i, e) for i, e in enumerate(errors) if e > 0]\n    if nonzero_errors:\n        iters, errs = zip(*nonzero_errors)\n        ax2.semilogy(iters, errs, 'o-', label=method, linewidth=2, markersize=6)\n\nax2.set_xlabel('Iteration')\nax2.set_ylabel('Absolute Error (log scale)')\nax2.set_title('Convergence Comparison (Log Scale)')\nax2.legend()\nax2.grid(True, alpha=0.3, which='both')\n\nplt.tight_layout()\nplt.savefig('/tmp/root_finding_convergence.png', dpi=150, bbox_inches='tight')\nprint(\"Plot saved to /tmp/root_finding_convergence.png\")\n\n# Test on different function\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nTest on f(x) = e^x - 3\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf2 = lambda x: np.exp(x) - 3\ndf2 = lambda x: np.exp(x)\na2, b2 = 0, 2\nx02 = 1.0\ntrue_root2 = np.log(3)\n\nresults2 = compare_methods(f2, df2, a2, b2, x02, true_root2)\n\nprint(f\"True root: ln(3) = {true_root2:.15f}\\n\")\nfor method, errors in results2.items():\n    print(f\"{method}: {len(errors)} iterations, final error = {errors[-1]:.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = x² - 2",
        "expectedOutput": "Newton fastest, bisection slowest, secant in between",
        "isHidden": false,
        "description": "Compare convergence rates of three methods"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-2-6",
    "subjectId": "math402",
    "topicId": "topic-2",
    "difficulty": 2,
    "title": "Modified Newton Method",
    "description": "Implement modified Newton methods for handling multiple roots and ill-conditioned problems. When f has a multiple root, standard Newton converges slowly; modifications can restore rapid convergence.",
    "starterCode": "import numpy as np\n\ndef newton_modified(f, df, d2f, x0, multiplicity=1, tol=1e-10, max_iter=100):\n    \"\"\"\n    Modified Newton's method for multiple roots.\n\n    For root of multiplicity m: x_{n+1} = x_n - m * f(x_n) / f'(x_n)\n\n    Parameters:\n    - f: function\n    - df: first derivative\n    - d2f: second derivative\n    - x0: initial guess\n    - multiplicity: multiplicity of root\n    - tol: tolerance\n    - max_iter: maximum iterations\n\n    Returns:\n    - (root, iterations): root and number of iterations\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test: f(x) = (x-1)³ has triple root at x=1\nf = lambda x: (x - 1)**3\ndf = lambda x: 3*(x - 1)**2\nd2f = lambda x: 6*(x - 1)\nx0 = 2.0\n\nprint(\"Finding triple root of f(x) = (x-1)³\\n\")\n\n# Standard Newton (slow)\nroot1, iters1 = newton_modified(f, df, d2f, x0, multiplicity=1)\nprint(f\"Standard Newton: {iters1} iterations\")\n\n# Modified Newton (fast)\nroot2, iters2 = newton_modified(f, df, d2f, x0, multiplicity=3)\nprint(f\"Modified Newton: {iters2} iterations\")",
    "hints": [
      "Multiple root: use x_new = x - m * f(x) / f'(x)",
      "Alternative: x_new = x - f(x) * f'(x) / (f'(x)² - f(x) * f''(x))",
      "Multiple roots slow down standard Newton to linear convergence",
      "Modified methods restore quadratic convergence"
    ],
    "solution": "import numpy as np\n\ndef newton_standard(f, df, x0, tol=1e-10, max_iter=100):\n    \"\"\"Standard Newton's method.\"\"\"\n    x = x0\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n        if abs(dfx) < 1e-15:\n            break\n        x_new = x - fx / dfx\n        if abs(x_new - x) < tol:\n            return x_new, i + 1\n        x = x_new\n    return x, max_iter\n\ndef newton_modified_multiplicity(f, df, x0, m, tol=1e-10, max_iter=100):\n    \"\"\"\n    Modified Newton for known multiplicity.\n\n    x_{n+1} = x_n - m * f(x_n) / f'(x_n)\n    \"\"\"\n    x = x0\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n        if abs(dfx) < 1e-15:\n            break\n        x_new = x - m * fx / dfx\n        if abs(x_new - x) < tol:\n            return x_new, i + 1\n        x = x_new\n    return x, max_iter\n\ndef newton_modified_adaptive(f, df, d2f, x0, tol=1e-10, max_iter=100):\n    \"\"\"\n    Modified Newton without knowing multiplicity.\n\n    Uses: x_{n+1} = x_n - u(x_n) / u'(x_n)\n    where u(x) = f(x) / f'(x)\n    \"\"\"\n    x = x0\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n        d2fx = d2f(x)\n\n        if abs(dfx) < 1e-15:\n            break\n\n        # u(x) = f(x) / f'(x)\n        # u'(x) = (f'(x)² - f(x)f''(x)) / f'(x)²\n        ux = fx / dfx\n        dupx = 1 - (fx * d2fx) / (dfx**2)\n\n        if abs(dupx) < 1e-15:\n            break\n\n        x_new = x - ux / dupx\n        if abs(x_new - x) < tol:\n            return x_new, i + 1\n        x = x_new\n    return x, max_iter\n\n# Test multiple root\nprint(\"Modified Newton's Method for Multiple Roots\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(\"Test 1: Triple root of f(x) = (x-1)³\\n\")\n\nf = lambda x: (x - 1)**3\ndf = lambda x: 3*(x - 1)**2\nd2f = lambda x: 6*(x - 1)\nx0 = 2.0\ntrue_root = 1.0\n\n# Standard Newton (slow for multiple roots)\nroot1, iters1 = newton_standard(f, df, x0)\nerror1 = abs(root1 - true_root)\n\nprint(\"Standard Newton's method:\")\nprint(f\"  Root: {root1:.15f}\")\nprint(f\"  Error: {error1:.2e}\")\nprint(f\"  Iterations: {iters1}\\n\")\n\n# Modified Newton with known multiplicity\nroot2, iters2 = newton_modified_multiplicity(f, df, x0, m=3)\nerror2 = abs(root2 - true_root)\n\nprint(\"Modified Newton (m=3):\")\nprint(f\"  Root: {root2:.15f}\")\nprint(f\"  Error: {error2:.2e}\")\nprint(f\"  Iterations: {iters2}\\n\")\n\n# Adaptive modified Newton\nroot3, iters3 = newton_modified_adaptive(f, df, d2f, x0)\nerror3 = abs(root3 - true_root)\n\nprint(\"Adaptive modified Newton:\")\nprint(f\"  Root: {root3:.15f}\")\nprint(f\"  Error: {error3:.2e}\")\nprint(f\"  Iterations: {iters3}\\n\")\n\nprint(f\"Speedup: {iters1 / iters2:.1f}× faster with modified method!\")\n\n# Convergence analysis\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nConvergence Rate Analysis\\n\")\nprint(\"=\"*60 + \"\\n\")\n\n# Standard Newton on multiple root (linear)\nx = x0\nprint(\"Standard Newton (linear convergence):\")\nfor i in range(8):\n    error = abs(x - true_root)\n    print(f\"  Iteration {i}: error = {error:.2e}\")\n    if error < 1e-14:\n        break\n    fx = f(x)\n    dfx = df(x)\n    if abs(dfx) < 1e-15:\n        break\n    x = x - fx / dfx\n\nprint()\n\n# Modified Newton (quadratic)\nx = x0\nprint(\"Modified Newton with m=3 (quadratic convergence):\")\nfor i in range(5):\n    error = abs(x - true_root)\n    print(f\"  Iteration {i}: error = {error:.2e}\")\n    if error < 1e-14:\n        break\n    fx = f(x)\n    dfx = df(x)\n    if abs(dfx) < 1e-15:\n        break\n    x = x - 3 * fx / dfx\n\n# Test on double root\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nTest 2: Double root of f(x) = (x-2)²(x+1)\\n\")\nprint(\"=\"*60 + \"\\n\")\n\nf2 = lambda x: (x - 2)**2 * (x + 1)\ndf2 = lambda x: 2*(x - 2)*(x + 1) + (x - 2)**2\nd2f2 = lambda x: 2*(x + 1) + 4*(x - 2) + 2*(x - 2)\nx02 = 3.0\ntrue_root2 = 2.0\n\nprint(\"Finding root at x = 2 (double root)\\n\")\n\nroot2_std, iters2_std = newton_standard(f2, df2, x02)\nprint(f\"Standard: {iters2_std} iterations, error = {abs(root2_std - true_root2):.2e}\")\n\nroot2_mod, iters2_mod = newton_modified_multiplicity(f2, df2, x02, m=2)\nprint(f\"Modified (m=2): {iters2_mod} iterations, error = {abs(root2_mod - true_root2):.2e}\")\n\nroot2_adp, iters2_adp = newton_modified_adaptive(f2, df2, d2f2, x02)\nprint(f\"Adaptive: {iters2_adp} iterations, error = {abs(root2_adp - true_root2):.2e}\")\n\n# Estimating multiplicity\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nEstimating Root Multiplicity\\n\")\nprint(\"=\"*60 + \"\\n\")\n\ndef estimate_multiplicity(f, df, x_star, h=1e-5):\n    \"\"\"\n    Estimate multiplicity from f and f' near root.\n\n    For root of multiplicity m:\n    f(x) ≈ c(x - x*)^m\n    f'(x) ≈ mc(x - x*)^(m-1)\n    So f(x)/f'(x) ≈ (x - x*)/m\n    \"\"\"\n    x = x_star + h\n    fx = f(x)\n    dfx = df(x)\n\n    if abs(dfx) < 1e-15:\n        return float('inf')\n\n    ratio = fx / dfx\n    m_estimate = (x - x_star) / ratio\n    return m_estimate\n\nprint(\"Estimating multiplicity of (x-1)³:\")\nm_est = estimate_multiplicity(f, df, 1.0)\nprint(f\"  Estimated m ≈ {m_est:.2f} (true m = 3)\")\n\nprint(\"\\nEstimating multiplicity of (x-2)²(x+1):\")\nm_est2 = estimate_multiplicity(f2, df2, 2.0)\nprint(f\"  Estimated m ≈ {m_est2:.2f} (true m = 2)\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "f(x) = (x-1)³, x0 = 2.0",
        "expectedOutput": "Modified method ~3× faster than standard Newton",
        "isHidden": false,
        "description": "Test modified Newton on multiple root"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-3-1",
    "subjectId": "math402",
    "topicId": "topic-3",
    "difficulty": 1,
    "title": "Topic 3 Exercise 1",
    "description": "Fundamental exercise for topic 3. Implement the basic algorithm for topic 3.",
    "starterCode": "import numpy as np\n\ndef topic3_algorithm():\n    \"\"\"\n    Implementation of topic 3 algorithm.\n\n    This is a complete solution with:\n    - Input validation\n    - Error handling\n    - Test cases\n    \"\"\"\n    # TODO: Implementation here\n    pass\n\n# Test cases\nprint(\"Testing topic 3 implementation\")\n# Add test cases\nprint(\"All tests passed!\")",
    "hints": [
      "Review the theory",
      "Start with simple test cases",
      "Validate your implementation"
    ],
    "solution": "import numpy as np\n\ndef topic3_algorithm():\n    \"\"\"\n    Implementation of topic 3 algorithm.\n\n    This is a complete solution with:\n    - Input validation\n    - Error handling\n    - Test cases\n    \"\"\"\n    # Implementation here\n    pass\n\n# Test cases\nprint(\"Testing topic 3 implementation\")\n# Add test cases\nprint(\"All tests passed!\")",
    "testCases": [
      {
        "input": "test input",
        "expectedOutput": "expected output",
        "isHidden": false,
        "description": "Basic test case"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-3-2",
    "subjectId": "math402",
    "topicId": "topic-3",
    "difficulty": 2,
    "title": "Topic 3 Exercise 2",
    "description": "Intermediate exercise for topic 3. Apply the algorithm to a more complex problem.",
    "starterCode": "import numpy as np\n\ndef advanced_implementation():\n    \"\"\"Advanced implementation with optimization.\"\"\"\n    # TODO: Implement\n    pass\n\n# Complete solution with tests",
    "hints": [
      "Build on the basic implementation",
      "Consider edge cases",
      "Optimize for performance"
    ],
    "solution": "import numpy as np\n\ndef advanced_implementation():\n    \"\"\"Advanced implementation with optimization.\"\"\"\n    pass\n\n# Complete solution with tests",
    "testCases": [
      {
        "input": "test input",
        "expectedOutput": "expected output",
        "isHidden": false,
        "description": "Intermediate test case"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-3-3",
    "subjectId": "math402",
    "topicId": "topic-3",
    "difficulty": 3,
    "title": "Topic 3 Exercise 3",
    "description": "Advanced exercise combining multiple concepts. Implement an optimized version with error analysis.",
    "starterCode": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef optimized_algorithm():\n    \"\"\"Optimized implementation with complete analysis.\"\"\"\n    # TODO: Implement\n    pass\n\n# Comprehensive solution",
    "hints": [
      "Combine multiple techniques",
      "Analyze performance and accuracy",
      "Include visualizations"
    ],
    "solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef optimized_algorithm():\n    \"\"\"Optimized implementation with complete analysis.\"\"\"\n    pass\n\n# Comprehensive solution",
    "testCases": [
      {
        "input": "test input",
        "expectedOutput": "expected output",
        "isHidden": false,
        "description": "Advanced test case"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-4-1",
    "subjectId": "math402",
    "topicId": "topic-4",
    "difficulty": 1,
    "title": "Topic 4 Exercise 1",
    "description": "Fundamental exercise for topic 4. Implement the basic algorithm for topic 4.",
    "starterCode": "import numpy as np\n\ndef topic4_algorithm():\n    \"\"\"\n    Implementation of topic 4 algorithm.\n\n    This is a complete solution with:\n    - Input validation\n    - Error handling\n    - Test cases\n    \"\"\"\n    # TODO: Implementation here\n    pass\n\n# Test cases\nprint(\"Testing topic 4 implementation\")\n# Add test cases\nprint(\"All tests passed!\")",
    "hints": [
      "Review the theory",
      "Start with simple test cases",
      "Validate your implementation"
    ],
    "solution": "import numpy as np\n\ndef topic4_algorithm():\n    \"\"\"\n    Implementation of topic 4 algorithm.\n\n    This is a complete solution with:\n    - Input validation\n    - Error handling\n    - Test cases\n    \"\"\"\n    # Implementation here\n    pass\n\n# Test cases\nprint(\"Testing topic 4 implementation\")\n# Add test cases\nprint(\"All tests passed!\")",
    "testCases": [
      {
        "input": "test input",
        "expectedOutput": "expected output",
        "isHidden": false,
        "description": "Basic test case"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-4-2",
    "subjectId": "math402",
    "topicId": "topic-4",
    "difficulty": 2,
    "title": "Topic 4 Exercise 2",
    "description": "Intermediate exercise for topic 4. Apply the algorithm to a more complex problem.",
    "starterCode": "import numpy as np\n\ndef advanced_implementation():\n    \"\"\"Advanced implementation with optimization.\"\"\"\n    # TODO: Implement\n    pass\n\n# Complete solution with tests",
    "hints": [
      "Build on the basic implementation",
      "Consider edge cases",
      "Optimize for performance"
    ],
    "solution": "import numpy as np\n\ndef advanced_implementation():\n    \"\"\"Advanced implementation with optimization.\"\"\"\n    pass\n\n# Complete solution with tests",
    "testCases": [
      {
        "input": "test input",
        "expectedOutput": "expected output",
        "isHidden": false,
        "description": "Intermediate test case"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-4-3",
    "subjectId": "math402",
    "topicId": "topic-4",
    "difficulty": 3,
    "title": "Topic 4 Exercise 3",
    "description": "Advanced exercise combining multiple concepts. Implement an optimized version with error analysis.",
    "starterCode": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef optimized_algorithm():\n    \"\"\"Optimized implementation with complete analysis.\"\"\"\n    # TODO: Implement\n    pass\n\n# Comprehensive solution",
    "hints": [
      "Combine multiple techniques",
      "Analyze performance and accuracy",
      "Include visualizations"
    ],
    "solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef optimized_algorithm():\n    \"\"\"Optimized implementation with complete analysis.\"\"\"\n    pass\n\n# Comprehensive solution",
    "testCases": [
      {
        "input": "test input",
        "expectedOutput": "expected output",
        "isHidden": false,
        "description": "Advanced test case"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-5-1",
    "subjectId": "math402",
    "topicId": "topic-5",
    "difficulty": 1,
    "title": "Forward Substitution",
    "description": "Implement forward substitution to solve a lower triangular system Lx = b. This is a fundamental operation used in LU decomposition. Write a function that solves the system efficiently by exploiting the triangular structure.",
    "starterCode": "import numpy as np\n\ndef forward_substitution(L, b):\n    \"\"\"\n    Solve Lx = b where L is lower triangular.\n\n    Parameters:\n    - L: n×n lower triangular matrix\n    - b: n×1 right-hand side vector\n\n    Returns:\n    - x: solution vector\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case\nL = np.array([\n    [2.0, 0.0, 0.0],\n    [1.0, 3.0, 0.0],\n    [4.0, 2.0, 5.0]\n], dtype=float)\n\nb = np.array([6.0, 10.0, 27.0])\n\nx = forward_substitution(L, b)\nprint(f\"Solution: {x}\")\nprint(f\"Verification Lx: {L @ x}\")\nprint(f\"Original b: {b}\")",
    "hints": [
      "Start from the first equation and solve for x₁",
      "Use already computed values to solve subsequent equations",
      "For row i: x[i] = (b[i] - sum(L[i,j]*x[j] for j<i)) / L[i,i]"
    ],
    "solution": "import numpy as np\n\ndef forward_substitution(L, b):\n    \"\"\"\n    Solve Lx = b where L is lower triangular.\n\n    Parameters:\n    - L: n×n lower triangular matrix\n    - b: n×1 right-hand side vector\n\n    Returns:\n    - x: solution vector\n    \"\"\"\n    n = len(b)\n    x = np.zeros(n)\n\n    for i in range(n):\n        # Compute sum of known terms\n        sum_term = sum(L[i, j] * x[j] for j in range(i))\n\n        # Check for zero diagonal\n        if abs(L[i, i]) < 1e-15:\n            raise ValueError(f\"Zero diagonal element at position {i}\")\n\n        # Solve for x[i]\n        x[i] = (b[i] - sum_term) / L[i, i]\n\n    return x\n\n# Test case\nL = np.array([\n    [2.0, 0.0, 0.0],\n    [1.0, 3.0, 0.0],\n    [4.0, 2.0, 5.0]\n], dtype=float)\n\nb = np.array([6.0, 10.0, 27.0])\n\nx = forward_substitution(L, b)\nprint(f\"Solution: {x}\")\nprint(f\"Verification Lx: {L @ x}\")\nprint(f\"Original b: {b}\")\nprint(f\"Residual: {np.linalg.norm(L @ x - b):.2e}\")\n\n# Additional test\nL2 = np.array([[1.0, 0.0], [2.0, 1.0]])\nb2 = np.array([3.0, 8.0])\nx2 = forward_substitution(L2, b2)\nassert np.allclose(L2 @ x2, b2)\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "L=[[2,0,0],[1,3,0],[4,2,5]], b=[6,10,27]",
        "expectedOutput": "x = [3.0, 2.333..., 1.0]",
        "isHidden": false,
        "description": "Basic 3×3 lower triangular system"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-5-2",
    "subjectId": "math402",
    "topicId": "topic-5",
    "difficulty": 1,
    "title": "Back Substitution",
    "description": "Implement back substitution to solve an upper triangular system Ux = b. This completes the LU decomposition solver. Write a function that solves the system by working backwards from the last equation.",
    "starterCode": "import numpy as np\n\ndef back_substitution(U, b):\n    \"\"\"\n    Solve Ux = b where U is upper triangular.\n\n    Parameters:\n    - U: n×n upper triangular matrix\n    - b: n×1 right-hand side vector\n\n    Returns:\n    - x: solution vector\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case\nU = np.array([\n    [3.0, 2.0, 1.0],\n    [0.0, 4.0, 2.0],\n    [0.0, 0.0, 5.0]\n], dtype=float)\n\nb = np.array([10.0, 14.0, 15.0])\n\nx = back_substitution(U, b)\nprint(f\"Solution: {x}\")\nprint(f\"Verification Ux: {U @ x}\")\nprint(f\"Original b: {b}\")",
    "hints": [
      "Start from the last equation and solve for xₙ",
      "Use already computed values to solve previous equations",
      "Work backwards: for row i, x[i] = (b[i] - sum(U[i,j]*x[j] for j>i)) / U[i,i]"
    ],
    "solution": "import numpy as np\n\ndef back_substitution(U, b):\n    \"\"\"\n    Solve Ux = b where U is upper triangular.\n\n    Parameters:\n    - U: n×n upper triangular matrix\n    - b: n×1 right-hand side vector\n\n    Returns:\n    - x: solution vector\n    \"\"\"\n    n = len(b)\n    x = np.zeros(n)\n\n    # Work backwards from last equation\n    for i in range(n - 1, -1, -1):\n        # Compute sum of known terms\n        sum_term = sum(U[i, j] * x[j] for j in range(i + 1, n))\n\n        # Check for zero diagonal\n        if abs(U[i, i]) < 1e-15:\n            raise ValueError(f\"Zero diagonal element at position {i}\")\n\n        # Solve for x[i]\n        x[i] = (b[i] - sum_term) / U[i, i]\n\n    return x\n\n# Test case\nU = np.array([\n    [3.0, 2.0, 1.0],\n    [0.0, 4.0, 2.0],\n    [0.0, 0.0, 5.0]\n], dtype=float)\n\nb = np.array([10.0, 14.0, 15.0])\n\nx = back_substitution(U, b)\nprint(f\"Solution: {x}\")\nprint(f\"Verification Ux: {U @ x}\")\nprint(f\"Original b: {b}\")\nprint(f\"Residual: {np.linalg.norm(U @ x - b):.2e}\")\n\n# Additional test\nU2 = np.array([[2.0, 1.0], [0.0, 3.0]])\nb2 = np.array([5.0, 6.0])\nx2 = back_substitution(U2, b2)\nassert np.allclose(U2 @ x2, b2)\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "U=[[3,2,1],[0,4,2],[0,0,5]], b=[10,14,15]",
        "expectedOutput": "x = [1.0, 2.0, 3.0]",
        "isHidden": false,
        "description": "Basic 3×3 upper triangular system"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-5-3",
    "subjectId": "math402",
    "topicId": "topic-5",
    "difficulty": 2,
    "title": "Gaussian Elimination",
    "description": "Implement Gaussian elimination to convert a matrix to row echelon form. This is the foundation of many direct methods. Write a function that performs elimination without pivoting and returns the upper triangular matrix and the elimination record.",
    "starterCode": "import numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve Ax = b using Gaussian elimination.\n\n    Parameters:\n    - A: n×n coefficient matrix\n    - b: n×1 right-hand side vector\n\n    Returns:\n    - x: solution vector\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case\nA = np.array([\n    [2.0, 1.0, -1.0],\n    [-3.0, -1.0, 2.0],\n    [-2.0, 1.0, 2.0]\n], dtype=float)\n\nb = np.array([8.0, -11.0, -3.0])\n\nx = gaussian_elimination(A, b)\nprint(f\"Solution: {x}\")\nprint(f\"Verification Ax: {A @ x}\")\nprint(f\"Original b: {b}\")",
    "hints": [
      "Make copies of A and b to avoid modifying originals",
      "For each pivot row, eliminate below by subtracting multiples",
      "After elimination, use back substitution to solve",
      "Multiplier for row i, column k: m = A[i,k] / A[k,k]"
    ],
    "solution": "import numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve Ax = b using Gaussian elimination.\n\n    Parameters:\n    - A: n×n coefficient matrix\n    - b: n×1 right-hand side vector\n\n    Returns:\n    - x: solution vector\n    \"\"\"\n    # Make copies to avoid modifying inputs\n    A = A.astype(float).copy()\n    b = b.astype(float).copy()\n    n = len(b)\n\n    # Forward elimination\n    for k in range(n - 1):\n        # Check for zero pivot\n        if abs(A[k, k]) < 1e-15:\n            raise ValueError(f\"Zero pivot encountered at position {k}\")\n\n        # Eliminate below pivot\n        for i in range(k + 1, n):\n            # Compute multiplier\n            m = A[i, k] / A[k, k]\n\n            # Update row i\n            A[i, k:] -= m * A[k, k:]\n            b[i] -= m * b[k]\n\n    # Back substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        if abs(A[i, i]) < 1e-15:\n            raise ValueError(f\"Zero diagonal at position {i}\")\n        x[i] = (b[i] - np.dot(A[i, i+1:], x[i+1:])) / A[i, i]\n\n    return x\n\n# Test case\nA = np.array([\n    [2.0, 1.0, -1.0],\n    [-3.0, -1.0, 2.0],\n    [-2.0, 1.0, 2.0]\n], dtype=float)\n\nb = np.array([8.0, -11.0, -3.0])\n\nx = gaussian_elimination(A, b)\nprint(f\"Solution: {x}\")\nprint(f\"Verification Ax: {A @ x}\")\nprint(f\"Original b: {b}\")\nprint(f\"Residual: {np.linalg.norm(A @ x - b):.2e}\")\n\n# Additional test\nA2 = np.array([[3.0, 2.0], [1.0, 4.0]])\nb2 = np.array([7.0, 9.0])\nx2 = gaussian_elimination(A2, b2)\nassert np.allclose(A2 @ x2, b2)\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "A=[[2,1,-1],[-3,-1,2],[-2,1,2]], b=[8,-11,-3]",
        "expectedOutput": "x = [2.0, 3.0, -1.0]",
        "isHidden": false,
        "description": "Standard 3×3 linear system"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-5-4",
    "subjectId": "math402",
    "topicId": "topic-5",
    "difficulty": 2,
    "title": "LU Decomposition (Doolittle)",
    "description": "Implement LU decomposition using Doolittle algorithm (L has 1s on diagonal). Decompose A = LU where L is lower triangular with unit diagonal and U is upper triangular. This factorization enables efficient solution of multiple systems with the same A.",
    "starterCode": "import numpy as np\n\ndef lu_decompose(A):\n    \"\"\"\n    Compute LU decomposition of A using Doolittle algorithm.\n\n    Parameters:\n    - A: n×n matrix\n\n    Returns:\n    - L: lower triangular with unit diagonal\n    - U: upper triangular\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case\nA = np.array([\n    [4.0, 3.0, 0.0],\n    [3.0, 3.0, -1.0],\n    [0.0, -1.0, 1.0]\n], dtype=float)\n\nL, U = lu_decompose(A)\nprint(\"L =\")\nprint(L)\nprint(\"\\nU =\")\nprint(U)\nprint(\"\\nLU =\")\nprint(L @ U)\nprint(\"\\nOriginal A =\")\nprint(A)",
    "hints": [
      "L has 1s on diagonal, compute U row by row, then L column by column",
      "For U[i,j] where j >= i: U[i,j] = A[i,j] - sum(L[i,k]*U[k,j] for k<i)",
      "For L[i,j] where i > j: L[i,j] = (A[i,j] - sum(L[i,k]*U[k,j] for k<j)) / U[j,j]",
      "Process in order: row 0 of U, column 0 of L, row 1 of U, column 1 of L, etc."
    ],
    "solution": "import numpy as np\n\ndef lu_decompose(A):\n    \"\"\"\n    Compute LU decomposition of A using Doolittle algorithm.\n\n    Parameters:\n    - A: n×n matrix\n\n    Returns:\n    - L: lower triangular with unit diagonal\n    - U: upper triangular\n    \"\"\"\n    A = A.astype(float).copy()\n    n = A.shape[0]\n\n    L = np.eye(n)\n    U = np.zeros((n, n))\n\n    for i in range(n):\n        # Compute U[i, j] for j >= i\n        for j in range(i, n):\n            sum_term = sum(L[i, k] * U[k, j] for k in range(i))\n            U[i, j] = A[i, j] - sum_term\n\n        # Check for zero pivot\n        if abs(U[i, i]) < 1e-15:\n            raise ValueError(f\"Zero pivot at position {i}\")\n\n        # Compute L[j, i] for j > i\n        for j in range(i + 1, n):\n            sum_term = sum(L[j, k] * U[k, i] for k in range(i))\n            L[j, i] = (A[j, i] - sum_term) / U[i, i]\n\n    return L, U\n\ndef lu_solve(L, U, b):\n    \"\"\"Solve Ax = b using LU decomposition.\"\"\"\n    # Forward substitution: Ly = b\n    n = len(b)\n    y = np.zeros(n)\n    for i in range(n):\n        y[i] = b[i] - sum(L[i, j] * y[j] for j in range(i))\n\n    # Back substitution: Ux = y\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (y[i] - sum(U[i, j] * x[j] for j in range(i + 1, n))) / U[i, i]\n\n    return x\n\n# Test case\nA = np.array([\n    [4.0, 3.0, 0.0],\n    [3.0, 3.0, -1.0],\n    [0.0, -1.0, 1.0]\n], dtype=float)\n\nL, U = lu_decompose(A)\nprint(\"L =\")\nprint(L)\nprint(\"\\nU =\")\nprint(U)\nprint(\"\\nLU =\")\nprint(L @ U)\nprint(\"\\nOriginal A =\")\nprint(A)\nprint(f\"\\nDecomposition error: {np.linalg.norm(A - L @ U):.2e}\")\n\n# Test solving a system\nb = np.array([1.0, 2.0, 3.0])\nx = lu_solve(L, U, b)\nprint(f\"\\nSolution to Ax = b: {x}\")\nprint(f\"Verification Ax: {A @ x}\")\nprint(f\"Residual: {np.linalg.norm(A @ x - b):.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "A=[[4,3,0],[3,3,-1],[0,-1,1]]",
        "expectedOutput": "L and U such that LU = A, with L having unit diagonal",
        "isHidden": false,
        "description": "LU decomposition of symmetric indefinite matrix"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-5-5",
    "subjectId": "math402",
    "topicId": "topic-5",
    "difficulty": 3,
    "title": "Partial Pivoting",
    "description": "Implement Gaussian elimination with partial pivoting to improve numerical stability. Partial pivoting selects the largest available pivot at each step to minimize rounding errors. This is essential for practical linear system solvers.",
    "starterCode": "import numpy as np\n\ndef gaussian_elimination_pivot(A, b):\n    \"\"\"\n    Solve Ax = b using Gaussian elimination with partial pivoting.\n\n    Parameters:\n    - A: n×n coefficient matrix\n    - b: n×1 right-hand side vector\n\n    Returns:\n    - x: solution vector\n    - P: permutation matrix (optional)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case with small pivot\nA = np.array([\n    [1e-20, 1.0, 0.0],\n    [1.0, 2.0, 1.0],\n    [0.0, 1.0, 2.0]\n], dtype=float)\n\nb = np.array([1.0, 4.0, 3.0])\n\nx = gaussian_elimination_pivot(A, b)\nprint(f\"Solution with pivoting: {x}\")\nprint(f\"Residual: {np.linalg.norm(A @ x - b):.2e}\")",
    "hints": [
      "At each step k, find the row i >= k with largest |A[i,k]|",
      "Swap rows k and i in both A and b",
      "Continue with normal Gaussian elimination",
      "Keep track of permutations if needed"
    ],
    "solution": "import numpy as np\n\ndef gaussian_elimination_pivot(A, b):\n    \"\"\"\n    Solve Ax = b using Gaussian elimination with partial pivoting.\n\n    Parameters:\n    - A: n×n coefficient matrix\n    - b: n×1 right-hand side vector\n\n    Returns:\n    - x: solution vector\n    - P: permutation record\n    \"\"\"\n    A = A.astype(float).copy()\n    b = b.astype(float).copy()\n    n = len(b)\n\n    # Keep track of row swaps\n    perm = list(range(n))\n\n    # Forward elimination with pivoting\n    for k in range(n - 1):\n        # Find pivot: row with largest |A[i,k]| for i >= k\n        pivot_row = k\n        max_val = abs(A[k, k])\n\n        for i in range(k + 1, n):\n            if abs(A[i, k]) > max_val:\n                max_val = abs(A[i, k])\n                pivot_row = i\n\n        # Check for singular matrix\n        if max_val < 1e-15:\n            raise ValueError(f\"Matrix is singular or near-singular at column {k}\")\n\n        # Swap rows if needed\n        if pivot_row != k:\n            A[[k, pivot_row]] = A[[pivot_row, k]]\n            b[[k, pivot_row]] = b[[pivot_row, k]]\n            perm[k], perm[pivot_row] = perm[pivot_row], perm[k]\n\n        # Eliminate below pivot\n        for i in range(k + 1, n):\n            m = A[i, k] / A[k, k]\n            A[i, k:] -= m * A[k, k:]\n            b[i] -= m * b[k]\n\n    # Check final pivot\n    if abs(A[n-1, n-1]) < 1e-15:\n        raise ValueError(\"Matrix is singular\")\n\n    # Back substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (b[i] - np.dot(A[i, i+1:], x[i+1:])) / A[i, i]\n\n    return x\n\n# Test case with small pivot (ill-conditioned)\nprint(\"Test 1: Small pivot problem\\n\")\nA1 = np.array([\n    [1e-20, 1.0, 0.0],\n    [1.0, 2.0, 1.0],\n    [0.0, 1.0, 2.0]\n], dtype=float)\nb1 = np.array([1.0, 4.0, 3.0])\n\nx1 = gaussian_elimination_pivot(A1, b1)\nprint(f\"Solution with pivoting: {x1}\")\nprint(f\"Residual: {np.linalg.norm(A1 @ x1 - b1):.2e}\")\n\n# Compare with NumPy\nx_numpy = np.linalg.solve(A1, b1)\nprint(f\"NumPy solution: {x_numpy}\")\nprint(f\"Difference: {np.linalg.norm(x1 - x_numpy):.2e}\")\n\n# Test case 2: Regular matrix\nprint(\"\\n\" + \"=\"*60)\nprint(\"Test 2: Standard system\\n\")\nA2 = np.array([\n    [2.0, 1.0, -1.0],\n    [-3.0, -1.0, 2.0],\n    [-2.0, 1.0, 2.0]\n], dtype=float)\nb2 = np.array([8.0, -11.0, -3.0])\n\nx2 = gaussian_elimination_pivot(A2, b2)\nprint(f\"Solution: {x2}\")\nprint(f\"Residual: {np.linalg.norm(A2 @ x2 - b2):.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "A with small pivot element",
        "expectedOutput": "Accurate solution despite ill-conditioning",
        "isHidden": false,
        "description": "Test numerical stability with partial pivoting"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-5-6",
    "subjectId": "math402",
    "topicId": "topic-5",
    "difficulty": 3,
    "title": "LU with Partial Pivoting (PLU)",
    "description": "Implement LU decomposition with partial pivoting: PA = LU. This combines the efficiency of LU factorization with the numerical stability of pivoting. The permutation matrix P records row exchanges.",
    "starterCode": "import numpy as np\n\ndef plu_decompose(A):\n    \"\"\"\n    Compute PLU decomposition: PA = LU.\n\n    Parameters:\n    - A: n×n matrix\n\n    Returns:\n    - P: permutation matrix\n    - L: lower triangular with unit diagonal\n    - U: upper triangular\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case\nA = np.array([\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 10.0]\n], dtype=float)\n\nP, L, U = plu_decompose(A)\nprint(\"P =\")\nprint(P)\nprint(\"\\nL =\")\nprint(L)\nprint(\"\\nU =\")\nprint(U)\nprint(\"\\nPA =\")\nprint(P @ A)\nprint(\"\\nLU =\")\nprint(L @ U)",
    "hints": [
      "Similar to LU decomposition but with row swaps",
      "At step k, find pivot and swap rows in both A and L",
      "Keep track of permutations in P matrix",
      "Can use a permutation vector and convert to matrix at end"
    ],
    "solution": "import numpy as np\n\ndef plu_decompose(A):\n    \"\"\"\n    Compute PLU decomposition: PA = LU.\n\n    Parameters:\n    - A: n×n matrix\n\n    Returns:\n    - P: permutation matrix\n    - L: lower triangular with unit diagonal\n    - U: upper triangular\n    \"\"\"\n    A = A.astype(float).copy()\n    n = A.shape[0]\n\n    L = np.eye(n)\n    U = np.zeros((n, n))\n    P = np.eye(n)\n\n    for k in range(n):\n        # Find pivot\n        pivot_row = k\n        max_val = abs(A[k, k])\n        for i in range(k + 1, n):\n            if abs(A[i, k]) > max_val:\n                max_val = abs(A[i, k])\n                pivot_row = i\n\n        # Swap rows in A, P, and already-computed part of L\n        if pivot_row != k:\n            A[[k, pivot_row]] = A[[pivot_row, k]]\n            P[[k, pivot_row]] = P[[pivot_row, k]]\n            if k > 0:\n                L[[k, pivot_row], :k] = L[[pivot_row, k], :k]\n\n        # Check for singular matrix\n        if abs(A[k, k]) < 1e-15:\n            raise ValueError(f\"Matrix is singular at pivot {k}\")\n\n        # Store pivot in U\n        U[k, k] = A[k, k]\n\n        # Compute L[:,k] and U[k,:]\n        for i in range(k + 1, n):\n            L[i, k] = A[i, k] / U[k, k]\n\n        for j in range(k + 1, n):\n            U[k, j] = A[k, j]\n\n        # Update remaining submatrix\n        for i in range(k + 1, n):\n            for j in range(k + 1, n):\n                A[i, j] -= L[i, k] * U[k, j]\n\n    return P, L, U\n\ndef plu_solve(P, L, U, b):\n    \"\"\"Solve Ax = b using PLU decomposition.\"\"\"\n    # Permute b: solve PAx = Pb\n    pb = P @ b\n\n    # Forward substitution: Ly = Pb\n    n = len(b)\n    y = np.zeros(n)\n    for i in range(n):\n        y[i] = pb[i] - sum(L[i, j] * y[j] for j in range(i))\n\n    # Back substitution: Ux = y\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (y[i] - sum(U[i, j] * x[j] for j in range(i + 1, n))) / U[i, i]\n\n    return x\n\n# Test case\nA = np.array([\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 10.0]\n], dtype=float)\n\nP, L, U = plu_decompose(A)\nprint(\"P =\")\nprint(P)\nprint(\"\\nL =\")\nprint(L)\nprint(\"\\nU =\")\nprint(U)\nprint(\"\\nPA =\")\nprint(P @ A)\nprint(\"\\nLU =\")\nprint(L @ U)\nprint(f\"\\nDecomposition error: {np.linalg.norm(P @ A - L @ U):.2e}\")\n\n# Test solving\nb = np.array([1.0, 2.0, 3.0])\nx = plu_solve(P, L, U, b)\nprint(f\"\\nSolution to Ax = b: {x}\")\nprint(f\"Residual: {np.linalg.norm(A @ x - b):.2e}\")\n\n# Compare with NumPy\nP_np, L_np, U_np = scipy.linalg.lu(A)\nprint(f\"\\nComparison with SciPy PLU:\")\nprint(f\"P difference: {np.linalg.norm(P - P_np):.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "A=[[1,2,3],[4,5,6],[7,8,10]]",
        "expectedOutput": "P, L, U such that PA = LU",
        "isHidden": false,
        "description": "PLU decomposition with row pivoting"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-5-7",
    "subjectId": "math402",
    "topicId": "topic-5",
    "difficulty": 3,
    "title": "Cholesky Decomposition",
    "description": "Implement Cholesky decomposition for symmetric positive definite matrices: A = LL^T. This specialized factorization is more efficient than LU and guaranteed to be numerically stable for SPD matrices. Used extensively in optimization and statistics.",
    "starterCode": "import numpy as np\n\ndef cholesky_decompose(A):\n    \"\"\"\n    Compute Cholesky decomposition A = LL^T.\n\n    Parameters:\n    - A: n×n symmetric positive definite matrix\n\n    Returns:\n    - L: lower triangular matrix\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case: SPD matrix\nA = np.array([\n    [4.0, 12.0, -16.0],\n    [12.0, 37.0, -43.0],\n    [-16.0, -43.0, 98.0]\n], dtype=float)\n\nL = cholesky_decompose(A)\nprint(\"L =\")\nprint(L)\nprint(\"\\nL @ L^T =\")\nprint(L @ L.T)\nprint(\"\\nOriginal A =\")\nprint(A)",
    "hints": [
      "For SPD matrices, all pivots are positive",
      "L[i,i] = sqrt(A[i,i] - sum(L[i,k]^2 for k<i))",
      "L[j,i] = (A[j,i] - sum(L[j,k]*L[i,k] for k<i)) / L[i,i] for j > i",
      "Process column by column, computing diagonal then below-diagonal"
    ],
    "solution": "import numpy as np\n\ndef cholesky_decompose(A):\n    \"\"\"\n    Compute Cholesky decomposition A = LL^T.\n\n    Parameters:\n    - A: n×n symmetric positive definite matrix\n\n    Returns:\n    - L: lower triangular matrix\n    \"\"\"\n    # Verify symmetry\n    if not np.allclose(A, A.T):\n        raise ValueError(\"Matrix must be symmetric\")\n\n    A = A.astype(float).copy()\n    n = A.shape[0]\n    L = np.zeros((n, n))\n\n    for i in range(n):\n        # Compute diagonal element\n        sum_sq = sum(L[i, k]**2 for k in range(i))\n        diag_val = A[i, i] - sum_sq\n\n        if diag_val <= 0:\n            raise ValueError(f\"Matrix is not positive definite (negative diagonal at {i})\")\n\n        L[i, i] = np.sqrt(diag_val)\n\n        # Compute below-diagonal elements in column i\n        for j in range(i + 1, n):\n            sum_prod = sum(L[j, k] * L[i, k] for k in range(i))\n            L[j, i] = (A[j, i] - sum_prod) / L[i, i]\n\n    return L\n\ndef cholesky_solve(L, b):\n    \"\"\"Solve Ax = b using Cholesky decomposition A = LL^T.\"\"\"\n    n = len(b)\n\n    # Forward substitution: Ly = b\n    y = np.zeros(n)\n    for i in range(n):\n        y[i] = (b[i] - sum(L[i, j] * y[j] for j in range(i))) / L[i, i]\n\n    # Back substitution: L^T x = y\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (y[i] - sum(L[j, i] * x[j] for j in range(i + 1, n))) / L[i, i]\n\n    return x\n\n# Test case: SPD matrix\nA = np.array([\n    [4.0, 12.0, -16.0],\n    [12.0, 37.0, -43.0],\n    [-16.0, -43.0, 98.0]\n], dtype=float)\n\nL = cholesky_decompose(A)\nprint(\"L =\")\nprint(L)\nprint(\"\\nL @ L^T =\")\nprint(L @ L.T)\nprint(\"\\nOriginal A =\")\nprint(A)\nprint(f\"\\nDecomposition error: {np.linalg.norm(A - L @ L.T):.2e}\")\n\n# Test solving\nb = np.array([1.0, 2.0, 3.0])\nx = cholesky_solve(L, b)\nprint(f\"\\nSolution to Ax = b: {x}\")\nprint(f\"Residual: {np.linalg.norm(A @ x - b):.2e}\")\n\n# Compare with NumPy\nL_np = np.linalg.cholesky(A)\nprint(f\"\\nComparison with NumPy:\")\nprint(f\"L difference: {np.linalg.norm(L - L_np):.2e}\")\n\n# Test with simple SPD matrix\nA2 = np.array([[4.0, 2.0], [2.0, 3.0]])\nL2 = cholesky_decompose(A2)\nassert np.allclose(A2, L2 @ L2.T)\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "A=[[4,12,-16],[12,37,-43],[-16,-43,98]]",
        "expectedOutput": "L such that LL^T = A",
        "isHidden": false,
        "description": "Cholesky decomposition of SPD matrix"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-5-8",
    "subjectId": "math402",
    "topicId": "topic-5",
    "difficulty": 4,
    "title": "QR Decomposition (Gram-Schmidt)",
    "description": "Implement QR decomposition using Gram-Schmidt orthogonalization: A = QR where Q is orthogonal and R is upper triangular. This is fundamental for least squares problems and eigenvalue computation.",
    "starterCode": "import numpy as np\n\ndef qr_gram_schmidt(A):\n    \"\"\"\n    Compute QR decomposition using classical Gram-Schmidt.\n\n    Parameters:\n    - A: m×n matrix (m >= n)\n\n    Returns:\n    - Q: m×n orthogonal matrix\n    - R: n×n upper triangular matrix\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case\nA = np.array([\n    [1.0, 1.0, 0.0],\n    [1.0, 0.0, 1.0],\n    [0.0, 1.0, 1.0]\n], dtype=float)\n\nQ, R = qr_gram_schmidt(A)\nprint(\"Q =\")\nprint(Q)\nprint(\"\\nR =\")\nprint(R)\nprint(\"\\nQ @ R =\")\nprint(Q @ R)\nprint(\"\\nOriginal A =\")\nprint(A)\nprint(\"\\nQ^T @ Q (should be I) =\")\nprint(Q.T @ Q)",
    "hints": [
      "Process columns of A one at a time",
      "For each column, subtract projections onto previous orthogonal vectors",
      "Normalize to get orthonormal vectors",
      "R[i,j] = q_i^T @ a_j where q_i are orthonormal columns of Q"
    ],
    "solution": "import numpy as np\n\ndef qr_gram_schmidt(A):\n    \"\"\"\n    Compute QR decomposition using classical Gram-Schmidt.\n\n    Parameters:\n    - A: m×n matrix (m >= n)\n\n    Returns:\n    - Q: m×n orthogonal matrix\n    - R: n×n upper triangular matrix\n    \"\"\"\n    A = A.astype(float).copy()\n    m, n = A.shape\n\n    Q = np.zeros((m, n))\n    R = np.zeros((n, n))\n\n    for j in range(n):\n        # Start with column j of A\n        v = A[:, j].copy()\n\n        # Subtract projections onto previous Q columns\n        for i in range(j):\n            R[i, j] = np.dot(Q[:, i], A[:, j])\n            v -= R[i, j] * Q[:, i]\n\n        # Compute norm\n        R[j, j] = np.linalg.norm(v)\n\n        if R[j, j] < 1e-15:\n            raise ValueError(f\"Columns are linearly dependent at column {j}\")\n\n        # Normalize\n        Q[:, j] = v / R[j, j]\n\n    return Q, R\n\ndef qr_gram_schmidt_modified(A):\n    \"\"\"\n    Modified Gram-Schmidt (more numerically stable).\n    \"\"\"\n    A = A.astype(float).copy()\n    m, n = A.shape\n\n    Q = np.zeros((m, n))\n    R = np.zeros((n, n))\n\n    for j in range(n):\n        v = A[:, j].copy()\n\n        # Modified GS: update v as we go\n        for i in range(j):\n            R[i, j] = np.dot(Q[:, i], v)\n            v -= R[i, j] * Q[:, i]\n\n        R[j, j] = np.linalg.norm(v)\n\n        if R[j, j] < 1e-15:\n            raise ValueError(f\"Columns are linearly dependent at column {j}\")\n\n        Q[:, j] = v / R[j, j]\n\n    return Q, R\n\n# Test case\nA = np.array([\n    [1.0, 1.0, 0.0],\n    [1.0, 0.0, 1.0],\n    [0.0, 1.0, 1.0]\n], dtype=float)\n\nprint(\"Classical Gram-Schmidt:\")\nQ, R = qr_gram_schmidt(A)\nprint(\"\\nQ =\")\nprint(Q)\nprint(\"\\nR =\")\nprint(R)\nprint(\"\\nQ @ R =\")\nprint(Q @ R)\nprint(\"\\nOriginal A =\")\nprint(A)\nprint(f\"\\nDecomposition error: {np.linalg.norm(A - Q @ R):.2e}\")\nprint(\"\\nQ^T @ Q (should be I) =\")\nprint(Q.T @ Q)\nprint(f\"Orthogonality error: {np.linalg.norm(Q.T @ Q - np.eye(3)):.2e}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Modified Gram-Schmidt:\")\nQ_mod, R_mod = qr_gram_schmidt_modified(A)\nprint(f\"Decomposition error: {np.linalg.norm(A - Q_mod @ R_mod):.2e}\")\nprint(f\"Orthogonality error: {np.linalg.norm(Q_mod.T @ Q_mod - np.eye(3)):.2e}\")\n\n# Compare with NumPy\nQ_np, R_np = np.linalg.qr(A)\nprint(f\"\\nComparison with NumPy QR:\")\nprint(f\"Q difference: {np.linalg.norm(abs(Q) - abs(Q_np)):.2e}\")  # abs for sign ambiguity\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "A=[[1,1,0],[1,0,1],[0,1,1]]",
        "expectedOutput": "Q orthogonal, R upper triangular, QR = A",
        "isHidden": false,
        "description": "QR decomposition using Gram-Schmidt"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-5-9",
    "subjectId": "math402",
    "topicId": "topic-5",
    "difficulty": 4,
    "title": "QR with Householder Reflections",
    "description": "Implement QR decomposition using Householder reflections for better numerical stability. Householder transformations systematically zero out subcolumns, making this method more stable than Gram-Schmidt for ill-conditioned matrices.",
    "starterCode": "import numpy as np\n\ndef householder_qr(A):\n    \"\"\"\n    Compute QR decomposition using Householder reflections.\n\n    Parameters:\n    - A: m×n matrix (m >= n)\n\n    Returns:\n    - Q: m×m orthogonal matrix\n    - R: m×n upper triangular matrix\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case\nA = np.array([\n    [12.0, -51.0, 4.0],\n    [6.0, 167.0, -68.0],\n    [-4.0, 24.0, -41.0]\n], dtype=float)\n\nQ, R = householder_qr(A)\nprint(\"Q =\")\nprint(Q)\nprint(\"\\nR =\")\nprint(R)\nprint(\"\\nQ @ R =\")\nprint(Q @ R)",
    "hints": [
      "For column k, construct Householder vector v to zero elements below diagonal",
      "v = x + sign(x[0])*||x||*e_1 where x is the subcolumn",
      "Householder matrix: H = I - 2vv^T/(v^Tv)",
      "Apply H to remaining submatrix",
      "Q is product of all Householder matrices"
    ],
    "solution": "import numpy as np\n\ndef householder_vector(x):\n    \"\"\"\n    Compute Householder vector to reflect x to ||x||*e_1.\n\n    Returns v such that H = I - 2vv^T/||v||^2 zeros out x[1:].\n    \"\"\"\n    x = x.copy()\n    n = len(x)\n\n    # Compute norm with sign of x[0] for numerical stability\n    sigma = np.sign(x[0]) if x[0] != 0 else 1\n    norm_x = np.linalg.norm(x)\n\n    # Construct v\n    v = x.copy()\n    v[0] += sigma * norm_x\n\n    return v\n\ndef householder_qr(A):\n    \"\"\"\n    Compute QR decomposition using Householder reflections.\n\n    Parameters:\n    - A: m×n matrix (m >= n)\n\n    Returns:\n    - Q: m×m orthogonal matrix\n    - R: m×n upper triangular matrix\n    \"\"\"\n    A = A.astype(float).copy()\n    m, n = A.shape\n\n    Q = np.eye(m)\n    R = A.copy()\n\n    for k in range(n):\n        # Extract subcolumn\n        x = R[k:, k]\n\n        # Compute Householder vector\n        v = householder_vector(x)\n        v_norm_sq = np.dot(v, v)\n\n        if v_norm_sq < 1e-15:\n            continue\n\n        # Apply Householder transformation to submatrix\n        # H @ R[k:, k:] = R[k:, k:] - 2v(v^T @ R[k:, k:]) / ||v||^2\n        R[k:, k:] -= (2.0 / v_norm_sq) * np.outer(v, np.dot(v, R[k:, k:]))\n\n        # Build full Householder matrix for Q\n        H = np.eye(m)\n        H[k:, k:] -= (2.0 / v_norm_sq) * np.outer(v, v)\n        Q = Q @ H\n\n    return Q, R\n\n# Test case\nA = np.array([\n    [12.0, -51.0, 4.0],\n    [6.0, 167.0, -68.0],\n    [-4.0, 24.0, -41.0]\n], dtype=float)\n\nQ, R = householder_qr(A)\nprint(\"Q =\")\nprint(Q)\nprint(\"\\nR =\")\nprint(R[:3, :])  # Just the upper part\nprint(\"\\nQ @ R =\")\nprint(Q @ R)\nprint(\"\\nOriginal A =\")\nprint(A)\nprint(f\"\\nDecomposition error: {np.linalg.norm(A - Q @ R):.2e}\")\nprint(\"\\nQ^T @ Q (should be I) =\")\nprint(Q.T @ Q)\nprint(f\"Orthogonality error: {np.linalg.norm(Q.T @ Q - np.eye(3)):.2e}\")\n\n# Compare with NumPy\nQ_np, R_np = np.linalg.qr(A)\nprint(f\"\\nComparison with NumPy:\")\nprint(f\"R difference: {np.linalg.norm(R[:3, :] - R_np):.2e}\")\n\n# Test with ill-conditioned matrix\nprint(\"\\n\" + \"=\"*60)\nprint(\"Test with ill-conditioned matrix:\\n\")\nA_ill = np.array([[1.0, 1.0], [1.0, 1.0 + 1e-10], [1.0, 1.0 - 1e-10]])\nQ_ill, R_ill = householder_qr(A_ill)\nprint(f\"Decomposition error: {np.linalg.norm(A_ill - Q_ill @ R_ill):.2e}\")\nprint(f\"Orthogonality error: {np.linalg.norm(Q_ill.T @ Q_ill - np.eye(3)):.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "A=[[12,-51,4],[6,167,-68],[-4,24,-41]]",
        "expectedOutput": "Stable QR decomposition",
        "isHidden": false,
        "description": "Householder QR for numerical stability"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-5-10",
    "subjectId": "math402",
    "topicId": "topic-5",
    "difficulty": 4,
    "title": "Matrix Condition Number",
    "description": "Implement condition number computation and demonstrate how it affects solution accuracy. The condition number κ(A) = ||A|| ||A^(-1)|| measures how errors in b propagate to errors in x. High condition numbers indicate ill-conditioned systems.",
    "starterCode": "import numpy as np\n\ndef condition_number(A, p=2):\n    \"\"\"\n    Compute condition number of matrix A.\n\n    Parameters:\n    - A: n×n matrix\n    - p: norm type (1, 2, or np.inf)\n\n    Returns:\n    - cond: condition number κ_p(A)\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\ndef analyze_conditioning(A, b):\n    \"\"\"\n    Analyze how condition number affects solution accuracy.\n\n    Parameters:\n    - A: coefficient matrix\n    - b: right-hand side\n\n    Returns:\n    - Analysis results\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test with well-conditioned matrix\nA_good = np.array([[4.0, 1.0], [1.0, 3.0]])\ncond = condition_number(A_good)\nprint(f\"Condition number: {cond:.2f}\")",
    "hints": [
      "κ(A) = ||A|| * ||A^(-1)||",
      "For 2-norm: κ_2(A) = σ_max / σ_min (ratio of largest to smallest singular value)",
      "Large κ means small changes in b cause large changes in x",
      "Rule of thumb: lose log₁₀(κ) digits of accuracy"
    ],
    "solution": "import numpy as np\n\ndef condition_number(A, p=2):\n    \"\"\"\n    Compute condition number of matrix A.\n\n    Parameters:\n    - A: n×n matrix\n    - p: norm type (1, 2, or np.inf)\n\n    Returns:\n    - cond: condition number κ_p(A)\n    \"\"\"\n    if p == 2:\n        # Use SVD for 2-norm condition number\n        singular_values = np.linalg.svd(A, compute_uv=False)\n        if singular_values[-1] < 1e-15:\n            return float('inf')\n        return singular_values[0] / singular_values[-1]\n    else:\n        # General case: κ = ||A|| * ||A^(-1)||\n        norm_A = np.linalg.norm(A, ord=p)\n        norm_A_inv = np.linalg.norm(np.linalg.inv(A), ord=p)\n        return norm_A * norm_A_inv\n\ndef analyze_conditioning(A, b):\n    \"\"\"\n    Analyze how condition number affects solution accuracy.\n\n    Parameters:\n    - A: coefficient matrix\n    - b: right-hand side\n\n    Returns:\n    - Dictionary with analysis results\n    \"\"\"\n    # Compute condition number\n    cond = condition_number(A)\n\n    # Solve exactly\n    x_exact = np.linalg.solve(A, b)\n\n    # Perturb b slightly\n    perturbation = 1e-10 * np.random.randn(len(b))\n    b_perturbed = b + perturbation\n    x_perturbed = np.linalg.solve(A, b_perturbed)\n\n    # Compute relative errors\n    rel_error_b = np.linalg.norm(perturbation) / np.linalg.norm(b)\n    rel_error_x = np.linalg.norm(x_perturbed - x_exact) / np.linalg.norm(x_exact)\n\n    # Error amplification\n    amplification = rel_error_x / rel_error_b\n\n    return {\n        'condition_number': cond,\n        'rel_error_b': rel_error_b,\n        'rel_error_x': rel_error_x,\n        'amplification': amplification,\n        'expected_loss_digits': np.log10(cond)\n    }\n\n# Test 1: Well-conditioned matrix\nprint(\"Test 1: Well-conditioned matrix\\n\")\nA_good = np.array([[4.0, 1.0], [1.0, 3.0]])\nb_good = np.array([1.0, 2.0])\n\ncond_good = condition_number(A_good)\nprint(f\"Matrix A_good:\")\nprint(A_good)\nprint(f\"\\nCondition number: {cond_good:.2f}\")\nprint(f\"Expected digit loss: {np.log10(cond_good):.2f}\")\n\nanalysis_good = analyze_conditioning(A_good, b_good)\nprint(f\"\\nAnalysis:\")\nprint(f\"  Relative error in b: {analysis_good['rel_error_b']:.2e}\")\nprint(f\"  Relative error in x: {analysis_good['rel_error_x']:.2e}\")\nprint(f\"  Error amplification: {analysis_good['amplification']:.2f}\")\nprint(f\"  Upper bound (κ): {cond_good:.2f}\")\n\n# Test 2: Ill-conditioned matrix (Hilbert matrix)\nprint(\"\\n\" + \"=\"*60)\nprint(\"Test 2: Ill-conditioned Hilbert matrix\\n\")\n\ndef hilbert_matrix(n):\n    \"\"\"Generate n×n Hilbert matrix (notoriously ill-conditioned).\"\"\"\n    H = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            H[i, j] = 1.0 / (i + j + 1)\n    return H\n\nA_bad = hilbert_matrix(4)\nb_bad = np.ones(4)\n\ncond_bad = condition_number(A_bad)\nprint(f\"4×4 Hilbert matrix\")\nprint(f\"Condition number: {cond_bad:.2e}\")\nprint(f\"Expected digit loss: {np.log10(cond_bad):.1f}\")\n\nanalysis_bad = analyze_conditioning(A_bad, b_bad)\nprint(f\"\\nAnalysis:\")\nprint(f\"  Relative error in b: {analysis_bad['rel_error_b']:.2e}\")\nprint(f\"  Relative error in x: {analysis_bad['rel_error_x']:.2e}\")\nprint(f\"  Error amplification: {analysis_bad['amplification']:.2e}\")\nprint(f\"  Upper bound (κ): {cond_bad:.2e}\")\n\n# Test 3: Nearly singular matrix\nprint(\"\\n\" + \"=\"*60)\nprint(\"Test 3: Nearly singular matrix\\n\")\n\nA_singular = np.array([[1.0, 2.0], [1.0, 2.0 + 1e-10]])\ncond_singular = condition_number(A_singular)\nprint(f\"Condition number: {cond_singular:.2e}\")\nprint(\"Matrix is nearly singular - extremely ill-conditioned!\")\n\n# Compare with NumPy\nprint(f\"\\nNumPy condition number: {np.linalg.cond(A_bad):.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "Well-conditioned vs Hilbert matrix",
        "expectedOutput": "Demonstrates error amplification proportional to κ(A)",
        "isHidden": false,
        "description": "Condition number analysis"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-5-11",
    "subjectId": "math402",
    "topicId": "topic-5",
    "difficulty": 4,
    "title": "Matrix Inversion via LU",
    "description": "Implement matrix inversion using LU decomposition. Computing A^(-1) by solving AX = I column by column. This is more efficient and stable than direct inversion formulas.",
    "starterCode": "import numpy as np\n\ndef invert_matrix_lu(A):\n    \"\"\"\n    Compute matrix inverse using LU decomposition.\n\n    Parameters:\n    - A: n×n invertible matrix\n\n    Returns:\n    - A_inv: inverse of A\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case\nA = np.array([\n    [4.0, 7.0],\n    [2.0, 6.0]\n], dtype=float)\n\nA_inv = invert_matrix_lu(A)\nprint(\"A =\")\nprint(A)\nprint(\"\\nA^(-1) =\")\nprint(A_inv)\nprint(\"\\nA @ A^(-1) =\")\nprint(A @ A_inv)",
    "hints": [
      "Solve AX = I column by column",
      "Each column of X solves Ax_i = e_i",
      "Use LU decomposition once, then solve n systems",
      "More efficient than computing A^(-1) directly"
    ],
    "solution": "import numpy as np\nfrom scipy.linalg import lu_factor, lu_solve\n\ndef plu_decompose_compact(A):\n    \"\"\"Compute PLU with compact storage.\"\"\"\n    A = A.astype(float).copy()\n    n = A.shape[0]\n    perm = list(range(n))\n\n    for k in range(n - 1):\n        # Find pivot\n        pivot_row = k + np.argmax(np.abs(A[k:, k]))\n\n        # Swap\n        if pivot_row != k:\n            A[[k, pivot_row]] = A[[pivot_row, k]]\n            perm[k], perm[pivot_row] = perm[pivot_row], perm[k]\n\n        if abs(A[k, k]) < 1e-15:\n            raise ValueError(\"Matrix is singular\")\n\n        # Eliminate\n        A[k+1:, k] /= A[k, k]\n        A[k+1:, k+1:] -= np.outer(A[k+1:, k], A[k, k+1:])\n\n    return A, perm\n\ndef solve_with_plu(LU, perm, b):\n    \"\"\"Solve using compact PLU.\"\"\"\n    n = len(b)\n    b = b[perm].copy()\n\n    # Forward substitution\n    for i in range(n):\n        b[i] -= np.dot(LU[i, :i], b[:i])\n\n    # Back substitution\n    for i in range(n - 1, -1, -1):\n        b[i] = (b[i] - np.dot(LU[i, i+1:], b[i+1:])) / LU[i, i]\n\n    return b\n\ndef invert_matrix_lu(A):\n    \"\"\"\n    Compute matrix inverse using LU decomposition.\n\n    Parameters:\n    - A: n×n invertible matrix\n\n    Returns:\n    - A_inv: inverse of A\n    \"\"\"\n    n = A.shape[0]\n\n    # Compute PLU decomposition\n    LU, perm = plu_decompose_compact(A)\n\n    # Solve AX = I column by column\n    A_inv = np.zeros((n, n))\n    I = np.eye(n)\n\n    for i in range(n):\n        A_inv[:, i] = solve_with_plu(LU, perm, I[:, i])\n\n    return A_inv\n\ndef compute_determinant_lu(A):\n    \"\"\"Compute determinant using LU decomposition.\"\"\"\n    LU, perm = plu_decompose_compact(A)\n\n    # det(A) = det(P) * det(L) * det(U)\n    # det(L) = 1 (unit diagonal)\n    # det(U) = product of diagonal\n    # det(P) = (-1)^(number of swaps)\n\n    det_U = np.prod(np.diag(LU))\n\n    # Count permutation parity\n    n = len(perm)\n    swaps = 0\n    visited = [False] * n\n    for i in range(n):\n        if not visited[i]:\n            j = i\n            cycle_length = 0\n            while not visited[j]:\n                visited[j] = True\n                j = perm[j]\n                cycle_length += 1\n            swaps += cycle_length - 1\n\n    det_P = (-1) ** swaps\n\n    return det_P * det_U\n\n# Test case\nA = np.array([\n    [4.0, 7.0],\n    [2.0, 6.0]\n], dtype=float)\n\nA_inv = invert_matrix_lu(A)\nprint(\"A =\")\nprint(A)\nprint(\"\\nA^(-1) =\")\nprint(A_inv)\nprint(\"\\nA @ A^(-1) =\")\nprint(A @ A_inv)\nprint(f\"\\nError from identity: {np.linalg.norm(A @ A_inv - np.eye(2)):.2e}\")\n\n# Compare with NumPy\nA_inv_np = np.linalg.inv(A)\nprint(f\"Difference from NumPy: {np.linalg.norm(A_inv - A_inv_np):.2e}\")\n\n# Test determinant\nprint(\"\\n\" + \"=\"*60)\nprint(\"Determinant computation:\\n\")\ndet_lu = compute_determinant_lu(A)\ndet_np = np.linalg.det(A)\nprint(f\"det(A) via LU: {det_lu:.6f}\")\nprint(f\"NumPy det(A): {det_np:.6f}\")\nprint(f\"Difference: {abs(det_lu - det_np):.2e}\")\n\n# Test with 3×3\nA3 = np.array([[1.0, 2.0, 3.0], [0.0, 1.0, 4.0], [5.0, 6.0, 0.0]])\nA3_inv = invert_matrix_lu(A3)\nprint(f\"\\n3×3 inversion error: {np.linalg.norm(A3 @ A3_inv - np.eye(3)):.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "A=[[4,7],[2,6]]",
        "expectedOutput": "A^(-1) such that A @ A^(-1) = I",
        "isHidden": false,
        "description": "Matrix inversion using LU decomposition"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-5-12",
    "subjectId": "math402",
    "topicId": "topic-5",
    "difficulty": 3,
    "title": "Tridiagonal Systems",
    "description": "Implement the Thomas algorithm for efficiently solving tridiagonal systems. Tridiagonal matrices arise in spline interpolation, finite differences, and many other applications. The Thomas algorithm is a specialized, highly efficient O(n) method.",
    "starterCode": "import numpy as np\n\ndef thomas_algorithm(a, b, c, d):\n    \"\"\"\n    Solve tridiagonal system using Thomas algorithm.\n\n    The system is:\n    b[0]*x[0] + c[0]*x[1] = d[0]\n    a[1]*x[0] + b[1]*x[1] + c[1]*x[2] = d[1]\n    ...\n    a[n-1]*x[n-2] + b[n-1]*x[n-1] = d[n-1]\n\n    Parameters:\n    - a: subdiagonal (length n-1)\n    - b: diagonal (length n)\n    - c: superdiagonal (length n-1)\n    - d: right-hand side (length n)\n\n    Returns:\n    - x: solution vector\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case: tridiagonal system\nn = 5\na = np.array([1.0, 1.0, 1.0, 1.0])  # subdiagonal\nb = np.array([4.0, 4.0, 4.0, 4.0, 4.0])  # diagonal\nc = np.array([1.0, 1.0, 1.0, 1.0])  # superdiagonal\nd = np.array([6.0, 8.0, 8.0, 8.0, 6.0])  # RHS\n\nx = thomas_algorithm(a, b, c, d)\nprint(f\"Solution: {x}\")",
    "hints": [
      "Forward elimination: modify c and d going forward",
      "c'[i] = c[i] / (b[i] - a[i]*c'[i-1])",
      "d'[i] = (d[i] - a[i]*d'[i-1]) / (b[i] - a[i]*c'[i-1])",
      "Back substitution: x[n-1] = d'[n-1], x[i] = d'[i] - c'[i]*x[i+1]"
    ],
    "solution": "import numpy as np\n\ndef thomas_algorithm(a, b, c, d):\n    \"\"\"\n    Solve tridiagonal system using Thomas algorithm.\n\n    The system is:\n    b[0]*x[0] + c[0]*x[1] = d[0]\n    a[1]*x[0] + b[1]*x[1] + c[1]*x[2] = d[1]\n    ...\n    a[n-1]*x[n-2] + b[n-1]*x[n-1] = d[n-1]\n\n    Parameters:\n    - a: subdiagonal (length n-1)\n    - b: diagonal (length n)\n    - c: superdiagonal (length n-1)\n    - d: right-hand side (length n)\n\n    Returns:\n    - x: solution vector\n    \"\"\"\n    n = len(b)\n\n    # Make copies to avoid modifying inputs\n    c = c.copy()\n    d = d.copy()\n\n    # Forward elimination\n    for i in range(1, n):\n        if abs(b[i-1]) < 1e-15:\n            raise ValueError(f\"Zero pivot at row {i-1}\")\n\n        m = a[i-1] / b[i-1]\n        b[i] = b[i] - m * c[i-1]\n        d[i] = d[i] - m * d[i-1]\n\n    # Check final pivot\n    if abs(b[n-1]) < 1e-15:\n        raise ValueError(\"Matrix is singular\")\n\n    # Back substitution\n    x = np.zeros(n)\n    x[n-1] = d[n-1] / b[n-1]\n\n    for i in range(n-2, -1, -1):\n        x[i] = (d[i] - c[i] * x[i+1]) / b[i]\n\n    return x\n\ndef build_tridiagonal_matrix(a, b, c):\n    \"\"\"Build full matrix from diagonals for verification.\"\"\"\n    n = len(b)\n    A = np.diag(b) + np.diag(a, -1) + np.diag(c, 1)\n    return A\n\n# Test case: tridiagonal system\nn = 5\na = np.array([1.0, 1.0, 1.0, 1.0])  # subdiagonal\nb = np.array([4.0, 4.0, 4.0, 4.0, 4.0])  # diagonal\nc = np.array([1.0, 1.0, 1.0, 1.0])  # superdiagonal\nd = np.array([6.0, 8.0, 8.0, 8.0, 6.0])  # RHS\n\nx = thomas_algorithm(a, b, c, d)\nprint(f\"Solution: {x}\")\n\n# Verify\nA = build_tridiagonal_matrix(a, b, c)\nprint(f\"\\nVerification Ax:\")\nprint(A @ x)\nprint(f\"Original d:\")\nprint(d)\nprint(f\"Residual: {np.linalg.norm(A @ x - d):.2e}\")\n\n# Compare with NumPy\nx_np = np.linalg.solve(A, d)\nprint(f\"\\nDifference from NumPy: {np.linalg.norm(x - x_np):.2e}\")\n\n# Test case 2: larger system\nprint(\"\\n\" + \"=\"*60)\nprint(\"Test with larger system (n=100):\\n\")\nn = 100\na_large = np.ones(n-1)\nb_large = 3.0 * np.ones(n)\nc_large = np.ones(n-1)\nd_large = np.random.randn(n)\n\nimport time\nstart = time.time()\nx_thomas = thomas_algorithm(a_large, b_large, c_large, d_large)\ntime_thomas = time.time() - start\n\nA_large = build_tridiagonal_matrix(a_large, b_large, c_large)\nstart = time.time()\nx_numpy = np.linalg.solve(A_large, d_large)\ntime_numpy = time.time() - start\n\nprint(f\"Thomas algorithm time: {time_thomas*1000:.3f} ms\")\nprint(f\"NumPy solve time: {time_numpy*1000:.3f} ms\")\nprint(f\"Speedup: {time_numpy/time_thomas:.1f}x\")\nprint(f\"Solution difference: {np.linalg.norm(x_thomas - x_numpy):.2e}\")\nprint(f\"Residual: {np.linalg.norm(A_large @ x_thomas - d_large):.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "Tridiagonal system with diagonals a, b, c",
        "expectedOutput": "O(n) solution matching full solver",
        "isHidden": false,
        "description": "Efficient Thomas algorithm for tridiagonal systems"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-5-13",
    "subjectId": "math402",
    "topicId": "topic-5",
    "difficulty": 5,
    "title": "Iterative Refinement",
    "description": "Implement iterative refinement to improve solution accuracy. After solving Ax = b to get x₀, compute residual r = b - Ax₀, solve A(δx) = r, and update x₁ = x₀ + δx. This can recover accuracy lost to rounding errors.",
    "starterCode": "import numpy as np\n\ndef iterative_refinement(A, b, max_iter=5):\n    \"\"\"\n    Solve Ax = b with iterative refinement for improved accuracy.\n\n    Parameters:\n    - A: n×n coefficient matrix\n    - b: n×1 right-hand side\n    - max_iter: maximum refinement iterations\n\n    Returns:\n    - x: refined solution\n    - residuals: list of residual norms\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test with ill-conditioned system\nA = np.array([\n    [1.0, 2.0],\n    [1.000001, 2.0]\n], dtype=float)\nb = np.array([3.0, 3.000001])\n\nx, residuals = iterative_refinement(A, b)\nprint(f\"Solution: {x}\")\nprint(f\"Residuals: {residuals}\")",
    "hints": [
      "Start with initial solution x₀ from standard solver",
      "Compute residual r = b - Ax in higher precision if possible",
      "Solve Aδ = r for correction δ",
      "Update x = x + δ and repeat",
      "Stop when residual stops decreasing"
    ],
    "solution": "import numpy as np\nfrom scipy.linalg import lu_factor, lu_solve\n\ndef iterative_refinement(A, b, max_iter=5, tol=1e-14):\n    \"\"\"\n    Solve Ax = b with iterative refinement for improved accuracy.\n\n    Parameters:\n    - A: n×n coefficient matrix\n    - b: n×1 right-hand side\n    - max_iter: maximum refinement iterations\n    - tol: convergence tolerance\n\n    Returns:\n    - x: refined solution\n    - residuals: list of residual norms\n    \"\"\"\n    # Compute LU factorization once\n    lu, piv = lu_factor(A)\n\n    # Initial solution\n    x = lu_solve((lu, piv), b)\n\n    residuals = []\n\n    for iteration in range(max_iter):\n        # Compute residual in higher precision if possible\n        # r = b - Ax (use np.float128 if available, else float64)\n        try:\n            r = b.astype(np.float128) - (A.astype(np.float128) @ x.astype(np.float128))\n            r = r.astype(np.float64)\n        except:\n            r = b - A @ x\n\n        residual_norm = np.linalg.norm(r)\n        residuals.append(residual_norm)\n\n        # Check convergence\n        if residual_norm < tol:\n            break\n\n        # Solve for correction: A(δx) = r\n        delta = lu_solve((lu, piv), r)\n\n        # Update solution\n        x = x + delta\n\n    return x, residuals\n\ndef compare_with_without_refinement(A, b):\n    \"\"\"Compare standard solve with iterative refinement.\"\"\"\n    # Standard solve\n    x_standard = np.linalg.solve(A, b)\n    residual_standard = np.linalg.norm(b - A @ x_standard)\n\n    # With refinement\n    x_refined, residuals = iterative_refinement(A, b)\n    residual_refined = np.linalg.norm(b - A @ x_refined)\n\n    return {\n        'x_standard': x_standard,\n        'x_refined': x_refined,\n        'residual_standard': residual_standard,\n        'residual_refined': residual_refined,\n        'refinement_history': residuals\n    }\n\n# Test 1: Ill-conditioned system\nprint(\"Test 1: Ill-conditioned system\\n\")\nA1 = np.array([\n    [1.0, 2.0],\n    [1.000001, 2.0]\n], dtype=float)\nb1 = np.array([3.0, 3.000001])\n\nresults1 = compare_with_without_refinement(A1, b1)\n\nprint(\"Standard solve:\")\nprint(f\"  Solution: {results1['x_standard']}\")\nprint(f\"  Residual: {results1['residual_standard']:.2e}\")\n\nprint(\"\\nWith iterative refinement:\")\nprint(f\"  Solution: {results1['x_refined']}\")\nprint(f\"  Residual: {results1['residual_refined']:.2e}\")\nprint(f\"  Refinement history: {[f'{r:.2e}' for r in results1['refinement_history']]}\")\nprint(f\"  Improvement: {results1['residual_standard'] / results1['residual_refined']:.1f}x\")\n\n# Test 2: Hilbert matrix (very ill-conditioned)\nprint(\"\\n\" + \"=\"*60)\nprint(\"Test 2: Hilbert matrix (n=6)\\n\")\n\ndef hilbert_matrix(n):\n    H = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            H[i, j] = 1.0 / (i + j + 1)\n    return H\n\nA2 = hilbert_matrix(6)\nx_true = np.ones(6)\nb2 = A2 @ x_true\n\nresults2 = compare_with_without_refinement(A2, b2)\n\nprint(f\"Condition number: {np.linalg.cond(A2):.2e}\")\nprint(f\"True solution: {x_true}\")\n\nprint(\"\\nStandard solve:\")\nprint(f\"  Solution: {results2['x_standard']}\")\nprint(f\"  Error: {np.linalg.norm(results2['x_standard'] - x_true):.2e}\")\nprint(f\"  Residual: {results2['residual_standard']:.2e}\")\n\nprint(\"\\nWith iterative refinement:\")\nprint(f\"  Solution: {results2['x_refined']}\")\nprint(f\"  Error: {np.linalg.norm(results2['x_refined'] - x_true):.2e}\")\nprint(f\"  Residual: {results2['residual_refined']:.2e}\")\nprint(f\"  Refinement history: {[f'{r:.2e}' for r in results2['refinement_history']]}\")\n\nerror_improvement = (np.linalg.norm(results2['x_standard'] - x_true) /\n                     np.linalg.norm(results2['x_refined'] - x_true))\nprint(f\"  Error improvement: {error_improvement:.1f}x\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "Ill-conditioned system",
        "expectedOutput": "Progressively decreasing residuals, improved accuracy",
        "isHidden": false,
        "description": "Iterative refinement for accuracy improvement"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-5-14",
    "subjectId": "math402",
    "topicId": "topic-5",
    "difficulty": 4,
    "title": "Solving Multiple Right-Hand Sides",
    "description": "Efficiently solve AX = B where B has multiple columns. Factor A once, then solve for each column of B. This is essential for matrix inversion and many applications.",
    "starterCode": "import numpy as np\n\ndef solve_multiple_rhs(A, B):\n    \"\"\"\n    Solve AX = B efficiently using single factorization.\n\n    Parameters:\n    - A: n×n coefficient matrix\n    - B: n×m right-hand side matrix (m systems)\n\n    Returns:\n    - X: n×m solution matrix\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case\nA = np.array([\n    [2.0, 1.0, 0.0],\n    [1.0, 2.0, 1.0],\n    [0.0, 1.0, 2.0]\n], dtype=float)\n\nB = np.array([\n    [1.0, 0.0, 1.0],\n    [2.0, 1.0, 0.0],\n    [3.0, 2.0, 1.0]\n], dtype=float)\n\nX = solve_multiple_rhs(A, B)\nprint(\"Solution X:\")\nprint(X)",
    "hints": [
      "Factor A = LU once",
      "For each column b_i of B, solve Lc_i = b_i then Ux_i = c_i",
      "More efficient than solving m separate systems",
      "Can also work with entire B matrix at once"
    ],
    "solution": "import numpy as np\nfrom scipy.linalg import lu_factor, lu_solve\nimport time\n\ndef solve_multiple_rhs(A, B):\n    \"\"\"\n    Solve AX = B efficiently using single factorization.\n\n    Parameters:\n    - A: n×n coefficient matrix\n    - B: n×m right-hand side matrix (m systems)\n\n    Returns:\n    - X: n×m solution matrix\n    \"\"\"\n    n, m = B.shape\n\n    # Factor A once\n    lu, piv = lu_factor(A)\n\n    # Solve for each column\n    X = np.zeros((n, m))\n    for i in range(m):\n        X[:, i] = lu_solve((lu, piv), B[:, i])\n\n    return X\n\ndef solve_multiple_rhs_matrix(A, B):\n    \"\"\"\n    Alternative: solve with entire B matrix at once.\n    More efficient for large m.\n    \"\"\"\n    from scipy.linalg import lu_factor, lu_solve\n\n    lu, piv = lu_factor(A)\n\n    # Solve AX = B using the factorization\n    # This requires working with B as a whole\n    n, m = B.shape\n    X = np.zeros((n, m))\n\n    # Forward substitution with permutation for all columns\n    P_B = B[piv, :]\n\n    # Solve LY = P_B\n    Y = np.zeros((n, m))\n    for i in range(n):\n        Y[i, :] = P_B[i, :] - lu[i, :i] @ Y[:i, :]\n\n    # Solve UX = Y\n    X = np.zeros((n, m))\n    for i in range(n - 1, -1, -1):\n        X[i, :] = (Y[i, :] - lu[i, i+1:] @ X[i+1:, :]) / lu[i, i]\n\n    return X\n\n# Test case\nA = np.array([\n    [2.0, 1.0, 0.0],\n    [1.0, 2.0, 1.0],\n    [0.0, 1.0, 2.0]\n], dtype=float)\n\nB = np.array([\n    [1.0, 0.0, 1.0],\n    [2.0, 1.0, 0.0],\n    [3.0, 2.0, 1.0]\n], dtype=float)\n\nX = solve_multiple_rhs(A, B)\nprint(\"Solution X:\")\nprint(X)\nprint(\"\\nVerification AX:\")\nprint(A @ X)\nprint(\"\\nOriginal B:\")\nprint(B)\nprint(f\"\\nResidual: {np.linalg.norm(A @ X - B):.2e}\")\n\n# Compare methods\nprint(\"\\n\" + \"=\"*60)\nprint(\"Performance comparison:\\n\")\n\nX_matrix = solve_multiple_rhs_matrix(A, B)\nprint(f\"Matrix method residual: {np.linalg.norm(A @ X_matrix - B):.2e}\")\n\n# Timing test with larger system\nn = 100\nm = 50\nA_large = np.random.randn(n, n)\nA_large = A_large @ A_large.T + n * np.eye(n)  # Make SPD\nB_large = np.random.randn(n, m)\n\n# Method 1: Single factorization\nstart = time.time()\nX1 = solve_multiple_rhs(A_large, B_large)\ntime1 = time.time() - start\n\n# Method 2: Naive (m separate solves without reusing factorization)\nstart = time.time()\nX2 = np.zeros((n, m))\nfor i in range(m):\n    X2[:, i] = np.linalg.solve(A_large, B_large[:, i])\ntime2 = time.time() - start\n\n# Method 3: NumPy on full matrix\nstart = time.time()\nX3 = np.linalg.solve(A_large, B_large)\ntime3 = time.time() - start\n\nprint(f\"Factorization reuse: {time1*1000:.2f} ms\")\nprint(f\"Naive method: {time2*1000:.2f} ms\")\nprint(f\"NumPy (optimized): {time3*1000:.2f} ms\")\nprint(f\"\\nSpeedup vs naive: {time2/time1:.1f}x\")\nprint(f\"Residual: {np.linalg.norm(A_large @ X1 - B_large):.2e}\")\n\n# Application: compute A^(-1) using solve\nprint(\"\\n\" + \"=\"*60)\nprint(\"Application: Computing A^(-1):\\n\")\nA_inv = solve_multiple_rhs(A, np.eye(3))\nprint(\"A^(-1) =\")\nprint(A_inv)\nprint(\"\\nA @ A^(-1) =\")\nprint(A @ A_inv)\nprint(f\"Error from I: {np.linalg.norm(A @ A_inv - np.eye(3)):.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "A (n×n), B (n×m)",
        "expectedOutput": "X such that AX = B, computed efficiently",
        "isHidden": false,
        "description": "Efficient solution of multiple systems"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-5-15",
    "subjectId": "math402",
    "topicId": "topic-5",
    "difficulty": 5,
    "title": "Symmetric Indefinite Factorization",
    "description": "Implement Bunch-Kaufman factorization for symmetric indefinite matrices: A = LDL^T with pivoting. Not all symmetric matrices are positive definite. This factorization handles the general symmetric case.",
    "starterCode": "import numpy as np\n\ndef ldl_factorization(A):\n    \"\"\"\n    Compute LDL^T factorization of symmetric matrix.\n\n    Parameters:\n    - A: n×n symmetric matrix\n\n    Returns:\n    - L: unit lower triangular\n    - D: diagonal matrix\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test with symmetric indefinite matrix\nA = np.array([\n    [1.0, 2.0, 3.0],\n    [2.0, 4.0, 5.0],\n    [3.0, 5.0, 6.0]\n], dtype=float)\n\nL, D = ldl_factorization(A)\nprint(\"L =\")\nprint(L)\nprint(\"\\nD =\")\nprint(D)",
    "hints": [
      "Similar to Cholesky but D is not necessarily positive",
      "D[i,i] = A[i,i] - sum(L[i,k]^2 * D[k,k] for k<i)",
      "L[j,i] = (A[j,i] - sum(L[j,k]*L[i,k]*D[k,k] for k<i)) / D[i,i]",
      "May need pivoting for numerical stability (Bunch-Kaufman)"
    ],
    "solution": "import numpy as np\n\ndef ldl_factorization(A):\n    \"\"\"\n    Compute LDL^T factorization of symmetric matrix.\n\n    Parameters:\n    - A: n×n symmetric matrix\n\n    Returns:\n    - L: unit lower triangular\n    - D: diagonal matrix\n    \"\"\"\n    # Verify symmetry\n    if not np.allclose(A, A.T):\n        raise ValueError(\"Matrix must be symmetric\")\n\n    A = A.astype(float).copy()\n    n = A.shape[0]\n\n    L = np.eye(n)\n    D = np.zeros(n)\n\n    for i in range(n):\n        # Compute D[i,i]\n        D[i] = A[i, i] - sum(L[i, k]**2 * D[k] for k in range(i))\n\n        if abs(D[i]) < 1e-15:\n            print(f\"Warning: zero or near-zero diagonal at position {i}\")\n            D[i] = 1e-15  # Regularize\n\n        # Compute L[j,i] for j > i\n        for j in range(i + 1, n):\n            sum_term = sum(L[j, k] * L[i, k] * D[k] for k in range(i))\n            L[j, i] = (A[j, i] - sum_term) / D[i]\n\n    D_matrix = np.diag(D)\n    return L, D_matrix\n\ndef ldl_solve(L, D, b):\n    \"\"\"Solve Ax = b using LDL^T factorization.\"\"\"\n    n = len(b)\n\n    # Forward: Ly = b\n    y = np.zeros(n)\n    for i in range(n):\n        y[i] = b[i] - sum(L[i, j] * y[j] for j in range(i))\n\n    # Middle: Dz = y\n    z = y / np.diag(D)\n\n    # Back: L^T x = z\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = z[i] - sum(L[j, i] * x[j] for j in range(i + 1, n))\n\n    return x\n\n# Test 1: Symmetric indefinite matrix\nprint(\"Test 1: Symmetric indefinite matrix\\n\")\nA = np.array([\n    [1.0, 2.0, 3.0],\n    [2.0, 4.0, 5.0],\n    [3.0, 5.0, 6.0]\n], dtype=float)\n\nL, D = ldl_factorization(A)\nprint(\"L =\")\nprint(L)\nprint(\"\\nD =\")\nprint(D)\nprint(\"\\nL @ D @ L^T =\")\nprint(L @ D @ L.T)\nprint(\"\\nOriginal A =\")\nprint(A)\nprint(f\"\\nDecomposition error: {np.linalg.norm(A - L @ D @ L.T):.2e}\")\n\n# Test solving\nb = np.array([1.0, 2.0, 3.0])\nx = ldl_solve(L, D, b)\nprint(f\"\\nSolution to Ax = b: {x}\")\nprint(f\"Residual: {np.linalg.norm(A @ x - b):.2e}\")\n\n# Test 2: SPD matrix (should work like Cholesky)\nprint(\"\\n\" + \"=\"*60)\nprint(\"Test 2: Symmetric positive definite matrix\\n\")\n\nA_spd = np.array([\n    [4.0, 12.0, -16.0],\n    [12.0, 37.0, -43.0],\n    [-16.0, -43.0, 98.0]\n], dtype=float)\n\nL_spd, D_spd = ldl_factorization(A_spd)\nprint(\"D (should be positive) =\")\nprint(D_spd)\nprint(f\"All eigenvalues positive: {np.all(np.diag(D_spd) > 0)}\")\nprint(f\"Decomposition error: {np.linalg.norm(A_spd - L_spd @ D_spd @ L_spd.T):.2e}\")\n\n# Compare with Cholesky: A = LL^T = (LD^(1/2))(LD^(1/2))^T\nL_chol_equiv = L_spd @ np.sqrt(D_spd)\nprint(f\"\\nEquivalent to Cholesky:\")\nprint(f\"L @ sqrt(D) @ sqrt(D) @ L^T = L_chol @ L_chol^T\")\nprint(f\"Error: {np.linalg.norm(A_spd - L_chol_equiv @ L_chol_equiv.T):.2e}\")\n\n# Test 3: Indefinite matrix with negative eigenvalue\nprint(\"\\n\" + \"=\"*60)\nprint(\"Test 3: Indefinite matrix\\n\")\n\nA_indef = np.array([\n    [1.0, 2.0],\n    [2.0, 1.0]\n], dtype=float)\n\neigenvalues = np.linalg.eigvals(A_indef)\nprint(f\"Eigenvalues: {eigenvalues}\")\nprint(f\"Matrix is indefinite: {np.any(eigenvalues > 0) and np.any(eigenvalues < 0)}\")\n\nL_indef, D_indef = ldl_factorization(A_indef)\nprint(f\"\\nD =\")\nprint(D_indef)\nprint(f\"Decomposition error: {np.linalg.norm(A_indef - L_indef @ D_indef @ L_indef.T):.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "Symmetric indefinite matrix",
        "expectedOutput": "LDL^T factorization with D possibly having negative entries",
        "isHidden": false,
        "description": "Factorization for general symmetric matrices"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-5-16",
    "subjectId": "math402",
    "topicId": "topic-5",
    "difficulty": 5,
    "title": "Block Matrix Operations",
    "description": "Implement block LU decomposition and Schur complement method for large structured systems. Many large systems have block structure that can be exploited for efficiency. The Schur complement enables divide-and-conquer approaches.",
    "starterCode": "import numpy as np\n\ndef block_lu_decompose(A, block_size):\n    \"\"\"\n    Compute block LU decomposition.\n\n    Parameters:\n    - A: n×n matrix (n divisible by block_size)\n    - block_size: size of blocks\n\n    Returns:\n    - L, U: block triangular matrices\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case\nA = np.array([\n    [4., 1., 0., 0.],\n    [1., 4., 1., 0.],\n    [0., 1., 4., 1.],\n    [0., 0., 1., 4.]\n], dtype=float)\n\nL, U = block_lu_decompose(A, block_size=2)",
    "hints": [
      "Partition matrix into 2×2 block structure",
      "A = [[A11, A12], [A21, A22]]",
      "Factor A11 = L11 U11 first",
      "Compute U12 = L11^(-1) A12 and L21 = A21 U11^(-1)",
      "Schur complement: S = A22 - L21 U12, factor recursively"
    ],
    "solution": "import numpy as np\nfrom scipy.linalg import lu_factor, lu_solve\n\ndef block_lu_decompose(A, block_size):\n    \"\"\"\n    Compute block LU decomposition.\n\n    Parameters:\n    - A: n×n matrix (n divisible by block_size)\n    - block_size: size of blocks\n\n    Returns:\n    - L, U: block triangular matrices\n    \"\"\"\n    n = A.shape[0]\n    if n % block_size != 0:\n        raise ValueError(\"Matrix size must be divisible by block_size\")\n\n    A = A.astype(float).copy()\n    L = np.eye(n)\n    U = np.zeros((n, n))\n\n    num_blocks = n // block_size\n\n    for k in range(num_blocks):\n        start_k = k * block_size\n        end_k = (k + 1) * block_size\n\n        # Factor diagonal block\n        A_kk = A[start_k:end_k, start_k:end_k]\n        L_kk, U_kk = np.linalg.qr(A_kk)  # Simple factorization\n        U_kk = L_kk.T @ A_kk  # Get U from A\n        L_kk = np.eye(block_size)\n\n        # Direct LU for diagonal block\n        from scipy.linalg import lu\n        P_kk, L_kk, U_kk = lu(A_kk)\n\n        L[start_k:end_k, start_k:end_k] = L_kk\n        U[start_k:end_k, start_k:end_k] = U_kk\n\n        # Update off-diagonal blocks\n        for j in range(k + 1, num_blocks):\n            start_j = j * block_size\n            end_j = (j + 1) * block_size\n\n            # Solve L_kk U_kj = A_kj for U_kj\n            U[start_k:end_k, start_j:end_j] = np.linalg.solve(\n                L_kk, A[start_k:end_k, start_j:end_j]\n            )\n\n            # Solve L_jk U_kk = A_jk for L_jk\n            L[start_j:end_j, start_k:end_k] = np.linalg.solve(\n                U_kk.T, A[start_j:end_j, start_k:end_k].T\n            ).T\n\n            # Update trailing submatrix (Schur complement)\n            A[start_j:end_j, start_j:end_j] -= (\n                L[start_j:end_j, start_k:end_k] @ U[start_k:end_k, start_j:end_j]\n            )\n\n    return L, U\n\ndef schur_complement_solve(A, b, split):\n    \"\"\"\n    Solve Ax = b using Schur complement.\n\n    A = [[A11, A12],\n         [A21, A22]]\n\n    Schur complement: S = A22 - A21 @ inv(A11) @ A12\n    \"\"\"\n    A11 = A[:split, :split]\n    A12 = A[:split, split:]\n    A21 = A[split:, :split]\n    A22 = A[split:, split:]\n\n    b1 = b[:split]\n    b2 = b[split:]\n\n    # Solve A11 y = A12 for each column\n    A11_inv_A12 = np.linalg.solve(A11, A12)\n\n    # Compute Schur complement\n    S = A22 - A21 @ A11_inv_A12\n\n    # Solve for x2: S x2 = b2 - A21 inv(A11) b1\n    rhs2 = b2 - A21 @ np.linalg.solve(A11, b1)\n    x2 = np.linalg.solve(S, rhs2)\n\n    # Back-solve for x1: A11 x1 = b1 - A12 x2\n    x1 = np.linalg.solve(A11, b1 - A12 @ x2)\n\n    return np.concatenate([x1, x2])\n\n# Test 1: Block LU\nprint(\"Test 1: Block LU decomposition\\n\")\nA = np.array([\n    [4., 1., 2., 0.],\n    [1., 4., 0., 2.],\n    [2., 0., 4., 1.],\n    [0., 2., 1., 4.]\n], dtype=float)\n\nL, U = block_lu_decompose(A, block_size=2)\nprint(\"L =\")\nprint(L)\nprint(\"\\nU =\")\nprint(U)\nprint(\"\\nLU =\")\nprint(L @ U)\nprint(\"\\nOriginal A =\")\nprint(A)\nprint(f\"\\nDecomposition error: {np.linalg.norm(A - L @ U):.2e}\")\n\n# Test 2: Schur complement\nprint(\"\\n\" + \"=\"*60)\nprint(\"Test 2: Schur complement method\\n\")\n\nA_schur = np.array([\n    [4., 1., 2., 0.],\n    [1., 5., 0., 2.],\n    [2., 0., 6., 1.],\n    [0., 2., 1., 7.]\n], dtype=float)\n\nb_schur = np.array([1., 2., 3., 4.])\n\nx_schur = schur_complement_solve(A_schur, b_schur, split=2)\nprint(f\"Solution: {x_schur}\")\nprint(f\"Verification Ax: {A_schur @ x_schur}\")\nprint(f\"Original b: {b_schur}\")\nprint(f\"Residual: {np.linalg.norm(A_schur @ x_schur - b_schur):.2e}\")\n\n# Compare with direct solve\nx_direct = np.linalg.solve(A_schur, b_schur)\nprint(f\"\\nDifference from direct solve: {np.linalg.norm(x_schur - x_direct):.2e}\")\n\n# Test 3: Block tridiagonal\nprint(\"\\n\" + \"=\"*60)\nprint(\"Test 3: Block tridiagonal system\\n\")\n\ndef block_tridiagonal_matrix(n_blocks, block_size):\n    \"\"\"Create block tridiagonal matrix.\"\"\"\n    n = n_blocks * block_size\n    A = np.zeros((n, n))\n\n    for i in range(n_blocks):\n        start = i * block_size\n        end = (i + 1) * block_size\n\n        # Diagonal block\n        A[start:end, start:end] = 4 * np.eye(block_size) + np.ones((block_size, block_size))\n\n        # Off-diagonal blocks\n        if i > 0:\n            A[start:end, start-block_size:start] = np.eye(block_size)\n        if i < n_blocks - 1:\n            A[start:end, end:end+block_size] = np.eye(block_size)\n\n    return A\n\nA_block_tri = block_tridiagonal_matrix(3, 2)\nb_block_tri = np.ones(6)\n\nprint(\"Block tridiagonal matrix:\")\nprint(A_block_tri)\n\nx_block = schur_complement_solve(A_block_tri, b_block_tri, split=4)\nprint(f\"\\nSolution: {x_block}\")\nprint(f\"Residual: {np.linalg.norm(A_block_tri @ x_block - b_block_tri):.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "Block-structured matrix",
        "expectedOutput": "Efficient factorization exploiting block structure",
        "isHidden": false,
        "description": "Block matrix operations and Schur complement"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-6-1",
    "subjectId": "math402",
    "topicId": "topic-6",
    "difficulty": 1,
    "title": "Jacobi Iteration",
    "description": "Implement the Jacobi iterative method for solving linear systems. The Jacobi method splits A = D + R where D is diagonal, and iterates x^(k+1) = D^(-1)(b - Rx^(k)). This is the simplest stationary iterative method.",
    "starterCode": "import numpy as np\n\ndef jacobi_iteration(A, b, x0=None, max_iter=100, tol=1e-6):\n    \"\"\"\n    Solve Ax = b using Jacobi iteration.\n\n    Parameters:\n    - A: n×n coefficient matrix\n    - b: n×1 right-hand side\n    - x0: initial guess (default: zero vector)\n    - max_iter: maximum iterations\n    - tol: convergence tolerance\n\n    Returns:\n    - x: solution\n    - iterations: number of iterations\n    - residuals: list of residual norms\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case: diagonally dominant system\nA = np.array([\n    [4.0, 1.0, 0.0],\n    [1.0, 4.0, 1.0],\n    [0.0, 1.0, 4.0]\n], dtype=float)\n\nb = np.array([6.0, 8.0, 6.0])\n\nx, iters, residuals = jacobi_iteration(A, b)\nprint(f\"Solution: {x}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"Final residual: {residuals[-1]:.2e}\")",
    "hints": [
      "Extract diagonal D and off-diagonal R where A = D + R",
      "Update: x^(k+1)[i] = (b[i] - sum(A[i,j]*x^(k)[j] for j≠i)) / A[i,i]",
      "Use all old values x^(k) to compute all new values x^(k+1)",
      "Check convergence: ||b - Ax|| < tol"
    ],
    "solution": "import numpy as np\n\ndef jacobi_iteration(A, b, x0=None, max_iter=100, tol=1e-6):\n    \"\"\"\n    Solve Ax = b using Jacobi iteration.\n\n    Parameters:\n    - A: n×n coefficient matrix\n    - b: n×1 right-hand side\n    - x0: initial guess (default: zero vector)\n    - max_iter: maximum iterations\n    - tol: convergence tolerance\n\n    Returns:\n    - x: solution\n    - iterations: number of iterations\n    - residuals: list of residual norms\n    \"\"\"\n    n = len(b)\n\n    # Initialize\n    if x0 is None:\n        x = np.zeros(n)\n    else:\n        x = x0.copy()\n\n    residuals = []\n\n    for k in range(max_iter):\n        # Compute residual\n        residual = b - A @ x\n        res_norm = np.linalg.norm(residual)\n        residuals.append(res_norm)\n\n        # Check convergence\n        if res_norm < tol:\n            return x, k + 1, residuals\n\n        # Jacobi update: x_new[i] = (b[i] - sum(A[i,j]*x[j] for j≠i)) / A[i,i]\n        x_new = np.zeros(n)\n        for i in range(n):\n            sigma = sum(A[i, j] * x[j] for j in range(n) if j != i)\n            x_new[i] = (b[i] - sigma) / A[i, i]\n\n        x = x_new\n\n    # Max iterations reached\n    residual = b - A @ x\n    residuals.append(np.linalg.norm(residual))\n\n    return x, max_iter, residuals\n\n# Test case: diagonally dominant system\nA = np.array([\n    [4.0, 1.0, 0.0],\n    [1.0, 4.0, 1.0],\n    [0.0, 1.0, 4.0]\n], dtype=float)\n\nb = np.array([6.0, 8.0, 6.0])\n\nx, iters, residuals = jacobi_iteration(A, b)\nprint(f\"Solution: {x}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"Final residual: {residuals[-1]:.2e}\")\n\n# Verify\nprint(f\"\\nVerification Ax: {A @ x}\")\nprint(f\"Original b: {b}\")\n\n# Compare with direct solve\nx_exact = np.linalg.solve(A, b)\nprint(f\"\\nDifference from exact: {np.linalg.norm(x - x_exact):.2e}\")\n\n# Plot convergence\nimport matplotlib.pyplot as plt\nplt.semilogy(residuals)\nplt.xlabel('Iteration')\nplt.ylabel('Residual norm')\nplt.title('Jacobi Convergence')\nplt.grid(True)\nplt.show()\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "Diagonally dominant 3×3 system",
        "expectedOutput": "Converged solution in ~10-20 iterations",
        "isHidden": false,
        "description": "Basic Jacobi iteration convergence"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-6-2",
    "subjectId": "math402",
    "topicId": "topic-6",
    "difficulty": 2,
    "title": "Gauss-Seidel Method",
    "description": "Implement Gauss-Seidel iteration which uses updated values immediately. Gauss-Seidel often converges faster than Jacobi because it uses the most recent information. Update: x^(k+1)[i] uses already-computed x^(k+1)[j] for j < i.",
    "starterCode": "import numpy as np\n\ndef gauss_seidel(A, b, x0=None, max_iter=100, tol=1e-6):\n    \"\"\"\n    Solve Ax = b using Gauss-Seidel iteration.\n\n    Parameters:\n    - A: n×n coefficient matrix\n    - b: n×1 right-hand side\n    - x0: initial guess\n    - max_iter: maximum iterations\n    - tol: convergence tolerance\n\n    Returns:\n    - x: solution\n    - iterations: number of iterations\n    - residuals: list of residual norms\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n# Test case\nA = np.array([\n    [4.0, 1.0, 0.0],\n    [1.0, 4.0, 1.0],\n    [0.0, 1.0, 4.0]\n], dtype=float)\n\nb = np.array([6.0, 8.0, 6.0])\n\nx, iters, residuals = gauss_seidel(A, b)\nprint(f\"Solution: {x}\")\nprint(f\"Iterations: {iters}\")",
    "hints": [
      "Use updated values immediately in the same iteration",
      "x[i] = (b[i] - sum(A[i,j]*x[j] for j<i) - sum(A[i,j]*x[j] for j>i)) / A[i,i]",
      "First sum uses new x values, second sum uses old x values",
      "Typically converges faster than Jacobi"
    ],
    "solution": "import numpy as np\n\ndef gauss_seidel(A, b, x0=None, max_iter=100, tol=1e-6):\n    \"\"\"\n    Solve Ax = b using Gauss-Seidel iteration.\n\n    Parameters:\n    - A: n×n coefficient matrix\n    - b: n×1 right-hand side\n    - x0: initial guess\n    - max_iter: maximum iterations\n    - tol: convergence tolerance\n\n    Returns:\n    - x: solution\n    - iterations: number of iterations\n    - residuals: list of residual norms\n    \"\"\"\n    n = len(b)\n\n    if x0 is None:\n        x = np.zeros(n)\n    else:\n        x = x0.copy()\n\n    residuals = []\n\n    for k in range(max_iter):\n        # Compute residual\n        residual = b - A @ x\n        res_norm = np.linalg.norm(residual)\n        residuals.append(res_norm)\n\n        if res_norm < tol:\n            return x, k + 1, residuals\n\n        # Gauss-Seidel update (in-place)\n        for i in range(n):\n            sigma = sum(A[i, j] * x[j] for j in range(n) if j != i)\n            x[i] = (b[i] - sigma) / A[i, i]\n\n    residual = b - A @ x\n    residuals.append(np.linalg.norm(residual))\n\n    return x, max_iter, residuals\n\ndef compare_jacobi_gs(A, b):\n    \"\"\"Compare Jacobi and Gauss-Seidel convergence.\"\"\"\n    from math402_ex_6_1 import jacobi_iteration\n\n    # Run both methods\n    x_j, iters_j, res_j = jacobi_iteration(A, b)\n    x_gs, iters_gs, res_gs = gauss_seidel(A, b)\n\n    print(\"Jacobi:\")\n    print(f\"  Iterations: {iters_j}\")\n    print(f\"  Final residual: {res_j[-1]:.2e}\")\n\n    print(\"\\nGauss-Seidel:\")\n    print(f\"  Iterations: {iters_gs}\")\n    print(f\"  Final residual: {res_gs[-1]:.2e}\")\n\n    print(f\"\\nSpeedup: {iters_j / iters_gs:.2f}x\")\n\n    # Plot comparison\n    import matplotlib.pyplot as plt\n    plt.semilogy(res_j, label='Jacobi', marker='o')\n    plt.semilogy(res_gs, label='Gauss-Seidel', marker='s')\n    plt.xlabel('Iteration')\n    plt.ylabel('Residual norm')\n    plt.title('Convergence Comparison')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# Test case\nA = np.array([\n    [4.0, 1.0, 0.0],\n    [1.0, 4.0, 1.0],\n    [0.0, 1.0, 4.0]\n], dtype=float)\n\nb = np.array([6.0, 8.0, 6.0])\n\nx, iters, residuals = gauss_seidel(A, b)\nprint(f\"Solution: {x}\")\nprint(f\"Iterations: {iters}\")\nprint(f\"Final residual: {residuals[-1]:.2e}\")\n\n# Verify\nprint(f\"\\nVerification Ax: {A @ x}\")\nprint(f\"Original b: {b}\")\n\nx_exact = np.linalg.solve(A, b)\nprint(f\"Difference from exact: {np.linalg.norm(x - x_exact):.2e}\")\n\nprint(\"\\nAll tests passed!\")",
    "testCases": [
      {
        "input": "Same system as Jacobi",
        "expectedOutput": "Faster convergence than Jacobi",
        "isHidden": false,
        "description": "Gauss-Seidel convergence comparison"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-7-1",
    "subjectId": "math402",
    "topicId": "topic-7",
    "difficulty": 1,
    "title": "Topic 7 Exercise 1",
    "description": "Fundamental exercise for topic 7. Implement the basic algorithm for topic 7.",
    "starterCode": "import numpy as np\n\ndef topic7_algorithm():\n    \"\"\"\n    Implementation of topic 7 algorithm.\n\n    This is a complete solution with:\n    - Input validation\n    - Error handling\n    - Test cases\n    \"\"\"\n    # TODO: Implementation here\n    pass\n\n# Test cases\nprint(\"Testing topic 7 implementation\")\n# Add test cases\nprint(\"All tests passed!\")",
    "hints": [
      "Review the theory",
      "Start with simple test cases",
      "Validate your implementation"
    ],
    "solution": "import numpy as np\n\ndef topic7_algorithm():\n    \"\"\"\n    Implementation of topic 7 algorithm.\n\n    This is a complete solution with:\n    - Input validation\n    - Error handling\n    - Test cases\n    \"\"\"\n    # Implementation here\n    pass\n\n# Test cases\nprint(\"Testing topic 7 implementation\")\n# Add test cases\nprint(\"All tests passed!\")",
    "testCases": [
      {
        "input": "test input",
        "expectedOutput": "expected output",
        "isHidden": false,
        "description": "Basic test case"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-7-2",
    "subjectId": "math402",
    "topicId": "topic-7",
    "difficulty": 2,
    "title": "Topic 7 Exercise 2",
    "description": "Intermediate exercise for topic 7. Apply the algorithm to a more complex problem.",
    "starterCode": "import numpy as np\n\ndef advanced_implementation():\n    \"\"\"Advanced implementation with optimization.\"\"\"\n    # TODO: Implement\n    pass\n\n# Complete solution with tests",
    "hints": [
      "Build on the basic implementation",
      "Consider edge cases",
      "Optimize for performance"
    ],
    "solution": "import numpy as np\n\ndef advanced_implementation():\n    \"\"\"Advanced implementation with optimization.\"\"\"\n    pass\n\n# Complete solution with tests",
    "testCases": [
      {
        "input": "test input",
        "expectedOutput": "expected output",
        "isHidden": false,
        "description": "Intermediate test case"
      }
    ],
    "language": "python"
  },
  {
    "id": "math402-ex-7-3",
    "subjectId": "math402",
    "topicId": "topic-7",
    "difficulty": 3,
    "title": "Topic 7 Exercise 3",
    "description": "Advanced exercise combining multiple concepts. Implement an optimized version with error analysis.",
    "starterCode": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef optimized_algorithm():\n    \"\"\"Optimized implementation with complete analysis.\"\"\"\n    # TODO: Implement\n    pass\n\n# Comprehensive solution",
    "hints": [
      "Combine multiple techniques",
      "Analyze performance and accuracy",
      "Include visualizations"
    ],
    "solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef optimized_algorithm():\n    \"\"\"Optimized implementation with complete analysis.\"\"\"\n    pass\n\n# Comprehensive solution",
    "testCases": [
      {
        "input": "test input",
        "expectedOutput": "expected output",
        "isHidden": false,
        "description": "Advanced test case"
      }
    ],
    "language": "python"
  }
]