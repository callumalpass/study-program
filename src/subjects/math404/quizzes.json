[
  {
    "id": "math404-quiz-1a",
    "subjectId": "math404",
    "topicId": "math404-topic-1",
    "title": "Problem Formulation - Fundamentals",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "Which of the following is NOT a component of an optimization problem?",
        "options": [
          "Decision variables",
          "Objective function",
          "Constraints",
          "Probability distribution"
        ],
        "correctAnswer": 3,
        "explanation": "Probability distributions are relevant for stochastic optimization but not required for deterministic optimization problems."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "Every local minimum of a convex optimization problem is also a global minimum.",
        "correctAnswer": true,
        "explanation": "This is a fundamental property of convex optimization. Convexity guarantees that local optima are global."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "For an unconstrained optimization problem, what is the first-order necessary condition for a local minimum at x*?",
        "options": [
          "∇f(x*) > 0",
          "∇f(x*) = 0",
          "∇²f(x*) > 0",
          "f(x*) = 0"
        ],
        "correctAnswer": 1,
        "explanation": "At a local minimum, the gradient must vanish: ∇f(x*) = 0. This is the first-order necessary condition (FONC)."
      },
      {
        "id": "q4",
        "type": "multiple_choice",
        "prompt": "What type of constraint is represented by g(x) ≤ 0?",
        "options": [
          "Equality constraint",
          "Inequality constraint",
          "Box constraint",
          "Nonnegativity constraint"
        ],
        "correctAnswer": 1,
        "explanation": "The form g(x) ≤ 0 represents an inequality constraint, which defines a half-space in the feasible region."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "A point is called _____ if it satisfies all constraints of an optimization problem.",
        "correctAnswer": "feasible",
        "explanation": "A feasible point satisfies all constraints. The set of all feasible points forms the feasible region."
      }
    ]
  },
  {
    "id": "math404-quiz-1b",
    "subjectId": "math404",
    "topicId": "math404-topic-1",
    "title": "Problem Formulation - Application",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "In a production problem, if x₁ represents units of product 1 and x₂ represents units of product 2, which constraint represents a labor limit of 100 hours if product 1 takes 2 hours and product 2 takes 3 hours?",
        "options": [
          "2x₁ + 3x₂ = 100",
          "2x₁ + 3x₂ ≤ 100",
          "2x₁ + 3x₂ ≥ 100",
          "x₁ + x₂ ≤ 100"
        ],
        "correctAnswer": 1,
        "explanation": "The total labor used (2x₁ + 3x₂) must not exceed the available 100 hours, giving us the inequality constraint 2x₁ + 3x₂ ≤ 100."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "A convex function always has exactly one local minimum.",
        "correctAnswer": false,
        "explanation": "A convex function may have no minimum (unbounded below), a unique minimum, or infinitely many minima (e.g., a constant function). However, if it has a local minimum, that is also a global minimum."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "For which type of optimization problem can we guarantee finding a global optimum in polynomial time?",
        "options": [
          "General nonlinear programming",
          "Integer linear programming",
          "Convex quadratic programming",
          "Mixed-integer nonlinear programming"
        ],
        "correctAnswer": 2,
        "explanation": "Convex quadratic programming can be solved to global optimality in polynomial time using interior-point methods. The other problem classes are generally NP-hard."
      },
      {
        "id": "q4",
        "type": "multiple_choice",
        "prompt": "What does LICQ (Linear Independence Constraint Qualification) require at a point x*?",
        "options": [
          "All constraints are active",
          "Gradients of active constraints are linearly independent",
          "The objective function is linear",
          "The Hessian is positive definite"
        ],
        "correctAnswer": 1,
        "explanation": "LICQ requires that the gradients of all active inequality constraints and all equality constraints are linearly independent at x*."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "In the standard form of an optimization problem, to convert max f(x) to a minimization, we minimize _____.",
        "correctAnswer": "-f(x)",
        "explanation": "Maximizing f(x) is equivalent to minimizing -f(x), since the point that gives the highest value of f gives the lowest value of -f."
      }
    ]
  },
  {
    "id": "math404-quiz-1c",
    "subjectId": "math404",
    "topicId": "math404-topic-1",
    "title": "Problem Formulation - Mastery",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "Consider f(x,y) = (x-2)² + (y-3)². What is the global minimum and where does it occur?",
        "options": [
          "f = 0 at (2,3)",
          "f = 1 at (1,2)",
          "f = 5 at (0,0)",
          "f = 13 at (3,2)"
        ],
        "correctAnswer": 0,
        "explanation": "This is a paraboloid with vertex at (2,3). The minimum value is 0, achieved when both squared terms equal zero, i.e., at (2,3)."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "If a constraint qualification fails at a local optimum, the KKT conditions may not hold.",
        "correctAnswer": true,
        "explanation": "Constraint qualifications (like LICQ) ensure that KKT conditions are necessary for optimality. If they fail, KKT may not be necessary."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "For a twice-differentiable function f, which condition is sufficient for x* to be a strict local minimum?",
        "options": [
          "∇f(x*) = 0 only",
          "∇f(x*) = 0 and ∇²f(x*) is positive semidefinite",
          "∇f(x*) = 0 and ∇²f(x*) is positive definite",
          "∇²f(x*) is positive definite only"
        ],
        "correctAnswer": 2,
        "explanation": "For a strict local minimum, we need ∇f(x*) = 0 (first-order condition) and ∇²f(x*) positive definite (second-order sufficient condition)."
      },
      {
        "id": "q4",
        "type": "multiple_choice",
        "prompt": "Which class of functions has the property that every local minimum is also a global minimum?",
        "options": [
          "Continuous functions",
          "Differentiable functions",
          "Convex functions",
          "Polynomial functions"
        ],
        "correctAnswer": 2,
        "explanation": "This is a defining property of convex functions. For convex f, any local minimum is automatically a global minimum."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "A function f is called _____ convex if f(λx + (1-λ)y) < λf(x) + (1-λ)f(y) for all x ≠ y and λ ∈ (0,1).",
        "correctAnswer": "strictly",
        "explanation": "Strict convexity requires the inequality to be strict (< instead of ≤) for distinct points, ensuring uniqueness of the minimum."
      }
    ]
  },
  {
    "id": "math404-quiz-2a",
    "subjectId": "math404",
    "topicId": "math404-topic-2",
    "title": "Linear Programming - Fundamentals",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "In standard form LP, what type of constraints are used?",
        "options": [
          "Only inequalities ≤",
          "Only inequalities ≥",
          "Only equalities with x ≥ 0",
          "Mix of equalities and inequalities"
        ],
        "correctAnswer": 2,
        "explanation": "Standard form LP has: min c^T x s.t. Ax = b, x ≥ 0 (equalities with nonnegativity)"
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "The simplex algorithm always finds the optimal solution in polynomial time.",
        "correctAnswer": false,
        "explanation": "The simplex algorithm has exponential worst-case complexity, though it performs well in practice."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "What is a basic feasible solution (BFS) in linear programming?",
        "options": [
          "Any feasible solution to the LP",
          "A vertex of the feasible polytope",
          "The optimal solution",
          "A solution in the interior of the feasible region"
        ],
        "correctAnswer": 1,
        "explanation": "A BFS corresponds to a vertex (extreme point) of the feasible polytope and has exactly n-m nonzero variables."
      },
      {
        "id": "q4",
        "type": "fill_blank",
        "prompt": "In the simplex method, the variable entering the basis is chosen based on the most negative _____ cost.",
        "correctAnswer": "reduced",
        "explanation": "The entering variable is selected as the one with the most negative reduced cost, indicating the steepest descent direction."
      },
      {
        "id": "q5",
        "type": "true_false",
        "prompt": "If an LP has an optimal solution, at least one basic feasible solution is optimal.",
        "correctAnswer": true,
        "explanation": "For LPs with bounded optimal solutions, there always exists an optimal BFS at a vertex of the feasible region."
      }
    ]
  },
  {
    "id": "math404-quiz-2b",
    "subjectId": "math404",
    "topicId": "math404-topic-2",
    "title": "Linear Programming - Application",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "When using the graphical method for a 2D LP, where is the optimal solution found?",
        "options": [
          "Always in the interior of the feasible region",
          "At a vertex of the feasible region",
          "On an edge of the feasible region",
          "Both B and C are possible"
        ],
        "correctAnswer": 3,
        "explanation": "The optimum is at a vertex or, if there are multiple optima, along an edge connecting optimal vertices."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "Degeneracy in linear programming means the LP has no feasible solution.",
        "correctAnswer": false,
        "explanation": "Degeneracy occurs when a BFS has more than n-m zero variables, which can cause simplex to cycle but doesn't imply infeasibility."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "What do shadow prices represent in sensitivity analysis?",
        "options": [
          "The cost of decision variables",
          "The marginal value of resources",
          "The reduced costs",
          "The objective function coefficients"
        ],
        "correctAnswer": 1,
        "explanation": "Shadow prices (dual variables) represent the rate of change in the optimal objective value per unit increase in a constraint's right-hand side."
      },
      {
        "id": "q4",
        "type": "multiple_choice",
        "prompt": "Interior-point methods for LP have what computational complexity?",
        "options": [
          "Exponential worst-case",
          "Pseudo-polynomial",
          "Polynomial worst-case",
          "Constant time"
        ],
        "correctAnswer": 2,
        "explanation": "Interior-point methods achieve polynomial worst-case complexity, typically O(n³) per iteration, unlike simplex which is exponential."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "The _____ test determines which variable leaves the basis in the simplex algorithm.",
        "correctAnswer": "minimum ratio",
        "explanation": "The minimum ratio test ensures feasibility is maintained when pivoting by determining how far we can move along the entering variable's direction."
      }
    ]
  },
  {
    "id": "math404-quiz-2c",
    "subjectId": "math404",
    "topicId": "math404-topic-2",
    "title": "Linear Programming - Mastery",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "Consider max 3x + 2y subject to x + y ≤ 4, 2x + y ≤ 6, x,y ≥ 0. What is the optimal value?",
        "options": [
          "10",
          "12",
          "14",
          "16"
        ],
        "correctAnswer": 1,
        "explanation": "The optimal solution is at vertex (2,2), giving objective value 3(2) + 2(2) = 10. We can verify this is optimal by checking all vertices."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "If the LP is unbounded, the simplex algorithm will identify this by finding a direction with negative reduced cost and no positive coefficients in the minimum ratio test.",
        "correctAnswer": true,
        "explanation": "Unboundedness is detected when an entering variable can increase indefinitely without violating any constraint (all ratios undefined)."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "In the simplex tableau, what indicates optimality has been reached?",
        "options": [
          "All basic variables are positive",
          "All reduced costs are non-negative (for minimization)",
          "The right-hand side is all positive",
          "The basis is identity"
        ],
        "correctAnswer": 1,
        "explanation": "For minimization, optimality is reached when all reduced costs are non-negative, meaning no improving direction exists."
      },
      {
        "id": "q4",
        "type": "multiple_choice",
        "prompt": "What is the relationship between the number of pivots in simplex and problem dimensions?",
        "options": [
          "Always exactly n pivots",
          "At most m pivots",
          "Can be exponential in worst case",
          "Always polynomial"
        ],
        "correctAnswer": 2,
        "explanation": "While simplex is polynomial on average, pathological cases (e.g., Klee-Minty cube) require exponentially many pivots."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "Bland's rule and _____ rule are anti-cycling strategies that guarantee simplex terminates.",
        "correctAnswer": "lexicographic",
        "explanation": "Both Bland's and lexicographic rules prevent cycling by imposing deterministic tie-breaking rules for entering/leaving variables."
      }
    ]
  },
  {
    "id": "math404-quiz-3a",
    "subjectId": "math404",
    "topicId": "math404-topic-3",
    "title": "Duality Theory - Fundamentals",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "What does weak duality guarantee for a primal maximization LP and its dual?",
        "options": [
          "Primal optimal ≤ Dual optimal",
          "Primal optimal ≥ Dual optimal",
          "Primal optimal = Dual optimal",
          "No relationship exists"
        ],
        "correctAnswer": 0,
        "explanation": "Weak duality states that any dual feasible solution provides an upper bound on the primal optimal value."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "If both primal and dual have feasible solutions, strong duality holds.",
        "correctAnswer": true,
        "explanation": "For linear programs, if both primal and dual are feasible, both have optimal solutions with equal values."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "In the dual of an LP, what does each dual variable correspond to?",
        "options": [
          "A primal decision variable",
          "A primal constraint",
          "An objective coefficient",
          "A basis element"
        ],
        "correctAnswer": 1,
        "explanation": "Each dual variable corresponds to a primal constraint, representing its shadow price or marginal value."
      },
      {
        "id": "q4",
        "type": "fill_blank",
        "prompt": "The dual of the dual LP is the _____ LP.",
        "correctAnswer": "primal",
        "explanation": "Taking the dual twice returns to the original primal problem, showing the symmetric relationship between primal and dual."
      },
      {
        "id": "q5",
        "type": "true_false",
        "prompt": "Strong duality can fail for linear programs.",
        "correctAnswer": false,
        "explanation": "Strong duality always holds for linear programs when both primal and dual are feasible. This is a fundamental property of LP."
      }
    ]
  },
  {
    "id": "math404-quiz-3b",
    "subjectId": "math404",
    "topicId": "math404-topic-3",
    "title": "Duality Theory - Application",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "What does complementary slackness state?",
        "options": [
          "Primal and dual are both feasible",
          "λᵢ(bᵢ - aᵢᵀx) = 0 for all i",
          "All constraints are binding",
          "The duality gap is positive"
        ],
        "correctAnswer": 1,
        "explanation": "Complementary slackness: for each constraint i, either it is tight (bᵢ = aᵢᵀx) or the dual variable is zero (λᵢ = 0)."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "If a primal constraint has positive slack at optimum, its dual variable must be zero.",
        "correctAnswer": true,
        "explanation": "This is complementary slackness: λᵢ · slack = 0, so positive slack implies λᵢ = 0."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "Farkas' Lemma provides a certificate for what property?",
        "options": [
          "Optimality",
          "Infeasibility",
          "Boundedness",
          "Degeneracy"
        ],
        "correctAnswer": 1,
        "explanation": "Farkas' Lemma gives a certificate of infeasibility: if no x satisfies Ax ≤ b, then there exists y such that Aᵀy = 0, bᵀy < 0, y ≥ 0."
      },
      {
        "id": "q4",
        "type": "multiple_choice",
        "prompt": "What economic interpretation do dual variables have?",
        "options": [
          "Production costs",
          "Shadow prices of resources",
          "Profit margins",
          "Inventory levels"
        ],
        "correctAnswer": 1,
        "explanation": "Dual variables are shadow prices, representing the marginal value of relaxing a constraint by one unit."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "The _____ gap is the difference between primal and dual objective values.",
        "correctAnswer": "duality",
        "explanation": "The duality gap measures the difference between primal and dual objectives. It is zero at optimality for LPs (strong duality)."
      }
    ]
  },
  {
    "id": "math404-quiz-3c",
    "subjectId": "math404",
    "topicId": "math404-topic-3",
    "title": "Duality Theory - Mastery",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "Given primal: max cᵀx s.t. Ax ≤ b, x ≥ 0, what is the dual?",
        "options": [
          "min bᵀy s.t. Aᵀy ≥ c, y ≥ 0",
          "max bᵀy s.t. Aᵀy ≤ c, y ≥ 0",
          "min bᵀy s.t. Aᵀy = c, y ≥ 0",
          "max bᵀy s.t. Aᵀy ≥ c, y ≥ 0"
        ],
        "correctAnswer": 0,
        "explanation": "The dual of max cᵀx s.t. Ax ≤ b, x ≥ 0 is min bᵀy s.t. Aᵀy ≥ c, y ≥ 0."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "The dual simplex method solves the dual problem directly without forming it explicitly.",
        "correctAnswer": false,
        "explanation": "The dual simplex maintains dual feasibility while seeking primal feasibility, working on the primal tableau but using dual logic."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "If the primal is unbounded, what can we conclude about the dual?",
        "options": [
          "Dual is also unbounded",
          "Dual is infeasible",
          "Dual is feasible",
          "Cannot determine"
        ],
        "correctAnswer": 1,
        "explanation": "If the primal is unbounded, weak duality implies the dual must be infeasible (otherwise dual feasible would bound primal)."
      },
      {
        "id": "q4",
        "type": "multiple_choice",
        "prompt": "For an equality constraint h(x) = 0 in the primal, what is the corresponding dual variable?",
        "options": [
          "Must be non-negative",
          "Must be non-positive",
          "Unrestricted in sign",
          "Must be zero"
        ],
        "correctAnswer": 2,
        "explanation": "Equality constraints in the primal correspond to unrestricted (free) dual variables that can be positive, negative, or zero."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "If a primal variable has a strict inequality bound (x > 0 not x ≥ 0), the corresponding dual constraint must hold with _____.",
        "correctAnswer": "equality",
        "explanation": "Strict inequalities in variable bounds correspond to equality in the dual constraints by complementary slackness."
      }
    ]
  },
  {
    "id": "math404-quiz-4a",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "title": "Convex Sets and Functions - Fundamentals",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "Which operation preserves convexity of a function?",
        "options": [
          "Taking the minimum of two convex functions",
          "Taking the maximum of two convex functions",
          "Multiplying by -1",
          "Composing with any function"
        ],
        "correctAnswer": 1,
        "explanation": "The pointwise maximum of convex functions is convex. Minimum can give non-convex results."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "The intersection of two convex sets is always convex.",
        "correctAnswer": true,
        "explanation": "The intersection of any collection of convex sets is convex."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "A convex set must be:",
        "options": [
          "Closed",
          "Bounded",
          "Contain all line segments between its points",
          "Contain the origin"
        ],
        "correctAnswer": 2,
        "explanation": "By definition, a convex set contains the line segment between any two of its points: λx + (1-λ)y ∈ C for all x,y ∈ C and λ ∈ [0,1]."
      },
      {
        "id": "q4",
        "type": "fill_blank",
        "prompt": "A function f is convex if its epigraph is a _____ set.",
        "correctAnswer": "convex",
        "explanation": "The epigraph {(x,t) : f(x) ≤ t} is convex if and only if f is a convex function."
      },
      {
        "id": "q5",
        "type": "true_false",
        "prompt": "All norms are convex functions.",
        "correctAnswer": true,
        "explanation": "All norms satisfy the triangle inequality and homogeneity, which implies convexity."
      }
    ]
  },
  {
    "id": "math404-quiz-4b",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "title": "Convex Sets and Functions - Application",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "Which of these is NOT a convex set?",
        "options": [
          "A hyperplane {x : aᵀx = b}",
          "A ball {x : ||x-c|| ≤ r}",
          "Union of two convex sets",
          "Intersection of halfspaces"
        ],
        "correctAnswer": 2,
        "explanation": "The union of convex sets is generally not convex. The other options are all convex."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "A function is convex if and only if its Hessian is positive semidefinite everywhere.",
        "correctAnswer": false,
        "explanation": "This is true for twice-differentiable functions, but convexity can be defined for non-differentiable functions as well."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "The first-order characterization of convexity states:",
        "options": [
          "f(y) ≥ f(x) for all x,y",
          "f(y) ≥ f(x) + ∇f(x)ᵀ(y-x)",
          "f(y) = f(x) + ∇f(x)ᵀ(y-x)",
          "∇f(x) = 0"
        ],
        "correctAnswer": 1,
        "explanation": "For differentiable convex f, the function lies above all its tangent hyperplanes: f(y) ≥ f(x) + ∇f(x)ᵀ(y-x)."
      },
      {
        "id": "q4",
        "type": "multiple_choice",
        "prompt": "Convex combinations use coefficients that:",
        "options": [
          "Can be any real numbers",
          "Must sum to 1",
          "Must be non-negative and sum to 1",
          "Must be positive"
        ],
        "correctAnswer": 2,
        "explanation": "Convex combinations require λᵢ ≥ 0 and Σλᵢ = 1, ensuring the result stays within the convex hull."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "The _____ hull of a set S is the smallest convex set containing S.",
        "correctAnswer": "convex",
        "explanation": "The convex hull is the set of all convex combinations of points in S, forming the smallest convex set containing S."
      }
    ]
  },
  {
    "id": "math404-quiz-4c",
    "subjectId": "math404",
    "topicId": "math404-topic-4",
    "title": "Convex Sets and Functions - Mastery",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "The conjugate function f*(y) = sup_x(yᵀx - f(x)) is always:",
        "options": [
          "Differentiable",
          "Convex",
          "Finite",
          "Strictly convex"
        ],
        "correctAnswer": 1,
        "explanation": "The conjugate function is always convex (as a supremum of affine functions), regardless of whether f is convex."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "The separating hyperplane theorem states that two disjoint convex sets can always be strictly separated.",
        "correctAnswer": false,
        "explanation": "They can be separated, but strict separation (positive distance) requires additional conditions like one set being compact."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "For a convex function f, which condition on the Hessian ensures strict convexity?",
        "options": [
          "∇²f(x) ≥ 0",
          "∇²f(x) > 0 (positive definite)",
          "∇²f(x) = 0",
          "det(∇²f(x)) > 0"
        ],
        "correctAnswer": 1,
        "explanation": "Strict convexity requires the Hessian to be positive definite (not just semidefinite) everywhere."
      },
      {
        "id": "q4",
        "type": "multiple_choice",
        "prompt": "A sublevel set {x : f(x) ≤ α} of a convex function is:",
        "options": [
          "Always empty",
          "Always convex",
          "Always bounded",
          "Never convex"
        ],
        "correctAnswer": 1,
        "explanation": "All sublevel sets of a convex function are convex sets, which is equivalent to the definition of convexity."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "Jensen's inequality states that for convex f: f(E[X]) _____ E[f(X)].",
        "correctAnswer": "≤",
        "explanation": "Jensen's inequality: for convex f, f(E[X]) ≤ E[f(X)]. The expectation of the function is at least the function of the expectation."
      }
    ]
  },
  {
    "id": "math404-quiz-5a",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "title": "Convex Optimization - Fundamentals",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "What makes semidefinite programming (SDP) special?",
        "options": [
          "Variables are vectors",
          "Constraints include matrix positive semidefiniteness",
          "Only linear objectives allowed",
          "Always has integer solutions"
        ],
        "correctAnswer": 1,
        "explanation": "SDP has constraints of the form X ⪰ 0 (matrix is positive semidefinite)."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "Every local minimum of a convex optimization problem is a global minimum.",
        "correctAnswer": true,
        "explanation": "This is a fundamental property of convex optimization that makes it tractable."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "Second-order cone programming (SOCP) involves constraints of the form:",
        "options": [
          "||Ax + b|| = c",
          "||Ax + b|| ≤ cᵀx + d",
          "xᵀQx ≤ 1",
          "Ax = b"
        ],
        "correctAnswer": 1,
        "explanation": "SOCP constraints have the form ||Ax + b|| ≤ cᵀx + d, defining a second-order (Lorentz) cone."
      },
      {
        "id": "q4",
        "type": "fill_blank",
        "prompt": "In geometric programming, a _____ is a sum of monomials.",
        "correctAnswer": "posynomial",
        "explanation": "A posynomial is a function of the form Σcᵢx₁^(aᵢ₁)···xₙ^(aᵢₙ) with cᵢ > 0, which becomes convex after log transformation."
      },
      {
        "id": "q5",
        "type": "true_false",
        "prompt": "Convex optimization problems can always be solved in polynomial time.",
        "correctAnswer": true,
        "explanation": "Convex optimization problems can be solved to arbitrary precision in polynomial time using interior-point methods."
      }
    ]
  },
  {
    "id": "math404-quiz-5b",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "title": "Convex Optimization - Application",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "What transformation makes geometric programming convex?",
        "options": [
          "Squaring variables",
          "Log transformation of variables and objective",
          "Adding slack variables",
          "Linearization"
        ],
        "correctAnswer": 1,
        "explanation": "Taking logarithms of variables (yᵢ = log xᵢ) transforms posynomial geometric programs into convex problems."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "Quadratic programming is a special case of semidefinite programming.",
        "correctAnswer": true,
        "explanation": "QP can be formulated as SDP by expressing the quadratic form as a matrix constraint."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "In robust optimization, the uncertainty set should be:",
        "options": [
          "As large as possible",
          "A singleton",
          "Convex for tractability",
          "Always unbounded"
        ],
        "correctAnswer": 2,
        "explanation": "A convex uncertainty set keeps the robust counterpart tractable, often remaining convex."
      },
      {
        "id": "q4",
        "type": "multiple_choice",
        "prompt": "A quasiconvex function has what property?",
        "options": [
          "Convex sublevel sets",
          "Convex epigraph",
          "Positive definite Hessian",
          "Linear gradient"
        ],
        "correctAnswer": 0,
        "explanation": "Quasiconvexity is defined by having convex sublevel sets, which is weaker than convexity."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "The max-cut SDP relaxation provides a _____ bound for the integer program.",
        "correctAnswer": "upper",
        "explanation": "SDP relaxations of combinatorial problems provide upper bounds (for maximization) that can be used in approximation algorithms."
      }
    ]
  },
  {
    "id": "math404-quiz-5c",
    "subjectId": "math404",
    "topicId": "math404-topic-5",
    "title": "Convex Optimization - Mastery",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "For matrix X ⪰ 0 to hold, all eigenvalues must be:",
        "options": [
          "Positive",
          "Non-negative",
          "Equal to 1",
          "Real"
        ],
        "correctAnswer": 1,
        "explanation": "Positive semidefinite (⪰ 0) requires all eigenvalues ≥ 0. Positive definite (≻ 0) requires all eigenvalues > 0."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "Quasiconvex optimization can be solved by bisection on the objective value.",
        "correctAnswer": true,
        "explanation": "Quasiconvex problems can be solved by binary search on α, checking feasibility of {x : f(x) ≤ α} at each step."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "The worst-case robust optimization formulation min_x max_{u∈U} f(x,u) is:",
        "options": [
          "Always convex",
          "Convex if f is convex in x for each u and U is convex",
          "Never convex",
          "Convex only if U is a singleton"
        ],
        "correctAnswer": 1,
        "explanation": "The pointwise maximum over u preserves convexity in x if each f(·,u) is convex and U is convex."
      },
      {
        "id": "q4",
        "type": "multiple_choice",
        "prompt": "Which problem class is most general?",
        "options": [
          "Linear Programming",
          "Quadratic Programming",
          "Second-Order Cone Programming",
          "Semidefinite Programming"
        ],
        "correctAnswer": 3,
        "explanation": "SDP is most general: LP ⊂ SOCP ⊂ SDP. Each can be reformulated as the next more general class."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "The Goemans-Williamson algorithm uses SDP to achieve a _____ approximation for Max-Cut.",
        "correctAnswer": "0.878",
        "explanation": "The Goemans-Williamson algorithm uses SDP relaxation with randomized rounding to achieve a 0.878-approximation ratio for Max-Cut."
      }
    ]
  },
  {
    "id": "math404-quiz-6a",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "title": "Gradient Methods - Fundamentals",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "What determines the convergence rate of gradient descent on a quadratic function?",
        "options": [
          "The objective value",
          "The condition number of the Hessian",
          "The dimension of the problem",
          "The initial point"
        ],
        "correctAnswer": 1,
        "explanation": "The condition number κ = λ_max/λ_min determines convergence rate: (κ-1)/(κ+1)."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "Newton's method converges quadratically near the optimum.",
        "correctAnswer": true,
        "explanation": "Newton's method achieves quadratic convergence when started sufficiently close to the optimum."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "The gradient descent update is: x_{k+1} = x_k - α_k ∇f(x_k). What does α_k represent?",
        "options": [
          "The gradient",
          "The step size (learning rate)",
          "The optimal solution",
          "The Hessian"
        ],
        "correctAnswer": 1,
        "explanation": "α_k is the step size or learning rate, determining how far to move in the negative gradient direction."
      },
      {
        "id": "q4",
        "type": "fill_blank",
        "prompt": "For L-smooth convex functions, gradient descent with appropriate step size achieves O(1/_____) convergence.",
        "correctAnswer": "k",
        "explanation": "GD on L-smooth convex functions converges as O(1/k), meaning f(x_k) - f(x*) ≤ C/k."
      },
      {
        "id": "q5",
        "type": "true_false",
        "prompt": "Gradient descent requires computing second derivatives.",
        "correctAnswer": false,
        "explanation": "Gradient descent only uses first-order information (gradients). Newton's method uses second derivatives (Hessian)."
      }
    ]
  },
  {
    "id": "math404-quiz-6b",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "title": "Gradient Methods - Application",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "The Armijo condition for line search requires:",
        "options": [
          "Exact minimization",
          "Sufficient decrease in objective",
          "Zero gradient",
          "Positive definite Hessian"
        ],
        "correctAnswer": 1,
        "explanation": "Armijo requires f(x + αd) ≤ f(x) + c·α·∇f(x)ᵀd for some c ∈ (0,1), ensuring sufficient decrease."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "Exact line search always outperforms backtracking line search in practice.",
        "correctAnswer": false,
        "explanation": "Exact line search is expensive to compute and backtracking is often more efficient overall despite using more iterations."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "BFGS is a:",
        "options": [
          "First-order method",
          "Quasi-Newton method",
          "Zero-order method",
          "Exact Newton method"
        ],
        "correctAnswer": 1,
        "explanation": "BFGS is a quasi-Newton method that approximates the Hessian using gradient information."
      },
      {
        "id": "q4",
        "type": "multiple_choice",
        "prompt": "For strongly convex functions with condition number κ, gradient descent converges as:",
        "options": [
          "O(1/k)",
          "O((κ-1)/(κ+1))^k",
          "O(1/k²)",
          "O(e^{-k})"
        ],
        "correctAnswer": 1,
        "explanation": "For μ-strongly convex, L-smooth functions, GD achieves linear convergence with rate ((κ-1)/(κ+1))^k where κ = L/μ."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "The _____ method uses only function evaluations, no gradients.",
        "correctAnswer": "Nelder-Mead",
        "explanation": "Nelder-Mead (simplex search) is a zero-order method using only function evaluations, suitable when gradients are unavailable."
      }
    ]
  },
  {
    "id": "math404-quiz-6c",
    "subjectId": "math404",
    "topicId": "math404-topic-6",
    "title": "Gradient Methods - Mastery",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "L-BFGS improves upon BFGS by:",
        "options": [
          "Using exact Hessian",
          "Storing only recent gradient pairs",
          "Using higher-order derivatives",
          "Requiring fewer iterations"
        ],
        "correctAnswer": 1,
        "explanation": "L-BFGS stores only the last m gradient differences, making it suitable for large-scale problems with O(mn) memory."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "Stochastic gradient descent uses the exact gradient at each iteration.",
        "correctAnswer": false,
        "explanation": "SGD uses stochastic gradient estimates (e.g., from mini-batches), not the full exact gradient."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "Coordinate descent updates:",
        "options": [
          "All variables simultaneously",
          "One variable at a time",
          "Random subsets of variables",
          "Only the largest gradient component"
        ],
        "correctAnswer": 1,
        "explanation": "Coordinate descent minimizes over one coordinate at a time, cycling or randomly selecting coordinates."
      },
      {
        "id": "q4",
        "type": "multiple_choice",
        "prompt": "The Wolfe conditions combine Armijo with a:",
        "options": [
          "Gradient condition",
          "Curvature condition",
          "Feasibility condition",
          "Optimality condition"
        ],
        "correctAnswer": 1,
        "explanation": "Wolfe conditions add a curvature condition |∇f(x+αd)ᵀd| ≤ c₂|∇f(x)ᵀd| to Armijo to ensure sufficient progress."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "For non-convex smooth functions, gradient descent converges to a _____ point.",
        "correctAnswer": "stationary",
        "explanation": "For non-convex problems, GD converges to stationary points where ∇f(x) = 0, which may be local minima, maxima, or saddle points."
      }
    ]
  },
  {
    "id": "math404-quiz-7a",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "title": "Constrained Optimization - Fundamentals",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "What does complementary slackness require?",
        "options": [
          "All constraints are active",
          "All multipliers are positive",
          "λᵢgᵢ(x) = 0 for all i",
          "The gradient is zero"
        ],
        "correctAnswer": 2,
        "explanation": "Complementary slackness: either the constraint is active (gᵢ = 0) or its multiplier is zero (λᵢ = 0)."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "KKT conditions are sufficient for optimality in convex problems.",
        "correctAnswer": true,
        "explanation": "For convex problems with constraint qualification, KKT conditions are both necessary and sufficient."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "The Lagrangian for min f(x) s.t. g(x) ≤ 0, h(x) = 0 is:",
        "options": [
          "L = f(x) + λᵀg(x)",
          "L = f(x) + λᵀg(x) + μᵀh(x)",
          "L = f(x) - λᵀg(x)",
          "L = f(x) + μᵀh(x)"
        ],
        "correctAnswer": 1,
        "explanation": "The Lagrangian is L(x,λ,μ) = f(x) + λᵀg(x) + μᵀh(x), with λ ≥ 0 for inequalities."
      },
      {
        "id": "q4",
        "type": "fill_blank",
        "prompt": "The KKT stationarity condition requires ∇_x L(x*,λ*,μ*) = _____.",
        "correctAnswer": "0",
        "explanation": "Stationarity requires the gradient of the Lagrangian with respect to x to be zero at the optimal point."
      },
      {
        "id": "q5",
        "type": "true_false",
        "prompt": "Inequality constraint multipliers in KKT must be non-negative.",
        "correctAnswer": true,
        "explanation": "For inequality constraints g(x) ≤ 0, the dual variables λ must satisfy λ ≥ 0 (dual feasibility)."
      }
    ]
  },
  {
    "id": "math404-quiz-7b",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "title": "Constrained Optimization - Application",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "The augmented Lagrangian method adds what to the Lagrangian?",
        "options": [
          "A linear penalty term",
          "A quadratic penalty term",
          "An exponential term",
          "A logarithmic barrier"
        ],
        "correctAnswer": 1,
        "explanation": "Augmented Lagrangian adds (ρ/2)||g(x)||² to improve conditioning while maintaining exact penalty at convergence."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "Penalty methods require penalty parameter ρ → ∞ for exact solutions.",
        "correctAnswer": true,
        "explanation": "Pure penalty methods require ρ → ∞ to enforce constraints exactly, which can cause ill-conditioning."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "Interior-point methods for NLP use:",
        "options": [
          "Exact penalties",
          "Barrier functions",
          "Active set strategies",
          "Gradient projection"
        ],
        "correctAnswer": 1,
        "explanation": "Interior-point methods use barrier functions (e.g., -log(-g(x))) to keep iterates in the interior of the feasible region."
      },
      {
        "id": "q4",
        "type": "multiple_choice",
        "prompt": "Sequential Quadratic Programming (SQP) solves a sequence of:",
        "options": [
          "Linear programs",
          "Quadratic programs",
          "Unconstrained problems",
          "Integer programs"
        ],
        "correctAnswer": 1,
        "explanation": "SQP solves QP subproblems that locally approximate the original NLP using quadratic models."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "The method of Lagrange multipliers applies to problems with _____ constraints only.",
        "correctAnswer": "equality",
        "explanation": "Classical Lagrange multipliers handle equality constraints. The KKT conditions generalize to include inequality constraints."
      }
    ]
  },
  {
    "id": "math404-quiz-7c",
    "subjectId": "math404",
    "topicId": "math404-topic-7",
    "title": "Constrained Optimization - Mastery",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "LICQ (Linear Independence Constraint Qualification) requires:",
        "options": [
          "All constraints are linear",
          "Gradients of active constraints are linearly independent",
          "The feasible region is convex",
          "All multipliers are positive"
        ],
        "correctAnswer": 1,
        "explanation": "LICQ requires linear independence of gradients of active inequality constraints and all equality constraints at x*."
      },
      {
        "id": "q2",
        "type": "true_false",
        "prompt": "If KKT conditions hold but constraint qualification fails, x* may not be optimal.",
        "correctAnswer": false,
        "explanation": "KKT conditions are sufficient for optimality in convex problems regardless of constraint qualification. CQ affects necessity."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "The central path in interior-point methods:",
        "options": [
          "Lies on the boundary",
          "Connects all vertices",
          "Passes through the interior approaching the optimum",
          "Is always a straight line"
        ],
        "correctAnswer": 2,
        "explanation": "The central path parameterized by barrier parameter μ lies in the interior and converges to the optimum as μ → 0."
      },
      {
        "id": "q4",
        "type": "multiple_choice",
        "prompt": "The QP subproblem in SQP approximates the original problem using:",
        "options": [
          "Linear objective and linear constraints",
          "Quadratic objective and linearized constraints",
          "Linear objective and quadratic constraints",
          "Quadratic objective and constraints"
        ],
        "correctAnswer": 1,
        "explanation": "SQP uses quadratic approximation of the objective (using Hessian or quasi-Newton) and first-order Taylor expansion of constraints."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "ALM (Augmented Lagrangian Method) is also known as the method of _____.",
        "correctAnswer": "multipliers",
        "explanation": "The Augmented Lagrangian Method is also called the Method of Multipliers, combining penalty methods with Lagrange multipliers."
      }
    ]
  }
]