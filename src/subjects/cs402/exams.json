[
  {
    "id": "cs402-exam-midterm",
    "subjectId": "cs402",
    "title": "CS402 Machine Learning Midterm Examination",
    "durationMinutes": 90,
    "instructions": [
      "This exam covers Topics 1-4: ML Overview, Linear/Logistic Regression, Classification, and Neural Networks.",
      "Answer all questions. There are 26 questions worth equal marks.",
      "For multiple choice, select the single best answer.",
      "Passing score is 65%.",
      "You may use a calculator for numerical questions."
    ],
    "questions": [
      {
        "id": "cs402-mid-q1",
        "type": "multiple_choice",
        "prompt": "What is the primary difference between supervised and unsupervised learning?",
        "options": [
          "Supervised learning does not require a training phase",
          "Unsupervised learning always produces better results",
          "Supervised learning uses labeled data; unsupervised learning does not",
          "Supervised learning is faster than unsupervised learning"
        ],
        "correctAnswer": 2,
        "explanation": "Supervised learning trains on input-output pairs (labeled data), while unsupervised learning finds patterns in unlabeled data without predefined targets."
      },
      {
        "id": "cs402-mid-q2",
        "type": "multiple_choice",
        "prompt": "Which of the following is NOT a common step in the machine learning workflow?",
        "options": [
          "Feature engineering",
          "Model evaluation and validation",
          "Data collection and preprocessing",
          "Manual code optimization for every prediction"
        ],
        "correctAnswer": 3,
        "explanation": "The ML workflow includes data collection, preprocessing, feature engineering, model training, and evaluation. Manual code optimization per prediction is not a standard ML workflow step."
      },
      {
        "id": "cs402-mid-q3",
        "type": "multiple_choice",
        "prompt": "In the bias-variance tradeoff, a model with high bias typically exhibits which behavior?",
        "options": [
          "High sensitivity to small changes in training data",
          "Perfect generalization on unseen data",
          "Underfitting: poor performance on both training and test data",
          "Overfitting: good training performance but poor test performance"
        ],
        "correctAnswer": 2,
        "explanation": "High bias models are too simple and underfit the data, performing poorly on both training and test sets. High variance models overfit."
      },
      {
        "id": "cs402-mid-q4",
        "type": "true_false",
        "prompt": "Feature scaling (normalization/standardization) is always necessary for all machine learning algorithms.",
        "correctAnswer": false,
        "explanation": "Feature scaling is important for distance-based algorithms (KNN, SVM) and gradient descent-based methods, but tree-based algorithms (Random Forest, XGBoost) are scale-invariant."
      },
      {
        "id": "cs402-mid-q5",
        "type": "multiple_choice",
        "prompt": "The cost function for linear regression (ordinary least squares) is:",
        "options": [
          "Cross-entropy loss",
          "Absolute error without squaring",
          "Hinge loss",
          "Mean Squared Error: J(w) = (1/2n) * sum((y - Xw)^2)"
        ],
        "correctAnswer": 3,
        "explanation": "Linear regression minimizes the Mean Squared Error (MSE), which measures the average squared difference between predictions and actual values."
      },
      {
        "id": "cs402-mid-q6",
        "type": "multiple_choice",
        "prompt": "What is the purpose of regularization in linear regression?",
        "options": [
          "To remove the need for feature scaling",
          "To increase model complexity",
          "To speed up training",
          "To prevent overfitting by penalizing large coefficients"
        ],
        "correctAnswer": 3,
        "explanation": "Regularization (L1/L2) adds a penalty term for large weights, preventing overfitting by constraining model complexity. L1 (Lasso) can also perform feature selection."
      },
      {
        "id": "cs402-mid-q7",
        "type": "multiple_choice",
        "prompt": "In L1 regularization (Lasso), which property makes it useful for feature selection?",
        "options": [
          "It always produces larger coefficients than L2",
          "It works only with categorical features",
          "It tends to produce sparse solutions with many coefficients exactly zero",
          "It requires less computation than L2"
        ],
        "correctAnswer": 2,
        "explanation": "L1 regularization produces sparse solutions by driving some coefficients to exactly zero, effectively performing automatic feature selection."
      },
      {
        "id": "cs402-mid-q8",
        "type": "multiple_choice",
        "prompt": "What transformation does logistic regression apply to the linear combination of features?",
        "options": [
          "Hyperbolic tangent: tanh(z)",
          "Sigmoid function: sigma(z) = 1 / (1 + e^(-z))",
          "No transformation; output is linear",
          "ReLU function: max(0, z)"
        ],
        "correctAnswer": 1,
        "explanation": "Logistic regression applies the sigmoid function to map the linear combination to a probability between 0 and 1."
      },
      {
        "id": "cs402-mid-q9",
        "type": "multiple_choice",
        "prompt": "The loss function used for logistic regression is:",
        "options": [
          "Mean squared error",
          "Binary cross-entropy (log loss)",
          "Huber loss",
          "Hinge loss"
        ],
        "correctAnswer": 1,
        "explanation": "Logistic regression uses binary cross-entropy loss, which measures the difference between predicted probabilities and actual binary labels."
      },
      {
        "id": "cs402-mid-q10",
        "type": "code_output",
        "prompt": "What is the predicted probability for the positive class?",
        "codeSnippet": "import numpy as np\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\nweights = np.array([0.5, -0.3, 0.8])\nbias = -0.2\nx = np.array([1.0, 2.0, 0.5])\n\nz = np.dot(weights, x) + bias\nprob = sigmoid(z)\nprint(f\"{prob:.4f}\")",
        "correctAnswer": "0.5250",
        "explanation": "z = 0.5*1 + (-0.3)*2 + 0.8*0.5 + (-0.2) = 0.5 - 0.6 + 0.4 - 0.2 = 0.1. sigmoid(0.1) = 1/(1+e^(-0.1)) â‰ˆ 0.5250"
      },
      {
        "id": "cs402-mid-q11",
        "type": "multiple_choice",
        "prompt": "What is the main advantage of decision trees over logistic regression?",
        "options": [
          "They can model non-linear relationships without explicit feature engineering",
          "They always have higher accuracy",
          "They are immune to overfitting",
          "They require less training data"
        ],
        "correctAnswer": 0,
        "explanation": "Decision trees can capture non-linear relationships and interactions automatically through their hierarchical splitting structure, without requiring manual feature engineering."
      },
      {
        "id": "cs402-mid-q12",
        "type": "multiple_choice",
        "prompt": "Which criterion is commonly used for splitting in classification decision trees?",
        "options": [
          "Euclidean distance",
          "Mean squared error",
          "Gini impurity or information gain (entropy)",
          "Cosine similarity"
        ],
        "correctAnswer": 2,
        "explanation": "Classification trees use Gini impurity or information gain (based on entropy) to find splits that best separate classes."
      },
      {
        "id": "cs402-mid-q13",
        "type": "multiple_choice",
        "prompt": "What technique does Random Forest use to reduce overfitting compared to a single decision tree?",
        "options": [
          "Using deeper trees",
          "Bagging (bootstrap aggregating) with feature randomization",
          "Training on more data",
          "Using a single best feature at each split"
        ],
        "correctAnswer": 1,
        "explanation": "Random Forest uses bagging (training on bootstrap samples) and random feature subsets at each split, reducing correlation between trees and variance in predictions."
      },
      {
        "id": "cs402-mid-q14",
        "type": "true_false",
        "prompt": "In a Random Forest, increasing the number of trees always leads to overfitting.",
        "correctAnswer": false,
        "explanation": "Unlike single decision trees, Random Forests do not overfit as more trees are added. More trees reduce variance and generally improve or stabilize performance."
      },
      {
        "id": "cs402-mid-q15",
        "type": "multiple_choice",
        "prompt": "Support Vector Machines (SVM) find the hyperplane that:",
        "options": [
          "Minimizes the total distance to all points",
          "Maximizes the margin between classes",
          "Passes through the centroid of each class",
          "Has the smallest number of support vectors"
        ],
        "correctAnswer": 1,
        "explanation": "SVMs find the hyperplane that maximizes the margin (distance) between the closest points of each class (support vectors)."
      },
      {
        "id": "cs402-mid-q16",
        "type": "multiple_choice",
        "prompt": "What is the purpose of the kernel trick in SVM?",
        "options": [
          "To speed up training by reducing dimensions",
          "To implicitly map data to higher dimensions where it becomes linearly separable",
          "To make predictions faster",
          "To eliminate the need for the margin"
        ],
        "correctAnswer": 1,
        "explanation": "The kernel trick allows SVMs to compute dot products in high-dimensional feature spaces without explicitly computing the transformation, enabling non-linear decision boundaries."
      },
      {
        "id": "cs402-mid-q17",
        "type": "multiple_choice",
        "prompt": "In a neural network, what is the purpose of the activation function?",
        "options": [
          "To normalize the input data",
          "To speed up training",
          "To introduce non-linearity, allowing the network to learn complex patterns",
          "To reduce the number of parameters"
        ],
        "correctAnswer": 2,
        "explanation": "Activation functions introduce non-linearity. Without them, a multi-layer network would collapse to a single linear transformation, unable to learn complex patterns."
      },
      {
        "id": "cs402-mid-q18",
        "type": "multiple_choice",
        "prompt": "Which activation function is most commonly used in hidden layers of modern deep networks?",
        "options": [
          "Sigmoid",
          "ReLU (Rectified Linear Unit)",
          "Step function",
          "Tanh"
        ],
        "correctAnswer": 1,
        "explanation": "ReLU (max(0, x)) is preferred for hidden layers because it doesn't saturate for positive values, reducing vanishing gradient problems and being computationally efficient."
      },
      {
        "id": "cs402-mid-q19",
        "type": "multiple_choice",
        "prompt": "What does backpropagation compute?",
        "options": [
          "The number of hidden layers needed",
          "The optimal learning rate",
          "Gradients of the loss with respect to all weights using the chain rule",
          "The activation values of all neurons"
        ],
        "correctAnswer": 2,
        "explanation": "Backpropagation efficiently computes gradients of the loss function with respect to all network weights by applying the chain rule from output to input layers."
      },
      {
        "id": "cs402-mid-q20",
        "type": "code_output",
        "prompt": "What is the output of this ReLU activation?",
        "codeSnippet": "import numpy as np\n\ndef relu(x):\n    return np.maximum(0, x)\n\ninput_values = np.array([-2.0, 0.0, 3.5, -0.5, 1.0])\noutput = relu(input_values)\nprint(output)",
        "correctAnswer": "[0.  0.  3.5 0.  1. ]",
        "explanation": "ReLU outputs max(0, x) for each element: -2->0, 0->0, 3.5->3.5, -0.5->0, 1->1."
      },
      {
        "id": "cs402-mid-q21",
        "type": "multiple_choice",
        "prompt": "What is the 'vanishing gradient problem' in deep neural networks?",
        "options": [
          "Gradients become too large, causing numerical overflow",
          "Gradients become extremely small in early layers, preventing effective learning",
          "The network produces zero output",
          "Training takes too long"
        ],
        "correctAnswer": 1,
        "explanation": "With certain activation functions (sigmoid, tanh), gradients can become very small when backpropagated through many layers, making it hard for early layers to learn."
      },
      {
        "id": "cs402-mid-q22",
        "type": "multiple_choice",
        "prompt": "Which optimizer adapts the learning rate for each parameter based on historical gradients?",
        "options": [
          "L-BFGS",
          "Adam (Adaptive Moment Estimation)",
          "Coordinate descent",
          "Standard gradient descent"
        ],
        "correctAnswer": 1,
        "explanation": "Adam maintains per-parameter learning rates by tracking first and second moments of gradients, adapting step sizes for faster and more stable convergence."
      },
      {
        "id": "cs402-mid-q23",
        "type": "true_false",
        "prompt": "Dropout randomly sets neurons to zero during training to prevent overfitting.",
        "correctAnswer": true,
        "explanation": "Dropout randomly 'drops' neurons during training with probability p, preventing co-adaptation and acting as an ensemble of smaller networks, reducing overfitting."
      },
      {
        "id": "cs402-mid-q24",
        "type": "multiple_choice",
        "prompt": "What is the softmax function used for in neural networks?",
        "options": [
          "Preventing vanishing gradients",
          "Converting output scores to probabilities that sum to 1 for multi-class classification",
          "Reducing overfitting",
          "Normalizing input features"
        ],
        "correctAnswer": 1,
        "explanation": "Softmax converts raw output scores (logits) into probabilities for multi-class classification by exponentiating and normalizing: softmax(z)_i = exp(z_i) / sum(exp(z_j))."
      },
      {
        "id": "cs402-mid-q25",
        "type": "multiple_choice",
        "prompt": "Batch normalization in neural networks:",
        "options": [
          "Eliminates the need for activation functions",
          "Only works during inference, not training",
          "Reduces the batch size to prevent overfitting",
          "Normalizes layer inputs to reduce internal covariate shift and enable higher learning rates"
        ],
        "correctAnswer": 3,
        "explanation": "Batch normalization normalizes activations within mini-batches, reducing internal covariate shift, allowing higher learning rates, and having a regularization effect."
      },
      {
        "id": "cs402-mid-q26",
        "type": "multiple_choice",
        "prompt": "When training a neural network, what is the purpose of the validation set?",
        "options": [
          "To normalize the training data",
          "To tune hyperparameters and monitor for overfitting without touching the test set",
          "To provide additional training examples",
          "To replace the test set for final evaluation"
        ],
        "correctAnswer": 1,
        "explanation": "The validation set is used during training to tune hyperparameters and detect overfitting, keeping the test set completely unseen for final unbiased evaluation."
      }
    ]
  },
  {
    "id": "cs402-exam-final",
    "subjectId": "cs402",
    "title": "CS402 Machine Learning Final Examination",
    "durationMinutes": 150,
    "instructions": [
      "This is a comprehensive exam covering all 7 topics in the Machine Learning course.",
      "Answer all questions. There are 42 questions worth equal marks.",
      "For multiple choice, select the single best answer.",
      "Passing score is 65%.",
      "Topics covered: ML Overview, Regression, Classification, Neural Networks, Deep Learning, Unsupervised Learning, and Model Evaluation."
    ],
    "questions": [
      {
        "id": "cs402-final-q1",
        "type": "multiple_choice",
        "prompt": "What is the primary advantage of deep learning over traditional machine learning methods?",
        "options": [
          "No need for labeled data",
          "Simpler model architectures",
          "Faster training on small datasets",
          "Automatic hierarchical feature learning from raw data"
        ],
        "correctAnswer": 3,
        "explanation": "Deep learning automatically discovers hierarchical representations, eliminating manual feature engineering required by traditional ML methods."
      },
      {
        "id": "cs402-final-q2",
        "type": "multiple_choice",
        "prompt": "According to the No Free Lunch theorem:",
        "options": [
          "More data always improves model performance",
          "No algorithm is universally best across all possible problems",
          "Deep learning is always better than classical ML",
          "Ensemble methods always outperform individual models"
        ],
        "correctAnswer": 1,
        "explanation": "The No Free Lunch theorem states that averaged over all possible problems, no algorithm outperforms random guessing. Algorithm selection must be problem-specific."
      },
      {
        "id": "cs402-final-q3",
        "type": "multiple_choice",
        "prompt": "Polynomial regression with degree 10 on 20 training points is likely to exhibit:",
        "options": [
          "No training error",
          "High bias (underfitting)",
          "Perfect generalization",
          "High variance (overfitting)"
        ],
        "correctAnswer": 3,
        "explanation": "A 10-degree polynomial has high capacity relative to 20 points, likely overfitting the training data (high variance) and generalizing poorly."
      },
      {
        "id": "cs402-final-q4",
        "type": "multiple_choice",
        "prompt": "The closed-form solution for linear regression weights is:",
        "options": [
          "w = (X^T X)^(-1) X^T y",
          "w = X^T y",
          "w = y / X",
          "w = X^(-1) y"
        ],
        "correctAnswer": 0,
        "explanation": "The normal equation w = (X^T X)^(-1) X^T y gives the optimal weights that minimize mean squared error in closed form."
      },
      {
        "id": "cs402-final-q5",
        "type": "multiple_choice",
        "prompt": "Ridge regression (L2 regularization) modifies the normal equation to:",
        "options": [
          "w = (X^T X + lambda*X)^(-1) y",
          "w = (X^T X + lambda*I)^(-1) X^T y",
          "w = (X^T X)^(-1) X^T y + lambda",
          "w = X^T y / lambda"
        ],
        "correctAnswer": 1,
        "explanation": "Ridge regression adds lambda*I to X^T X before inversion, shrinking coefficients toward zero and ensuring the matrix is invertible."
      },
      {
        "id": "cs402-final-q6",
        "type": "true_false",
        "prompt": "Logistic regression can only be used for binary classification.",
        "correctAnswer": false,
        "explanation": "While binary logistic regression is common, multinomial (softmax) regression extends it to multi-class classification with K classes."
      },
      {
        "id": "cs402-final-q7",
        "type": "multiple_choice",
        "prompt": "The decision boundary of logistic regression is:",
        "options": [
          "Determined by the kernel function",
          "Always curved",
          "A linear hyperplane in feature space",
          "A circle around one class"
        ],
        "correctAnswer": 2,
        "explanation": "Logistic regression produces a linear decision boundary where w^T x + b = 0. The sigmoid function provides probabilities but doesn't change the boundary shape."
      },
      {
        "id": "cs402-final-q8",
        "type": "multiple_choice",
        "prompt": "Gini impurity for a node with 50% class A and 50% class B is:",
        "options": [
          "0.5",
          "1.0",
          "0.0",
          "0.25"
        ],
        "correctAnswer": 0,
        "explanation": "Gini = 1 - sum(p_i^2) = 1 - (0.5^2 + 0.5^2) = 1 - 0.5 = 0.5. Maximum impurity occurs with equal class proportions."
      },
      {
        "id": "cs402-final-q9",
        "type": "multiple_choice",
        "prompt": "What is the main difference between bagging and boosting?",
        "options": [
          "Bagging trains models independently in parallel; boosting trains sequentially focusing on errors",
          "Bagging uses decision trees; boosting uses neural networks",
          "Bagging reduces bias; boosting reduces variance",
          "They are identical ensemble methods"
        ],
        "correctAnswer": 0,
        "explanation": "Bagging trains independent models on bootstrap samples (reducing variance). Boosting trains sequential models where each focuses on previous errors (reducing bias)."
      },
      {
        "id": "cs402-final-q10",
        "type": "multiple_choice",
        "prompt": "In SVM, the parameter C controls:",
        "options": [
          "The learning rate",
          "The kernel function type",
          "The tradeoff between margin size and classification errors",
          "The number of support vectors"
        ],
        "correctAnswer": 2,
        "explanation": "Large C prioritizes correct classification (smaller margin, potential overfitting). Small C allows more errors for larger margin (potential underfitting)."
      },
      {
        "id": "cs402-final-q11",
        "type": "code_output",
        "prompt": "What shape will the output tensor have?",
        "codeSnippet": "import numpy as np\n\n# Input: (batch=32, height=28, width=28, channels=1)\n# Conv2d: 16 filters, 3x3 kernel, stride=1, padding='same'\n# MaxPool: 2x2, stride=2\n\nprint(\"After conv: (32, 28, 28, 16)\")\nprint(\"After pool: (32, 14, 14, 16)\")",
        "correctAnswer": "After conv: (32, 28, 28, 16)\nAfter pool: (32, 14, 14, 16)",
        "explanation": "With 'same' padding and stride 1, conv keeps spatial dims. 16 filters give 16 channels. MaxPool 2x2 stride 2 halves spatial dimensions."
      },
      {
        "id": "cs402-final-q12",
        "type": "multiple_choice",
        "prompt": "What is the purpose of pooling layers in CNNs?",
        "options": [
          "To increase the number of channels",
          "To normalize activations",
          "To reduce spatial dimensions and provide translation invariance",
          "To add non-linearity"
        ],
        "correctAnswer": 2,
        "explanation": "Pooling reduces spatial dimensions (reducing computation and parameters) and provides some translation invariance by summarizing local regions."
      },
      {
        "id": "cs402-final-q13",
        "type": "multiple_choice",
        "prompt": "ResNet introduced skip connections (residual connections) to solve:",
        "options": [
          "Memory constraints",
          "The degradation problem in very deep networks",
          "Vanishing gradients only",
          "Computational efficiency"
        ],
        "correctAnswer": 1,
        "explanation": "Skip connections allow training very deep networks by enabling gradient flow and letting layers learn residual mappings, solving the degradation problem."
      },
      {
        "id": "cs402-final-q14",
        "type": "multiple_choice",
        "prompt": "In a basic RNN, what problem limits its ability to learn long-range dependencies?",
        "options": [
          "Too few parameters",
          "Lack of non-linearity",
          "Sequential processing speed",
          "Vanishing/exploding gradients during backpropagation through time"
        ],
        "correctAnswer": 3,
        "explanation": "In basic RNNs, gradients can vanish or explode when backpropagated through many time steps, making it hard to learn long-range dependencies."
      },
      {
        "id": "cs402-final-q15",
        "type": "multiple_choice",
        "prompt": "LSTMs address the vanishing gradient problem using:",
        "options": [
          "Higher learning rates",
          "Larger hidden layers",
          "Bidirectional processing",
          "Gating mechanisms that control information flow through cell state"
        ],
        "correctAnswer": 3,
        "explanation": "LSTMs use forget, input, and output gates to control what information to keep or discard, allowing gradients to flow through the cell state unimpeded."
      },
      {
        "id": "cs402-final-q16",
        "type": "multiple_choice",
        "prompt": "What is the key innovation of the Transformer architecture?",
        "options": [
          "Deeper networks with residual connections",
          "Convolutional layers for sequence processing",
          "Recurrent connections with gating",
          "Self-attention mechanism that processes all positions in parallel"
        ],
        "correctAnswer": 3,
        "explanation": "Transformers use self-attention to compute relationships between all positions simultaneously, eliminating sequential dependencies and enabling parallelization."
      },
      {
        "id": "cs402-final-q17",
        "type": "multiple_choice",
        "prompt": "The scaled dot-product attention formula is:",
        "options": [
          "Attention(Q,K,V) = tanh(QK^T) * V",
          "Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V",
          "Attention(Q,K,V) = Q + K + V",
          "Attention(Q,K,V) = sigmoid(QKV)"
        ],
        "correctAnswer": 1,
        "explanation": "Attention computes similarity via QK^T, scales by sqrt(d_k) for stable gradients, applies softmax for weights, then multiplies by values V."
      },
      {
        "id": "cs402-final-q18",
        "type": "true_false",
        "prompt": "Multi-head attention allows the model to attend to information from different representation subspaces.",
        "correctAnswer": true,
        "explanation": "Multi-head attention runs multiple attention operations in parallel with different learned projections, allowing the model to focus on different aspects of the input."
      },
      {
        "id": "cs402-final-q19",
        "type": "multiple_choice",
        "prompt": "Transfer learning in deep learning typically involves:",
        "options": [
          "Using the same architecture for all problems",
          "Randomly initializing weights",
          "Transferring data between training and test sets",
          "Using a pre-trained model and fine-tuning it for a new task"
        ],
        "correctAnswer": 3,
        "explanation": "Transfer learning leverages models pre-trained on large datasets (e.g., ImageNet), then fine-tunes them for specific tasks, dramatically reducing data and training requirements."
      },
      {
        "id": "cs402-final-q20",
        "type": "multiple_choice",
        "prompt": "What distinguishes BERT from GPT in their Transformer usage?",
        "options": [
          "BERT is smaller than GPT",
          "BERT uses RNNs; GPT uses Transformers",
          "They are architecturally identical",
          "BERT uses bidirectional encoder; GPT uses unidirectional decoder"
        ],
        "correctAnswer": 3,
        "explanation": "BERT uses the Transformer encoder with bidirectional context. GPT uses the decoder with causal (left-to-right) masking for autoregressive generation."
      },
      {
        "id": "cs402-final-q21",
        "type": "multiple_choice",
        "prompt": "K-means clustering minimizes:",
        "options": [
          "Within-cluster sum of squared distances to centroids",
          "Between-cluster distances",
          "The number of clusters",
          "The maximum distance between any two points"
        ],
        "correctAnswer": 0,
        "explanation": "K-means minimizes the within-cluster sum of squares (inertia): the total squared distance from each point to its assigned cluster centroid."
      },
      {
        "id": "cs402-final-q22",
        "type": "multiple_choice",
        "prompt": "Which clustering algorithm can discover clusters of arbitrary shape?",
        "options": [
          "K-medoids",
          "DBSCAN (Density-Based Spatial Clustering)",
          "K-means",
          "Gaussian Mixture Models (spherical covariance)"
        ],
        "correctAnswer": 1,
        "explanation": "DBSCAN uses density-based clustering, connecting points in dense regions regardless of shape. K-means assumes spherical clusters around centroids."
      },
      {
        "id": "cs402-final-q23",
        "type": "multiple_choice",
        "prompt": "In hierarchical clustering, the 'linkage' criterion determines:",
        "options": [
          "The order of data points",
          "The final number of clusters",
          "Whether to use agglomerative or divisive approach",
          "How distance between clusters is computed when merging"
        ],
        "correctAnswer": 3,
        "explanation": "Linkage (single, complete, average, Ward's) defines how inter-cluster distance is measured when deciding which clusters to merge."
      },
      {
        "id": "cs402-final-q24",
        "type": "multiple_choice",
        "prompt": "PCA (Principal Component Analysis) finds directions that:",
        "options": [
          "Maximize variance in the projected data",
          "Minimize the number of features",
          "Maximize classification accuracy",
          "Preserve cluster structure"
        ],
        "correctAnswer": 0,
        "explanation": "PCA finds orthogonal directions (principal components) that capture maximum variance in the data, enabling dimensionality reduction with minimal information loss."
      },
      {
        "id": "cs402-final-q25",
        "type": "true_false",
        "prompt": "PCA components are always orthogonal to each other.",
        "correctAnswer": true,
        "explanation": "By construction, principal components are orthogonal (perpendicular) eigenvectors of the covariance matrix, ensuring uncorrelated projections."
      },
      {
        "id": "cs402-final-q26",
        "type": "multiple_choice",
        "prompt": "t-SNE is primarily used for:",
        "options": [
          "Visualization of high-dimensional data in 2D or 3D",
          "Feature selection",
          "Classification",
          "Regression"
        ],
        "correctAnswer": 0,
        "explanation": "t-SNE is a non-linear dimensionality reduction technique designed for visualization, preserving local neighborhood structure in low dimensions."
      },
      {
        "id": "cs402-final-q27",
        "type": "multiple_choice",
        "prompt": "What does the silhouette score measure in clustering?",
        "options": [
          "The optimal number of clusters directly",
          "The number of outliers",
          "How similar points are to their own cluster compared to other clusters",
          "Computational complexity"
        ],
        "correctAnswer": 2,
        "explanation": "Silhouette score measures cluster cohesion (intra-cluster distance) vs separation (nearest-cluster distance). Range [-1, 1], higher is better."
      },
      {
        "id": "cs402-final-q28",
        "type": "multiple_choice",
        "prompt": "When is accuracy a misleading metric for classification?",
        "options": [
          "When classes are highly imbalanced",
          "When there are exactly two classes",
          "When using cross-validation",
          "When the model is a neural network"
        ],
        "correctAnswer": 0,
        "explanation": "With imbalanced classes, a model predicting only the majority class achieves high accuracy while being useless. Use precision, recall, F1, or AUC instead."
      },
      {
        "id": "cs402-final-q29",
        "type": "multiple_choice",
        "prompt": "Precision is defined as:",
        "options": [
          "(TP + TN) / Total",
          "TP / (TP + FN) - Of actual positives, how many were found",
          "TP / (TP + FP) - Of predicted positives, how many are correct",
          "FP / (FP + TN)"
        ],
        "correctAnswer": 2,
        "explanation": "Precision = TP/(TP+FP) measures the fraction of predicted positives that are actually positive. High precision means few false alarms."
      },
      {
        "id": "cs402-final-q30",
        "type": "multiple_choice",
        "prompt": "Recall (Sensitivity) is important when:",
        "options": [
          "Missing positive cases is very costly (e.g., disease detection)",
          "False positives are costly",
          "Classes are perfectly balanced",
          "The model is simple"
        ],
        "correctAnswer": 0,
        "explanation": "Recall = TP/(TP+FN) measures the fraction of actual positives detected. High recall is critical when missing cases is dangerous (medical, fraud)."
      },
      {
        "id": "cs402-final-q31",
        "type": "multiple_choice",
        "prompt": "The F1 score is:",
        "options": [
          "The harmonic mean of precision and recall",
          "The arithmetic mean of precision and recall",
          "Precision times recall",
          "Accuracy for imbalanced datasets"
        ],
        "correctAnswer": 0,
        "explanation": "F1 = 2 * (Precision * Recall) / (Precision + Recall) is the harmonic mean, balancing precision and recall. It's low if either is low."
      },
      {
        "id": "cs402-final-q32",
        "type": "multiple_choice",
        "prompt": "ROC-AUC measures:",
        "options": [
          "Training time efficiency",
          "The probability that a random positive ranks higher than a random negative",
          "The F1 score averaged over thresholds",
          "Classification accuracy at the optimal threshold"
        ],
        "correctAnswer": 1,
        "explanation": "AUC equals the probability that a randomly chosen positive example has a higher predicted probability than a randomly chosen negative example."
      },
      {
        "id": "cs402-final-q33",
        "type": "true_false",
        "prompt": "For highly imbalanced data, precision-recall curves are often more informative than ROC curves.",
        "correctAnswer": true,
        "explanation": "ROC can be overly optimistic for imbalanced data due to large TN count inflating specificity. PR curves focus on the positive class performance."
      },
      {
        "id": "cs402-final-q34",
        "type": "multiple_choice",
        "prompt": "Mean Squared Error (MSE) vs Mean Absolute Error (MAE): MSE is more sensitive to:",
        "options": [
          "Computational speed",
          "Small errors",
          "Outliers and large errors",
          "The number of features"
        ],
        "correctAnswer": 2,
        "explanation": "MSE squares errors, so large errors are penalized quadratically. MAE treats all errors linearly, making it more robust to outliers."
      },
      {
        "id": "cs402-final-q35",
        "type": "multiple_choice",
        "prompt": "R-squared (R^2) represents:",
        "options": [
          "The number of features used",
          "The mean absolute error",
          "The correlation between predictions and actuals",
          "The proportion of variance in y explained by the model"
        ],
        "correctAnswer": 3,
        "explanation": "R^2 = 1 - (SS_res/SS_tot) measures what fraction of target variance is explained by the model. 1 is perfect, 0 means no better than predicting the mean."
      },
      {
        "id": "cs402-final-q36",
        "type": "multiple_choice",
        "prompt": "K-fold cross-validation provides:",
        "options": [
          "Faster training than a single split",
          "More robust performance estimates by averaging over multiple train-test splits",
          "Automatic hyperparameter tuning",
          "A way to increase training data size"
        ],
        "correctAnswer": 1,
        "explanation": "K-fold CV trains and tests k times on different splits, averaging results for a more reliable performance estimate with lower variance."
      },
      {
        "id": "cs402-final-q37",
        "type": "multiple_choice",
        "prompt": "Grid search for hyperparameter tuning:",
        "options": [
          "Uses Bayesian optimization",
          "Exhaustively tries all combinations in a predefined parameter grid",
          "Only works for neural networks",
          "Randomly samples parameter combinations"
        ],
        "correctAnswer": 1,
        "explanation": "Grid search evaluates every combination in the Cartesian product of parameter values. Thorough but computationally expensive for large grids."
      },
      {
        "id": "cs402-final-q38",
        "type": "multiple_choice",
        "prompt": "Random search often outperforms grid search because:",
        "options": [
          "It always finds the global optimum",
          "It uses less memory",
          "It requires fewer total evaluations",
          "It explores more unique values of important hyperparameters"
        ],
        "correctAnswer": 3,
        "explanation": "When some hyperparameters matter more than others, random search explores more unique values of important parameters, often finding better configurations."
      },
      {
        "id": "cs402-final-q39",
        "type": "true_false",
        "prompt": "Nested cross-validation is used to get unbiased estimates of model performance when hyperparameters are tuned.",
        "correctAnswer": true,
        "explanation": "Nested CV uses an outer loop for performance estimation and inner loop for hyperparameter tuning, preventing hyperparameter overfitting to the validation set."
      },
      {
        "id": "cs402-final-q40",
        "type": "multiple_choice",
        "prompt": "Early stopping in neural network training:",
        "options": [
          "Uses a smaller batch size",
          "Removes neurons from the network",
          "Stops training when validation performance stops improving to prevent overfitting",
          "Reduces the learning rate during training"
        ],
        "correctAnswer": 2,
        "explanation": "Early stopping monitors validation loss and stops training when it stops improving (or starts increasing), preventing the model from overfitting to training data."
      },
      {
        "id": "cs402-final-q41",
        "type": "multiple_choice",
        "prompt": "When comparing two models using cross-validation, which statistical test is appropriate?",
        "options": [
          "ANOVA",
          "Chi-squared test",
          "Paired t-test on the k fold scores",
          "Independent t-test"
        ],
        "correctAnswer": 2,
        "explanation": "Since both models are evaluated on the same folds, their scores are paired. A paired t-test accounts for this dependency when testing for significant differences."
      },
      {
        "id": "cs402-final-q42",
        "type": "multiple_choice",
        "prompt": "Ensemble methods (Random Forest, Gradient Boosting) typically work well because they:",
        "options": [
          "Require less training data",
          "Reduce variance (bagging) or bias (boosting) by combining multiple models",
          "Are always faster than single models",
          "Use simpler base models"
        ],
        "correctAnswer": 1,
        "explanation": "Bagging (Random Forest) reduces variance by averaging uncorrelated models. Boosting reduces bias by sequentially correcting errors. Both improve over single models."
      }
    ]
  }
]
