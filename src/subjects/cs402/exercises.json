[
  {
    "id": "cs402-t1-ex01",
    "subjectId": "cs402",
    "topicId": "cs402-topic-1",
    "title": "Load and Explore Dataset",
    "difficulty": 1,
    "description": "Load a CSV dataset using pandas and perform basic exploratory data analysis.\n\nRequirements:\n- Load data from 'data.csv'\n- Display first 5 rows\n- Show data types and missing values\n- Calculate basic statistics\n- Display shape of dataset",
    "starterCode": "import pandas as pd\nimport numpy as np\n\n# Load the dataset\ndf = None\n\n# Display information\n# TODO: Implement exploration",
    "solution": "import pandas as pd\nimport numpy as np\n\n# Load the dataset\ndf = pd.read_csv('data.csv')\n\n# Display first 5 rows\nprint(df.head())\n\n# Show data types and missing values\nprint(\"\\nData Info:\")\nprint(df.info())\n\nprint(\"\\nMissing Values:\")\nprint(df.isnull().sum())\n\n# Calculate basic statistics\nprint(\"\\nBasic Statistics:\")\nprint(df.describe())\n\n# Display shape\nprint(f\"\\nDataset Shape: {df.shape}\")",
    "testCases": [],
    "hints": [
      "Use pd.read_csv() to load CSV files",
      "df.head() shows the first few rows",
      "df.info() provides data types and null counts",
      "df.describe() gives statistical summary",
      "df.shape returns (rows, columns)"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t1-ex02",
    "subjectId": "cs402",
    "topicId": "cs402-topic-1",
    "title": "Handle Missing Values",
    "difficulty": 2,
    "description": "Implement different strategies for handling missing values in a dataset.\n\nRequirements:\n- Identify columns with missing values\n- Remove rows with any missing values\n- Fill numeric columns with mean\n- Fill categorical columns with mode\n- Forward fill time series data",
    "starterCode": "import pandas as pd\nimport numpy as np\n\ndef handle_missing_values(df, strategy='mean'):\n    \"\"\"\n    Handle missing values using specified strategy.\n\n    Args:\n        df: DataFrame with missing values\n        strategy: 'drop', 'mean', 'median', 'mode', 'ffill'\n\n    Returns:\n        DataFrame with handled missing values\n    \"\"\"\n    # TODO: Implement missing value handling\n    pass",
    "solution": "import pandas as pd\nimport numpy as np\n\ndef handle_missing_values(df, strategy='mean'):\n    \"\"\"\n    Handle missing values using specified strategy.\n\n    Args:\n        df: DataFrame with missing values\n        strategy: 'drop', 'mean', 'median', 'mode', 'ffill'\n\n    Returns:\n        DataFrame with handled missing values\n    \"\"\"\n    df_clean = df.copy()\n\n    if strategy == 'drop':\n        df_clean = df_clean.dropna()\n    elif strategy == 'mean':\n        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n        df_clean[numeric_cols] = df_clean[numeric_cols].fillna(df_clean[numeric_cols].mean())\n    elif strategy == 'median':\n        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n        df_clean[numeric_cols] = df_clean[numeric_cols].fillna(df_clean[numeric_cols].median())\n    elif strategy == 'mode':\n        for col in df_clean.columns:\n            df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n    elif strategy == 'ffill':\n        df_clean = df_clean.fillna(method='ffill')\n\n    return df_clean",
    "testCases": [],
    "hints": [
      "Use df.dropna() to remove rows with missing values",
      "df.fillna() can fill with specific values",
      "df.mean(), df.median(), df.mode() calculate statistics",
      "select_dtypes() helps identify numeric columns",
      "Forward fill propagates last valid observation forward"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t1-ex03",
    "subjectId": "cs402",
    "topicId": "cs402-topic-1",
    "title": "Detect and Remove Outliers",
    "difficulty": 2,
    "description": "Detect outliers using IQR method and optionally remove them.\n\nRequirements:\n- Calculate Q1, Q3, and IQR for numeric columns\n- Identify outliers using 1.5 * IQR rule\n- Return boolean mask of outliers\n- Optionally remove outlier rows",
    "starterCode": "import pandas as pd\nimport numpy as np\n\ndef detect_outliers(df, columns=None, remove=False):\n    \"\"\"\n    Detect outliers using IQR method.\n\n    Args:\n        df: Input DataFrame\n        columns: List of columns to check (None = all numeric)\n        remove: If True, remove outlier rows\n\n    Returns:\n        If remove=False: boolean mask\n        If remove=True: cleaned DataFrame\n    \"\"\"\n    # TODO: Implement outlier detection\n    pass",
    "solution": "import pandas as pd\nimport numpy as np\n\ndef detect_outliers(df, columns=None, remove=False):\n    \"\"\"\n    Detect outliers using IQR method.\n\n    Args:\n        df: Input DataFrame\n        columns: List of columns to check (None = all numeric)\n        remove: If True, remove outlier rows\n\n    Returns:\n        If remove=False: boolean mask\n        If remove=True: cleaned DataFrame\n    \"\"\"\n    if columns is None:\n        columns = df.select_dtypes(include=[np.number]).columns\n\n    outlier_mask = pd.Series([False] * len(df), index=df.index)\n\n    for col in columns:\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n\n        col_outliers = (df[col] < lower_bound) | (df[col] > upper_bound)\n        outlier_mask = outlier_mask | col_outliers\n\n    if remove:\n        return df[~outlier_mask]\n    else:\n        return outlier_mask",
    "testCases": [],
    "hints": [
      "Q1 is the 25th percentile, Q3 is the 75th percentile",
      "IQR = Q3 - Q1",
      "Outliers are below Q1 - 1.5*IQR or above Q3 + 1.5*IQR",
      "Use quantile() method to calculate percentiles",
      "Combine boolean masks with | (or) operator"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t1-ex04",
    "subjectId": "cs402",
    "topicId": "cs402-topic-1",
    "title": "Implement Label Encoding",
    "difficulty": 2,
    "description": "Convert categorical variables to numeric labels.\n\nRequirements:\n- Identify categorical columns\n- Map each unique category to an integer\n- Return encoded DataFrame and mapping dictionary\n- Handle unseen categories in transform",
    "starterCode": "import pandas as pd\nimport numpy as np\n\nclass LabelEncoder:\n    def __init__(self):\n        self.mapping = {}\n\n    def fit(self, data):\n        \"\"\"Learn the mapping from categories to integers.\"\"\"\n        # TODO: Implement fit\n        pass\n\n    def transform(self, data):\n        \"\"\"Transform categories to integers.\"\"\"\n        # TODO: Implement transform\n        pass\n\n    def fit_transform(self, data):\n        \"\"\"Fit and transform in one step.\"\"\"\n        self.fit(data)\n        return self.transform(data)",
    "solution": "import pandas as pd\nimport numpy as np\n\nclass LabelEncoder:\n    def __init__(self):\n        self.mapping = {}\n\n    def fit(self, data):\n        \"\"\"Learn the mapping from categories to integers.\"\"\"\n        unique_values = sorted(data.unique())\n        self.mapping = {val: idx for idx, val in enumerate(unique_values)}\n        return self\n\n    def transform(self, data):\n        \"\"\"Transform categories to integers.\"\"\"\n        return data.map(self.mapping).fillna(-1).astype(int)\n\n    def fit_transform(self, data):\n        \"\"\"Fit and transform in one step.\"\"\"\n        self.fit(data)\n        return self.transform(data)",
    "testCases": [],
    "hints": [
      "Use data.unique() to get unique categories",
      "Create dictionary mapping categories to integers",
      "Use map() to apply the mapping",
      "Handle missing mappings with fillna()",
      "Sort categories for consistent encoding"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t1-ex05",
    "subjectId": "cs402",
    "topicId": "cs402-topic-1",
    "title": "Implement One-Hot Encoding",
    "difficulty": 2,
    "description": "Convert categorical variables to binary columns (one-hot encoding).\n\nRequirements:\n- Create binary column for each category\n- Handle multiple categorical columns\n- Avoid dummy variable trap (drop first column)\n- Return DataFrame with encoded columns",
    "starterCode": "import pandas as pd\nimport numpy as np\n\ndef one_hot_encode(df, columns, drop_first=True):\n    \"\"\"\n    One-hot encode categorical columns.\n\n    Args:\n        df: Input DataFrame\n        columns: List of columns to encode\n        drop_first: Drop first category to avoid multicollinearity\n\n    Returns:\n        DataFrame with one-hot encoded columns\n    \"\"\"\n    # TODO: Implement one-hot encoding\n    pass",
    "solution": "import pandas as pd\nimport numpy as np\n\ndef one_hot_encode(df, columns, drop_first=True):\n    \"\"\"\n    One-hot encode categorical columns.\n\n    Args:\n        df: Input DataFrame\n        columns: List of columns to encode\n        drop_first: Drop first category to avoid multicollinearity\n\n    Returns:\n        DataFrame with one-hot encoded columns\n    \"\"\"\n    df_encoded = df.copy()\n\n    for col in columns:\n        # Create dummy variables\n        dummies = pd.get_dummies(df_encoded[col], prefix=col, drop_first=drop_first)\n\n        # Drop original column and add dummy columns\n        df_encoded = df_encoded.drop(col, axis=1)\n        df_encoded = pd.concat([df_encoded, dummies], axis=1)\n\n    return df_encoded",
    "testCases": [],
    "hints": [
      "Use pd.get_dummies() for one-hot encoding",
      "Set prefix parameter to include original column name",
      "drop_first=True removes one category to avoid dummy trap",
      "Use pd.concat() to combine DataFrames",
      "Drop original column after creating dummies"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t1-ex06",
    "subjectId": "cs402",
    "topicId": "cs402-topic-1",
    "title": "Implement Min-Max Scaling",
    "difficulty": 2,
    "description": "Scale features to a fixed range [0, 1] using min-max normalization.\n\nRequirements:\n- Calculate min and max for each feature\n- Scale features: (x - min) / (max - min)\n- Handle case where min = max\n- Implement fit and transform methods",
    "starterCode": "import numpy as np\n\nclass MinMaxScaler:\n    def __init__(self):\n        self.min_ = None\n        self.max_ = None\n\n    def fit(self, X):\n        \"\"\"Learn min and max from training data.\"\"\"\n        # TODO: Implement fit\n        pass\n\n    def transform(self, X):\n        \"\"\"Scale features to [0, 1] range.\"\"\"\n        # TODO: Implement transform\n        pass\n\n    def fit_transform(self, X):\n        \"\"\"Fit and transform in one step.\"\"\"\n        self.fit(X)\n        return self.transform(X)",
    "solution": "import numpy as np\n\nclass MinMaxScaler:\n    def __init__(self):\n        self.min_ = None\n        self.max_ = None\n\n    def fit(self, X):\n        \"\"\"Learn min and max from training data.\"\"\"\n        self.min_ = np.min(X, axis=0)\n        self.max_ = np.max(X, axis=0)\n        return self\n\n    def transform(self, X):\n        \"\"\"Scale features to [0, 1] range.\"\"\"\n        X = np.array(X)\n        # Handle case where min = max\n        range_ = self.max_ - self.min_\n        range_[range_ == 0] = 1\n\n        X_scaled = (X - self.min_) / range_\n        return X_scaled\n\n    def fit_transform(self, X):\n        \"\"\"Fit and transform in one step.\"\"\"\n        self.fit(X)\n        return self.transform(X)",
    "testCases": [],
    "hints": [
      "Use np.min() and np.max() with axis=0 for column-wise",
      "Formula: (x - min) / (max - min)",
      "When min = max, set range to 1 to avoid division by zero",
      "Store min and max during fit for later transform",
      "Ensure X is numpy array for vectorized operations"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t1-ex07",
    "subjectId": "cs402",
    "topicId": "cs402-topic-1",
    "title": "Implement Standard Scaling",
    "difficulty": 2,
    "description": "Standardize features by removing mean and scaling to unit variance.\n\nRequirements:\n- Calculate mean and standard deviation\n- Scale features: (x - mean) / std\n- Handle case where std = 0\n- Implement fit and transform methods",
    "starterCode": "import numpy as np\n\nclass StandardScaler:\n    def __init__(self):\n        self.mean_ = None\n        self.std_ = None\n\n    def fit(self, X):\n        \"\"\"Learn mean and std from training data.\"\"\"\n        # TODO: Implement fit\n        pass\n\n    def transform(self, X):\n        \"\"\"Standardize features.\"\"\"\n        # TODO: Implement transform\n        pass\n\n    def fit_transform(self, X):\n        \"\"\"Fit and transform in one step.\"\"\"\n        self.fit(X)\n        return self.transform(X)",
    "solution": "import numpy as np\n\nclass StandardScaler:\n    def __init__(self):\n        self.mean_ = None\n        self.std_ = None\n\n    def fit(self, X):\n        \"\"\"Learn mean and std from training data.\"\"\"\n        self.mean_ = np.mean(X, axis=0)\n        self.std_ = np.std(X, axis=0)\n        return self\n\n    def transform(self, X):\n        \"\"\"Standardize features.\"\"\"\n        X = np.array(X)\n        # Handle case where std = 0\n        std = self.std_.copy()\n        std[std == 0] = 1\n\n        X_scaled = (X - self.mean_) / std\n        return X_scaled\n\n    def fit_transform(self, X):\n        \"\"\"Fit and transform in one step.\"\"\"\n        self.fit(X)\n        return self.transform(X)",
    "testCases": [],
    "hints": [
      "Use np.mean() and np.std() with axis=0",
      "Formula: (x - mean) / std",
      "When std = 0, set it to 1 to avoid division by zero",
      "Standardized data has mean=0 and std=1",
      "Store statistics during fit for consistent transform"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t1-ex08",
    "subjectId": "cs402",
    "topicId": "cs402-topic-1",
    "title": "Implement Train-Test Split",
    "difficulty": 2,
    "description": "Split dataset into training and testing sets.\n\nRequirements:\n- Support stratified splitting for classification\n- Shuffle data before splitting\n- Handle both numpy arrays and pandas DataFrames\n- Return X_train, X_test, y_train, y_test",
    "starterCode": "import numpy as np\nimport pandas as pd\n\ndef train_test_split(X, y, test_size=0.2, random_state=None, stratify=None):\n    \"\"\"\n    Split data into training and testing sets.\n\n    Args:\n        X: Features\n        y: Target\n        test_size: Proportion for test set\n        random_state: Random seed\n        stratify: If not None, data is split in a stratified fashion\n\n    Returns:\n        X_train, X_test, y_train, y_test\n    \"\"\"\n    # TODO: Implement train-test split\n    pass",
    "solution": "import numpy as np\nimport pandas as pd\n\ndef train_test_split(X, y, test_size=0.2, random_state=None, stratify=None):\n    \"\"\"\n    Split data into training and testing sets.\n\n    Args:\n        X: Features\n        y: Target\n        test_size: Proportion for test set\n        random_state: Random seed\n        stratify: If not None, data is split in a stratified fashion\n\n    Returns:\n        X_train, X_test, y_train, y_test\n    \"\"\"\n    if random_state is not None:\n        np.random.seed(random_state)\n\n    n_samples = len(X)\n    n_test = int(n_samples * test_size)\n\n    if stratify is not None:\n        # Stratified split\n        unique_classes = np.unique(stratify)\n        train_idx = []\n        test_idx = []\n\n        for cls in unique_classes:\n            cls_idx = np.where(stratify == cls)[0]\n            np.random.shuffle(cls_idx)\n            n_cls_test = int(len(cls_idx) * test_size)\n            test_idx.extend(cls_idx[:n_cls_test])\n            train_idx.extend(cls_idx[n_cls_test:])\n\n        train_idx = np.array(train_idx)\n        test_idx = np.array(test_idx)\n    else:\n        # Random split\n        indices = np.arange(n_samples)\n        np.random.shuffle(indices)\n        test_idx = indices[:n_test]\n        train_idx = indices[n_test:]\n\n    # Handle pandas DataFrame\n    if isinstance(X, pd.DataFrame):\n        X_train = X.iloc[train_idx]\n        X_test = X.iloc[test_idx]\n        y_train = y.iloc[train_idx]\n        y_test = y.iloc[test_idx]\n    else:\n        X_train = X[train_idx]\n        X_test = X[test_idx]\n        y_train = y[train_idx]\n        y_test = y[test_idx]\n\n    return X_train, X_test, y_train, y_test",
    "testCases": [],
    "hints": [
      "Use np.random.shuffle() to randomize data",
      "Calculate test size as proportion of total samples",
      "For stratified split, maintain class proportions",
      "Use np.where() to find indices of each class",
      "Handle both numpy arrays and pandas DataFrames"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t1-ex09",
    "subjectId": "cs402",
    "topicId": "cs402-topic-1",
    "title": "Create Polynomial Features",
    "difficulty": 3,
    "description": "Generate polynomial and interaction features from input features.\n\nRequirements:\n- Create features up to specified degree\n- Include interaction terms (x1*x2)\n- Include bias term (column of ones)\n- Handle multiple features efficiently",
    "starterCode": "import numpy as np\nfrom itertools import combinations_with_replacement\n\nclass PolynomialFeatures:\n    def __init__(self, degree=2, include_bias=True):\n        self.degree = degree\n        self.include_bias = include_bias\n\n    def fit_transform(self, X):\n        \"\"\"Generate polynomial features.\"\"\"\n        # TODO: Implement polynomial feature generation\n        pass",
    "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\n\nclass PolynomialFeatures:\n    def __init__(self, degree=2, include_bias=True):\n        self.degree = degree\n        self.include_bias = include_bias\n\n    def fit_transform(self, X):\n        \"\"\"Generate polynomial features.\"\"\"\n        X = np.array(X)\n        n_samples, n_features = X.shape\n\n        # Generate all combinations of features up to degree\n        combinations = []\n        for d in range(0 if self.include_bias else 1, self.degree + 1):\n            for combo in combinations_with_replacement(range(n_features), d):\n                combinations.append(combo)\n\n        # Create polynomial features\n        X_poly = np.zeros((n_samples, len(combinations)))\n        for i, combo in enumerate(combinations):\n            if len(combo) == 0:\n                # Bias term\n                X_poly[:, i] = 1\n            else:\n                # Product of features in combination\n                X_poly[:, i] = np.prod(X[:, combo], axis=1)\n\n        return X_poly",
    "testCases": [],
    "hints": [
      "Use itertools.combinations_with_replacement for feature combinations",
      "For degree=2 with 2 features: [1, x1, x2, x1^2, x1*x2, x2^2]",
      "np.prod() computes product along axis",
      "Empty combination represents bias term (1)",
      "Number of features grows quickly with degree"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t1-ex10",
    "subjectId": "cs402",
    "topicId": "cs402-topic-1",
    "title": "Implement Feature Selection by Correlation",
    "difficulty": 3,
    "description": "Select features based on correlation with target variable.\n\nRequirements:\n- Calculate correlation between each feature and target\n- Select top k features with highest absolute correlation\n- Return selected feature indices and correlations\n- Handle multicollinearity (highly correlated features)",
    "starterCode": "import numpy as np\nimport pandas as pd\n\ndef select_features_by_correlation(X, y, k=10, threshold=0.9):\n    \"\"\"\n    Select features by correlation with target.\n\n    Args:\n        X: Feature matrix\n        y: Target variable\n        k: Number of features to select\n        threshold: Remove features with correlation > threshold\n\n    Returns:\n        selected_indices: Indices of selected features\n        correlations: Correlation values\n    \"\"\"\n    # TODO: Implement feature selection\n    pass",
    "solution": "import numpy as np\nimport pandas as pd\n\ndef select_features_by_correlation(X, y, k=10, threshold=0.9):\n    \"\"\"\n    Select features by correlation with target.\n\n    Args:\n        X: Feature matrix\n        y: Target variable\n        k: Number of features to select\n        threshold: Remove features with correlation > threshold\n\n    Returns:\n        selected_indices: Indices of selected features\n        correlations: Correlation values\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n\n    # Calculate correlation with target\n    correlations = []\n    for i in range(X.shape[1]):\n        corr = np.corrcoef(X[:, i], y)[0, 1]\n        correlations.append(abs(corr))\n\n    correlations = np.array(correlations)\n\n    # Sort by correlation\n    sorted_indices = np.argsort(correlations)[::-1]\n\n    # Select features, avoiding multicollinearity\n    selected_indices = []\n    for idx in sorted_indices:\n        if len(selected_indices) >= k:\n            break\n\n        # Check correlation with already selected features\n        is_collinear = False\n        for selected_idx in selected_indices:\n            corr = abs(np.corrcoef(X[:, idx], X[:, selected_idx])[0, 1])\n            if corr > threshold:\n                is_collinear = True\n                break\n\n        if not is_collinear:\n            selected_indices.append(idx)\n\n    return np.array(selected_indices), correlations[selected_indices]",
    "testCases": [],
    "hints": [
      "Use np.corrcoef() to calculate correlation",
      "Take absolute value to consider both positive and negative correlation",
      "np.argsort()[::-1] sorts in descending order",
      "Check correlation between features to avoid multicollinearity",
      "Stop when k features are selected"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t1-ex11",
    "subjectId": "cs402",
    "topicId": "cs402-topic-1",
    "title": "Handle Imbalanced Datasets",
    "difficulty": 3,
    "description": "Implement techniques to handle imbalanced classification datasets.\n\nRequirements:\n- Random oversampling of minority class\n- Random undersampling of majority class\n- SMOTE-like synthetic sample generation\n- Return balanced dataset",
    "starterCode": "import numpy as np\n\ndef balance_dataset(X, y, method='oversample'):\n    \"\"\"\n    Balance imbalanced dataset.\n\n    Args:\n        X: Feature matrix\n        y: Target labels\n        method: 'oversample', 'undersample', or 'smote'\n\n    Returns:\n        X_balanced, y_balanced\n    \"\"\"\n    # TODO: Implement dataset balancing\n    pass",
    "solution": "import numpy as np\n\ndef balance_dataset(X, y, method='oversample'):\n    \"\"\"\n    Balance imbalanced dataset.\n\n    Args:\n        X: Feature matrix\n        y: Target labels\n        method: 'oversample', 'undersample', or 'smote'\n\n    Returns:\n        X_balanced, y_balanced\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n\n    classes, counts = np.unique(y, return_counts=True)\n\n    if method == 'oversample':\n        # Oversample minority classes\n        max_count = np.max(counts)\n        X_balanced = []\n        y_balanced = []\n\n        for cls in classes:\n            cls_idx = np.where(y == cls)[0]\n            X_cls = X[cls_idx]\n            y_cls = y[cls_idx]\n\n            # Resample with replacement\n            n_samples = max_count - len(cls_idx)\n            if n_samples > 0:\n                resample_idx = np.random.choice(len(cls_idx), n_samples, replace=True)\n                X_cls = np.vstack([X_cls, X_cls[resample_idx]])\n                y_cls = np.hstack([y_cls, y_cls[resample_idx]])\n\n            X_balanced.append(X_cls)\n            y_balanced.append(y_cls)\n\n        X_balanced = np.vstack(X_balanced)\n        y_balanced = np.hstack(y_balanced)\n\n    elif method == 'undersample':\n        # Undersample majority classes\n        min_count = np.min(counts)\n        X_balanced = []\n        y_balanced = []\n\n        for cls in classes:\n            cls_idx = np.where(y == cls)[0]\n            # Sample without replacement\n            sample_idx = np.random.choice(cls_idx, min_count, replace=False)\n            X_balanced.append(X[sample_idx])\n            y_balanced.append(y[sample_idx])\n\n        X_balanced = np.vstack(X_balanced)\n        y_balanced = np.hstack(y_balanced)\n\n    elif method == 'smote':\n        # Simple SMOTE: create synthetic samples between neighbors\n        max_count = np.max(counts)\n        X_balanced = []\n        y_balanced = []\n\n        for cls in classes:\n            cls_idx = np.where(y == cls)[0]\n            X_cls = X[cls_idx]\n            y_cls = y[cls_idx]\n\n            n_synthetic = max_count - len(cls_idx)\n            if n_synthetic > 0:\n                # Generate synthetic samples\n                for _ in range(n_synthetic):\n                    idx1, idx2 = np.random.choice(len(cls_idx), 2, replace=True)\n                    # Linear interpolation\n                    alpha = np.random.random()\n                    synthetic = X_cls[idx1] + alpha * (X_cls[idx2] - X_cls[idx1])\n                    X_cls = np.vstack([X_cls, synthetic])\n                    y_cls = np.hstack([y_cls, cls])\n\n            X_balanced.append(X_cls)\n            y_balanced.append(y_cls)\n\n        X_balanced = np.vstack(X_balanced)\n        y_balanced = np.hstack(y_balanced)\n\n    return X_balanced, y_balanced",
    "testCases": [],
    "hints": [
      "Use np.unique() with return_counts=True to find class distribution",
      "Oversample: duplicate minority class samples randomly",
      "Undersample: randomly select subset of majority class",
      "SMOTE: create synthetic samples between existing samples",
      "Linear interpolation: x_new = x1 + alpha * (x2 - x1)"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t1-ex12",
    "subjectId": "cs402",
    "topicId": "cs402-topic-1",
    "title": "Implement K-Fold Cross-Validation Indices",
    "difficulty": 3,
    "description": "Generate indices for k-fold cross-validation.\n\nRequirements:\n- Split data into k folds\n- Return train and validation indices for each fold\n- Support stratified k-fold for classification\n- Handle edge cases (k > n_samples)",
    "starterCode": "import numpy as np\n\ndef k_fold_indices(n_samples, k=5, stratify=None):\n    \"\"\"\n    Generate k-fold cross-validation indices.\n\n    Args:\n        n_samples: Number of samples\n        k: Number of folds\n        stratify: Labels for stratified k-fold\n\n    Yields:\n        train_idx, val_idx for each fold\n    \"\"\"\n    # TODO: Implement k-fold index generation\n    pass",
    "solution": "import numpy as np\n\ndef k_fold_indices(n_samples, k=5, stratify=None):\n    \"\"\"\n    Generate k-fold cross-validation indices.\n\n    Args:\n        n_samples: Number of samples\n        k: Number of folds\n        stratify: Labels for stratified k-fold\n\n    Yields:\n        train_idx, val_idx for each fold\n    \"\"\"\n    if k > n_samples:\n        raise ValueError(f\"k={k} cannot be greater than n_samples={n_samples}\")\n\n    if stratify is not None:\n        # Stratified k-fold\n        unique_classes = np.unique(stratify)\n        fold_indices = [[] for _ in range(k)]\n\n        for cls in unique_classes:\n            cls_idx = np.where(stratify == cls)[0]\n            np.random.shuffle(cls_idx)\n            # Distribute class samples across folds\n            for i, idx in enumerate(cls_idx):\n                fold_indices[i % k].append(idx)\n\n        # Convert to arrays\n        fold_indices = [np.array(fold) for fold in fold_indices]\n    else:\n        # Regular k-fold\n        indices = np.arange(n_samples)\n        np.random.shuffle(indices)\n        fold_indices = np.array_split(indices, k)\n\n    # Generate train/val splits\n    for i in range(k):\n        val_idx = fold_indices[i]\n        train_idx = np.concatenate([fold_indices[j] for j in range(k) if j != i])\n        yield train_idx, val_idx",
    "testCases": [],
    "hints": [
      "Divide samples into k approximately equal folds",
      "For each fold: use fold as validation, rest as training",
      "Stratified: maintain class proportions in each fold",
      "Use np.array_split() to divide into k parts",
      "Yield train and validation indices for each fold"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t1-ex13",
    "subjectId": "cs402",
    "topicId": "cs402-topic-1",
    "title": "Feature Binning",
    "difficulty": 2,
    "description": "Discretize continuous features into bins.\n\nRequirements:\n- Support equal-width binning\n- Support equal-frequency (quantile) binning\n- Return bin edges and transformed data\n- Handle edge cases",
    "starterCode": "import numpy as np\n\ndef bin_features(X, n_bins=5, strategy='uniform'):\n    \"\"\"\n    Bin continuous features.\n\n    Args:\n        X: Feature array\n        n_bins: Number of bins\n        strategy: 'uniform' (equal width) or 'quantile' (equal frequency)\n\n    Returns:\n        X_binned, bin_edges\n    \"\"\"\n    # TODO: Implement feature binning\n    pass",
    "solution": "import numpy as np\n\ndef bin_features(X, n_bins=5, strategy='uniform'):\n    \"\"\"\n    Bin continuous features.\n\n    Args:\n        X: Feature array\n        n_bins: Number of bins\n        strategy: 'uniform' (equal width) or 'quantile' (equal frequency)\n\n    Returns:\n        X_binned, bin_edges\n    \"\"\"\n    X = np.array(X)\n\n    if strategy == 'uniform':\n        # Equal-width bins\n        min_val = np.min(X)\n        max_val = np.max(X)\n        bin_edges = np.linspace(min_val, max_val, n_bins + 1)\n    elif strategy == 'quantile':\n        # Equal-frequency bins\n        quantiles = np.linspace(0, 100, n_bins + 1)\n        bin_edges = np.percentile(X, quantiles)\n        # Ensure unique edges\n        bin_edges = np.unique(bin_edges)\n    else:\n        raise ValueError(f\"Unknown strategy: {strategy}\")\n\n    # Bin the data\n    X_binned = np.digitize(X, bin_edges[:-1]) - 1\n    # Clip to valid range\n    X_binned = np.clip(X_binned, 0, n_bins - 1)\n\n    return X_binned, bin_edges",
    "testCases": [],
    "hints": [
      "Uniform: use np.linspace() between min and max",
      "Quantile: use np.percentile() for equal-frequency bins",
      "np.digitize() assigns values to bins",
      "Subtract 1 from digitize result for 0-based indexing",
      "Use np.clip() to ensure bins are in valid range"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t1-ex14",
    "subjectId": "cs402",
    "topicId": "cs402-topic-1",
    "title": "Calculate Feature Importance",
    "difficulty": 3,
    "description": "Calculate feature importance using permutation importance.\n\nRequirements:\n- Train a simple model\n- For each feature, permute values and measure performance drop\n- Larger drop = more important feature\n- Return feature importance scores",
    "starterCode": "import numpy as np\n\ndef permutation_importance(model, X, y, metric='accuracy', n_repeats=10):\n    \"\"\"\n    Calculate permutation feature importance.\n\n    Args:\n        model: Trained model with predict method\n        X: Feature matrix\n        y: True labels\n        metric: 'accuracy' or 'mse'\n        n_repeats: Number of permutations per feature\n\n    Returns:\n        importances: Array of importance scores\n    \"\"\"\n    # TODO: Implement permutation importance\n    pass",
    "solution": "import numpy as np\n\ndef permutation_importance(model, X, y, metric='accuracy', n_repeats=10):\n    \"\"\"\n    Calculate permutation feature importance.\n\n    Args:\n        model: Trained model with predict method\n        X: Feature matrix\n        y: True labels\n        metric: 'accuracy' or 'mse'\n        n_repeats: Number of permutations per feature\n\n    Returns:\n        importances: Array of importance scores\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    n_features = X.shape[1]\n\n    # Calculate baseline score\n    y_pred = model.predict(X)\n    if metric == 'accuracy':\n        baseline_score = np.mean(y_pred == y)\n    elif metric == 'mse':\n        baseline_score = -np.mean((y_pred - y) ** 2)  # Negative for consistency\n\n    importances = np.zeros(n_features)\n\n    # Permute each feature\n    for i in range(n_features):\n        scores = []\n        for _ in range(n_repeats):\n            X_permuted = X.copy()\n            # Permute feature i\n            X_permuted[:, i] = np.random.permutation(X_permuted[:, i])\n\n            # Calculate score with permuted feature\n            y_pred = model.predict(X_permuted)\n            if metric == 'accuracy':\n                score = np.mean(y_pred == y)\n            elif metric == 'mse':\n                score = -np.mean((y_pred - y) ** 2)\n\n            scores.append(score)\n\n        # Importance is drop in performance\n        importances[i] = baseline_score - np.mean(scores)\n\n    return importances",
    "testCases": [],
    "hints": [
      "Calculate baseline performance with original features",
      "Permute each feature independently",
      "Measure performance drop after permutation",
      "Average over multiple permutations for stability",
      "Important features cause large performance drop when permuted"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t1-ex15",
    "subjectId": "cs402",
    "topicId": "cs402-topic-1",
    "title": "Time Series Train-Test Split",
    "difficulty": 2,
    "description": "Split time series data maintaining temporal order.\n\nRequirements:\n- Preserve temporal order (no shuffling)\n- Split by date or index\n- Support multiple time-based splits\n- Return train and test indices",
    "starterCode": "import numpy as np\nimport pandas as pd\n\ndef time_series_split(data, n_splits=5, test_size=None):\n    \"\"\"\n    Split time series data preserving temporal order.\n\n    Args:\n        data: Time series data or indices\n        n_splits: Number of splits\n        test_size: Size of test set (if None, use expanding window)\n\n    Yields:\n        train_idx, test_idx for each split\n    \"\"\"\n    # TODO: Implement time series split\n    pass",
    "solution": "import numpy as np\nimport pandas as pd\n\ndef time_series_split(data, n_splits=5, test_size=None):\n    \"\"\"\n    Split time series data preserving temporal order.\n\n    Args:\n        data: Time series data or indices\n        n_splits: Number of splits\n        test_size: Size of test set (if None, use expanding window)\n\n    Yields:\n        train_idx, test_idx for each split\n    \"\"\"\n    n_samples = len(data)\n\n    if test_size is None:\n        # Expanding window: each split adds more training data\n        test_size = n_samples // (n_splits + 1)\n\n    for i in range(n_splits):\n        # Test set starts after training set\n        test_start = (i + 1) * test_size\n        test_end = test_start + test_size\n\n        if test_end > n_samples:\n            test_end = n_samples\n\n        train_idx = np.arange(0, test_start)\n        test_idx = np.arange(test_start, test_end)\n\n        if len(test_idx) > 0:\n            yield train_idx, test_idx",
    "testCases": [],
    "hints": [
      "Never shuffle time series data",
      "Training data always comes before test data",
      "Expanding window: training set grows with each split",
      "Test sets should be consecutive time periods",
      "Ensure test set exists for each split"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t1-ex16",
    "subjectId": "cs402",
    "topicId": "cs402-topic-1",
    "title": "Data Augmentation for Tabular Data",
    "difficulty": 3,
    "description": "Augment tabular dataset by adding noise and synthetic samples.\n\nRequirements:\n- Add Gaussian noise to numeric features\n- Create synthetic samples via interpolation\n- Preserve data distribution\n- Return augmented dataset",
    "starterCode": "import numpy as np\n\ndef augment_tabular_data(X, y=None, n_augment=None, noise_level=0.1):\n    \"\"\"\n    Augment tabular data with noise and synthetic samples.\n\n    Args:\n        X: Feature matrix\n        y: Labels (optional)\n        n_augment: Number of synthetic samples to create\n        noise_level: Standard deviation of Gaussian noise\n\n    Returns:\n        X_augmented, y_augmented (if y provided)\n    \"\"\"\n    # TODO: Implement data augmentation\n    pass",
    "solution": "import numpy as np\n\ndef augment_tabular_data(X, y=None, n_augment=None, noise_level=0.1):\n    \"\"\"\n    Augment tabular data with noise and synthetic samples.\n\n    Args:\n        X: Feature matrix\n        y: Labels (optional)\n        n_augment: Number of synthetic samples to create\n        noise_level: Standard deviation of Gaussian noise\n\n    Returns:\n        X_augmented, y_augmented (if y provided)\n    \"\"\"\n    X = np.array(X)\n    n_samples, n_features = X.shape\n\n    if n_augment is None:\n        n_augment = n_samples // 2\n\n    X_synthetic = []\n    y_synthetic = [] if y is not None else None\n\n    for _ in range(n_augment):\n        # Select two random samples\n        idx1, idx2 = np.random.choice(n_samples, 2, replace=True)\n\n        # Create synthetic sample via interpolation\n        alpha = np.random.random()\n        x_new = X[idx1] + alpha * (X[idx2] - X[idx1])\n\n        # Add Gaussian noise\n        noise = np.random.normal(0, noise_level * np.std(X, axis=0), n_features)\n        x_new = x_new + noise\n\n        X_synthetic.append(x_new)\n\n        if y is not None:\n            # Use label from closer sample\n            y_new = y[idx1] if alpha < 0.5 else y[idx2]\n            y_synthetic.append(y_new)\n\n    X_augmented = np.vstack([X, np.array(X_synthetic)])\n\n    if y is not None:\n        y_augmented = np.hstack([y, np.array(y_synthetic)])\n        return X_augmented, y_augmented\n    else:\n        return X_augmented",
    "testCases": [],
    "hints": [
      "Interpolate between existing samples: x_new = x1 + alpha*(x2-x1)",
      "Add small Gaussian noise scaled by feature std",
      "For labels, use label of closer sample",
      "noise_level controls augmentation strength",
      "Use np.std() to scale noise by feature variance"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t2-ex01",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Implement Simple Linear Regression",
    "difficulty": 2,
    "description": "Implement linear regression from scratch using normal equation.\n\nRequirements:\n- Calculate optimal weights: w = (X^T X)^-1 X^T y\n- Add bias term (column of ones)\n- Implement fit and predict methods\n- Return coefficients and intercept",
    "starterCode": "import numpy as np\n\nclass LinearRegression:\n    def __init__(self):\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        \"\"\"Fit linear regression using normal equation.\"\"\"\n        # TODO: Implement fit\n        pass\n\n    def predict(self, X):\n        \"\"\"Predict using linear model.\"\"\"\n        # TODO: Implement predict\n        pass",
    "solution": "import numpy as np\n\nclass LinearRegression:\n    def __init__(self):\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        \"\"\"Fit linear regression using normal equation.\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n\n        # Add bias term\n        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n\n        # Normal equation: w = (X^T X)^-1 X^T y\n        theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n\n        self.bias = theta[0]\n        self.weights = theta[1:]\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using linear model.\"\"\"\n        X = np.array(X)\n        return X @ self.weights + self.bias",
    "testCases": [],
    "hints": [
      "Add column of ones to X for bias term",
      "Use @ operator for matrix multiplication",
      "np.linalg.inv() computes matrix inverse",
      "First element of theta is bias, rest are weights",
      "Normal equation: w = (X^T X)^-1 X^T y"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t2-ex02",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Calculate Mean Squared Error",
    "difficulty": 1,
    "description": "Implement MSE cost function for regression.\n\nRequirements:\n- Calculate MSE: (1/n) * Î£(y_pred - y_true)^2\n- Handle both vectors and scalars\n- Return single float value",
    "starterCode": "import numpy as np\n\ndef mean_squared_error(y_true, y_pred):\n    \"\"\"\n    Calculate mean squared error.\n\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n\n    Returns:\n        MSE as float\n    \"\"\"\n    # TODO: Implement MSE\n    pass",
    "solution": "import numpy as np\n\ndef mean_squared_error(y_true, y_pred):\n    \"\"\"\n    Calculate mean squared error.\n\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n\n    Returns:\n        MSE as float\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    mse = np.mean((y_pred - y_true) ** 2)\n    return mse",
    "testCases": [],
    "hints": [
      "MSE = average of squared errors",
      "Use np.mean() for averaging",
      "Square the differences: (y_pred - y_true) ** 2",
      "MSE is always non-negative",
      "Perfect predictions give MSE = 0"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t2-ex03",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Implement Gradient Descent for Linear Regression",
    "difficulty": 3,
    "description": "Implement batch gradient descent to optimize linear regression.\n\nRequirements:\n- Initialize weights randomly\n- Calculate gradient: dw = (2/n) * X^T * (X*w - y)\n- Update weights: w = w - learning_rate * gradient\n- Track cost history\n- Implement early stopping",
    "starterCode": "import numpy as np\n\nclass GradientDescentRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n        self.cost_history = []\n\n    def fit(self, X, y):\n        \"\"\"Fit using gradient descent.\"\"\"\n        # TODO: Implement gradient descent\n        pass\n\n    def predict(self, X):\n        \"\"\"Predict using linear model.\"\"\"\n        # TODO: Implement predict\n        pass",
    "solution": "import numpy as np\n\nclass GradientDescentRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n        self.cost_history = []\n\n    def fit(self, X, y):\n        \"\"\"Fit using gradient descent.\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n        n_samples, n_features = X.shape\n\n        # Initialize parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # Gradient descent\n        for _ in range(self.n_iterations):\n            # Forward pass\n            y_pred = X @ self.weights + self.bias\n\n            # Calculate cost (MSE)\n            cost = np.mean((y_pred - y) ** 2)\n            self.cost_history.append(cost)\n\n            # Calculate gradients\n            dw = (2 / n_samples) * X.T @ (y_pred - y)\n            db = (2 / n_samples) * np.sum(y_pred - y)\n\n            # Update parameters\n            self.weights -= self.learning_rate * dw\n            self.bias -= self.learning_rate * db\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using linear model.\"\"\"\n        X = np.array(X)\n        return X @ self.weights + self.bias",
    "testCases": [],
    "hints": [
      "Initialize weights to zeros or small random values",
      "Gradient for weights: dw = (2/n) * X^T * (y_pred - y)",
      "Gradient for bias: db = (2/n) * sum(y_pred - y)",
      "Update: w = w - learning_rate * dw",
      "Cost should decrease over iterations"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t2-ex04",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Implement Stochastic Gradient Descent",
    "difficulty": 3,
    "description": "Implement SGD where gradient is computed on single random sample each iteration.\n\nRequirements:\n- Randomly shuffle data each epoch\n- Update weights after each sample\n- Implement multiple epochs\n- Track cost after each epoch",
    "starterCode": "import numpy as np\n\nclass SGDRegression:\n    def __init__(self, learning_rate=0.01, n_epochs=100):\n        self.learning_rate = learning_rate\n        self.n_epochs = n_epochs\n        self.weights = None\n        self.bias = None\n        self.cost_history = []\n\n    def fit(self, X, y):\n        \"\"\"Fit using stochastic gradient descent.\"\"\"\n        # TODO: Implement SGD\n        pass\n\n    def predict(self, X):\n        \"\"\"Predict using linear model.\"\"\"\n        # TODO: Implement predict\n        pass",
    "solution": "import numpy as np\n\nclass SGDRegression:\n    def __init__(self, learning_rate=0.01, n_epochs=100):\n        self.learning_rate = learning_rate\n        self.n_epochs = n_epochs\n        self.weights = None\n        self.bias = None\n        self.cost_history = []\n\n    def fit(self, X, y):\n        \"\"\"Fit using stochastic gradient descent.\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n        n_samples, n_features = X.shape\n\n        # Initialize parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # SGD epochs\n        for epoch in range(self.n_epochs):\n            # Shuffle data\n            indices = np.random.permutation(n_samples)\n\n            # Update on each sample\n            for idx in indices:\n                xi = X[idx]\n                yi = y[idx]\n\n                # Forward pass\n                y_pred = xi @ self.weights + self.bias\n\n                # Calculate gradients for single sample\n                error = y_pred - yi\n                dw = 2 * xi * error\n                db = 2 * error\n\n                # Update parameters\n                self.weights -= self.learning_rate * dw\n                self.bias -= self.learning_rate * db\n\n            # Calculate cost after epoch\n            y_pred_all = X @ self.weights + self.bias\n            cost = np.mean((y_pred_all - y) ** 2)\n            self.cost_history.append(cost)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using linear model.\"\"\"\n        X = np.array(X)\n        return X @ self.weights + self.bias",
    "testCases": [],
    "hints": [
      "Shuffle data at the start of each epoch",
      "Update weights after each single sample",
      "Gradient for one sample: dw = 2 * x * error",
      "SGD is noisier but faster than batch GD",
      "Calculate full cost after each epoch for monitoring"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t2-ex05",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Implement Mini-Batch Gradient Descent",
    "difficulty": 3,
    "description": "Implement mini-batch GD where gradient is computed on small batches.\n\nRequirements:\n- Split data into mini-batches\n- Update weights after each batch\n- Shuffle data each epoch\n- Support variable batch sizes",
    "starterCode": "import numpy as np\n\nclass MiniBatchGD:\n    def __init__(self, learning_rate=0.01, n_epochs=100, batch_size=32):\n        self.learning_rate = learning_rate\n        self.n_epochs = n_epochs\n        self.batch_size = batch_size\n        self.weights = None\n        self.bias = None\n        self.cost_history = []\n\n    def fit(self, X, y):\n        \"\"\"Fit using mini-batch gradient descent.\"\"\"\n        # TODO: Implement mini-batch GD\n        pass\n\n    def predict(self, X):\n        \"\"\"Predict using linear model.\"\"\"\n        # TODO: Implement predict\n        pass",
    "solution": "import numpy as np\n\nclass MiniBatchGD:\n    def __init__(self, learning_rate=0.01, n_epochs=100, batch_size=32):\n        self.learning_rate = learning_rate\n        self.n_epochs = n_epochs\n        self.batch_size = batch_size\n        self.weights = None\n        self.bias = None\n        self.cost_history = []\n\n    def fit(self, X, y):\n        \"\"\"Fit using mini-batch gradient descent.\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n        n_samples, n_features = X.shape\n\n        # Initialize parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # Mini-batch GD epochs\n        for epoch in range(self.n_epochs):\n            # Shuffle data\n            indices = np.random.permutation(n_samples)\n            X_shuffled = X[indices]\n            y_shuffled = y[indices]\n\n            # Process mini-batches\n            for i in range(0, n_samples, self.batch_size):\n                batch_end = min(i + self.batch_size, n_samples)\n                X_batch = X_shuffled[i:batch_end]\n                y_batch = y_shuffled[i:batch_end]\n                batch_size_actual = len(X_batch)\n\n                # Forward pass\n                y_pred = X_batch @ self.weights + self.bias\n\n                # Calculate gradients for batch\n                dw = (2 / batch_size_actual) * X_batch.T @ (y_pred - y_batch)\n                db = (2 / batch_size_actual) * np.sum(y_pred - y_batch)\n\n                # Update parameters\n                self.weights -= self.learning_rate * dw\n                self.bias -= self.learning_rate * db\n\n            # Calculate cost after epoch\n            y_pred_all = X @ self.weights + self.bias\n            cost = np.mean((y_pred_all - y) ** 2)\n            self.cost_history.append(cost)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using linear model.\"\"\"\n        X = np.array(X)\n        return X @ self.weights + self.bias",
    "testCases": [],
    "hints": [
      "Mini-batch is between SGD (batch=1) and batch GD (batch=n)",
      "Shuffle data at start of each epoch",
      "Process data in chunks of batch_size",
      "Handle last batch which may be smaller",
      "Typical batch sizes: 32, 64, 128, 256"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t2-ex06",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Implement L2 Regularization (Ridge)",
    "difficulty": 3,
    "description": "Add L2 regularization to linear regression to prevent overfitting.\n\nRequirements:\n- Add penalty term: Î» * ||w||^2\n- Modified cost: MSE + Î» * sum(w^2)\n- Modified gradient: gradient + 2Î»w\n- Implement with gradient descent",
    "starterCode": "import numpy as np\n\nclass RidgeRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000, alpha=1.0):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.alpha = alpha  # Regularization strength\n        self.weights = None\n        self.bias = None\n        self.cost_history = []\n\n    def fit(self, X, y):\n        \"\"\"Fit Ridge regression with L2 regularization.\"\"\"\n        # TODO: Implement Ridge regression\n        pass\n\n    def predict(self, X):\n        \"\"\"Predict using linear model.\"\"\"\n        # TODO: Implement predict\n        pass",
    "solution": "import numpy as np\n\nclass RidgeRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000, alpha=1.0):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.alpha = alpha  # Regularization strength\n        self.weights = None\n        self.bias = None\n        self.cost_history = []\n\n    def fit(self, X, y):\n        \"\"\"Fit Ridge regression with L2 regularization.\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n        n_samples, n_features = X.shape\n\n        # Initialize parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # Gradient descent with L2 regularization\n        for _ in range(self.n_iterations):\n            # Forward pass\n            y_pred = X @ self.weights + self.bias\n\n            # Calculate cost with L2 penalty\n            mse = np.mean((y_pred - y) ** 2)\n            l2_penalty = self.alpha * np.sum(self.weights ** 2)\n            cost = mse + l2_penalty\n            self.cost_history.append(cost)\n\n            # Calculate gradients with L2 regularization\n            dw = (2 / n_samples) * X.T @ (y_pred - y) + 2 * self.alpha * self.weights\n            db = (2 / n_samples) * np.sum(y_pred - y)\n\n            # Update parameters\n            self.weights -= self.learning_rate * dw\n            self.bias -= self.learning_rate * db\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using linear model.\"\"\"\n        X = np.array(X)\n        return X @ self.weights + self.bias",
    "testCases": [],
    "hints": [
      "L2 penalty: alpha * sum(w^2)",
      "Modified gradient: original_gradient + 2*alpha*w",
      "Bias is not regularized",
      "Larger alpha = more regularization = simpler model",
      "Ridge shrinks weights toward zero"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t2-ex07",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Implement L1 Regularization (Lasso)",
    "difficulty": 3,
    "description": "Add L1 regularization to linear regression for feature selection.\n\nRequirements:\n- Add penalty term: Î» * ||w||_1\n- Modified cost: MSE + Î» * sum(|w|)\n- Modified gradient uses sign of weights\n- Can drive weights exactly to zero",
    "starterCode": "import numpy as np\n\nclass LassoRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000, alpha=1.0):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.alpha = alpha\n        self.weights = None\n        self.bias = None\n        self.cost_history = []\n\n    def fit(self, X, y):\n        \"\"\"Fit Lasso regression with L1 regularization.\"\"\"\n        # TODO: Implement Lasso regression\n        pass\n\n    def predict(self, X):\n        \"\"\"Predict using linear model.\"\"\"\n        # TODO: Implement predict\n        pass",
    "solution": "import numpy as np\n\nclass LassoRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000, alpha=1.0):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.alpha = alpha\n        self.weights = None\n        self.bias = None\n        self.cost_history = []\n\n    def fit(self, X, y):\n        \"\"\"Fit Lasso regression with L1 regularization.\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n        n_samples, n_features = X.shape\n\n        # Initialize parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # Gradient descent with L1 regularization\n        for _ in range(self.n_iterations):\n            # Forward pass\n            y_pred = X @ self.weights + self.bias\n\n            # Calculate cost with L1 penalty\n            mse = np.mean((y_pred - y) ** 2)\n            l1_penalty = self.alpha * np.sum(np.abs(self.weights))\n            cost = mse + l1_penalty\n            self.cost_history.append(cost)\n\n            # Calculate gradients with L1 regularization\n            # L1 gradient is sign of weight\n            dw = (2 / n_samples) * X.T @ (y_pred - y) + self.alpha * np.sign(self.weights)\n            db = (2 / n_samples) * np.sum(y_pred - y)\n\n            # Update parameters\n            self.weights -= self.learning_rate * dw\n            self.bias -= self.learning_rate * db\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using linear model.\"\"\"\n        X = np.array(X)\n        return X @ self.weights + self.bias",
    "testCases": [],
    "hints": [
      "L1 penalty: alpha * sum(|w|)",
      "L1 gradient: alpha * sign(w)",
      "np.sign() returns -1, 0, or 1",
      "L1 can drive weights exactly to zero",
      "Lasso performs automatic feature selection"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t2-ex08",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Implement Sigmoid Function",
    "difficulty": 1,
    "description": "Implement sigmoid activation function for logistic regression.\n\nRequirements:\n- Sigmoid: Ï(z) = 1 / (1 + e^(-z))\n- Handle numerical stability (large positive/negative values)\n- Return values in range (0, 1)",
    "starterCode": "import numpy as np\n\ndef sigmoid(z):\n    \"\"\"\n    Compute sigmoid function.\n\n    Args:\n        z: Input (can be scalar, vector, or matrix)\n\n    Returns:\n        Sigmoid output in range (0, 1)\n    \"\"\"\n    # TODO: Implement sigmoid\n    pass",
    "solution": "import numpy as np\n\ndef sigmoid(z):\n    \"\"\"\n    Compute sigmoid function.\n\n    Args:\n        z: Input (can be scalar, vector, or matrix)\n\n    Returns:\n        Sigmoid output in range (0, 1)\n    \"\"\"\n    # Clip z for numerical stability\n    z = np.clip(z, -500, 500)\n    return 1 / (1 + np.exp(-z))",
    "testCases": [],
    "hints": [
      "Sigmoid formula: 1 / (1 + exp(-z))",
      "Clip z to prevent overflow in exp()",
      "Sigmoid(0) = 0.5",
      "Sigmoid approaches 1 as z â â",
      "Sigmoid approaches 0 as z â -â"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t2-ex09",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Implement Binary Cross-Entropy Loss",
    "difficulty": 2,
    "description": "Implement binary cross-entropy loss for logistic regression.\n\nRequirements:\n- Loss: -[y*log(p) + (1-y)*log(1-p)]\n- Handle numerical stability\n- Return average loss over samples",
    "starterCode": "import numpy as np\n\ndef binary_cross_entropy(y_true, y_pred):\n    \"\"\"\n    Calculate binary cross-entropy loss.\n\n    Args:\n        y_true: True labels (0 or 1)\n        y_pred: Predicted probabilities\n\n    Returns:\n        Average BCE loss\n    \"\"\"\n    # TODO: Implement BCE loss\n    pass",
    "solution": "import numpy as np\n\ndef binary_cross_entropy(y_true, y_pred):\n    \"\"\"\n    Calculate binary cross-entropy loss.\n\n    Args:\n        y_true: True labels (0 or 1)\n        y_pred: Predicted probabilities\n\n    Returns:\n        Average BCE loss\n    \"\"\"\n    # Clip predictions for numerical stability\n    epsilon = 1e-15\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n\n    # Binary cross-entropy formula\n    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\n    return loss",
    "testCases": [],
    "hints": [
      "Clip predictions to avoid log(0)",
      "Formula: -[y*log(p) + (1-y)*log(1-p)]",
      "Take mean over all samples",
      "Loss is minimized when y_pred = y_true",
      "Small epsilon (e.g., 1e-15) prevents log(0)"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t2-ex10",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Implement Logistic Regression",
    "difficulty": 3,
    "description": "Implement binary logistic regression from scratch.\n\nRequirements:\n- Use sigmoid activation\n- Binary cross-entropy loss\n- Gradient descent optimization\n- Predict probabilities and classes",
    "starterCode": "import numpy as np\n\nclass LogisticRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n        self.cost_history = []\n\n    def sigmoid(self, z):\n        \"\"\"Sigmoid activation function.\"\"\"\n        # TODO: Implement sigmoid\n        pass\n\n    def fit(self, X, y):\n        \"\"\"Fit logistic regression using gradient descent.\"\"\"\n        # TODO: Implement fit\n        pass\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities.\"\"\"\n        # TODO: Implement predict_proba\n        pass\n\n    def predict(self, X, threshold=0.5):\n        \"\"\"Predict class labels.\"\"\"\n        # TODO: Implement predict\n        pass",
    "solution": "import numpy as np\n\nclass LogisticRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n        self.cost_history = []\n\n    def sigmoid(self, z):\n        \"\"\"Sigmoid activation function.\"\"\"\n        z = np.clip(z, -500, 500)\n        return 1 / (1 + np.exp(-z))\n\n    def fit(self, X, y):\n        \"\"\"Fit logistic regression using gradient descent.\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n        n_samples, n_features = X.shape\n\n        # Initialize parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # Gradient descent\n        for _ in range(self.n_iterations):\n            # Forward pass\n            z = X @ self.weights + self.bias\n            y_pred = self.sigmoid(z)\n\n            # Calculate cost (binary cross-entropy)\n            epsilon = 1e-15\n            y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n            cost = -np.mean(y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped))\n            self.cost_history.append(cost)\n\n            # Calculate gradients\n            dw = (1 / n_samples) * X.T @ (y_pred - y)\n            db = (1 / n_samples) * np.sum(y_pred - y)\n\n            # Update parameters\n            self.weights -= self.learning_rate * dw\n            self.bias -= self.learning_rate * db\n\n        return self\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities.\"\"\"\n        X = np.array(X)\n        z = X @ self.weights + self.bias\n        return self.sigmoid(z)\n\n    def predict(self, X, threshold=0.5):\n        \"\"\"Predict class labels.\"\"\"\n        probas = self.predict_proba(X)\n        return (probas >= threshold).astype(int)",
    "testCases": [],
    "hints": [
      "Use sigmoid to convert linear output to probabilities",
      "Binary cross-entropy is the appropriate loss function",
      "Gradient: dw = (1/n) * X^T * (y_pred - y)",
      "Predict class 1 if probability >= threshold",
      "Clip probabilities for numerical stability in loss calculation"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t2-ex11",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Implement Logistic Regression with L2 Regularization",
    "difficulty": 3,
    "description": "Add L2 regularization to logistic regression.\n\nRequirements:\n- Add L2 penalty to cost function\n- Modify gradient with regularization term\n- Prevent overfitting on training data",
    "starterCode": "import numpy as np\n\nclass RegularizedLogisticRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000, alpha=1.0):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.alpha = alpha\n        self.weights = None\n        self.bias = None\n        self.cost_history = []\n\n    def sigmoid(self, z):\n        \"\"\"Sigmoid activation function.\"\"\"\n        z = np.clip(z, -500, 500)\n        return 1 / (1 + np.exp(-z))\n\n    def fit(self, X, y):\n        \"\"\"Fit regularized logistic regression.\"\"\"\n        # TODO: Implement fit with L2 regularization\n        pass\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities.\"\"\"\n        # TODO: Implement predict_proba\n        pass\n\n    def predict(self, X, threshold=0.5):\n        \"\"\"Predict class labels.\"\"\"\n        # TODO: Implement predict\n        pass",
    "solution": "import numpy as np\n\nclass RegularizedLogisticRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000, alpha=1.0):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.alpha = alpha\n        self.weights = None\n        self.bias = None\n        self.cost_history = []\n\n    def sigmoid(self, z):\n        \"\"\"Sigmoid activation function.\"\"\"\n        z = np.clip(z, -500, 500)\n        return 1 / (1 + np.exp(-z))\n\n    def fit(self, X, y):\n        \"\"\"Fit regularized logistic regression.\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n        n_samples, n_features = X.shape\n\n        # Initialize parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # Gradient descent with L2 regularization\n        for _ in range(self.n_iterations):\n            # Forward pass\n            z = X @ self.weights + self.bias\n            y_pred = self.sigmoid(z)\n\n            # Calculate cost with L2 penalty\n            epsilon = 1e-15\n            y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n            bce = -np.mean(y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped))\n            l2_penalty = (self.alpha / (2 * n_samples)) * np.sum(self.weights ** 2)\n            cost = bce + l2_penalty\n            self.cost_history.append(cost)\n\n            # Calculate gradients with L2 regularization\n            dw = (1 / n_samples) * X.T @ (y_pred - y) + (self.alpha / n_samples) * self.weights\n            db = (1 / n_samples) * np.sum(y_pred - y)\n\n            # Update parameters\n            self.weights -= self.learning_rate * dw\n            self.bias -= self.learning_rate * db\n\n        return self\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities.\"\"\"\n        X = np.array(X)\n        z = X @ self.weights + self.bias\n        return self.sigmoid(z)\n\n    def predict(self, X, threshold=0.5):\n        \"\"\"Predict class labels.\"\"\"\n        probas = self.predict_proba(X)\n        return (probas >= threshold).astype(int)",
    "testCases": [],
    "hints": [
      "L2 penalty: (alpha/(2*n)) * sum(w^2)",
      "Modified gradient: gradient + (alpha/n) * w",
      "Bias is typically not regularized",
      "Regularization prevents large weight values",
      "Higher alpha = stronger regularization"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t2-ex12",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Implement One-vs-Rest Multiclass Classification",
    "difficulty": 4,
    "description": "Extend binary logistic regression to multiclass using One-vs-Rest.\n\nRequirements:\n- Train one binary classifier per class\n- Predict using all classifiers\n- Return class with highest probability\n- Handle k classes",
    "starterCode": "import numpy as np\n\nclass OneVsRestClassifier:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.classifiers = []\n        self.classes = None\n\n    def fit(self, X, y):\n        \"\"\"Train one binary classifier per class.\"\"\"\n        # TODO: Implement One-vs-Rest training\n        pass\n\n    def predict_proba(self, X):\n        \"\"\"Predict probabilities for each class.\"\"\"\n        # TODO: Implement predict_proba\n        pass\n\n    def predict(self, X):\n        \"\"\"Predict class labels.\"\"\"\n        # TODO: Implement predict\n        pass",
    "solution": "import numpy as np\n\nclass BinaryLogisticRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n\n    def sigmoid(self, z):\n        z = np.clip(z, -500, 500)\n        return 1 / (1 + np.exp(-z))\n\n    def fit(self, X, y):\n        X = np.array(X)\n        y = np.array(y)\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        for _ in range(self.n_iterations):\n            z = X @ self.weights + self.bias\n            y_pred = self.sigmoid(z)\n            dw = (1 / n_samples) * X.T @ (y_pred - y)\n            db = (1 / n_samples) * np.sum(y_pred - y)\n            self.weights -= self.learning_rate * dw\n            self.bias -= self.learning_rate * db\n        return self\n\n    def predict_proba(self, X):\n        X = np.array(X)\n        z = X @ self.weights + self.bias\n        return self.sigmoid(z)\n\nclass OneVsRestClassifier:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.classifiers = []\n        self.classes = None\n\n    def fit(self, X, y):\n        \"\"\"Train one binary classifier per class.\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n        self.classes = np.unique(y)\n\n        # Train one classifier per class\n        for cls in self.classes:\n            # Create binary labels: 1 if class matches, 0 otherwise\n            y_binary = (y == cls).astype(int)\n\n            # Train binary classifier\n            clf = BinaryLogisticRegression(self.learning_rate, self.n_iterations)\n            clf.fit(X, y_binary)\n            self.classifiers.append(clf)\n\n        return self\n\n    def predict_proba(self, X):\n        \"\"\"Predict probabilities for each class.\"\"\"\n        # Get predictions from all classifiers\n        probas = np.array([clf.predict_proba(X) for clf in self.classifiers]).T\n        # Normalize so probabilities sum to 1\n        probas = probas / np.sum(probas, axis=1, keepdims=True)\n        return probas\n\n    def predict(self, X):\n        \"\"\"Predict class labels.\"\"\"\n        probas = self.predict_proba(X)\n        return self.classes[np.argmax(probas, axis=1)]",
    "testCases": [],
    "hints": [
      "Train k binary classifiers for k classes",
      "Each classifier: current class vs all others",
      "Create binary labels: 1 for target class, 0 for others",
      "Predict: class with highest probability",
      "Normalize probabilities to sum to 1"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t2-ex13",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Implement Softmax Function",
    "difficulty": 2,
    "description": "Implement softmax activation for multiclass classification.\n\nRequirements:\n- Convert logits to probabilities\n- Ensure numerical stability\n- Probabilities sum to 1\n- Handle batches of inputs",
    "starterCode": "import numpy as np\n\ndef softmax(z):\n    \"\"\"\n    Compute softmax activation.\n\n    Args:\n        z: Logits (n_samples, n_classes) or (n_classes,)\n\n    Returns:\n        Probabilities that sum to 1\n    \"\"\"\n    # TODO: Implement softmax\n    pass",
    "solution": "import numpy as np\n\ndef softmax(z):\n    \"\"\"\n    Compute softmax activation.\n\n    Args:\n        z: Logits (n_samples, n_classes) or (n_classes,)\n\n    Returns:\n        Probabilities that sum to 1\n    \"\"\"\n    z = np.array(z)\n\n    # Subtract max for numerical stability\n    if z.ndim == 1:\n        z_shifted = z - np.max(z)\n        exp_z = np.exp(z_shifted)\n        return exp_z / np.sum(exp_z)\n    else:\n        z_shifted = z - np.max(z, axis=1, keepdims=True)\n        exp_z = np.exp(z_shifted)\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)",
    "testCases": [],
    "hints": [
      "Softmax: exp(z_i) / sum(exp(z_j))",
      "Subtract max(z) before exp for numerical stability",
      "Output probabilities sum to 1",
      "Handle both single samples and batches",
      "Use keepdims=True for correct broadcasting"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t2-ex14",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Implement Learning Rate Decay",
    "difficulty": 2,
    "description": "Implement learning rate schedules for better convergence.\n\nRequirements:\n- Step decay: reduce by factor every n epochs\n- Exponential decay: multiply by decay rate\n- Time-based decay: decrease over iterations\n- Return updated learning rate",
    "starterCode": "import numpy as np\n\nclass LearningRateScheduler:\n    def __init__(self, initial_lr=0.1):\n        self.initial_lr = initial_lr\n        self.current_lr = initial_lr\n\n    def step_decay(self, epoch, drop_rate=0.5, epochs_drop=10):\n        \"\"\"Step decay: reduce LR every epochs_drop.\"\"\"\n        # TODO: Implement step decay\n        pass\n\n    def exponential_decay(self, epoch, decay_rate=0.95):\n        \"\"\"Exponential decay: lr = lr0 * decay_rate^epoch.\"\"\"\n        # TODO: Implement exponential decay\n        pass\n\n    def time_decay(self, epoch, decay_rate=0.01):\n        \"\"\"Time-based decay: lr = lr0 / (1 + decay_rate * epoch).\"\"\"\n        # TODO: Implement time decay\n        pass",
    "solution": "import numpy as np\n\nclass LearningRateScheduler:\n    def __init__(self, initial_lr=0.1):\n        self.initial_lr = initial_lr\n        self.current_lr = initial_lr\n\n    def step_decay(self, epoch, drop_rate=0.5, epochs_drop=10):\n        \"\"\"Step decay: reduce LR every epochs_drop.\"\"\"\n        self.current_lr = self.initial_lr * (drop_rate ** (epoch // epochs_drop))\n        return self.current_lr\n\n    def exponential_decay(self, epoch, decay_rate=0.95):\n        \"\"\"Exponential decay: lr = lr0 * decay_rate^epoch.\"\"\"\n        self.current_lr = self.initial_lr * (decay_rate ** epoch)\n        return self.current_lr\n\n    def time_decay(self, epoch, decay_rate=0.01):\n        \"\"\"Time-based decay: lr = lr0 / (1 + decay_rate * epoch).\"\"\"\n        self.current_lr = self.initial_lr / (1 + decay_rate * epoch)\n        return self.current_lr",
    "testCases": [],
    "hints": [
      "Step decay: lr *= drop_rate every epochs_drop",
      "Exponential: lr = lr0 * decay_rate^epoch",
      "Time-based: lr = lr0 / (1 + decay * epoch)",
      "Learning rate should decrease over time",
      "Helps with fine-tuning in later epochs"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t2-ex15",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Implement Early Stopping",
    "difficulty": 3,
    "description": "Implement early stopping to prevent overfitting during training.\n\nRequirements:\n- Monitor validation loss\n- Stop if no improvement for patience epochs\n- Save best model weights\n- Restore best weights after stopping",
    "starterCode": "import numpy as np\n\nclass EarlyStopping:\n    def __init__(self, patience=10, min_delta=0.001):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.best_loss = None\n        self.counter = 0\n        self.best_weights = None\n\n    def __call__(self, val_loss, model):\n        \"\"\"\n        Check if training should stop.\n\n        Args:\n            val_loss: Current validation loss\n            model: Model with weights attribute\n\n        Returns:\n            True if should stop, False otherwise\n        \"\"\"\n        # TODO: Implement early stopping logic\n        pass",
    "solution": "import numpy as np\n\nclass EarlyStopping:\n    def __init__(self, patience=10, min_delta=0.001):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.best_loss = None\n        self.counter = 0\n        self.best_weights = None\n\n    def __call__(self, val_loss, model):\n        \"\"\"\n        Check if training should stop.\n\n        Args:\n            val_loss: Current validation loss\n            model: Model with weights attribute\n\n        Returns:\n            True if should stop, False otherwise\n        \"\"\"\n        if self.best_loss is None:\n            # First epoch\n            self.best_loss = val_loss\n            self.best_weights = model.weights.copy()\n            return False\n\n        if val_loss < self.best_loss - self.min_delta:\n            # Improvement found\n            self.best_loss = val_loss\n            self.best_weights = model.weights.copy()\n            self.counter = 0\n            return False\n        else:\n            # No improvement\n            self.counter += 1\n            if self.counter >= self.patience:\n                # Restore best weights\n                model.weights = self.best_weights\n                return True\n            return False",
    "testCases": [],
    "hints": [
      "Track best validation loss seen so far",
      "Increment counter if no improvement",
      "Reset counter when improvement found",
      "Stop when counter reaches patience",
      "Save and restore best model weights"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t2-ex16",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Calculate R-Squared Score",
    "difficulty": 2,
    "description": "Implement RÂ² (coefficient of determination) for regression evaluation.\n\nRequirements:\n- Calculate total sum of squares (TSS)\n- Calculate residual sum of squares (RSS)\n- RÂ² = 1 - (RSS / TSS)\n- Return score between -â and 1",
    "starterCode": "import numpy as np\n\ndef r2_score(y_true, y_pred):\n    \"\"\"\n    Calculate R-squared score.\n\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n\n    Returns:\n        RÂ² score\n    \"\"\"\n    # TODO: Implement RÂ² score\n    pass",
    "solution": "import numpy as np\n\ndef r2_score(y_true, y_pred):\n    \"\"\"\n    Calculate R-squared score.\n\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n\n    Returns:\n        RÂ² score\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Total sum of squares (variance in y)\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n\n    # Residual sum of squares (variance not explained)\n    rss = np.sum((y_true - y_pred) ** 2)\n\n    # RÂ² = 1 - (RSS / TSS)\n    r2 = 1 - (rss / tss)\n\n    return r2",
    "testCases": [],
    "hints": [
      "TSS = sum((y_true - y_mean)^2)",
      "RSS = sum((y_true - y_pred)^2)",
      "RÂ² = 1 - RSS/TSS",
      "RÂ² = 1 means perfect predictions",
      "RÂ² = 0 means predictions equal to mean"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t3-ex01",
    "subjectId": "cs402",
    "topicId": "cs402-topic-3",
    "title": "Calculate Gini Impurity",
    "difficulty": 2,
    "description": "Implement Gini impurity for decision tree splitting.\n\nRequirements:\n- Calculate class probabilities\n- Gini = 1 - Î£(p_iÂ²)\n- Return impurity score (0 = pure, 0.5 = mixed for binary)\n- Handle multiple classes",
    "starterCode": "import numpy as np\n\ndef gini_impurity(y):\n    \"\"\"\n    Calculate Gini impurity of a node.\n\n    Args:\n        y: Labels in the node\n\n    Returns:\n        Gini impurity score\n    \"\"\"\n    # TODO: Implement Gini impurity\n    pass",
    "solution": "import numpy as np\n\ndef gini_impurity(y):\n    \"\"\"\n    Calculate Gini impurity of a node.\n\n    Args:\n        y: Labels in the node\n\n    Returns:\n        Gini impurity score\n    \"\"\"\n    if len(y) == 0:\n        return 0\n\n    # Calculate class probabilities\n    classes, counts = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n\n    # Gini = 1 - sum(p_i^2)\n    gini = 1 - np.sum(probabilities ** 2)\n\n    return gini",
    "testCases": [],
    "hints": [
      "Gini = 1 - sum of squared probabilities",
      "Pure node (all same class) has Gini = 0",
      "Maximum impurity for binary: Gini = 0.5",
      "Use np.unique() to count class occurrences",
      "Probabilities = counts / total samples"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t3-ex02",
    "subjectId": "cs402",
    "topicId": "cs402-topic-3",
    "title": "Calculate Entropy",
    "difficulty": 2,
    "description": "Implement entropy for information gain calculation.\n\nRequirements:\n- Calculate class probabilities\n- Entropy = -Î£(p_i * log2(p_i))\n- Handle p = 0 case\n- Return entropy score",
    "starterCode": "import numpy as np\n\ndef entropy(y):\n    \"\"\"\n    Calculate entropy of a node.\n\n    Args:\n        y: Labels in the node\n\n    Returns:\n        Entropy score\n    \"\"\"\n    # TODO: Implement entropy\n    pass",
    "solution": "import numpy as np\n\ndef entropy(y):\n    \"\"\"\n    Calculate entropy of a node.\n\n    Args:\n        y: Labels in the node\n\n    Returns:\n        Entropy score\n    \"\"\"\n    if len(y) == 0:\n        return 0\n\n    # Calculate class probabilities\n    classes, counts = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n\n    # Entropy = -sum(p_i * log2(p_i))\n    # Filter out zero probabilities to avoid log(0)\n    probabilities = probabilities[probabilities > 0]\n    ent = -np.sum(probabilities * np.log2(probabilities))\n\n    return ent",
    "testCases": [],
    "hints": [
      "Entropy = -sum(p * log2(p))",
      "Pure node has entropy = 0",
      "Filter out zero probabilities before log",
      "Use np.log2() for base-2 logarithm",
      "Maximum entropy for binary: 1 bit"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t3-ex03",
    "subjectId": "cs402",
    "topicId": "cs402-topic-3",
    "title": "Calculate Information Gain",
    "difficulty": 2,
    "description": "Calculate information gain for a split decision.\n\nRequirements:\n- Calculate parent entropy\n- Calculate weighted child entropies\n- Information Gain = parent_entropy - weighted_child_entropy\n- Return gain value",
    "starterCode": "import numpy as np\n\ndef information_gain(y_parent, y_left, y_right):\n    \"\"\"\n    Calculate information gain from a split.\n\n    Args:\n        y_parent: Labels before split\n        y_left: Labels in left child\n        y_right: Labels in right child\n\n    Returns:\n        Information gain\n    \"\"\"\n    # TODO: Implement information gain\n    pass",
    "solution": "import numpy as np\n\ndef entropy(y):\n    \"\"\"Calculate entropy.\"\"\"\n    if len(y) == 0:\n        return 0\n    classes, counts = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    probabilities = probabilities[probabilities > 0]\n    return -np.sum(probabilities * np.log2(probabilities))\n\ndef information_gain(y_parent, y_left, y_right):\n    \"\"\"\n    Calculate information gain from a split.\n\n    Args:\n        y_parent: Labels before split\n        y_left: Labels in left child\n        y_right: Labels in right child\n\n    Returns:\n        Information gain\n    \"\"\"\n    n = len(y_parent)\n    n_left = len(y_left)\n    n_right = len(y_right)\n\n    # Parent entropy\n    parent_entropy = entropy(y_parent)\n\n    # Weighted child entropy\n    if n_left == 0 or n_right == 0:\n        return 0\n\n    child_entropy = (n_left / n) * entropy(y_left) + (n_right / n) * entropy(y_right)\n\n    # Information gain\n    gain = parent_entropy - child_entropy\n\n    return gain",
    "testCases": [],
    "hints": [
      "Information Gain = H(parent) - weighted_H(children)",
      "Weight by proportion of samples in each child",
      "Higher gain = better split",
      "If all samples go to one child, gain = 0",
      "Choose split with maximum information gain"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t3-ex04",
    "subjectId": "cs402",
    "topicId": "cs402-topic-3",
    "title": "Find Best Split",
    "difficulty": 3,
    "description": "Find the best feature and threshold to split a node.\n\nRequirements:\n- Try all features and thresholds\n- Calculate information gain for each split\n- Return best feature index and threshold\n- Handle continuous features",
    "starterCode": "import numpy as np\n\ndef find_best_split(X, y):\n    \"\"\"\n    Find best feature and threshold for splitting.\n\n    Args:\n        X: Feature matrix (n_samples, n_features)\n        y: Labels\n\n    Returns:\n        best_feature: Index of best feature\n        best_threshold: Best threshold value\n        best_gain: Information gain achieved\n    \"\"\"\n    # TODO: Implement best split finding\n    pass",
    "solution": "import numpy as np\n\ndef entropy(y):\n    \"\"\"Calculate entropy.\"\"\"\n    if len(y) == 0:\n        return 0\n    classes, counts = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    probabilities = probabilities[probabilities > 0]\n    return -np.sum(probabilities * np.log2(probabilities))\n\ndef information_gain(y_parent, y_left, y_right):\n    \"\"\"Calculate information gain.\"\"\"\n    n = len(y_parent)\n    n_left = len(y_left)\n    n_right = len(y_right)\n\n    if n_left == 0 or n_right == 0:\n        return 0\n\n    parent_entropy = entropy(y_parent)\n    child_entropy = (n_left / n) * entropy(y_left) + (n_right / n) * entropy(y_right)\n    return parent_entropy - child_entropy\n\ndef find_best_split(X, y):\n    \"\"\"\n    Find best feature and threshold for splitting.\n\n    Args:\n        X: Feature matrix (n_samples, n_features)\n        y: Labels\n\n    Returns:\n        best_feature: Index of best feature\n        best_threshold: Best threshold value\n        best_gain: Information gain achieved\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    n_samples, n_features = X.shape\n\n    best_gain = -1\n    best_feature = None\n    best_threshold = None\n\n    # Try each feature\n    for feature_idx in range(n_features):\n        feature_values = X[:, feature_idx]\n\n        # Try each unique value as threshold\n        thresholds = np.unique(feature_values)\n\n        for threshold in thresholds:\n            # Split data\n            left_mask = feature_values <= threshold\n            right_mask = ~left_mask\n\n            if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n                continue\n\n            y_left = y[left_mask]\n            y_right = y[right_mask]\n\n            # Calculate gain\n            gain = information_gain(y, y_left, y_right)\n\n            if gain > best_gain:\n                best_gain = gain\n                best_feature = feature_idx\n                best_threshold = threshold\n\n    return best_feature, best_threshold, best_gain",
    "testCases": [],
    "hints": [
      "Try every feature and every unique value as threshold",
      "Split data: left (<=threshold), right (>threshold)",
      "Calculate information gain for each split",
      "Keep track of best split seen so far",
      "Skip splits that put all samples in one child"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t3-ex05",
    "subjectId": "cs402",
    "topicId": "cs402-topic-3",
    "title": "Implement Decision Tree Node",
    "difficulty": 3,
    "description": "Create a node class for decision tree structure.\n\nRequirements:\n- Store feature, threshold for internal nodes\n- Store value for leaf nodes\n- Track left and right children\n- Implement is_leaf method",
    "starterCode": "class TreeNode:\n    def __init__(self):\n        \"\"\"Initialize tree node.\"\"\"\n        # TODO: Define node attributes\n        pass\n\n    def is_leaf(self):\n        \"\"\"Check if node is a leaf.\"\"\"\n        # TODO: Implement is_leaf\n        pass",
    "solution": "class TreeNode:\n    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n        \"\"\"\n        Initialize tree node.\n\n        For internal nodes: feature, threshold, left, right are set\n        For leaf nodes: value is set\n        \"\"\"\n        self.feature = feature      # Feature index to split on\n        self.threshold = threshold  # Threshold value for split\n        self.left = left           # Left child node\n        self.right = right         # Right child node\n        self.value = value         # Class label for leaf node\n\n    def is_leaf(self):\n        \"\"\"Check if node is a leaf.\"\"\"\n        return self.value is not None",
    "testCases": [],
    "hints": [
      "Internal nodes have feature and threshold",
      "Leaf nodes have value (class label)",
      "Left and right store child nodes",
      "Node is leaf if it has no children",
      "value is None for internal nodes"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t3-ex06",
    "subjectId": "cs402",
    "topicId": "cs402-topic-3",
    "title": "Implement Basic Decision Tree",
    "difficulty": 4,
    "description": "Implement a basic decision tree classifier from scratch.\n\nRequirements:\n- Recursive tree building\n- Stop at max depth or pure nodes\n- Use Gini impurity for splitting\n- Implement fit and predict methods",
    "starterCode": "import numpy as np\n\nclass DecisionTreeClassifier:\n    def __init__(self, max_depth=5, min_samples_split=2):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.root = None\n\n    def fit(self, X, y):\n        \"\"\"Build decision tree.\"\"\"\n        # TODO: Implement tree building\n        pass\n\n    def predict(self, X):\n        \"\"\"Predict class labels.\"\"\"\n        # TODO: Implement prediction\n        pass",
    "solution": "import numpy as np\n\nclass TreeNode:\n    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.value = value\n\n    def is_leaf(self):\n        return self.value is not None\n\nclass DecisionTreeClassifier:\n    def __init__(self, max_depth=5, min_samples_split=2):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.root = None\n\n    def gini_impurity(self, y):\n        \"\"\"Calculate Gini impurity.\"\"\"\n        if len(y) == 0:\n            return 0\n        classes, counts = np.unique(y, return_counts=True)\n        probabilities = counts / len(y)\n        return 1 - np.sum(probabilities ** 2)\n\n    def split(self, X, y, feature, threshold):\n        \"\"\"Split data based on feature and threshold.\"\"\"\n        left_mask = X[:, feature] <= threshold\n        right_mask = ~left_mask\n        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n\n    def find_best_split(self, X, y):\n        \"\"\"Find best split.\"\"\"\n        best_gini = float('inf')\n        best_feature = None\n        best_threshold = None\n\n        for feature_idx in range(X.shape[1]):\n            thresholds = np.unique(X[:, feature_idx])\n            for threshold in thresholds:\n                left_mask = X[:, feature_idx] <= threshold\n                right_mask = ~left_mask\n\n                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n                    continue\n\n                y_left, y_right = y[left_mask], y[right_mask]\n                n = len(y)\n                gini = (len(y_left) / n) * self.gini_impurity(y_left) + \\\n                       (len(y_right) / n) * self.gini_impurity(y_right)\n\n                if gini < best_gini:\n                    best_gini = gini\n                    best_feature = feature_idx\n                    best_threshold = threshold\n\n        return best_feature, best_threshold\n\n    def build_tree(self, X, y, depth=0):\n        \"\"\"Recursively build tree.\"\"\"\n        n_samples = len(y)\n        n_classes = len(np.unique(y))\n\n        # Stopping criteria\n        if depth >= self.max_depth or n_classes == 1 or n_samples < self.min_samples_split:\n            leaf_value = np.argmax(np.bincount(y))\n            return TreeNode(value=leaf_value)\n\n        # Find best split\n        feature, threshold = self.find_best_split(X, y)\n\n        if feature is None:\n            leaf_value = np.argmax(np.bincount(y))\n            return TreeNode(value=leaf_value)\n\n        # Split and recurse\n        X_left, X_right, y_left, y_right = self.split(X, y, feature, threshold)\n        left_child = self.build_tree(X_left, y_left, depth + 1)\n        right_child = self.build_tree(X_right, y_right, depth + 1)\n\n        return TreeNode(feature=feature, threshold=threshold, left=left_child, right=right_child)\n\n    def fit(self, X, y):\n        \"\"\"Build decision tree.\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n        self.root = self.build_tree(X, y)\n        return self\n\n    def predict_sample(self, x, node):\n        \"\"\"Predict single sample.\"\"\"\n        if node.is_leaf():\n            return node.value\n\n        if x[node.feature] <= node.threshold:\n            return self.predict_sample(x, node.left)\n        else:\n            return self.predict_sample(x, node.right)\n\n    def predict(self, X):\n        \"\"\"Predict class labels.\"\"\"\n        X = np.array(X)\n        return np.array([self.predict_sample(x, self.root) for x in X])",
    "testCases": [],
    "hints": [
      "Build tree recursively from root to leaves",
      "Stop when max_depth reached or node is pure",
      "Choose split with minimum Gini impurity",
      "Leaf nodes store most common class",
      "Traverse tree for prediction: left if <=threshold, right otherwise"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t3-ex07",
    "subjectId": "cs402",
    "topicId": "cs402-topic-3",
    "title": "Calculate Feature Importance for Decision Tree",
    "difficulty": 3,
    "description": "Calculate feature importance based on impurity reduction.\n\nRequirements:\n- Track total impurity reduction per feature\n- Weight by number of samples\n- Normalize importances to sum to 1\n- Return array of importances",
    "starterCode": "import numpy as np\n\ndef calculate_feature_importance(tree, n_features):\n    \"\"\"\n    Calculate feature importance for decision tree.\n\n    Args:\n        tree: Trained decision tree with root node\n        n_features: Number of features\n\n    Returns:\n        importances: Array of feature importances\n    \"\"\"\n    # TODO: Implement feature importance calculation\n    pass",
    "solution": "import numpy as np\n\ndef calculate_feature_importance(tree, n_features):\n    \"\"\"\n    Calculate feature importance for decision tree.\n\n    Args:\n        tree: Trained decision tree with root node\n        n_features: Number of features\n\n    Returns:\n        importances: Array of feature importances\n    \"\"\"\n    importances = np.zeros(n_features)\n\n    def traverse(node, n_samples):\n        \"\"\"Recursively calculate importance.\"\"\"\n        if node.is_leaf():\n            return\n\n        # Importance = (samples at node / total) * impurity reduction\n        # For simplicity, we'll count how often each feature is used\n        importances[node.feature] += n_samples\n\n        # Recurse to children (approximate sample counts)\n        if node.left:\n            traverse(node.left, n_samples / 2)\n        if node.right:\n            traverse(node.right, n_samples / 2)\n\n    # Start traversal\n    traverse(tree.root, 1.0)\n\n    # Normalize to sum to 1\n    if np.sum(importances) > 0:\n        importances = importances / np.sum(importances)\n\n    return importances",
    "testCases": [],
    "hints": [
      "Traverse tree and sum importance per feature",
      "Importance increases when feature is used for splitting",
      "Weight by number of samples at node",
      "Normalize so importances sum to 1",
      "Features used higher in tree are more important"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t3-ex08",
    "subjectId": "cs402",
    "topicId": "cs402-topic-3",
    "title": "Implement Bootstrap Sampling",
    "difficulty": 2,
    "description": "Implement bootstrap sampling for ensemble methods.\n\nRequirements:\n- Sample n samples with replacement\n- Return sampled data and out-of-bag indices\n- Maintain approximately 63% unique samples\n- Support both X and y",
    "starterCode": "import numpy as np\n\ndef bootstrap_sample(X, y, random_state=None):\n    \"\"\"\n    Create bootstrap sample.\n\n    Args:\n        X: Feature matrix\n        y: Labels\n        random_state: Random seed\n\n    Returns:\n        X_sample, y_sample, oob_indices\n    \"\"\"\n    # TODO: Implement bootstrap sampling\n    pass",
    "solution": "import numpy as np\n\ndef bootstrap_sample(X, y, random_state=None):\n    \"\"\"\n    Create bootstrap sample.\n\n    Args:\n        X: Feature matrix\n        y: Labels\n        random_state: Random seed\n\n    Returns:\n        X_sample, y_sample, oob_indices\n    \"\"\"\n    if random_state is not None:\n        np.random.seed(random_state)\n\n    n_samples = len(X)\n\n    # Sample with replacement\n    sample_indices = np.random.choice(n_samples, n_samples, replace=True)\n\n    X_sample = X[sample_indices]\n    y_sample = y[sample_indices]\n\n    # Out-of-bag samples (not selected)\n    all_indices = set(range(n_samples))\n    sampled_indices = set(sample_indices)\n    oob_indices = np.array(list(all_indices - sampled_indices))\n\n    return X_sample, y_sample, oob_indices",
    "testCases": [],
    "hints": [
      "Bootstrap: sample n times with replacement",
      "Use np.random.choice() with replace=True",
      "OOB samples are those not selected",
      "About 37% of samples are OOB on average",
      "OOB samples can be used for validation"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t3-ex09",
    "subjectId": "cs402",
    "topicId": "cs402-topic-3",
    "title": "Implement Random Forest Classifier",
    "difficulty": 4,
    "description": "Implement Random Forest using multiple decision trees.\n\nRequirements:\n- Train multiple trees on bootstrap samples\n- Use random feature subset for each split\n- Aggregate predictions by majority voting\n- Track out-of-bag error",
    "starterCode": "import numpy as np\n\nclass RandomForestClassifier:\n    def __init__(self, n_estimators=10, max_depth=5, max_features='sqrt'):\n        self.n_estimators = n_estimators\n        self.max_depth = max_depth\n        self.max_features = max_features\n        self.trees = []\n\n    def fit(self, X, y):\n        \"\"\"Train random forest.\"\"\"\n        # TODO: Implement random forest training\n        pass\n\n    def predict(self, X):\n        \"\"\"Predict using majority voting.\"\"\"\n        # TODO: Implement prediction\n        pass",
    "solution": "import numpy as np\n\nclass SimpleDecisionTree:\n    \"\"\"Simplified decision tree for random forest.\"\"\"\n    def __init__(self, max_depth=5, max_features=None):\n        self.max_depth = max_depth\n        self.max_features = max_features\n        self.tree = None\n\n    def fit(self, X, y):\n        self.n_features = X.shape[1]\n        if self.max_features is None:\n            self.max_features = self.n_features\n        self.tree = self._build_tree(X, y, 0)\n        return self\n\n    def _build_tree(self, X, y, depth):\n        if depth >= self.max_depth or len(np.unique(y)) == 1 or len(y) < 2:\n            return {'value': np.argmax(np.bincount(y))}\n\n        # Random feature subset\n        feature_indices = np.random.choice(self.n_features,\n                                          min(self.max_features, self.n_features),\n                                          replace=False)\n\n        best_gain = -1\n        best_feature = None\n        best_threshold = None\n\n        for feature_idx in feature_indices:\n            thresholds = np.unique(X[:, feature_idx])\n            for threshold in thresholds:\n                left_mask = X[:, feature_idx] <= threshold\n                if np.sum(left_mask) == 0 or np.sum(~left_mask) == 0:\n                    continue\n\n                y_left, y_right = y[left_mask], y[~left_mask]\n                n = len(y)\n                gini = (len(y_left) / n) * (1 - np.sum((np.bincount(y_left) / len(y_left)) ** 2)) + \\\n                       (len(y_right) / n) * (1 - np.sum((np.bincount(y_right) / len(y_right)) ** 2))\n\n                gain = 1 - gini\n                if gain > best_gain:\n                    best_gain = gain\n                    best_feature = feature_idx\n                    best_threshold = threshold\n\n        if best_feature is None:\n            return {'value': np.argmax(np.bincount(y))}\n\n        left_mask = X[:, best_feature] <= best_threshold\n        return {\n            'feature': best_feature,\n            'threshold': best_threshold,\n            'left': self._build_tree(X[left_mask], y[left_mask], depth + 1),\n            'right': self._build_tree(X[~left_mask], y[~left_mask], depth + 1)\n        }\n\n    def predict(self, X):\n        return np.array([self._predict_sample(x, self.tree) for x in X])\n\n    def _predict_sample(self, x, node):\n        if 'value' in node:\n            return node['value']\n        if x[node['feature']] <= node['threshold']:\n            return self._predict_sample(x, node['left'])\n        return self._predict_sample(x, node['right'])\n\nclass RandomForestClassifier:\n    def __init__(self, n_estimators=10, max_depth=5, max_features='sqrt'):\n        self.n_estimators = n_estimators\n        self.max_depth = max_depth\n        self.max_features = max_features\n        self.trees = []\n\n    def fit(self, X, y):\n        \"\"\"Train random forest.\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n\n        n_features = X.shape[1]\n        if self.max_features == 'sqrt':\n            max_features = int(np.sqrt(n_features))\n        elif self.max_features == 'log2':\n            max_features = int(np.log2(n_features))\n        else:\n            max_features = n_features\n\n        self.trees = []\n        for _ in range(self.n_estimators):\n            # Bootstrap sample\n            n_samples = len(X)\n            indices = np.random.choice(n_samples, n_samples, replace=True)\n            X_sample = X[indices]\n            y_sample = y[indices]\n\n            # Train tree\n            tree = SimpleDecisionTree(max_depth=self.max_depth, max_features=max_features)\n            tree.fit(X_sample, y_sample)\n            self.trees.append(tree)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using majority voting.\"\"\"\n        X = np.array(X)\n\n        # Get predictions from all trees\n        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n\n        # Majority voting\n        predictions = []\n        for i in range(len(X)):\n            predictions.append(np.argmax(np.bincount(tree_preds[:, i].astype(int))))\n\n        return np.array(predictions)",
    "testCases": [],
    "hints": [
      "Train each tree on bootstrap sample",
      "Use random subset of features for each split",
      "Common max_features: sqrt(n_features) for classification",
      "Aggregate predictions by majority voting",
      "More trees generally improve performance"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t3-ex10",
    "subjectId": "cs402",
    "topicId": "cs402-topic-3",
    "title": "Implement K-Nearest Neighbors",
    "difficulty": 3,
    "description": "Implement KNN classifier from scratch.\n\nRequirements:\n- Calculate distances to all training samples\n- Find k nearest neighbors\n- Use majority voting for classification\n- Support different distance metrics",
    "starterCode": "import numpy as np\n\nclass KNNClassifier:\n    def __init__(self, n_neighbors=5, metric='euclidean'):\n        self.n_neighbors = n_neighbors\n        self.metric = metric\n        self.X_train = None\n        self.y_train = None\n\n    def fit(self, X, y):\n        \"\"\"Store training data.\"\"\"\n        # TODO: Implement fit\n        pass\n\n    def predict(self, X):\n        \"\"\"Predict using KNN.\"\"\"\n        # TODO: Implement predict\n        pass",
    "solution": "import numpy as np\n\nclass KNNClassifier:\n    def __init__(self, n_neighbors=5, metric='euclidean'):\n        self.n_neighbors = n_neighbors\n        self.metric = metric\n        self.X_train = None\n        self.y_train = None\n\n    def fit(self, X, y):\n        \"\"\"Store training data.\"\"\"\n        self.X_train = np.array(X)\n        self.y_train = np.array(y)\n        return self\n\n    def distance(self, x1, x2):\n        \"\"\"Calculate distance between two points.\"\"\"\n        if self.metric == 'euclidean':\n            return np.sqrt(np.sum((x1 - x2) ** 2))\n        elif self.metric == 'manhattan':\n            return np.sum(np.abs(x1 - x2))\n        else:\n            raise ValueError(f\"Unknown metric: {self.metric}\")\n\n    def predict_sample(self, x):\n        \"\"\"Predict single sample.\"\"\"\n        # Calculate distances to all training samples\n        distances = [self.distance(x, x_train) for x_train in self.X_train]\n\n        # Get k nearest neighbors\n        k_indices = np.argsort(distances)[:self.n_neighbors]\n        k_nearest_labels = self.y_train[k_indices]\n\n        # Majority voting\n        most_common = np.argmax(np.bincount(k_nearest_labels))\n        return most_common\n\n    def predict(self, X):\n        \"\"\"Predict using KNN.\"\"\"\n        X = np.array(X)\n        return np.array([self.predict_sample(x) for x in X])",
    "testCases": [],
    "hints": [
      "KNN is instance-based: store training data",
      "For each test sample, find k nearest training samples",
      "Use Euclidean distance: sqrt(sum((x1-x2)^2))",
      "np.argsort() returns indices that sort array",
      "Majority voting: most common class among k neighbors"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t3-ex11",
    "subjectId": "cs402",
    "topicId": "cs402-topic-3",
    "title": "Implement Naive Bayes Classifier",
    "difficulty": 3,
    "description": "Implement Gaussian Naive Bayes from scratch.\n\nRequirements:\n- Calculate prior probabilities P(y)\n- Calculate mean and variance for each feature per class\n- Use Gaussian likelihood P(x|y)\n- Predict using Bayes theorem",
    "starterCode": "import numpy as np\n\nclass GaussianNaiveBayes:\n    def __init__(self):\n        self.classes = None\n        self.priors = {}\n        self.means = {}\n        self.variances = {}\n\n    def fit(self, X, y):\n        \"\"\"Train Naive Bayes.\"\"\"\n        # TODO: Implement fit\n        pass\n\n    def predict(self, X):\n        \"\"\"Predict using Bayes theorem.\"\"\"\n        # TODO: Implement predict\n        pass",
    "solution": "import numpy as np\n\nclass GaussianNaiveBayes:\n    def __init__(self):\n        self.classes = None\n        self.priors = {}\n        self.means = {}\n        self.variances = {}\n\n    def fit(self, X, y):\n        \"\"\"Train Naive Bayes.\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n        self.classes = np.unique(y)\n\n        # Calculate statistics for each class\n        for cls in self.classes:\n            X_cls = X[y == cls]\n\n            # Prior probability\n            self.priors[cls] = len(X_cls) / len(X)\n\n            # Mean and variance for each feature\n            self.means[cls] = np.mean(X_cls, axis=0)\n            self.variances[cls] = np.var(X_cls, axis=0) + 1e-9  # Add small value for stability\n\n        return self\n\n    def gaussian_likelihood(self, x, mean, variance):\n        \"\"\"Calculate Gaussian probability density.\"\"\"\n        exponent = -((x - mean) ** 2) / (2 * variance)\n        return (1 / np.sqrt(2 * np.pi * variance)) * np.exp(exponent)\n\n    def predict_sample(self, x):\n        \"\"\"Predict single sample using Bayes theorem.\"\"\"\n        posteriors = {}\n\n        for cls in self.classes:\n            # Start with prior\n            posterior = np.log(self.priors[cls])\n\n            # Multiply likelihoods (add log likelihoods)\n            for i in range(len(x)):\n                likelihood = self.gaussian_likelihood(x[i], self.means[cls][i], self.variances[cls][i])\n                posterior += np.log(likelihood + 1e-9)  # Add small value to avoid log(0)\n\n            posteriors[cls] = posterior\n\n        # Return class with highest posterior\n        return max(posteriors, key=posteriors.get)\n\n    def predict(self, X):\n        \"\"\"Predict using Bayes theorem.\"\"\"\n        X = np.array(X)\n        return np.array([self.predict_sample(x) for x in X])",
    "testCases": [],
    "hints": [
      "Naive Bayes assumes features are independent",
      "Prior: P(y) = count(y) / total samples",
      "Gaussian likelihood: (1/â(2ÏÏÂ²)) * exp(-(x-Î¼)Â²/(2ÏÂ²))",
      "Use log probabilities to avoid underflow",
      "Predict class with highest posterior probability"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t3-ex12",
    "subjectId": "cs402",
    "topicId": "cs402-topic-3",
    "title": "Calculate SVM Margin",
    "difficulty": 3,
    "description": "Calculate the margin of a linear SVM.\n\nRequirements:\n- Given weights and bias\n- Calculate distance from hyperplane\n- Margin = 2 / ||w||\n- Identify support vectors",
    "starterCode": "import numpy as np\n\ndef calculate_margin(w, b, X, y):\n    \"\"\"\n    Calculate SVM margin and identify support vectors.\n\n    Args:\n        w: Weight vector\n        b: Bias term\n        X: Feature matrix\n        y: Labels (-1 or +1)\n\n    Returns:\n        margin: Margin width\n        support_vectors: Indices of support vectors\n    \"\"\"\n    # TODO: Implement margin calculation\n    pass",
    "solution": "import numpy as np\n\ndef calculate_margin(w, b, X, y):\n    \"\"\"\n    Calculate SVM margin and identify support vectors.\n\n    Args:\n        w: Weight vector\n        b: Bias term\n        X: Feature matrix\n        y: Labels (-1 or +1)\n\n    Returns:\n        margin: Margin width\n        support_vectors: Indices of support vectors\n    \"\"\"\n    w = np.array(w)\n    X = np.array(X)\n    y = np.array(y)\n\n    # Margin = 2 / ||w||\n    margin = 2 / np.linalg.norm(w)\n\n    # Distance from hyperplane: y * (wÂ·x + b)\n    distances = y * (X @ w + b)\n\n    # Support vectors are on the margin: distance = 1\n    support_vectors = np.where(np.abs(distances - 1) < 1e-6)[0]\n\n    return margin, support_vectors",
    "testCases": [],
    "hints": [
      "Decision boundary: wÂ·x + b = 0",
      "Margin boundaries: wÂ·x + b = Â±1",
      "Margin width: 2 / ||w||",
      "Support vectors lie on margin boundaries",
      "Distance from hyperplane: |wÂ·x + b| / ||w||"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t3-ex13",
    "subjectId": "cs402",
    "topicId": "cs402-topic-3",
    "title": "Implement Linear SVM with SGD",
    "difficulty": 4,
    "description": "Implement linear SVM using stochastic gradient descent.\n\nRequirements:\n- Hinge loss: max(0, 1 - y*(wÂ·x + b))\n- Add L2 regularization\n- Update weights using SGD\n- Support binary classification",
    "starterCode": "import numpy as np\n\nclass LinearSVM:\n    def __init__(self, learning_rate=0.01, lambda_param=0.01, n_epochs=1000):\n        self.learning_rate = learning_rate\n        self.lambda_param = lambda_param\n        self.n_epochs = n_epochs\n        self.w = None\n        self.b = None\n\n    def fit(self, X, y):\n        \"\"\"Train SVM using SGD.\"\"\"\n        # TODO: Implement SVM training\n        pass\n\n    def predict(self, X):\n        \"\"\"Predict class labels.\"\"\"\n        # TODO: Implement predict\n        pass",
    "solution": "import numpy as np\n\nclass LinearSVM:\n    def __init__(self, learning_rate=0.01, lambda_param=0.01, n_epochs=1000):\n        self.learning_rate = learning_rate\n        self.lambda_param = lambda_param\n        self.n_epochs = n_epochs\n        self.w = None\n        self.b = None\n\n    def fit(self, X, y):\n        \"\"\"Train SVM using SGD.\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n\n        # Convert labels to -1, +1\n        y = np.where(y <= 0, -1, 1)\n\n        n_samples, n_features = X.shape\n\n        # Initialize parameters\n        self.w = np.zeros(n_features)\n        self.b = 0\n\n        # SGD\n        for epoch in range(self.n_epochs):\n            for i in range(n_samples):\n                xi = X[i]\n                yi = y[i]\n\n                # Hinge loss condition\n                if yi * (np.dot(self.w, xi) + self.b) >= 1:\n                    # Correct classification\n                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w)\n                else:\n                    # Misclassification or within margin\n                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w - yi * xi)\n                    self.b -= self.learning_rate * (-yi)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict class labels.\"\"\"\n        X = np.array(X)\n        linear_output = X @ self.w + self.b\n        return np.sign(linear_output).astype(int)",
    "testCases": [],
    "hints": [
      "Hinge loss: max(0, 1 - y*(wÂ·x + b))",
      "If y*(wÂ·x + b) >= 1: only update regularization term",
      "If y*(wÂ·x + b) < 1: update both hinge loss and regularization",
      "Gradient for regularization: 2*lambda*w",
      "Gradient for hinge loss: -y*x"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t3-ex14",
    "subjectId": "cs402",
    "topicId": "cs402-topic-3",
    "title": "Implement RBF Kernel",
    "difficulty": 3,
    "description": "Implement Radial Basis Function (Gaussian) kernel for SVM.\n\nRequirements:\n- Calculate RBF kernel: K(x, y) = exp(-Î³||x-y||Â²)\n- Support kernel matrix computation\n- Handle gamma parameter\n- Efficient pairwise distance calculation",
    "starterCode": "import numpy as np\n\ndef rbf_kernel(X, Y=None, gamma=1.0):\n    \"\"\"\n    Calculate RBF kernel matrix.\n\n    Args:\n        X: First set of samples (n_samples_1, n_features)\n        Y: Second set of samples (n_samples_2, n_features)\n           If None, use Y = X\n        gamma: Kernel coefficient\n\n    Returns:\n        Kernel matrix (n_samples_1, n_samples_2)\n    \"\"\"\n    # TODO: Implement RBF kernel\n    pass",
    "solution": "import numpy as np\n\ndef rbf_kernel(X, Y=None, gamma=1.0):\n    \"\"\"\n    Calculate RBF kernel matrix.\n\n    Args:\n        X: First set of samples (n_samples_1, n_features)\n        Y: Second set of samples (n_samples_2, n_features)\n           If None, use Y = X\n        gamma: Kernel coefficient\n\n    Returns:\n        Kernel matrix (n_samples_1, n_samples_2)\n    \"\"\"\n    X = np.array(X)\n    if Y is None:\n        Y = X\n    else:\n        Y = np.array(Y)\n\n    # Calculate pairwise squared Euclidean distances\n    # ||x - y||^2 = ||x||^2 + ||y||^2 - 2*xÂ·y\n    X_norm = np.sum(X ** 2, axis=1).reshape(-1, 1)\n    Y_norm = np.sum(Y ** 2, axis=1).reshape(1, -1)\n    distances_squared = X_norm + Y_norm - 2 * X @ Y.T\n\n    # RBF kernel: exp(-gamma * ||x - y||^2)\n    K = np.exp(-gamma * distances_squared)\n\n    return K",
    "testCases": [],
    "hints": [
      "RBF kernel: exp(-gamma * ||x-y||Â²)",
      "Efficient computation: ||x-y||Â² = ||x||Â² + ||y||Â² - 2xÂ·y",
      "gamma controls kernel width",
      "High gamma: narrow kernel, low gamma: wide kernel",
      "Kernel matrix is symmetric if X = Y"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t3-ex15",
    "subjectId": "cs402",
    "topicId": "cs402-topic-3",
    "title": "Implement Weighted KNN",
    "difficulty": 3,
    "description": "Implement weighted KNN where closer neighbors have higher influence.\n\nRequirements:\n- Weight by inverse distance\n- Handle zero distances\n- Use weighted voting for classification\n- Support different distance metrics",
    "starterCode": "import numpy as np\n\nclass WeightedKNN:\n    def __init__(self, n_neighbors=5, weights='distance'):\n        self.n_neighbors = n_neighbors\n        self.weights = weights  # 'uniform' or 'distance'\n        self.X_train = None\n        self.y_train = None\n\n    def fit(self, X, y):\n        \"\"\"Store training data.\"\"\"\n        # TODO: Implement fit\n        pass\n\n    def predict(self, X):\n        \"\"\"Predict using weighted KNN.\"\"\"\n        # TODO: Implement weighted prediction\n        pass",
    "solution": "import numpy as np\n\nclass WeightedKNN:\n    def __init__(self, n_neighbors=5, weights='distance'):\n        self.n_neighbors = n_neighbors\n        self.weights = weights  # 'uniform' or 'distance'\n        self.X_train = None\n        self.y_train = None\n\n    def fit(self, X, y):\n        \"\"\"Store training data.\"\"\"\n        self.X_train = np.array(X)\n        self.y_train = np.array(y)\n        return self\n\n    def predict_sample(self, x):\n        \"\"\"Predict single sample with weighted voting.\"\"\"\n        # Calculate distances\n        distances = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))\n\n        # Get k nearest neighbors\n        k_indices = np.argsort(distances)[:self.n_neighbors]\n        k_distances = distances[k_indices]\n        k_labels = self.y_train[k_indices]\n\n        if self.weights == 'uniform':\n            # Standard majority voting\n            return np.argmax(np.bincount(k_labels))\n        elif self.weights == 'distance':\n            # Weighted voting\n            # Weight = 1 / distance (handle zero distance)\n            weights = np.where(k_distances == 0, 1e10, 1 / k_distances)\n\n            # Weighted voting for each class\n            classes = np.unique(self.y_train)\n            weighted_votes = np.zeros(len(classes))\n\n            for i, cls in enumerate(classes):\n                weighted_votes[i] = np.sum(weights[k_labels == cls])\n\n            return classes[np.argmax(weighted_votes)]\n\n    def predict(self, X):\n        \"\"\"Predict using weighted KNN.\"\"\"\n        X = np.array(X)\n        return np.array([self.predict_sample(x) for x in X])",
    "testCases": [],
    "hints": [
      "Weight by inverse distance: w = 1/d",
      "Handle zero distance: set very large weight",
      "Weighted vote: sum weights for each class",
      "Closer neighbors have more influence",
      "Uniform weights = standard KNN"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t3-ex16",
    "subjectId": "cs402",
    "topicId": "cs402-topic-3",
    "title": "Implement AdaBoost",
    "difficulty": 4,
    "description": "Implement AdaBoost ensemble method with decision stumps.\n\nRequirements:\n- Use decision stumps (depth-1 trees) as weak learners\n- Update sample weights based on errors\n- Calculate classifier weights (alpha)\n- Combine classifiers for final prediction",
    "starterCode": "import numpy as np\n\nclass AdaBoost:\n    def __init__(self, n_estimators=50):\n        self.n_estimators = n_estimators\n        self.alphas = []\n        self.classifiers = []\n\n    def fit(self, X, y):\n        \"\"\"Train AdaBoost.\"\"\"\n        # TODO: Implement AdaBoost training\n        pass\n\n    def predict(self, X):\n        \"\"\"Predict using weighted combination.\"\"\"\n        # TODO: Implement prediction\n        pass",
    "solution": "import numpy as np\n\nclass DecisionStump:\n    \"\"\"Simple decision stump (depth-1 tree).\"\"\"\n    def __init__(self):\n        self.feature_idx = None\n        self.threshold = None\n        self.polarity = 1\n\n    def fit(self, X, y, sample_weights):\n        n_samples, n_features = X.shape\n        best_error = float('inf')\n\n        for feature_idx in range(n_features):\n            thresholds = np.unique(X[:, feature_idx])\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    predictions = np.ones(n_samples)\n                    predictions[X[:, feature_idx] < threshold] = -1\n                    if polarity == -1:\n                        predictions = -predictions\n\n                    error = np.sum(sample_weights[predictions != y])\n\n                    if error < best_error:\n                        best_error = error\n                        self.feature_idx = feature_idx\n                        self.threshold = threshold\n                        self.polarity = polarity\n\n        return self\n\n    def predict(self, X):\n        predictions = np.ones(len(X))\n        predictions[X[:, self.feature_idx] < self.threshold] = -1\n        if self.polarity == -1:\n            predictions = -predictions\n        return predictions\n\nclass AdaBoost:\n    def __init__(self, n_estimators=50):\n        self.n_estimators = n_estimators\n        self.alphas = []\n        self.classifiers = []\n\n    def fit(self, X, y):\n        \"\"\"Train AdaBoost.\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n\n        # Convert labels to -1, +1\n        y = np.where(y == 0, -1, 1)\n\n        n_samples = len(X)\n\n        # Initialize weights uniformly\n        sample_weights = np.ones(n_samples) / n_samples\n\n        for _ in range(self.n_estimators):\n            # Train weak classifier\n            stump = DecisionStump()\n            stump.fit(X, y, sample_weights)\n\n            # Get predictions\n            predictions = stump.predict(X)\n\n            # Calculate weighted error\n            error = np.sum(sample_weights[predictions != y])\n\n            # Avoid division by zero\n            error = np.clip(error, 1e-10, 1 - 1e-10)\n\n            # Calculate classifier weight\n            alpha = 0.5 * np.log((1 - error) / error)\n\n            # Update sample weights\n            sample_weights *= np.exp(-alpha * y * predictions)\n            sample_weights /= np.sum(sample_weights)\n\n            # Store classifier and weight\n            self.classifiers.append(stump)\n            self.alphas.append(alpha)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using weighted combination.\"\"\"\n        X = np.array(X)\n\n        # Weighted sum of classifier predictions\n        clf_preds = np.array([alpha * clf.predict(X) for alpha, clf in zip(self.alphas, self.classifiers)])\n        final_pred = np.sum(clf_preds, axis=0)\n\n        return np.sign(final_pred).astype(int)",
    "testCases": [],
    "hints": [
      "Initialize sample weights uniformly: 1/n",
      "Train weak classifier on weighted samples",
      "Calculate error: sum of weights where prediction wrong",
      "Alpha = 0.5 * log((1-error)/error)",
      "Update weights: w *= exp(-alpha * y * prediction)"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t4-ex01",
    "subjectId": "cs402",
    "topicId": "cs402-topic-4",
    "title": "Implement Step Activation Function",
    "difficulty": 1,
    "description": "Implement the step (Heaviside) activation function for perceptron.\n\nRequirements:\n- Return 1 if input >= 0\n- Return 0 if input < 0\n- Handle arrays and scalars\n- Used in binary perceptron",
    "starterCode": "import numpy as np\n\ndef step_activation(x):\n    \"\"\"\n    Step activation function.\n\n    Args:\n        x: Input (scalar or array)\n\n    Returns:\n        0 or 1\n    \"\"\"\n    # TODO: Implement step activation\n    pass",
    "solution": "import numpy as np\n\ndef step_activation(x):\n    \"\"\"\n    Step activation function.\n\n    Args:\n        x: Input (scalar or array)\n\n    Returns:\n        0 or 1\n    \"\"\"\n    return np.where(x >= 0, 1, 0)",
    "testCases": [],
    "hints": [
      "Step function: f(x) = 1 if x >= 0, else 0",
      "Use np.where() for vectorized operation",
      "Also called Heaviside function",
      "Non-differentiable at x = 0",
      "Used in original perceptron"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t4-ex02",
    "subjectId": "cs402",
    "topicId": "cs402-topic-4",
    "title": "Implement Binary Perceptron",
    "difficulty": 3,
    "description": "Implement a single perceptron for binary classification.\n\nRequirements:\n- Initialize weights randomly\n- Update rule: w = w + Î± * (y - Å·) * x\n- Train until convergence or max iterations\n- Use step activation function",
    "starterCode": "import numpy as np\n\nclass Perceptron:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        \"\"\"Train perceptron.\"\"\"\n        # TODO: Implement perceptron training\n        pass\n\n    def predict(self, X):\n        \"\"\"Predict class labels.\"\"\"\n        # TODO: Implement predict\n        pass",
    "solution": "import numpy as np\n\nclass Perceptron:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        \"\"\"Train perceptron.\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n        n_samples, n_features = X.shape\n\n        # Initialize weights and bias\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # Training loop\n        for _ in range(self.n_iterations):\n            for i in range(n_samples):\n                xi = X[i]\n                yi = y[i]\n\n                # Forward pass\n                linear_output = np.dot(self.weights, xi) + self.bias\n                y_pred = 1 if linear_output >= 0 else 0\n\n                # Update weights if prediction is wrong\n                error = yi - y_pred\n                self.weights += self.learning_rate * error * xi\n                self.bias += self.learning_rate * error\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict class labels.\"\"\"\n        X = np.array(X)\n        linear_output = X @ self.weights + self.bias\n        return np.where(linear_output >= 0, 1, 0)",
    "testCases": [],
    "hints": [
      "Perceptron update rule: w = w + Î±*(y - Å·)*x",
      "Only update when prediction is wrong",
      "Initialize weights to zeros or small random values",
      "Linear output: wÂ·x + b",
      "Apply step function for binary classification"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t4-ex03",
    "subjectId": "cs402",
    "topicId": "cs402-topic-4",
    "title": "Implement ReLU Activation",
    "difficulty": 1,
    "description": "Implement ReLU (Rectified Linear Unit) activation function.\n\nRequirements:\n- ReLU(x) = max(0, x)\n- Implement derivative for backpropagation\n- Handle arrays efficiently\n- Most common activation in hidden layers",
    "starterCode": "import numpy as np\n\ndef relu(x):\n    \"\"\"ReLU activation function.\"\"\"\n    # TODO: Implement ReLU\n    pass\n\ndef relu_derivative(x):\n    \"\"\"Derivative of ReLU.\"\"\"\n    # TODO: Implement ReLU derivative\n    pass",
    "solution": "import numpy as np\n\ndef relu(x):\n    \"\"\"ReLU activation function.\"\"\"\n    return np.maximum(0, x)\n\ndef relu_derivative(x):\n    \"\"\"Derivative of ReLU.\"\"\"\n    return np.where(x > 0, 1, 0)",
    "testCases": [],
    "hints": [
      "ReLU: f(x) = max(0, x)",
      "Derivative: 1 if x > 0, else 0",
      "Use np.maximum() for ReLU",
      "ReLU helps avoid vanishing gradient",
      "Most popular activation for hidden layers"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t4-ex04",
    "subjectId": "cs402",
    "topicId": "cs402-topic-4",
    "title": "Implement Tanh Activation",
    "difficulty": 1,
    "description": "Implement hyperbolic tangent activation function.\n\nRequirements:\n- tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n- Implement derivative\n- Output range: (-1, 1)\n- Zero-centered unlike sigmoid",
    "starterCode": "import numpy as np\n\ndef tanh(x):\n    \"\"\"Tanh activation function.\"\"\"\n    # TODO: Implement tanh\n    pass\n\ndef tanh_derivative(x):\n    \"\"\"Derivative of tanh.\"\"\"\n    # TODO: Implement tanh derivative\n    pass",
    "solution": "import numpy as np\n\ndef tanh(x):\n    \"\"\"Tanh activation function.\"\"\"\n    return np.tanh(x)\n\ndef tanh_derivative(x):\n    \"\"\"Derivative of tanh.\"\"\"\n    # Derivative: 1 - tanh^2(x)\n    return 1 - np.tanh(x) ** 2",
    "testCases": [],
    "hints": [
      "tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))",
      "Use np.tanh() for implementation",
      "Derivative: 1 - tanhÂ²(x)",
      "Output range: (-1, 1)",
      "Zero-centered, better than sigmoid"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t4-ex05",
    "subjectId": "cs402",
    "topicId": "cs402-topic-4",
    "title": "Implement Leaky ReLU",
    "difficulty": 2,
    "description": "Implement Leaky ReLU to prevent dying ReLU problem.\n\nRequirements:\n- Leaky ReLU: x if x > 0, else alpha*x\n- Implement derivative\n- Small slope for negative values (alpha=0.01)\n- Prevents dead neurons",
    "starterCode": "import numpy as np\n\ndef leaky_relu(x, alpha=0.01):\n    \"\"\"Leaky ReLU activation function.\"\"\"\n    # TODO: Implement Leaky ReLU\n    pass\n\ndef leaky_relu_derivative(x, alpha=0.01):\n    \"\"\"Derivative of Leaky ReLU.\"\"\"\n    # TODO: Implement derivative\n    pass",
    "solution": "import numpy as np\n\ndef leaky_relu(x, alpha=0.01):\n    \"\"\"Leaky ReLU activation function.\"\"\"\n    return np.where(x > 0, x, alpha * x)\n\ndef leaky_relu_derivative(x, alpha=0.01):\n    \"\"\"Derivative of Leaky ReLU.\"\"\"\n    return np.where(x > 0, 1, alpha)",
    "testCases": [],
    "hints": [
      "Leaky ReLU: f(x) = x if x > 0, else Î±*x",
      "Derivative: 1 if x > 0, else Î±",
      "Common Î± values: 0.01, 0.1",
      "Prevents dying ReLU problem",
      "Small gradient for negative inputs"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t4-ex06",
    "subjectId": "cs402",
    "topicId": "cs402-topic-4",
    "title": "Implement Forward Pass",
    "difficulty": 2,
    "description": "Implement forward propagation for a single layer.\n\nRequirements:\n- Calculate z = Wx + b\n- Apply activation function\n- Return both pre-activation (z) and activation (a)\n- Support batch processing",
    "starterCode": "import numpy as np\n\ndef forward_pass(X, W, b, activation='relu'):\n    \"\"\"\n    Forward pass for single layer.\n\n    Args:\n        X: Input (n_samples, n_features)\n        W: Weights (n_features, n_neurons)\n        b: Bias (n_neurons,)\n        activation: Activation function name\n\n    Returns:\n        z: Pre-activation (n_samples, n_neurons)\n        a: Activation output (n_samples, n_neurons)\n    \"\"\"\n    # TODO: Implement forward pass\n    pass",
    "solution": "import numpy as np\n\ndef relu(x):\n    return np.maximum(0, x)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef forward_pass(X, W, b, activation='relu'):\n    \"\"\"\n    Forward pass for single layer.\n\n    Args:\n        X: Input (n_samples, n_features)\n        W: Weights (n_features, n_neurons)\n        b: Bias (n_neurons,)\n        activation: Activation function name\n\n    Returns:\n        z: Pre-activation (n_samples, n_neurons)\n        a: Activation output (n_samples, n_neurons)\n    \"\"\"\n    # Linear transformation\n    z = X @ W + b\n\n    # Apply activation\n    if activation == 'relu':\n        a = relu(z)\n    elif activation == 'sigmoid':\n        a = sigmoid(z)\n    elif activation == 'tanh':\n        a = tanh(z)\n    elif activation == 'linear':\n        a = z\n    else:\n        raise ValueError(f\"Unknown activation: {activation}\")\n\n    return z, a",
    "testCases": [],
    "hints": [
      "Linear transformation: z = X @ W + b",
      "Apply activation to z to get a",
      "Store z for backpropagation",
      "Broadcasting handles bias addition",
      "Support multiple activation functions"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t4-ex07",
    "subjectId": "cs402",
    "topicId": "cs402-topic-4",
    "title": "Implement Backward Pass for Output Layer",
    "difficulty": 3,
    "description": "Implement backpropagation for output layer.\n\nRequirements:\n- Calculate loss gradient\n- Compute weight gradient: dW = X^T @ dL/da\n- Compute bias gradient: db = sum(dL/da)\n- Return gradients for weights and bias",
    "starterCode": "import numpy as np\n\ndef backward_output_layer(X, y, a, loss='mse'):\n    \"\"\"\n    Backward pass for output layer.\n\n    Args:\n        X: Input to this layer (n_samples, n_features)\n        y: True labels (n_samples, n_outputs)\n        a: Output activations (n_samples, n_outputs)\n        loss: Loss function ('mse' or 'cross_entropy')\n\n    Returns:\n        dW: Weight gradient (n_features, n_outputs)\n        db: Bias gradient (n_outputs,)\n        dX: Gradient w.r.t input (n_samples, n_features)\n    \"\"\"\n    # TODO: Implement backward pass for output layer\n    pass",
    "solution": "import numpy as np\n\ndef backward_output_layer(X, y, a, loss='mse'):\n    \"\"\"\n    Backward pass for output layer.\n\n    Args:\n        X: Input to this layer (n_samples, n_features)\n        y: True labels (n_samples, n_outputs)\n        a: Output activations (n_samples, n_outputs)\n        loss: Loss function ('mse' or 'cross_entropy')\n\n    Returns:\n        dW: Weight gradient (n_features, n_outputs)\n        db: Bias gradient (n_outputs,)\n        dX: Gradient w.r.t input (n_samples, n_features)\n    \"\"\"\n    n_samples = X.shape[0]\n\n    # Gradient of loss w.r.t. output activation\n    if loss == 'mse':\n        # MSE: (a - y)\n        da = a - y\n    elif loss == 'cross_entropy':\n        # Binary cross-entropy with sigmoid\n        da = a - y\n    else:\n        raise ValueError(f\"Unknown loss: {loss}\")\n\n    # Gradients\n    dW = (1 / n_samples) * X.T @ da\n    db = (1 / n_samples) * np.sum(da, axis=0)\n    dX = da @ W.T  # For propagating to previous layer\n\n    return dW, db, dX",
    "testCases": [],
    "hints": [
      "For MSE: gradient = a - y",
      "For cross-entropy + sigmoid: gradient = a - y",
      "Weight gradient: dW = (1/n) * X^T @ da",
      "Bias gradient: db = (1/n) * sum(da)",
      "Gradient w.r.t. input: dX = da @ W^T"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t4-ex08",
    "subjectId": "cs402",
    "topicId": "cs402-topic-4",
    "title": "Implement Backward Pass for Hidden Layer",
    "difficulty": 3,
    "description": "Implement backpropagation for hidden layer.\n\nRequirements:\n- Calculate gradient from next layer\n- Apply activation derivative\n- Compute weight and bias gradients\n- Return gradients for current and previous layer",
    "starterCode": "import numpy as np\n\ndef backward_hidden_layer(X, W, z, dA_next, activation='relu'):\n    \"\"\"\n    Backward pass for hidden layer.\n\n    Args:\n        X: Input to this layer (n_samples, n_features)\n        W: Weights (n_features, n_neurons)\n        z: Pre-activation (n_samples, n_neurons)\n        dA_next: Gradient from next layer (n_samples, n_neurons)\n        activation: Activation function\n\n    Returns:\n        dW: Weight gradient (n_features, n_neurons)\n        db: Bias gradient (n_neurons,)\n        dX: Gradient w.r.t input (n_samples, n_features)\n    \"\"\"\n    # TODO: Implement backward pass for hidden layer\n    pass",
    "solution": "import numpy as np\n\ndef relu_derivative(x):\n    return np.where(x > 0, 1, 0)\n\ndef sigmoid_derivative(x):\n    s = 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n    return s * (1 - s)\n\ndef tanh_derivative(x):\n    return 1 - np.tanh(x) ** 2\n\ndef backward_hidden_layer(X, W, z, dA_next, activation='relu'):\n    \"\"\"\n    Backward pass for hidden layer.\n\n    Args:\n        X: Input to this layer (n_samples, n_features)\n        W: Weights (n_features, n_neurons)\n        z: Pre-activation (n_samples, n_neurons)\n        dA_next: Gradient from next layer (n_samples, n_neurons)\n        activation: Activation function\n\n    Returns:\n        dW: Weight gradient (n_features, n_neurons)\n        db: Bias gradient (n_neurons,)\n        dX: Gradient w.r.t input (n_samples, n_features)\n    \"\"\"\n    n_samples = X.shape[0]\n\n    # Apply activation derivative\n    if activation == 'relu':\n        dz = dA_next * relu_derivative(z)\n    elif activation == 'sigmoid':\n        dz = dA_next * sigmoid_derivative(z)\n    elif activation == 'tanh':\n        dz = dA_next * tanh_derivative(z)\n    elif activation == 'linear':\n        dz = dA_next\n    else:\n        raise ValueError(f\"Unknown activation: {activation}\")\n\n    # Calculate gradients\n    dW = (1 / n_samples) * X.T @ dz\n    db = (1 / n_samples) * np.sum(dz, axis=0)\n    dX = dz @ W.T\n\n    return dW, db, dX",
    "testCases": [],
    "hints": [
      "Chain rule: dz = dA_next * activation_derivative(z)",
      "Weight gradient: dW = (1/n) * X^T @ dz",
      "Bias gradient: db = (1/n) * sum(dz)",
      "Propagate to previous layer: dX = dz @ W^T",
      "Activation derivative depends on pre-activation z"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t4-ex09",
    "subjectId": "cs402",
    "topicId": "cs402-topic-4",
    "title": "Implement Xavier/Glorot Initialization",
    "difficulty": 2,
    "description": "Implement Xavier initialization for neural network weights.\n\nRequirements:\n- Initialize weights from uniform distribution\n- Range: [-â(6/(n_in + n_out)), â(6/(n_in + n_out))]\n- Initialize bias to zeros\n- Helps maintain gradient variance",
    "starterCode": "import numpy as np\n\ndef xavier_initialization(n_in, n_out, random_state=None):\n    \"\"\"\n    Xavier/Glorot weight initialization.\n\n    Args:\n        n_in: Number of input units\n        n_out: Number of output units\n        random_state: Random seed\n\n    Returns:\n        W: Initialized weights (n_in, n_out)\n        b: Initialized bias (n_out,)\n    \"\"\"\n    # TODO: Implement Xavier initialization\n    pass",
    "solution": "import numpy as np\n\ndef xavier_initialization(n_in, n_out, random_state=None):\n    \"\"\"\n    Xavier/Glorot weight initialization.\n\n    Args:\n        n_in: Number of input units\n        n_out: Number of output units\n        random_state: Random seed\n\n    Returns:\n        W: Initialized weights (n_in, n_out)\n        b: Initialized bias (n_out,)\n    \"\"\"\n    if random_state is not None:\n        np.random.seed(random_state)\n\n    # Xavier uniform initialization\n    limit = np.sqrt(6 / (n_in + n_out))\n    W = np.random.uniform(-limit, limit, (n_in, n_out))\n\n    # Bias initialized to zeros\n    b = np.zeros(n_out)\n\n    return W, b",
    "testCases": [],
    "hints": [
      "Xavier uniform: W ~ U(-â(6/(n_in+n_out)), â(6/(n_in+n_out)))",
      "Xavier normal: W ~ N(0, â(2/(n_in+n_out)))",
      "Bias initialized to zeros",
      "Helps avoid vanishing/exploding gradients",
      "Works well with tanh and sigmoid"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t4-ex10",
    "subjectId": "cs402",
    "topicId": "cs402-topic-4",
    "title": "Implement He Initialization",
    "difficulty": 2,
    "description": "Implement He initialization for ReLU networks.\n\nRequirements:\n- Initialize weights from normal distribution\n- Mean: 0, Variance: 2/n_in\n- Initialize bias to zeros\n- Optimized for ReLU activation",
    "starterCode": "import numpy as np\n\ndef he_initialization(n_in, n_out, random_state=None):\n    \"\"\"\n    He weight initialization for ReLU networks.\n\n    Args:\n        n_in: Number of input units\n        n_out: Number of output units\n        random_state: Random seed\n\n    Returns:\n        W: Initialized weights (n_in, n_out)\n        b: Initialized bias (n_out,)\n    \"\"\"\n    # TODO: Implement He initialization\n    pass",
    "solution": "import numpy as np\n\ndef he_initialization(n_in, n_out, random_state=None):\n    \"\"\"\n    He weight initialization for ReLU networks.\n\n    Args:\n        n_in: Number of input units\n        n_out: Number of output units\n        random_state: Random seed\n\n    Returns:\n        W: Initialized weights (n_in, n_out)\n        b: Initialized bias (n_out,)\n    \"\"\"\n    if random_state is not None:\n        np.random.seed(random_state)\n\n    # He normal initialization\n    std = np.sqrt(2 / n_in)\n    W = np.random.normal(0, std, (n_in, n_out))\n\n    # Bias initialized to zeros\n    b = np.zeros(n_out)\n\n    return W, b",
    "testCases": [],
    "hints": [
      "He normal: W ~ N(0, â(2/n_in))",
      "He uniform: W ~ U(-â(6/n_in), â(6/n_in))",
      "Optimized for ReLU and its variants",
      "Variance: 2/n_in",
      "Bias initialized to zeros"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t4-ex11",
    "subjectId": "cs402",
    "topicId": "cs402-topic-4",
    "title": "Implement 2-Layer Neural Network",
    "difficulty": 4,
    "description": "Implement a complete 2-layer neural network with backpropagation.\n\nRequirements:\n- One hidden layer, one output layer\n- Forward propagation through both layers\n- Backpropagation to compute gradients\n- Gradient descent for training\n- Support different activation functions",
    "starterCode": "import numpy as np\n\nclass TwoLayerNN:\n    def __init__(self, n_features, n_hidden, n_output, learning_rate=0.01):\n        self.learning_rate = learning_rate\n        # Initialize weights and biases\n        # TODO: Initialize parameters\n        pass\n\n    def forward(self, X):\n        \"\"\"Forward propagation.\"\"\"\n        # TODO: Implement forward pass\n        pass\n\n    def backward(self, X, y):\n        \"\"\"Backward propagation.\"\"\"\n        # TODO: Implement backward pass\n        pass\n\n    def fit(self, X, y, n_epochs=1000):\n        \"\"\"Train the network.\"\"\"\n        # TODO: Implement training loop\n        pass\n\n    def predict(self, X):\n        \"\"\"Make predictions.\"\"\"\n        # TODO: Implement predict\n        pass",
    "solution": "import numpy as np\n\nclass TwoLayerNN:\n    def __init__(self, n_features, n_hidden, n_output, learning_rate=0.01):\n        self.learning_rate = learning_rate\n\n        # He initialization for hidden layer (ReLU)\n        self.W1 = np.random.randn(n_features, n_hidden) * np.sqrt(2 / n_features)\n        self.b1 = np.zeros(n_hidden)\n\n        # Xavier initialization for output layer (sigmoid)\n        limit = np.sqrt(6 / (n_hidden + n_output))\n        self.W2 = np.random.uniform(-limit, limit, (n_hidden, n_output))\n        self.b2 = np.zeros(n_output)\n\n        # Cache for backpropagation\n        self.cache = {}\n\n    def relu(self, x):\n        return np.maximum(0, x)\n\n    def relu_derivative(self, x):\n        return np.where(x > 0, 1, 0)\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n\n    def forward(self, X):\n        \"\"\"Forward propagation.\"\"\"\n        # Hidden layer\n        z1 = X @ self.W1 + self.b1\n        a1 = self.relu(z1)\n\n        # Output layer\n        z2 = a1 @ self.W2 + self.b2\n        a2 = self.sigmoid(z2)\n\n        # Cache for backward pass\n        self.cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2}\n\n        return a2\n\n    def backward(self, y):\n        \"\"\"Backward propagation.\"\"\"\n        X = self.cache['X']\n        a1 = self.cache['a1']\n        a2 = self.cache['a2']\n        z1 = self.cache['z1']\n\n        n_samples = X.shape[0]\n\n        # Output layer gradients\n        dz2 = a2 - y\n        dW2 = (1 / n_samples) * a1.T @ dz2\n        db2 = (1 / n_samples) * np.sum(dz2, axis=0)\n\n        # Hidden layer gradients\n        da1 = dz2 @ self.W2.T\n        dz1 = da1 * self.relu_derivative(z1)\n        dW1 = (1 / n_samples) * X.T @ dz1\n        db1 = (1 / n_samples) * np.sum(dz1, axis=0)\n\n        return dW1, db1, dW2, db2\n\n    def fit(self, X, y, n_epochs=1000):\n        \"\"\"Train the network.\"\"\"\n        X = np.array(X)\n        y = np.array(y).reshape(-1, 1) if y.ndim == 1 else np.array(y)\n\n        for epoch in range(n_epochs):\n            # Forward pass\n            a2 = self.forward(X)\n\n            # Backward pass\n            dW1, db1, dW2, db2 = self.backward(y)\n\n            # Update parameters\n            self.W1 -= self.learning_rate * dW1\n            self.b1 -= self.learning_rate * db1\n            self.W2 -= self.learning_rate * dW2\n            self.b2 -= self.learning_rate * db2\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Make predictions.\"\"\"\n        X = np.array(X)\n        a2 = self.forward(X)\n        return (a2 >= 0.5).astype(int)",
    "testCases": [],
    "hints": [
      "Forward: X -> hidden (ReLU) -> output (sigmoid)",
      "Backward: compute gradients layer by layer",
      "Use chain rule for hidden layer gradients",
      "Update weights: W -= learning_rate * dW",
      "Cache intermediate values for backpropagation"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t4-ex12",
    "subjectId": "cs402",
    "topicId": "cs402-topic-4",
    "title": "Implement Batch Normalization",
    "difficulty": 4,
    "description": "Implement batch normalization for neural networks.\n\nRequirements:\n- Normalize: (x - mean) / sqrt(var + Îµ)\n- Scale and shift: Î³*x_norm + Î²\n- Calculate running mean/var for inference\n- Implement forward and backward pass",
    "starterCode": "import numpy as np\n\nclass BatchNormalization:\n    def __init__(self, n_features, epsilon=1e-5, momentum=0.9):\n        self.epsilon = epsilon\n        self.momentum = momentum\n\n        # Learnable parameters\n        self.gamma = np.ones(n_features)\n        self.beta = np.zeros(n_features)\n\n        # Running statistics for inference\n        self.running_mean = np.zeros(n_features)\n        self.running_var = np.ones(n_features)\n\n    def forward(self, X, training=True):\n        \"\"\"Forward pass.\"\"\"\n        # TODO: Implement forward pass\n        pass\n\n    def backward(self, dout):\n        \"\"\"Backward pass.\"\"\"\n        # TODO: Implement backward pass\n        pass",
    "solution": "import numpy as np\n\nclass BatchNormalization:\n    def __init__(self, n_features, epsilon=1e-5, momentum=0.9):\n        self.epsilon = epsilon\n        self.momentum = momentum\n\n        # Learnable parameters\n        self.gamma = np.ones(n_features)\n        self.beta = np.zeros(n_features)\n\n        # Running statistics for inference\n        self.running_mean = np.zeros(n_features)\n        self.running_var = np.ones(n_features)\n\n        # Cache for backward pass\n        self.cache = {}\n\n    def forward(self, X, training=True):\n        \"\"\"Forward pass.\"\"\"\n        if training:\n            # Calculate batch statistics\n            batch_mean = np.mean(X, axis=0)\n            batch_var = np.var(X, axis=0)\n\n            # Normalize\n            X_norm = (X - batch_mean) / np.sqrt(batch_var + self.epsilon)\n\n            # Scale and shift\n            out = self.gamma * X_norm + self.beta\n\n            # Update running statistics\n            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n\n            # Cache for backward pass\n            self.cache = {\n                'X': X,\n                'X_norm': X_norm,\n                'batch_mean': batch_mean,\n                'batch_var': batch_var\n            }\n        else:\n            # Use running statistics for inference\n            X_norm = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n            out = self.gamma * X_norm + self.beta\n\n        return out\n\n    def backward(self, dout):\n        \"\"\"Backward pass.\"\"\"\n        X = self.cache['X']\n        X_norm = self.cache['X_norm']\n        batch_mean = self.cache['batch_mean']\n        batch_var = self.cache['batch_var']\n\n        n_samples = X.shape[0]\n\n        # Gradients for gamma and beta\n        dgamma = np.sum(dout * X_norm, axis=0)\n        dbeta = np.sum(dout, axis=0)\n\n        # Gradient for X_norm\n        dX_norm = dout * self.gamma\n\n        # Gradient for variance\n        dvar = np.sum(dX_norm * (X - batch_mean) * -0.5 * (batch_var + self.epsilon) ** -1.5, axis=0)\n\n        # Gradient for mean\n        dmean = np.sum(dX_norm * -1 / np.sqrt(batch_var + self.epsilon), axis=0) + \\\n                dvar * np.sum(-2 * (X - batch_mean), axis=0) / n_samples\n\n        # Gradient for X\n        dX = dX_norm / np.sqrt(batch_var + self.epsilon) + \\\n             dvar * 2 * (X - batch_mean) / n_samples + \\\n             dmean / n_samples\n\n        return dX, dgamma, dbeta",
    "testCases": [],
    "hints": [
      "Normalize: (x - mean) / sqrt(var + Îµ)",
      "Scale and shift: Î³ * x_norm + Î²",
      "Use batch statistics during training",
      "Use running statistics during inference",
      "Update running stats with momentum"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t4-ex13",
    "subjectId": "cs402",
    "topicId": "cs402-topic-4",
    "title": "Implement Dropout",
    "difficulty": 3,
    "description": "Implement dropout regularization for neural networks.\n\nRequirements:\n- Randomly set neurons to zero during training\n- Scale remaining activations by 1/(1-p)\n- Disable dropout during inference\n- Return mask for backpropagation",
    "starterCode": "import numpy as np\n\nclass Dropout:\n    def __init__(self, dropout_rate=0.5):\n        self.dropout_rate = dropout_rate\n        self.mask = None\n\n    def forward(self, X, training=True):\n        \"\"\"Forward pass with dropout.\"\"\"\n        # TODO: Implement forward pass\n        pass\n\n    def backward(self, dout):\n        \"\"\"Backward pass with dropout.\"\"\"\n        # TODO: Implement backward pass\n        pass",
    "solution": "import numpy as np\n\nclass Dropout:\n    def __init__(self, dropout_rate=0.5):\n        self.dropout_rate = dropout_rate\n        self.mask = None\n\n    def forward(self, X, training=True):\n        \"\"\"Forward pass with dropout.\"\"\"\n        if training:\n            # Create dropout mask\n            self.mask = np.random.rand(*X.shape) > self.dropout_rate\n\n            # Apply mask and scale\n            out = X * self.mask / (1 - self.dropout_rate)\n        else:\n            # No dropout during inference\n            out = X\n\n        return out\n\n    def backward(self, dout):\n        \"\"\"Backward pass with dropout.\"\"\"\n        # Apply same mask to gradients\n        dX = dout * self.mask / (1 - self.dropout_rate)\n        return dX",
    "testCases": [],
    "hints": [
      "Create binary mask: np.random.rand() > dropout_rate",
      "Scale by 1/(1-p) during training (inverted dropout)",
      "No dropout during inference",
      "Apply same mask during backward pass",
      "Typical dropout rates: 0.2-0.5"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t4-ex14",
    "subjectId": "cs402",
    "topicId": "cs402-topic-4",
    "title": "Implement Momentum Optimizer",
    "difficulty": 3,
    "description": "Implement momentum-based gradient descent optimization.\n\nRequirements:\n- Maintain velocity for each parameter\n- Update: v = Î²*v + (1-Î²)*gradient\n- Parameter update: Î¸ = Î¸ - Î±*v\n- Accelerates convergence",
    "starterCode": "import numpy as np\n\nclass MomentumOptimizer:\n    def __init__(self, learning_rate=0.01, momentum=0.9):\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.velocities = {}\n\n    def update(self, params, grads):\n        \"\"\"\n        Update parameters using momentum.\n\n        Args:\n            params: Dictionary of parameters\n            grads: Dictionary of gradients\n\n        Returns:\n            Updated parameters\n        \"\"\"\n        # TODO: Implement momentum update\n        pass",
    "solution": "import numpy as np\n\nclass MomentumOptimizer:\n    def __init__(self, learning_rate=0.01, momentum=0.9):\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.velocities = {}\n\n    def update(self, params, grads):\n        \"\"\"\n        Update parameters using momentum.\n\n        Args:\n            params: Dictionary of parameters\n            grads: Dictionary of gradients\n\n        Returns:\n            Updated parameters\n        \"\"\"\n        # Initialize velocities if first update\n        if not self.velocities:\n            for key in params:\n                self.velocities[key] = np.zeros_like(params[key])\n\n        # Update parameters\n        for key in params:\n            # Update velocity\n            self.velocities[key] = self.momentum * self.velocities[key] + \\\n                                  (1 - self.momentum) * grads[key]\n\n            # Update parameter\n            params[key] -= self.learning_rate * self.velocities[key]\n\n        return params",
    "testCases": [],
    "hints": [
      "Velocity: v = Î²*v + (1-Î²)*gradient",
      "Update: Î¸ = Î¸ - Î±*v",
      "Initialize velocities to zeros",
      "Typical momentum: 0.9 or 0.99",
      "Helps escape local minima"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t4-ex15",
    "subjectId": "cs402",
    "topicId": "cs402-topic-4",
    "title": "Implement Adam Optimizer",
    "difficulty": 4,
    "description": "Implement Adam (Adaptive Moment Estimation) optimizer.\n\nRequirements:\n- Maintain first moment (mean) and second moment (variance)\n- Bias correction for moments\n- Adaptive learning rate per parameter\n- Combine momentum and RMSprop",
    "starterCode": "import numpy as np\n\nclass AdamOptimizer:\n    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.m = {}  # First moment\n        self.v = {}  # Second moment\n        self.t = 0   # Time step\n\n    def update(self, params, grads):\n        \"\"\"Update parameters using Adam.\"\"\"\n        # TODO: Implement Adam update\n        pass",
    "solution": "import numpy as np\n\nclass AdamOptimizer:\n    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.m = {}  # First moment\n        self.v = {}  # Second moment\n        self.t = 0   # Time step\n\n    def update(self, params, grads):\n        \"\"\"Update parameters using Adam.\"\"\"\n        # Initialize moments if first update\n        if not self.m:\n            for key in params:\n                self.m[key] = np.zeros_like(params[key])\n                self.v[key] = np.zeros_like(params[key])\n\n        # Increment time step\n        self.t += 1\n\n        # Update parameters\n        for key in params:\n            # Update biased first moment estimate\n            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n\n            # Update biased second moment estimate\n            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)\n\n            # Bias correction\n            m_hat = self.m[key] / (1 - self.beta1 ** self.t)\n            v_hat = self.v[key] / (1 - self.beta2 ** self.t)\n\n            # Update parameter\n            params[key] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n\n        return params",
    "testCases": [],
    "hints": [
      "First moment: m = Î²1*m + (1-Î²1)*gradient",
      "Second moment: v = Î²2*v + (1-Î²2)*gradientÂ²",
      "Bias correction: m_hat = m/(1-Î²1^t)",
      "Update: Î¸ = Î¸ - Î±*m_hat/(âv_hat + Îµ)",
      "Default: Î²1=0.9, Î²2=0.999, Îµ=1e-8"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t4-ex16",
    "subjectId": "cs402",
    "topicId": "cs402-topic-4",
    "title": "Implement Gradient Clipping",
    "difficulty": 2,
    "description": "Implement gradient clipping to prevent exploding gradients.\n\nRequirements:\n- Clip gradients by value\n- Clip gradients by norm\n- Support both methods\n- Prevent exploding gradient problem",
    "starterCode": "import numpy as np\n\ndef clip_gradients(grads, method='value', threshold=1.0):\n    \"\"\"\n    Clip gradients to prevent explosion.\n\n    Args:\n        grads: Dictionary of gradients\n        method: 'value' or 'norm'\n        threshold: Clipping threshold\n\n    Returns:\n        Clipped gradients\n    \"\"\"\n    # TODO: Implement gradient clipping\n    pass",
    "solution": "import numpy as np\n\ndef clip_gradients(grads, method='value', threshold=1.0):\n    \"\"\"\n    Clip gradients to prevent explosion.\n\n    Args:\n        grads: Dictionary of gradients\n        method: 'value' or 'norm'\n        threshold: Clipping threshold\n\n    Returns:\n        Clipped gradients\n    \"\"\"\n    clipped_grads = {}\n\n    if method == 'value':\n        # Clip each gradient value\n        for key in grads:\n            clipped_grads[key] = np.clip(grads[key], -threshold, threshold)\n\n    elif method == 'norm':\n        # Clip by global norm\n        # Calculate global norm\n        total_norm = 0\n        for key in grads:\n            total_norm += np.sum(grads[key] ** 2)\n        total_norm = np.sqrt(total_norm)\n\n        # Clip if norm exceeds threshold\n        clip_coef = threshold / (total_norm + 1e-6)\n        if clip_coef < 1:\n            for key in grads:\n                clipped_grads[key] = grads[key] * clip_coef\n        else:\n            clipped_grads = grads\n\n    else:\n        raise ValueError(f\"Unknown clipping method: {method}\")\n\n    return clipped_grads",
    "testCases": [],
    "hints": [
      "Value clipping: np.clip(grad, -threshold, threshold)",
      "Norm clipping: scale gradient if ||grad|| > threshold",
      "Global norm: sqrt(sum of all gradient squares)",
      "Clip coefficient: threshold / norm",
      "Helps with exploding gradients in RNNs"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t5-ex01",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Implement Basic CNN for Image Classification",
    "difficulty": 3,
    "description": "Build a convolutional neural network for MNIST digit classification.\n\nRequirements:\n- 2 convolutional layers with ReLU and max pooling\n- 2 fully connected layers\n- Softmax output for 10 classes\n- Train using Adam optimizer\n- Achieve >95% test accuracy",
    "starterCode": "import torch\nimport torch.nn as nn\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        # Define layers\n        \n    def forward(self, x):\n        # Implement forward pass\n        pass\n        \n# Training loop\nmodel = SimpleCNN()\n# TODO: Implement training",
    "solution": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n        self.fc2 = nn.Linear(128, 10)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.pool(self.relu(self.conv1(x)))  # 28->14\n        x = self.pool(self.relu(self.conv2(x)))  # 14->7\n        x = x.view(-1, 64 * 7 * 7)\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Training\nmodel = SimpleCNN()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(10):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()",
    "testCases": [],
    "hints": [
      "Start by defining Conv2d layers with appropriate input/output channels",
      "Use MaxPool2d after each convolutional layer to reduce spatial dimensions",
      "Flatten the output before passing to fully connected layers",
      "Use CrossEntropyLoss for multi-class classification",
      "Adam optimizer works well with learning rate around 0.001"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-5-2",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Calculate Convolutional Layer Output Size",
    "difficulty": 1,
    "description": "Calculate the output dimensions of a convolutional layer.\n\nRequirements:\n- Given input size (H, W), kernel size (K), stride (S), and padding (P)\n- Apply formula: output = floor((input + 2*padding - kernel) / stride) + 1\n- Handle both height and width dimensions\n- Return output shape as tuple",
    "starterCode": "import numpy as np\n\ndef conv_output_size(input_size, kernel_size, stride=1, padding=0):\n    \"\"\"\n    Calculate output size after convolution.\n\n    Args:\n        input_size: tuple (height, width)\n        kernel_size: int or tuple\n        stride: int or tuple\n        padding: int or tuple\n\n    Returns:\n        tuple: (output_height, output_width)\n    \"\"\"\n    # TODO: Implement\n    pass\n\n# Test\nprint(conv_output_size((28, 28), 3, 1, 1))  # Should output (28, 28)",
    "solution": "import numpy as np\n\ndef conv_output_size(input_size, kernel_size, stride=1, padding=0):\n    \"\"\"\n    Calculate output size after convolution.\n\n    Args:\n        input_size: tuple (height, width)\n        kernel_size: int or tuple\n        stride: int or tuple\n        padding: int or tuple\n\n    Returns:\n        tuple: (output_height, output_width)\n    \"\"\"\n    h_in, w_in = input_size\n\n    # Handle scalar values\n    if isinstance(kernel_size, int):\n        k_h, k_w = kernel_size, kernel_size\n    else:\n        k_h, k_w = kernel_size\n\n    if isinstance(stride, int):\n        s_h, s_w = stride, stride\n    else:\n        s_h, s_w = stride\n\n    if isinstance(padding, int):\n        p_h, p_w = padding, padding\n    else:\n        p_h, p_w = padding\n\n    # Apply formula\n    h_out = (h_in + 2*p_h - k_h) // s_h + 1\n    w_out = (w_in + 2*p_w - k_w) // s_w + 1\n\n    return (h_out, w_out)\n\n# Test\nprint(conv_output_size((28, 28), 3, 1, 1))  # (28, 28)\nprint(conv_output_size((32, 32), 5, 2, 0))  # (14, 14)",
    "testCases": [],
    "hints": [
      "Use the formula: output = (input + 2*padding - kernel) / stride + 1",
      "Handle both scalar and tuple inputs for kernel, stride, and padding",
      "Use integer division (//) to get the floor of the result",
      "Apply the formula independently to height and width",
      "Common case: same padding (padding=k//2) keeps dimensions the same with stride=1"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-5-3",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Implement Max Pooling Layer",
    "difficulty": 2,
    "description": "Implement 2D max pooling from scratch.\n\nRequirements:\n- Apply max pooling with specified pool size and stride\n- Handle batch and channel dimensions\n- Return pooled output and optionally indices for unpooling\n- Support both overlapping and non-overlapping pooling",
    "starterCode": "import numpy as np\n\ndef max_pool2d(x, pool_size=2, stride=None):\n    \"\"\"\n    Apply 2D max pooling.\n\n    Args:\n        x: input tensor (batch, channels, height, width)\n        pool_size: size of pooling window\n        stride: stride (defaults to pool_size)\n\n    Returns:\n        pooled output\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import numpy as np\n\ndef max_pool2d(x, pool_size=2, stride=None):\n    \"\"\"\n    Apply 2D max pooling.\n\n    Args:\n        x: input tensor (batch, channels, height, width)\n        pool_size: size of pooling window\n        stride: stride (defaults to pool_size)\n\n    Returns:\n        pooled output\n    \"\"\"\n    if stride is None:\n        stride = pool_size\n\n    batch, channels, h_in, w_in = x.shape\n\n    # Calculate output dimensions\n    h_out = (h_in - pool_size) // stride + 1\n    w_out = (w_in - pool_size) // stride + 1\n\n    # Initialize output\n    output = np.zeros((batch, channels, h_out, w_out))\n\n    # Apply max pooling\n    for b in range(batch):\n        for c in range(channels):\n            for i in range(h_out):\n                for j in range(w_out):\n                    h_start = i * stride\n                    w_start = j * stride\n                    h_end = h_start + pool_size\n                    w_end = w_start + pool_size\n\n                    window = x[b, c, h_start:h_end, w_start:w_end]\n                    output[b, c, i, j] = np.max(window)\n\n    return output",
    "testCases": [],
    "hints": [
      "Calculate output dimensions based on pool size and stride",
      "Use nested loops to iterate over output positions",
      "Extract the pooling window using slicing",
      "Apply np.max to the window to get the maximum value",
      "Handle all batch and channel dimensions"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-5-4",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Build Simple RNN Cell",
    "difficulty": 3,
    "description": "Implement a basic RNN cell from scratch.\n\nRequirements:\n- Implement forward pass: h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b)\n- Initialize weights randomly\n- Process a sequence of inputs\n- Return all hidden states\n- Support batch processing",
    "starterCode": "import numpy as np\n\nclass RNNCell:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        # Initialize weights\n\n    def forward(self, x, h_prev):\n        \"\"\"\n        Forward pass for one time step.\n\n        Args:\n            x: input (batch, input_size)\n            h_prev: previous hidden state (batch, hidden_size)\n\n        Returns:\n            h_next: next hidden state\n        \"\"\"\n        # TODO: Implement\n        pass\n\n    def forward_sequence(self, x_seq):\n        \"\"\"Process entire sequence.\"\"\"\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\n\nclass RNNCell:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights with Xavier initialization\n        self.W_xh = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / (input_size + hidden_size))\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * np.sqrt(2.0 / (hidden_size + hidden_size))\n        self.b = np.zeros(hidden_size)\n\n    def forward(self, x, h_prev):\n        \"\"\"\n        Forward pass for one time step.\n\n        Args:\n            x: input (batch, input_size)\n            h_prev: previous hidden state (batch, hidden_size)\n\n        Returns:\n            h_next: next hidden state\n        \"\"\"\n        h_next = np.tanh(x @ self.W_xh + h_prev @ self.W_hh + self.b)\n        return h_next\n\n    def forward_sequence(self, x_seq):\n        \"\"\"\n        Process entire sequence.\n\n        Args:\n            x_seq: input sequence (batch, seq_len, input_size)\n\n        Returns:\n            h_seq: all hidden states (batch, seq_len, hidden_size)\n        \"\"\"\n        batch_size, seq_len, _ = x_seq.shape\n        h = np.zeros((batch_size, self.hidden_size))\n        h_seq = []\n\n        for t in range(seq_len):\n            h = self.forward(x_seq[:, t, :], h)\n            h_seq.append(h)\n\n        return np.stack(h_seq, axis=1)",
    "testCases": [],
    "hints": [
      "Use Xavier initialization for weights: scale by sqrt(2/(fan_in+fan_out))",
      "The hidden state is computed as: h = tanh(x*W_xh + h*W_hh + b)",
      "Use @ operator for matrix multiplication",
      "For sequences, iterate through time steps maintaining hidden state",
      "Stack all hidden states to create output sequence"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-5-5",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Implement LSTM Gates",
    "difficulty": 4,
    "description": "Implement LSTM cell with all four gates.\n\nRequirements:\n- Forget gate: f_t = sigmoid(W_f * [h_{t-1}, x_t] + b_f)\n- Input gate: i_t = sigmoid(W_i * [h_{t-1}, x_t] + b_i)\n- Cell candidate: c_tilde = tanh(W_c * [h_{t-1}, x_t] + b_c)\n- Output gate: o_t = sigmoid(W_o * [h_{t-1}, x_t] + b_o)\n- Update cell state and hidden state",
    "starterCode": "import numpy as np\n\nclass LSTMCell:\n    def __init__(self, input_size, hidden_size):\n        # Initialize weights for all gates\n        pass\n\n    def forward(self, x, h_prev, c_prev):\n        \"\"\"\n        LSTM forward pass.\n\n        Args:\n            x: input (batch, input_size)\n            h_prev: previous hidden state\n            c_prev: previous cell state\n\n        Returns:\n            h_next, c_next: next hidden and cell states\n        \"\"\"\n        # TODO: Implement all gates\n        pass",
    "solution": "import numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass LSTMCell:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Combined input size\n        combined_size = input_size + hidden_size\n        scale = np.sqrt(2.0 / (combined_size + hidden_size))\n\n        # Initialize weights for all gates\n        self.W_f = np.random.randn(combined_size, hidden_size) * scale\n        self.b_f = np.ones(hidden_size)  # Forget gate bias to 1\n\n        self.W_i = np.random.randn(combined_size, hidden_size) * scale\n        self.b_i = np.zeros(hidden_size)\n\n        self.W_c = np.random.randn(combined_size, hidden_size) * scale\n        self.b_c = np.zeros(hidden_size)\n\n        self.W_o = np.random.randn(combined_size, hidden_size) * scale\n        self.b_o = np.zeros(hidden_size)\n\n    def forward(self, x, h_prev, c_prev):\n        \"\"\"\n        LSTM forward pass.\n\n        Args:\n            x: input (batch, input_size)\n            h_prev: previous hidden state\n            c_prev: previous cell state\n\n        Returns:\n            h_next, c_next: next hidden and cell states\n        \"\"\"\n        # Concatenate input and hidden state\n        combined = np.concatenate([h_prev, x], axis=1)\n\n        # Forget gate\n        f_t = sigmoid(combined @ self.W_f + self.b_f)\n\n        # Input gate\n        i_t = sigmoid(combined @ self.W_i + self.b_i)\n\n        # Cell candidate\n        c_tilde = np.tanh(combined @ self.W_c + self.b_c)\n\n        # Output gate\n        o_t = sigmoid(combined @ self.W_o + self.b_o)\n\n        # Update cell state\n        c_next = f_t * c_prev + i_t * c_tilde\n\n        # Update hidden state\n        h_next = o_t * np.tanh(c_next)\n\n        return h_next, c_next",
    "testCases": [],
    "hints": [
      "Concatenate h_prev and x before computing gates",
      "Initialize forget gate bias to 1 to avoid vanishing gradients initially",
      "Use sigmoid activation for all gates (forget, input, output)",
      "Use tanh for cell candidate and final cell state activation",
      "Cell state: c_t = f_t * c_{t-1} + i_t * c_tilde",
      "Hidden state: h_t = o_t * tanh(c_t)"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-5-6",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Implement Attention Mechanism",
    "difficulty": 4,
    "description": "Build basic attention mechanism (Bahdanau-style).\n\nRequirements:\n- Compute attention scores: score(h_t, h_s) = v^T * tanh(W1*h_t + W2*h_s)\n- Apply softmax to get attention weights\n- Compute context vector as weighted sum\n- Return attention weights and context",
    "starterCode": "import numpy as np\n\nclass Attention:\n    def __init__(self, hidden_size):\n        # Initialize weight matrices\n        pass\n\n    def forward(self, query, keys, values):\n        \"\"\"\n        Apply attention mechanism.\n\n        Args:\n            query: decoder hidden state (batch, hidden_size)\n            keys: encoder hidden states (batch, seq_len, hidden_size)\n            values: encoder hidden states (batch, seq_len, hidden_size)\n\n        Returns:\n            context: weighted context vector\n            weights: attention weights\n        \"\"\"\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\n\ndef softmax(x, axis=-1):\n    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n\nclass Attention:\n    def __init__(self, hidden_size):\n        self.hidden_size = hidden_size\n\n        # Initialize weight matrices\n        scale = np.sqrt(2.0 / hidden_size)\n        self.W1 = np.random.randn(hidden_size, hidden_size) * scale\n        self.W2 = np.random.randn(hidden_size, hidden_size) * scale\n        self.v = np.random.randn(hidden_size) * scale\n\n    def forward(self, query, keys, values):\n        \"\"\"\n        Apply attention mechanism.\n\n        Args:\n            query: decoder hidden state (batch, hidden_size)\n            keys: encoder hidden states (batch, seq_len, hidden_size)\n            values: encoder hidden states (batch, seq_len, hidden_size)\n\n        Returns:\n            context: weighted context vector\n            weights: attention weights\n        \"\"\"\n        batch_size, seq_len, _ = keys.shape\n\n        # Expand query for broadcasting\n        query_expanded = query[:, np.newaxis, :]  # (batch, 1, hidden)\n\n        # Compute scores: v^T * tanh(W1*query + W2*keys)\n        # query: (batch, 1, hidden) @ W1: (hidden, hidden) -> (batch, 1, hidden)\n        # keys: (batch, seq_len, hidden) @ W2: (hidden, hidden) -> (batch, seq_len, hidden)\n        query_proj = query_expanded @ self.W1\n        keys_proj = keys @ self.W2\n\n        # Add and apply tanh\n        combined = np.tanh(query_proj + keys_proj)  # (batch, seq_len, hidden)\n\n        # Apply v to get scores\n        scores = combined @ self.v  # (batch, seq_len)\n\n        # Compute attention weights\n        weights = softmax(scores, axis=1)  # (batch, seq_len)\n\n        # Compute context vector\n        weights_expanded = weights[:, :, np.newaxis]  # (batch, seq_len, 1)\n        context = np.sum(weights_expanded * values, axis=1)  # (batch, hidden)\n\n        return context, weights",
    "testCases": [],
    "hints": [
      "Expand query dimension to enable broadcasting with keys",
      "Project both query and keys with their respective weight matrices",
      "Apply tanh activation after combining projections",
      "Multiply by v vector to get scalar scores for each position",
      "Use softmax to convert scores to attention weights",
      "Compute context as weighted sum of values"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-5-7",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Build Multi-Head Attention",
    "difficulty": 5,
    "description": "Implement multi-head self-attention (as in Transformers).\n\nRequirements:\n- Split input into multiple attention heads\n- Apply scaled dot-product attention per head\n- Concatenate heads and apply output projection\n- Support masking for decoder self-attention",
    "starterCode": "import numpy as np\n\nclass MultiHeadAttention:\n    def __init__(self, d_model, num_heads):\n        self.d_model = d_model\n        self.num_heads = num_heads\n        # Initialize weights\n\n    def forward(self, query, key, value, mask=None):\n        \"\"\"\n        Multi-head attention.\n\n        Args:\n            query, key, value: input tensors (batch, seq_len, d_model)\n            mask: optional mask (batch, seq_len, seq_len)\n\n        Returns:\n            output: (batch, seq_len, d_model)\n        \"\"\"\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\n\ndef softmax(x, axis=-1):\n    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n\nclass MultiHeadAttention:\n    def __init__(self, d_model, num_heads):\n        assert d_model % num_heads == 0\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n\n        # Initialize weight matrices\n        scale = np.sqrt(2.0 / d_model)\n        self.W_q = np.random.randn(d_model, d_model) * scale\n        self.W_k = np.random.randn(d_model, d_model) * scale\n        self.W_v = np.random.randn(d_model, d_model) * scale\n        self.W_o = np.random.randn(d_model, d_model) * scale\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        \"\"\"\n        Compute scaled dot-product attention.\n\n        Args:\n            Q, K, V: (batch, num_heads, seq_len, d_k)\n            mask: (batch, 1, seq_len, seq_len)\n        \"\"\"\n        # Compute attention scores\n        scores = Q @ K.transpose(0, 1, 3, 2) / np.sqrt(self.d_k)\n\n        # Apply mask if provided\n        if mask is not None:\n            scores = scores + (mask * -1e9)\n\n        # Apply softmax\n        attn_weights = softmax(scores, axis=-1)\n\n        # Apply attention to values\n        output = attn_weights @ V\n\n        return output\n\n    def forward(self, query, key, value, mask=None):\n        \"\"\"\n        Multi-head attention.\n\n        Args:\n            query, key, value: input tensors (batch, seq_len, d_model)\n            mask: optional mask (batch, seq_len, seq_len)\n\n        Returns:\n            output: (batch, seq_len, d_model)\n        \"\"\"\n        batch_size, seq_len, _ = query.shape\n\n        # Linear projections\n        Q = query @ self.W_q  # (batch, seq_len, d_model)\n        K = key @ self.W_k\n        V = value @ self.W_v\n\n        # Reshape for multi-head: (batch, seq_len, num_heads, d_k)\n        Q = Q.reshape(batch_size, seq_len, self.num_heads, self.d_k)\n        K = K.reshape(batch_size, seq_len, self.num_heads, self.d_k)\n        V = V.reshape(batch_size, seq_len, self.num_heads, self.d_k)\n\n        # Transpose to (batch, num_heads, seq_len, d_k)\n        Q = Q.transpose(0, 2, 1, 3)\n        K = K.transpose(0, 2, 1, 3)\n        V = V.transpose(0, 2, 1, 3)\n\n        # Apply attention\n        if mask is not None:\n            mask = mask[:, np.newaxis, :, :]  # Add head dimension\n\n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n\n        # Concatenate heads: (batch, seq_len, d_model)\n        attn_output = attn_output.transpose(0, 2, 1, 3)\n        attn_output = attn_output.reshape(batch_size, seq_len, self.d_model)\n\n        # Final linear projection\n        output = attn_output @ self.W_o\n\n        return output",
    "testCases": [],
    "hints": [
      "Project Q, K, V using learned weight matrices",
      "Split d_model into num_heads, each with dimension d_k = d_model/num_heads",
      "Reshape and transpose to get (batch, num_heads, seq_len, d_k)",
      "Compute scaled dot-product: softmax(Q*K^T/sqrt(d_k))*V",
      "Apply mask before softmax by adding large negative values",
      "Concatenate heads and apply final linear projection"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-5-8",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Implement Positional Encoding",
    "difficulty": 2,
    "description": "Create positional encoding for Transformer models.\n\nRequirements:\n- Use sinusoidal functions: PE(pos, 2i) = sin(pos/10000^(2i/d_model))\n- Even dimensions use sine, odd dimensions use cosine\n- Support variable sequence lengths\n- Return encoding matrix of shape (seq_len, d_model)",
    "starterCode": "import numpy as np\n\ndef positional_encoding(seq_len, d_model):\n    \"\"\"\n    Generate positional encoding.\n\n    Args:\n        seq_len: sequence length\n        d_model: model dimension\n\n    Returns:\n        pe: positional encoding (seq_len, d_model)\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import numpy as np\n\ndef positional_encoding(seq_len, d_model):\n    \"\"\"\n    Generate positional encoding.\n\n    Args:\n        seq_len: sequence length\n        d_model: model dimension\n\n    Returns:\n        pe: positional encoding (seq_len, d_model)\n    \"\"\"\n    # Initialize matrix\n    pe = np.zeros((seq_len, d_model))\n\n    # Create position indices\n    position = np.arange(seq_len)[:, np.newaxis]  # (seq_len, 1)\n\n    # Create dimension indices\n    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n\n    # Apply sine to even indices\n    pe[:, 0::2] = np.sin(position * div_term)\n\n    # Apply cosine to odd indices\n    pe[:, 1::2] = np.cos(position * div_term)\n\n    return pe",
    "testCases": [],
    "hints": [
      "Create position array from 0 to seq_len-1",
      "Compute division term: 10000^(2i/d_model) for each dimension pair",
      "Use np.arange(0, d_model, 2) to get even dimension indices",
      "Apply sin to even dimensions: PE[:, 0::2]",
      "Apply cos to odd dimensions: PE[:, 1::2]",
      "Use broadcasting to compute all positions at once"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-5-9",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Build Transformer Encoder Block",
    "difficulty": 4,
    "description": "Implement a complete Transformer encoder block.\n\nRequirements:\n- Multi-head self-attention with residual connection and layer norm\n- Position-wise feed-forward network (two linear layers with ReLU)\n- Residual connections after each sublayer\n- Layer normalization after residual connections",
    "starterCode": "import torch\nimport torch.nn as nn\n\nclass TransformerEncoderBlock(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        # Initialize layers\n\n    def forward(self, x, mask=None):\n        # TODO: Implement\n        pass",
    "solution": "import torch\nimport torch.nn as nn\n\nclass TransformerEncoderBlock(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n\n        # Multi-head attention\n        self.attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n\n        # Feed-forward network\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model)\n        )\n\n        # Layer normalization\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n        # Dropout\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Forward pass.\n\n        Args:\n            x: input (batch, seq_len, d_model)\n            mask: attention mask (seq_len, seq_len)\n\n        Returns:\n            output (batch, seq_len, d_model)\n        \"\"\"\n        # Multi-head attention with residual\n        attn_output, _ = self.attention(x, x, x, attn_mask=mask)\n        x = self.norm1(x + self.dropout1(attn_output))\n\n        # Feed-forward with residual\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout2(ffn_output))\n\n        return x",
    "testCases": [],
    "hints": [
      "Use nn.MultiheadAttention with batch_first=True",
      "Feed-forward network: Linear -> ReLU -> Dropout -> Linear",
      "Apply residual connection: output = norm(x + sublayer(x))",
      "Use LayerNorm after each residual connection",
      "Typical d_ff is 4 times d_model (e.g., 2048 for d_model=512)"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-5-10",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Implement Batch Normalization",
    "difficulty": 2,
    "description": "Build batch normalization layer from scratch.\n\nRequirements:\n- Normalize across batch dimension: (x - mean) / sqrt(var + epsilon)\n- Support training and inference modes\n- Include learnable scale (gamma) and shift (beta) parameters\n- Track running statistics for inference",
    "starterCode": "import numpy as np\n\nclass BatchNorm1d:\n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        # Initialize parameters\n\n    def forward(self, x, training=True):\n        \"\"\"\n        Apply batch normalization.\n\n        Args:\n            x: input (batch, features)\n            training: whether in training mode\n\n        Returns:\n            normalized output\n        \"\"\"\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\n\nclass BatchNorm1d:\n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n\n        # Learnable parameters\n        self.gamma = np.ones(num_features)\n        self.beta = np.zeros(num_features)\n\n        # Running statistics\n        self.running_mean = np.zeros(num_features)\n        self.running_var = np.ones(num_features)\n\n    def forward(self, x, training=True):\n        \"\"\"\n        Apply batch normalization.\n\n        Args:\n            x: input (batch, features)\n            training: whether in training mode\n\n        Returns:\n            normalized output\n        \"\"\"\n        if training:\n            # Compute batch statistics\n            batch_mean = np.mean(x, axis=0)\n            batch_var = np.var(x, axis=0)\n\n            # Normalize\n            x_norm = (x - batch_mean) / np.sqrt(batch_var + self.eps)\n\n            # Update running statistics\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n        else:\n            # Use running statistics\n            x_norm = (x - self.running_mean) / np.sqrt(self.running_var + self.eps)\n\n        # Scale and shift\n        output = self.gamma * x_norm + self.beta\n\n        return output",
    "testCases": [],
    "hints": [
      "Compute mean and variance across batch dimension (axis=0)",
      "Normalize: (x - mean) / sqrt(var + eps)",
      "Apply learnable affine transformation: gamma * x_norm + beta",
      "Update running statistics with exponential moving average",
      "Use running statistics during inference, batch statistics during training"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-5-11",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Implement Dropout Layer",
    "difficulty": 1,
    "description": "Create dropout regularization from scratch.\n\nRequirements:\n- Randomly drop neurons with probability p during training\n- Scale remaining activations by 1/(1-p)\n- Keep all neurons during inference\n- Support different dropout rates",
    "starterCode": "import numpy as np\n\nclass Dropout:\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def forward(self, x, training=True):\n        \"\"\"\n        Apply dropout.\n\n        Args:\n            x: input tensor\n            training: whether in training mode\n\n        Returns:\n            output with dropout applied\n        \"\"\"\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\n\nclass Dropout:\n    def __init__(self, p=0.5):\n        self.p = p\n        self.mask = None\n\n    def forward(self, x, training=True):\n        \"\"\"\n        Apply dropout.\n\n        Args:\n            x: input tensor\n            training: whether in training mode\n\n        Returns:\n            output with dropout applied\n        \"\"\"\n        if training:\n            # Create dropout mask\n            self.mask = np.random.binomial(1, 1 - self.p, size=x.shape)\n\n            # Apply mask and scale\n            output = x * self.mask / (1 - self.p)\n        else:\n            # No dropout during inference\n            output = x\n\n        return output",
    "testCases": [],
    "hints": [
      "Use np.random.binomial to create binary mask with probability (1-p)",
      "Multiply input by mask to drop neurons",
      "Scale by 1/(1-p) to maintain expected value",
      "During inference, return input unchanged",
      "Store mask for potential use in backward pass"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-5-12",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Build ResNet Residual Block",
    "difficulty": 3,
    "description": "Implement a ResNet residual block with skip connection.\n\nRequirements:\n- Two convolutional layers with batch norm and ReLU\n- Skip connection that adds input to output\n- Handle dimension mismatch with 1x1 convolution\n- Support stride for downsampling",
    "starterCode": "import torch\nimport torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        # Define layers\n\n    def forward(self, x):\n        # TODO: Implement\n        pass",
    "solution": "import torch\nimport torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n\n        # Main path\n        self.conv1 = nn.Conv2d(in_channels, out_channels,\n                               kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(out_channels, out_channels,\n                               kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        # Skip connection\n        self.skip = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.skip = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n                         stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass.\n\n        Args:\n            x: input (batch, channels, height, width)\n\n        Returns:\n            output with residual connection\n        \"\"\"\n        identity = x\n\n        # Main path\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        # Add skip connection\n        out += self.skip(identity)\n        out = self.relu(out)\n\n        return out",
    "testCases": [],
    "hints": [
      "Use Conv2d -> BatchNorm2d -> ReLU pattern",
      "Apply stride in first convolution for downsampling",
      "Use 1x1 convolution in skip connection when dimensions change",
      "Add skip connection before final ReLU activation",
      "Set bias=False in convolutions when using batch norm"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-5-13",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Implement Transfer Learning",
    "difficulty": 3,
    "description": "Fine-tune a pre-trained CNN for new classification task.\n\nRequirements:\n- Load pre-trained ResNet model\n- Freeze early layers (feature extraction)\n- Replace final layer for new classes\n- Train only the new classifier\n- Compare with training from scratch",
    "starterCode": "import torch\nimport torch.nn as nn\nfrom torchvision import models\n\ndef setup_transfer_learning(num_classes, freeze_features=True):\n    \"\"\"\n    Setup model for transfer learning.\n\n    Args:\n        num_classes: number of classes in new task\n        freeze_features: whether to freeze feature layers\n\n    Returns:\n        model ready for transfer learning\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import torch\nimport torch.nn as nn\nfrom torchvision import models\n\ndef setup_transfer_learning(num_classes, freeze_features=True):\n    \"\"\"\n    Setup model for transfer learning.\n\n    Args:\n        num_classes: number of classes in new task\n        freeze_features: whether to freeze feature layers\n\n    Returns:\n        model ready for transfer learning\n    \"\"\"\n    # Load pre-trained ResNet\n    model = models.resnet18(pretrained=True)\n\n    # Freeze feature extraction layers\n    if freeze_features:\n        for param in model.parameters():\n            param.requires_grad = False\n\n    # Replace final layer\n    num_features = model.fc.in_features\n    model.fc = nn.Linear(num_features, num_classes)\n\n    return model\n\n# Example usage\nmodel = setup_transfer_learning(num_classes=10, freeze_features=True)\n\n# Create optimizer that only updates classifier\noptimizer = torch.optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters()),\n    lr=0.001\n)\n\n# Training loop (simplified)\ncriterion = nn.CrossEntropyLoss()\nfor epoch in range(10):\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()",
    "testCases": [],
    "hints": [
      "Use torchvision.models to load pre-trained models",
      "Set requires_grad=False to freeze parameters",
      "Replace model.fc (final layer) with new Linear layer",
      "Use filter(lambda p: p.requires_grad) to get trainable parameters",
      "Transfer learning typically uses lower learning rate"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-5-14",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Implement GRU Cell",
    "difficulty": 3,
    "description": "Build Gated Recurrent Unit (GRU) cell from scratch.\n\nRequirements:\n- Reset gate: r_t = sigmoid(W_r * [h_{t-1}, x_t])\n- Update gate: z_t = sigmoid(W_z * [h_{t-1}, x_t])\n- Candidate: h_tilde = tanh(W_h * [r_t * h_{t-1}, x_t])\n- Final state: h_t = (1 - z_t) * h_{t-1} + z_t * h_tilde",
    "starterCode": "import numpy as np\n\nclass GRUCell:\n    def __init__(self, input_size, hidden_size):\n        # Initialize weights\n        pass\n\n    def forward(self, x, h_prev):\n        \"\"\"\n        GRU forward pass.\n\n        Args:\n            x: input (batch, input_size)\n            h_prev: previous hidden state\n\n        Returns:\n            h_next: next hidden state\n        \"\"\"\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass GRUCell:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        combined_size = input_size + hidden_size\n        scale = np.sqrt(2.0 / (combined_size + hidden_size))\n\n        # Initialize weights\n        self.W_r = np.random.randn(combined_size, hidden_size) * scale\n        self.b_r = np.zeros(hidden_size)\n\n        self.W_z = np.random.randn(combined_size, hidden_size) * scale\n        self.b_z = np.zeros(hidden_size)\n\n        self.W_h = np.random.randn(combined_size, hidden_size) * scale\n        self.b_h = np.zeros(hidden_size)\n\n    def forward(self, x, h_prev):\n        \"\"\"\n        GRU forward pass.\n\n        Args:\n            x: input (batch, input_size)\n            h_prev: previous hidden state (batch, hidden_size)\n\n        Returns:\n            h_next: next hidden state\n        \"\"\"\n        # Concatenate input and hidden\n        combined = np.concatenate([h_prev, x], axis=1)\n\n        # Reset gate\n        r_t = sigmoid(combined @ self.W_r + self.b_r)\n\n        # Update gate\n        z_t = sigmoid(combined @ self.W_z + self.b_z)\n\n        # Candidate hidden state\n        combined_reset = np.concatenate([r_t * h_prev, x], axis=1)\n        h_tilde = np.tanh(combined_reset @ self.W_h + self.b_h)\n\n        # Final hidden state\n        h_next = (1 - z_t) * h_prev + z_t * h_tilde\n\n        return h_next",
    "testCases": [],
    "hints": [
      "GRU has fewer parameters than LSTM (no separate cell state)",
      "Reset gate controls how much past information to forget",
      "Update gate decides between keeping old state or using new candidate",
      "Apply reset gate before computing candidate state",
      "Final state is interpolation between previous and candidate states"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-5-15",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Build Seq2Seq with Attention",
    "difficulty": 5,
    "description": "Implement sequence-to-sequence model with attention mechanism.\n\nRequirements:\n- LSTM-based encoder that processes input sequence\n- LSTM-based decoder with attention over encoder outputs\n- Attention mechanism to compute context vector at each step\n- Teacher forcing during training\n- Beam search for inference",
    "starterCode": "import torch\nimport torch.nn as nn\n\nclass Seq2SeqAttention(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size):\n        super().__init__()\n        # Define encoder, decoder, attention\n\n    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n        # TODO: Implement\n        pass",
    "solution": "import torch\nimport torch.nn as nn\nimport random\n\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n\n    def forward(self, src):\n        embedded = self.embedding(src)\n        outputs, (hidden, cell) = self.lstm(embedded)\n        return outputs, hidden, cell\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n        self.v = nn.Linear(hidden_size, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs):\n        # hidden: (1, batch, hidden)\n        # encoder_outputs: (batch, seq_len, hidden)\n        batch_size = encoder_outputs.shape[0]\n        seq_len = encoder_outputs.shape[1]\n\n        hidden = hidden.repeat(seq_len, 1, 1).transpose(0, 1)\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n        attention = self.v(energy).squeeze(2)\n        return torch.softmax(attention, dim=1)\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.attention = Attention(hidden_size)\n        self.lstm = nn.LSTM(embed_size + hidden_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, input, hidden, cell, encoder_outputs):\n        input = input.unsqueeze(1)  # (batch, 1)\n        embedded = self.embedding(input)  # (batch, 1, embed)\n\n        attn_weights = self.attention(hidden, encoder_outputs)\n        attn_weights = attn_weights.unsqueeze(1)\n        context = torch.bmm(attn_weights, encoder_outputs)\n\n        lstm_input = torch.cat((embedded, context), dim=2)\n        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n\n        prediction = self.fc(output.squeeze(1))\n        return prediction, hidden, cell\n\nclass Seq2SeqAttention(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, embed_size, hidden_size)\n        self.decoder = Decoder(vocab_size, embed_size, hidden_size)\n\n    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n        batch_size = src.shape[0]\n        tgt_len = tgt.shape[1]\n        vocab_size = self.decoder.fc.out_features\n\n        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(src.device)\n        encoder_outputs, hidden, cell = self.encoder(src)\n\n        input = tgt[:, 0]\n        for t in range(1, tgt_len):\n            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n            outputs[:, t] = output\n\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = tgt[:, t] if teacher_force else top1\n\n        return outputs",
    "testCases": [],
    "hints": [
      "Encoder processes entire source sequence and returns all outputs",
      "Decoder generates one token at a time using attention",
      "Compute attention weights over all encoder outputs",
      "Context vector is weighted sum of encoder outputs",
      "Use teacher forcing: feed ground truth instead of prediction during training",
      "For inference, use greedy decoding or beam search"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-5-16",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Implement Data Augmentation for Images",
    "difficulty": 2,
    "description": "Create custom data augmentation pipeline for image classification.\n\nRequirements:\n- Random horizontal flip\n- Random rotation (-15 to +15 degrees)\n- Random crop and resize\n- Color jittering (brightness, contrast, saturation)\n- Normalization with ImageNet statistics",
    "starterCode": "import torch\nfrom torchvision import transforms\n\ndef create_augmentation_pipeline(train=True):\n    \"\"\"\n    Create data augmentation pipeline.\n\n    Args:\n        train: whether for training (augment) or validation (no augment)\n\n    Returns:\n        transforms.Compose pipeline\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import torch\nfrom torchvision import transforms\n\ndef create_augmentation_pipeline(train=True):\n    \"\"\"\n    Create data augmentation pipeline.\n\n    Args:\n        train: whether for training (augment) or validation (no augment)\n\n    Returns:\n        transforms.Compose pipeline\n    \"\"\"\n    # ImageNet statistics\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n\n    if train:\n        # Training pipeline with augmentation\n        pipeline = transforms.Compose([\n            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomRotation(degrees=15),\n            transforms.ColorJitter(\n                brightness=0.2,\n                contrast=0.2,\n                saturation=0.2,\n                hue=0.1\n            ),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std),\n            transforms.RandomErasing(p=0.3)  # Random erasing augmentation\n        ])\n    else:\n        # Validation pipeline without augmentation\n        pipeline = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std)\n        ])\n\n    return pipeline\n\n# Usage example\ntrain_transform = create_augmentation_pipeline(train=True)\nval_transform = create_augmentation_pipeline(train=False)\n\n# Apply to dataset\nfrom torchvision.datasets import ImageFolder\ntrain_dataset = ImageFolder(root='train/', transform=train_transform)\nval_dataset = ImageFolder(root='val/', transform=val_transform)",
    "testCases": [],
    "hints": [
      "Use transforms.Compose to chain multiple transformations",
      "RandomResizedCrop combines random crop and resize",
      "Set probability p for random transformations",
      "Apply ColorJitter before ToTensor for efficiency",
      "Always normalize as the last step (after ToTensor)",
      "No augmentation for validation, only resize and center crop"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t6-ex01",
    "subjectId": "cs402",
    "topicId": "cs402-topic-6",
    "title": "Implement K-Means from Scratch",
    "difficulty": 3,
    "description": "Build k-means clustering algorithm without sklearn.\n\nRequirements:\n- Initialize centroids using k-means++\n- Implement assignment and update steps\n- Track convergence (when centroids stop moving)\n- Return cluster assignments and final centroids",
    "starterCode": "import numpy as np\n\nclass KMeans:\n    def __init__(self, n_clusters=3, max_iters=100):\n        self.n_clusters = n_clusters\n        self.max_iters = max_iters\n        \n    def fit(self, X):\n        # Initialize centroids\n        # Iterate until convergence\n        pass\n        \n    def predict(self, X):\n        # Assign to nearest centroid\n        pass",
    "solution": "import numpy as np\n\nclass KMeans:\n    def __init__(self, n_clusters=3, max_iters=100):\n        self.n_clusters = n_clusters\n        self.max_iters = max_iters\n        self.centroids = None\n        \n    def fit(self, X):\n        # K-means++ initialization\n        centroids = [X[np.random.randint(len(X))]]\n        for _ in range(1, self.n_clusters):\n            distances = np.array([min([np.linalg.norm(x-c)**2 for c in centroids]) for x in X])\n            probs = distances / distances.sum()\n            centroids.append(X[np.random.choice(len(X), p=probs)])\n        self.centroids = np.array(centroids)\n        \n        # Main loop\n        for _ in range(self.max_iters):\n            # Assign\n            labels = self.predict(X)\n            # Update\n            new_centroids = np.array([X[labels == k].mean(axis=0) \n                                     for k in range(self.n_clusters)])\n            if np.allclose(new_centroids, self.centroids):\n                break\n            self.centroids = new_centroids\n        return self\n        \n    def predict(self, X):\n        distances = np.array([[np.linalg.norm(x - c) for c in self.centroids] for x in X])\n        return np.argmin(distances, axis=1)",
    "testCases": [],
    "hints": [
      "Use k-means++ initialization to choose initial centroids wisely",
      "In each iteration, assign each point to the nearest centroid",
      "Update centroids by taking the mean of all points assigned to each cluster",
      "Check convergence by comparing old and new centroids",
      "Use np.linalg.norm to calculate Euclidean distances"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-6-2",
    "subjectId": "cs402",
    "topicId": "cs402-topic-6",
    "title": "Calculate Silhouette Score",
    "difficulty": 2,
    "description": "Compute silhouette score to evaluate clustering quality.\n\nRequirements:\n- For each point, compute a(i) = average distance to points in same cluster\n- Compute b(i) = min average distance to points in other clusters\n- Silhouette = (b(i) - a(i)) / max(a(i), b(i))\n- Return mean silhouette score across all points",
    "starterCode": "import numpy as np\n\ndef silhouette_score(X, labels):\n    \"\"\"\n    Compute silhouette score.\n\n    Args:\n        X: data points (n_samples, n_features)\n        labels: cluster assignments (n_samples,)\n\n    Returns:\n        mean silhouette score\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import numpy as np\n\ndef silhouette_score(X, labels):\n    \"\"\"\n    Compute silhouette score.\n\n    Args:\n        X: data points (n_samples, n_features)\n        labels: cluster assignments (n_samples,)\n\n    Returns:\n        mean silhouette score\n    \"\"\"\n    n_samples = len(X)\n    silhouette_vals = np.zeros(n_samples)\n\n    for i in range(n_samples):\n        # Get cluster of current point\n        cluster_i = labels[i]\n\n        # Points in same cluster\n        same_cluster = X[labels == cluster_i]\n\n        # a(i): mean distance to points in same cluster\n        if len(same_cluster) > 1:\n            a_i = np.mean([np.linalg.norm(X[i] - x) for x in same_cluster if not np.array_equal(x, X[i])])\n        else:\n            a_i = 0\n\n        # b(i): min mean distance to points in other clusters\n        b_i = float('inf')\n        for cluster_j in np.unique(labels):\n            if cluster_j != cluster_i:\n                other_cluster = X[labels == cluster_j]\n                mean_dist = np.mean([np.linalg.norm(X[i] - x) for x in other_cluster])\n                b_i = min(b_i, mean_dist)\n\n        # Compute silhouette\n        if max(a_i, b_i) > 0:\n            silhouette_vals[i] = (b_i - a_i) / max(a_i, b_i)\n        else:\n            silhouette_vals[i] = 0\n\n    return np.mean(silhouette_vals)",
    "testCases": [],
    "hints": [
      "a(i) measures cohesion: how close point is to its cluster",
      "b(i) measures separation: distance to nearest other cluster",
      "Silhouette ranges from -1 (wrong cluster) to +1 (perfect)",
      "Handle single-point clusters by setting a(i) = 0",
      "Use Euclidean distance for distance calculations"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-6-3",
    "subjectId": "cs402",
    "topicId": "cs402-topic-6",
    "title": "Implement Elbow Method",
    "difficulty": 1,
    "description": "Determine optimal number of clusters using elbow method.\n\nRequirements:\n- Run k-means for k = 1 to max_k\n- Compute within-cluster sum of squares (WCSS) for each k\n- Plot WCSS vs k to find \"elbow\"\n- Return WCSS values for all k",
    "starterCode": "import numpy as np\nfrom sklearn.cluster import KMeans\n\ndef elbow_method(X, max_k=10):\n    \"\"\"\n    Apply elbow method to find optimal k.\n\n    Args:\n        X: data points\n        max_k: maximum k to try\n\n    Returns:\n        wcss: list of WCSS for each k\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef elbow_method(X, max_k=10):\n    \"\"\"\n    Apply elbow method to find optimal k.\n\n    Args:\n        X: data points\n        max_k: maximum k to try\n\n    Returns:\n        wcss: list of WCSS for each k\n    \"\"\"\n    wcss = []\n\n    for k in range(1, max_k + 1):\n        kmeans = KMeans(n_clusters=k, random_state=42)\n        kmeans.fit(X)\n\n        # Compute within-cluster sum of squares\n        wcss.append(kmeans.inertia_)\n\n    return wcss\n\n# Usage with plotting\nX = np.random.randn(300, 2)\nwcss = elbow_method(X, max_k=10)\n\nplt.plot(range(1, 11), wcss, marker='o')\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('WCSS')\nplt.title('Elbow Method')\nplt.show()",
    "testCases": [],
    "hints": [
      "WCSS is the sum of squared distances from points to their centroids",
      "KMeans.inertia_ gives the WCSS directly",
      "Plot shows decreasing WCSS as k increases",
      "Look for \"elbow\" where improvement slows down",
      "Optimal k is at the elbow point"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-6-4",
    "subjectId": "cs402",
    "topicId": "cs402-topic-6",
    "title": "Build Hierarchical Clustering",
    "difficulty": 3,
    "description": "Implement agglomerative hierarchical clustering.\n\nRequirements:\n- Start with each point as its own cluster\n- Use single linkage (minimum distance between clusters)\n- Merge closest clusters iteratively\n- Build dendrogram structure\n- Support different linkage methods",
    "starterCode": "import numpy as np\n\nclass HierarchicalClustering:\n    def __init__(self, linkage='single'):\n        self.linkage = linkage\n\n    def fit(self, X, n_clusters=2):\n        \"\"\"\n        Perform hierarchical clustering.\n\n        Args:\n            X: data points\n            n_clusters: number of final clusters\n\n        Returns:\n            cluster labels\n        \"\"\"\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\n\nclass HierarchicalClustering:\n    def __init__(self, linkage='single'):\n        self.linkage = linkage\n\n    def compute_distance(self, cluster1, cluster2):\n        \"\"\"Compute distance between two clusters.\"\"\"\n        if self.linkage == 'single':\n            # Minimum distance\n            return np.min([np.linalg.norm(p1 - p2)\n                          for p1 in cluster1 for p2 in cluster2])\n        elif self.linkage == 'complete':\n            # Maximum distance\n            return np.max([np.linalg.norm(p1 - p2)\n                          for p1 in cluster1 for p2 in cluster2])\n        elif self.linkage == 'average':\n            # Average distance\n            return np.mean([np.linalg.norm(p1 - p2)\n                           for p1 in cluster1 for p2 in cluster2])\n\n    def fit(self, X, n_clusters=2):\n        \"\"\"\n        Perform hierarchical clustering.\n\n        Args:\n            X: data points (n_samples, n_features)\n            n_clusters: number of final clusters\n\n        Returns:\n            cluster labels\n        \"\"\"\n        # Initialize: each point is a cluster\n        clusters = [[i] for i in range(len(X))]\n\n        # Merge until we have n_clusters\n        while len(clusters) > n_clusters:\n            # Find closest pair of clusters\n            min_dist = float('inf')\n            merge_i, merge_j = 0, 1\n\n            for i in range(len(clusters)):\n                for j in range(i + 1, len(clusters)):\n                    cluster_i = X[clusters[i]]\n                    cluster_j = X[clusters[j]]\n                    dist = self.compute_distance(cluster_i, cluster_j)\n\n                    if dist < min_dist:\n                        min_dist = dist\n                        merge_i, merge_j = i, j\n\n            # Merge closest clusters\n            clusters[merge_i].extend(clusters[merge_j])\n            clusters.pop(merge_j)\n\n        # Create label array\n        labels = np.zeros(len(X), dtype=int)\n        for cluster_id, cluster in enumerate(clusters):\n            for point_id in cluster:\n                labels[point_id] = cluster_id\n\n        return labels",
    "testCases": [],
    "hints": [
      "Start with n clusters (one per point)",
      "Single linkage: distance = min distance between any two points",
      "Complete linkage: distance = max distance between any two points",
      "Average linkage: distance = mean of all pairwise distances",
      "Merge closest clusters until reaching desired number"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-6-5",
    "subjectId": "cs402",
    "topicId": "cs402-topic-6",
    "title": "Implement DBSCAN",
    "difficulty": 4,
    "description": "Build DBSCAN density-based clustering from scratch.\n\nRequirements:\n- Find core points (points with >= min_samples neighbors within eps)\n- Expand clusters from core points\n- Mark border points and noise\n- Handle arbitrary cluster shapes",
    "starterCode": "import numpy as np\n\nclass DBSCAN:\n    def __init__(self, eps=0.5, min_samples=5):\n        self.eps = eps\n        self.min_samples = min_samples\n\n    def fit_predict(self, X):\n        \"\"\"\n        Perform DBSCAN clustering.\n\n        Args:\n            X: data points\n\n        Returns:\n            labels (-1 for noise)\n        \"\"\"\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\n\nclass DBSCAN:\n    def __init__(self, eps=0.5, min_samples=5):\n        self.eps = eps\n        self.min_samples = min_samples\n\n    def get_neighbors(self, X, point_idx):\n        \"\"\"Find all neighbors within eps distance.\"\"\"\n        neighbors = []\n        for i in range(len(X)):\n            if np.linalg.norm(X[point_idx] - X[i]) <= self.eps:\n                neighbors.append(i)\n        return neighbors\n\n    def expand_cluster(self, X, labels, point_idx, neighbors, cluster_id):\n        \"\"\"Expand cluster from a core point.\"\"\"\n        labels[point_idx] = cluster_id\n\n        i = 0\n        while i < len(neighbors):\n            neighbor_idx = neighbors[i]\n\n            # If noise, change to border point\n            if labels[neighbor_idx] == -1:\n                labels[neighbor_idx] = cluster_id\n\n            # If unvisited\n            elif labels[neighbor_idx] == 0:\n                labels[neighbor_idx] = cluster_id\n\n                # Check if neighbor is core point\n                neighbor_neighbors = self.get_neighbors(X, neighbor_idx)\n                if len(neighbor_neighbors) >= self.min_samples:\n                    neighbors.extend(neighbor_neighbors)\n\n            i += 1\n\n    def fit_predict(self, X):\n        \"\"\"\n        Perform DBSCAN clustering.\n\n        Args:\n            X: data points (n_samples, n_features)\n\n        Returns:\n            labels (-1 for noise, 0+ for clusters)\n        \"\"\"\n        n_samples = len(X)\n        labels = np.zeros(n_samples, dtype=int)  # 0 = unvisited\n        cluster_id = 0\n\n        for point_idx in range(n_samples):\n            # Skip if already processed\n            if labels[point_idx] != 0:\n                continue\n\n            # Find neighbors\n            neighbors = self.get_neighbors(X, point_idx)\n\n            # Mark as noise if not enough neighbors\n            if len(neighbors) < self.min_samples:\n                labels[point_idx] = -1\n            else:\n                # Create new cluster\n                cluster_id += 1\n                self.expand_cluster(X, labels, point_idx, neighbors, cluster_id)\n\n        return labels",
    "testCases": [],
    "hints": [
      "Core point: has at least min_samples neighbors within eps",
      "Border point: not core but within eps of a core point",
      "Noise point: neither core nor border",
      "Use breadth-first search to expand clusters",
      "Label noise as -1, clusters as 1, 2, 3, ..."
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-6-6",
    "subjectId": "cs402",
    "topicId": "cs402-topic-6",
    "title": "Implement PCA from Scratch",
    "difficulty": 3,
    "description": "Build Principal Component Analysis without sklearn.\n\nRequirements:\n- Center the data by subtracting mean\n- Compute covariance matrix\n- Find eigenvectors and eigenvalues\n- Sort by eigenvalues (descending)\n- Project data onto top k components",
    "starterCode": "import numpy as np\n\nclass PCA:\n    def __init__(self, n_components=2):\n        self.n_components = n_components\n        self.components = None\n        self.mean = None\n\n    def fit(self, X):\n        # TODO: Implement\n        pass\n\n    def transform(self, X):\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\n\nclass PCA:\n    def __init__(self, n_components=2):\n        self.n_components = n_components\n        self.components = None\n        self.mean = None\n        self.explained_variance = None\n\n    def fit(self, X):\n        \"\"\"\n        Fit PCA model.\n\n        Args:\n            X: data matrix (n_samples, n_features)\n        \"\"\"\n        # Center the data\n        self.mean = np.mean(X, axis=0)\n        X_centered = X - self.mean\n\n        # Compute covariance matrix\n        cov_matrix = np.cov(X_centered.T)\n\n        # Compute eigenvectors and eigenvalues\n        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n        # Sort by eigenvalues (descending)\n        idx = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n\n        # Store top k components\n        self.components = eigenvectors[:, :self.n_components]\n        self.explained_variance = eigenvalues[:self.n_components]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Project data onto principal components.\n\n        Args:\n            X: data matrix\n\n        Returns:\n            transformed data\n        \"\"\"\n        # Center and project\n        X_centered = X - self.mean\n        return X_centered @ self.components\n\n    def fit_transform(self, X):\n        \"\"\"Fit and transform in one step.\"\"\"\n        self.fit(X)\n        return self.transform(X)\n\n    def explained_variance_ratio(self):\n        \"\"\"Return proportion of variance explained by each component.\"\"\"\n        total_var = np.sum(self.explained_variance)\n        return self.explained_variance / total_var",
    "testCases": [],
    "hints": [
      "Center data by subtracting mean before computing covariance",
      "Covariance matrix: C = (1/n) * X^T * X",
      "Use np.linalg.eig to compute eigenvectors and eigenvalues",
      "Sort eigenvectors by eigenvalues in descending order",
      "Project data: X_transformed = X_centered @ eigenvectors",
      "Explained variance is proportional to eigenvalues"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-6-7",
    "subjectId": "cs402",
    "topicId": "cs402-topic-6",
    "title": "Calculate Explained Variance",
    "difficulty": 1,
    "description": "Compute explained variance ratio for PCA components.\n\nRequirements:\n- Use eigenvalues from PCA\n- Calculate proportion of variance for each component\n- Return cumulative explained variance\n- Help determine number of components needed",
    "starterCode": "import numpy as np\n\ndef explained_variance_ratio(eigenvalues, n_components=None):\n    \"\"\"\n    Compute explained variance ratio.\n\n    Args:\n        eigenvalues: eigenvalues from PCA\n        n_components: number of components (or None for all)\n\n    Returns:\n        individual and cumulative explained variance\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import numpy as np\n\ndef explained_variance_ratio(eigenvalues, n_components=None):\n    \"\"\"\n    Compute explained variance ratio.\n\n    Args:\n        eigenvalues: eigenvalues from PCA\n        n_components: number of components (or None for all)\n\n    Returns:\n        individual and cumulative explained variance\n    \"\"\"\n    # Sort eigenvalues descending\n    eigenvalues = np.sort(eigenvalues)[::-1]\n\n    if n_components is not None:\n        eigenvalues = eigenvalues[:n_components]\n\n    # Compute total variance\n    total_var = np.sum(eigenvalues)\n\n    # Individual explained variance\n    explained_var = eigenvalues / total_var\n\n    # Cumulative explained variance\n    cumulative_var = np.cumsum(explained_var)\n\n    return explained_var, cumulative_var\n\n# Example usage\neigenvalues = np.array([50, 30, 15, 5])\nindividual, cumulative = explained_variance_ratio(eigenvalues)\n\nprint(\"Individual:\", individual)\n# Output: [0.5  0.3  0.15 0.05]\n\nprint(\"Cumulative:\", cumulative)\n# Output: [0.5  0.8  0.95 1.0]\n\n# First 2 components explain 80% of variance",
    "testCases": [],
    "hints": [
      "Total variance is sum of all eigenvalues",
      "Each component explains: eigenvalue / total_variance",
      "Cumulative variance: running sum of explained variance",
      "Use np.cumsum for cumulative sum",
      "Common threshold: keep components until 95% variance explained"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-6-8",
    "subjectId": "cs402",
    "topicId": "cs402-topic-6",
    "title": "Implement Kernel PCA",
    "difficulty": 4,
    "description": "Build kernel PCA for non-linear dimensionality reduction.\n\nRequirements:\n- Support RBF (Gaussian) kernel\n- Compute kernel matrix K\n- Center kernel matrix\n- Perform eigendecomposition\n- Project new data using kernel trick",
    "starterCode": "import numpy as np\n\nclass KernelPCA:\n    def __init__(self, n_components=2, kernel='rbf', gamma=1.0):\n        self.n_components = n_components\n        self.kernel = kernel\n        self.gamma = gamma\n\n    def fit(self, X):\n        # TODO: Implement\n        pass\n\n    def transform(self, X):\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\n\nclass KernelPCA:\n    def __init__(self, n_components=2, kernel='rbf', gamma=1.0):\n        self.n_components = n_components\n        self.kernel = kernel\n        self.gamma = gamma\n        self.X_fit = None\n        self.alphas = None\n\n    def rbf_kernel(self, X1, X2):\n        \"\"\"Compute RBF kernel matrix.\"\"\"\n        sq_dists = np.sum(X1**2, axis=1, keepdims=True) +                    np.sum(X2**2, axis=1) - 2 * X1 @ X2.T\n        return np.exp(-self.gamma * sq_dists)\n\n    def fit(self, X):\n        \"\"\"\n        Fit kernel PCA.\n\n        Args:\n            X: data matrix (n_samples, n_features)\n        \"\"\"\n        self.X_fit = X\n        n_samples = X.shape[0]\n\n        # Compute kernel matrix\n        K = self.rbf_kernel(X, X)\n\n        # Center kernel matrix\n        one_n = np.ones((n_samples, n_samples)) / n_samples\n        K_centered = K - one_n @ K - K @ one_n + one_n @ K @ one_n\n\n        # Compute eigenvectors\n        eigenvalues, eigenvectors = np.linalg.eigh(K_centered)\n\n        # Sort by eigenvalues (descending)\n        idx = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n\n        # Normalize eigenvectors\n        # alphas = eigenvectors / sqrt(eigenvalues)\n        self.alphas = eigenvectors[:, :self.n_components] /                       np.sqrt(eigenvalues[:self.n_components])\n\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Project data onto principal components.\n\n        Args:\n            X: data matrix\n\n        Returns:\n            transformed data\n        \"\"\"\n        K = self.rbf_kernel(X, self.X_fit)\n\n        # Center kernel matrix\n        n_samples = self.X_fit.shape[0]\n        one_n = np.ones((len(X), n_samples)) / n_samples\n        K_fit = self.rbf_kernel(self.X_fit, self.X_fit)\n\n        K_centered = K - one_n @ K_fit -                      K @ (np.ones((n_samples, n_samples)) / n_samples) +                      one_n @ K_fit @ (np.ones((n_samples, n_samples)) / n_samples)\n\n        # Project\n        return K_centered @ self.alphas",
    "testCases": [],
    "hints": [
      "RBF kernel: K(x,y) = exp(-gamma * ||x-y||^2)",
      "Center kernel matrix using: K_c = K - 1*K - K*1 + 1*K*1",
      "Eigenvectors must be normalized: alpha = v / sqrt(lambda)",
      "For new data, compute kernel with training data",
      "Kernel PCA can capture non-linear structure"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-6-9",
    "subjectId": "cs402",
    "topicId": "cs402-topic-6",
    "title": "Implement t-SNE Gradient",
    "difficulty": 5,
    "description": "Implement core gradient computation for t-SNE.\n\nRequirements:\n- Compute pairwise similarities in high dimension (Gaussian)\n- Compute pairwise similarities in low dimension (t-distribution)\n- Calculate KL divergence gradient\n- Support perplexity parameter",
    "starterCode": "import numpy as np\n\ndef tsne_gradient(X_high, X_low, perplexity=30):\n    \"\"\"\n    Compute t-SNE gradient.\n\n    Args:\n        X_high: high-dimensional data\n        X_low: current low-dimensional embedding\n        perplexity: perplexity parameter\n\n    Returns:\n        gradient with respect to X_low\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import numpy as np\n\ndef compute_p_matrix(X, perplexity=30):\n    \"\"\"Compute pairwise affinities in high dimension.\"\"\"\n    n = len(X)\n    P = np.zeros((n, n))\n\n    # Target entropy from perplexity\n    target_entropy = np.log(perplexity)\n\n    for i in range(n):\n        # Binary search for sigma\n        sigma = 1.0\n        for _ in range(50):\n            # Compute p_j|i\n            diff = X[i] - X\n            distances = np.sum(diff**2, axis=1)\n            exp_vals = np.exp(-distances / (2 * sigma**2))\n            exp_vals[i] = 0  # Set p_i|i = 0\n\n            sum_exp = np.sum(exp_vals)\n            if sum_exp == 0:\n                p_i = np.zeros(n)\n            else:\n                p_i = exp_vals / sum_exp\n\n            # Compute entropy\n            p_i_nonzero = p_i[p_i > 0]\n            entropy = -np.sum(p_i_nonzero * np.log2(p_i_nonzero))\n\n            # Adjust sigma based on entropy\n            if entropy > target_entropy:\n                sigma *= 1.1\n            else:\n                sigma *= 0.9\n\n        P[i] = p_i\n\n    # Symmetrize\n    P = (P + P.T) / (2 * n)\n    return np.maximum(P, 1e-12)\n\ndef compute_q_matrix(X_low):\n    \"\"\"Compute pairwise affinities in low dimension (t-distribution).\"\"\"\n    n = len(X_low)\n    diff = X_low[:, np.newaxis] - X_low\n    distances = np.sum(diff**2, axis=2)\n\n    # t-distribution with 1 degree of freedom\n    Q = 1 / (1 + distances)\n    np.fill_diagonal(Q, 0)\n\n    Q = Q / np.sum(Q)\n    return np.maximum(Q, 1e-12)\n\ndef tsne_gradient(X_high, X_low, perplexity=30):\n    \"\"\"\n    Compute t-SNE gradient.\n\n    Args:\n        X_high: high-dimensional data (n_samples, n_features_high)\n        X_low: current low-dimensional embedding (n_samples, n_features_low)\n        perplexity: perplexity parameter\n\n    Returns:\n        gradient with respect to X_low\n    \"\"\"\n    n = len(X_high)\n\n    # Compute P and Q matrices\n    P = compute_p_matrix(X_high, perplexity)\n    Q = compute_q_matrix(X_low)\n\n    # Compute gradient\n    PQ_diff = P - Q\n    grad = np.zeros_like(X_low)\n\n    for i in range(n):\n        diff = X_low[i] - X_low\n        distances = np.sum(diff**2, axis=1)\n        weights = PQ_diff[i] * (1 / (1 + distances))\n        grad[i] = 4 * np.sum(weights[:, np.newaxis] * diff, axis=0)\n\n    return grad",
    "testCases": [],
    "hints": [
      "High-dim similarities use Gaussian kernel",
      "Low-dim similarities use t-distribution (heavy tails)",
      "Perplexity controls effective number of neighbors",
      "Use binary search to find appropriate sigma for each point",
      "Gradient: 4 * sum((p_ij - q_ij) * (y_i - y_j) / (1 + ||y_i - y_j||^2))",
      "t-SNE minimizes KL divergence between P and Q"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-6-10",
    "subjectId": "cs402",
    "topicId": "cs402-topic-6",
    "title": "Build Gaussian Mixture Model",
    "difficulty": 4,
    "description": "Implement GMM using Expectation-Maximization algorithm.\n\nRequirements:\n- E-step: compute responsibilities (posterior probabilities)\n- M-step: update means, covariances, and mixing coefficients\n- Handle multiple Gaussian components\n- Support diagonal covariance matrices",
    "starterCode": "import numpy as np\n\nclass GaussianMixture:\n    def __init__(self, n_components=3, max_iter=100):\n        self.n_components = n_components\n        self.max_iter = max_iter\n\n    def fit(self, X):\n        # TODO: Implement EM algorithm\n        pass",
    "solution": "import numpy as np\n\nclass GaussianMixture:\n    def __init__(self, n_components=3, max_iter=100):\n        self.n_components = n_components\n        self.max_iter = max_iter\n        self.means = None\n        self.covs = None\n        self.mixing_coefs = None\n\n    def gaussian_pdf(self, X, mean, cov):\n        \"\"\"Compute Gaussian PDF.\"\"\"\n        n_features = X.shape[1]\n        diff = X - mean\n        cov_inv = np.linalg.inv(cov)\n        cov_det = np.linalg.det(cov)\n\n        norm_const = 1 / np.sqrt((2 * np.pi)**n_features * cov_det)\n        exponent = -0.5 * np.sum((diff @ cov_inv) * diff, axis=1)\n\n        return norm_const * np.exp(exponent)\n\n    def fit(self, X):\n        \"\"\"\n        Fit GMM using EM algorithm.\n\n        Args:\n            X: data matrix (n_samples, n_features)\n        \"\"\"\n        n_samples, n_features = X.shape\n\n        # Initialize parameters\n        # Random initialization of means\n        idx = np.random.choice(n_samples, self.n_components, replace=False)\n        self.means = X[idx]\n\n        # Initialize covariances as identity\n        self.covs = [np.eye(n_features) for _ in range(self.n_components)]\n\n        # Initialize mixing coefficients uniformly\n        self.mixing_coefs = np.ones(self.n_components) / self.n_components\n\n        for iteration in range(self.max_iter):\n            # E-step: compute responsibilities\n            responsibilities = np.zeros((n_samples, self.n_components))\n\n            for k in range(self.n_components):\n                responsibilities[:, k] = self.mixing_coefs[k] *                     self.gaussian_pdf(X, self.means[k], self.covs[k])\n\n            # Normalize responsibilities\n            responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n\n            # M-step: update parameters\n            N_k = responsibilities.sum(axis=0)\n\n            # Update means\n            for k in range(self.n_components):\n                self.means[k] = (responsibilities[:, k:k+1] * X).sum(axis=0) / N_k[k]\n\n            # Update covariances\n            for k in range(self.n_components):\n                diff = X - self.means[k]\n                self.covs[k] = (responsibilities[:, k:k+1] * diff).T @ diff / N_k[k]\n                # Add small value to diagonal for numerical stability\n                self.covs[k] += np.eye(n_features) * 1e-6\n\n            # Update mixing coefficients\n            self.mixing_coefs = N_k / n_samples\n\n        return self\n\n    def predict_proba(self, X):\n        \"\"\"Return probability of each component.\"\"\"\n        n_samples = X.shape[0]\n        probs = np.zeros((n_samples, self.n_components))\n\n        for k in range(self.n_components):\n            probs[:, k] = self.mixing_coefs[k] *                 self.gaussian_pdf(X, self.means[k], self.covs[k])\n\n        return probs / probs.sum(axis=1, keepdims=True)\n\n    def predict(self, X):\n        \"\"\"Return most likely component.\"\"\"\n        return np.argmax(self.predict_proba(X), axis=1)",
    "testCases": [],
    "hints": [
      "E-step: compute gamma_ik = pi_k * N(x_i | mu_k, Sigma_k)",
      "Normalize responsibilities to sum to 1",
      "M-step: update means as weighted average of points",
      "Update covariances using weighted outer products",
      "Mixing coefficients: pi_k = N_k / N (proportion of points)",
      "Add small value to covariance diagonal for numerical stability"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-6-11",
    "subjectId": "cs402",
    "topicId": "cs402-topic-6",
    "title": "Implement Anomaly Detection",
    "difficulty": 3,
    "description": "Build statistical anomaly detection using Gaussian distribution.\n\nRequirements:\n- Fit Gaussian distribution to training data\n- Compute probability density for new points\n- Flag points below threshold as anomalies\n- Support multivariate Gaussian",
    "starterCode": "import numpy as np\n\nclass AnomalyDetector:\n    def __init__(self, threshold=0.01):\n        self.threshold = threshold\n        self.mean = None\n        self.cov = None\n\n    def fit(self, X):\n        # TODO: Implement\n        pass\n\n    def predict(self, X):\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\n\nclass AnomalyDetector:\n    def __init__(self, threshold=0.01):\n        self.threshold = threshold\n        self.mean = None\n        self.cov = None\n\n    def fit(self, X):\n        \"\"\"\n        Fit Gaussian distribution to data.\n\n        Args:\n            X: training data (normal examples)\n        \"\"\"\n        self.mean = np.mean(X, axis=0)\n        self.cov = np.cov(X.T)\n\n        # Ensure covariance is positive definite\n        self.cov += np.eye(len(self.cov)) * 1e-6\n\n        return self\n\n    def probability_density(self, X):\n        \"\"\"\n        Compute probability density under Gaussian.\n\n        Args:\n            X: data points\n\n        Returns:\n            probability densities\n        \"\"\"\n        n_features = X.shape[1]\n\n        # Compute multivariate Gaussian PDF\n        diff = X - self.mean\n        cov_inv = np.linalg.inv(self.cov)\n        cov_det = np.linalg.det(self.cov)\n\n        norm_const = 1 / np.sqrt((2 * np.pi)**n_features * cov_det)\n        exponent = -0.5 * np.sum((diff @ cov_inv) * diff, axis=1)\n\n        return norm_const * np.exp(exponent)\n\n    def predict(self, X):\n        \"\"\"\n        Predict anomalies.\n\n        Args:\n            X: data points\n\n        Returns:\n            labels (1 = normal, -1 = anomaly)\n        \"\"\"\n        probabilities = self.probability_density(X)\n        return np.where(probabilities >= self.threshold, 1, -1)\n\n    def decision_function(self, X):\n        \"\"\"Return anomaly scores (negative log probability).\"\"\"\n        probabilities = self.probability_density(X)\n        return -np.log(probabilities + 1e-10)",
    "testCases": [],
    "hints": [
      "Fit multivariate Gaussian to normal training data",
      "Probability density: p(x) = N(x | mu, Sigma)",
      "Points with low probability are anomalies",
      "Choose threshold based on validation set",
      "Can also use negative log probability as anomaly score",
      "Add small value to covariance diagonal for stability"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-6-12",
    "subjectId": "cs402",
    "topicId": "cs402-topic-6",
    "title": "Build Isolation Forest",
    "difficulty": 4,
    "description": "Implement Isolation Forest for anomaly detection.\n\nRequirements:\n- Build random isolation trees\n- Anomalies are easier to isolate (shorter paths)\n- Compute anomaly score based on path length\n- Support multiple trees (ensemble)",
    "starterCode": "import numpy as np\n\nclass IsolationTree:\n    def __init__(self, height_limit):\n        self.height_limit = height_limit\n\n    def fit(self, X, current_height=0):\n        # TODO: Implement\n        pass\n\nclass IsolationForest:\n    def __init__(self, n_trees=100):\n        self.n_trees = n_trees\n        self.trees = []\n\n    def fit(self, X):\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\n\nclass IsolationTree:\n    def __init__(self, height_limit):\n        self.height_limit = height_limit\n        self.split_feature = None\n        self.split_value = None\n        self.left = None\n        self.right = None\n        self.size = 0\n\n    def fit(self, X, current_height=0):\n        \"\"\"\n        Build isolation tree.\n\n        Args:\n            X: data points\n            current_height: current depth in tree\n        \"\"\"\n        self.size = len(X)\n\n        # Stop if height limit reached or only one point\n        if current_height >= self.height_limit or len(X) <= 1:\n            return self\n\n        # Random split\n        n_features = X.shape[1]\n        self.split_feature = np.random.randint(0, n_features)\n\n        feature_values = X[:, self.split_feature]\n        min_val, max_val = feature_values.min(), feature_values.max()\n\n        if min_val == max_val:\n            return self\n\n        self.split_value = np.random.uniform(min_val, max_val)\n\n        # Split data\n        left_mask = feature_values < self.split_value\n        right_mask = ~left_mask\n\n        if left_mask.sum() > 0:\n            self.left = IsolationTree(self.height_limit)\n            self.left.fit(X[left_mask], current_height + 1)\n\n        if right_mask.sum() > 0:\n            self.right = IsolationTree(self.height_limit)\n            self.right.fit(X[right_mask], current_height + 1)\n\n        return self\n\n    def path_length(self, x, current_height=0):\n        \"\"\"Compute path length for a point.\"\"\"\n        if self.split_feature is None:\n            # External node\n            return current_height + self._c(self.size)\n\n        if x[self.split_feature] < self.split_value:\n            if self.left is not None:\n                return self.left.path_length(x, current_height + 1)\n        else:\n            if self.right is not None:\n                return self.right.path_length(x, current_height + 1)\n\n        return current_height + self._c(self.size)\n\n    def _c(self, n):\n        \"\"\"Average path length of unsuccessful search in BST.\"\"\"\n        if n <= 1:\n            return 0\n        return 2 * (np.log(n - 1) + 0.5772156649) - 2 * (n - 1) / n\n\nclass IsolationForest:\n    def __init__(self, n_trees=100, sample_size=256):\n        self.n_trees = n_trees\n        self.sample_size = sample_size\n        self.trees = []\n\n    def fit(self, X):\n        \"\"\"\n        Build isolation forest.\n\n        Args:\n            X: training data\n        \"\"\"\n        n_samples = len(X)\n        height_limit = int(np.ceil(np.log2(self.sample_size)))\n\n        self.trees = []\n        for _ in range(self.n_trees):\n            # Sample subset\n            sample_idx = np.random.choice(n_samples,\n                                         min(self.sample_size, n_samples),\n                                         replace=False)\n            X_sample = X[sample_idx]\n\n            # Build tree\n            tree = IsolationTree(height_limit)\n            tree.fit(X_sample)\n            self.trees.append(tree)\n\n        return self\n\n    def anomaly_score(self, X):\n        \"\"\"\n        Compute anomaly scores.\n\n        Args:\n            X: data points\n\n        Returns:\n            scores (higher = more anomalous)\n        \"\"\"\n        avg_path_lengths = np.zeros(len(X))\n\n        for tree in self.trees:\n            for i, x in enumerate(X):\n                avg_path_lengths[i] += tree.path_length(x)\n\n        avg_path_lengths /= len(self.trees)\n\n        # Normalize\n        c = self.trees[0]._c(self.sample_size)\n        scores = 2 ** (-avg_path_lengths / c)\n\n        return scores\n\n    def predict(self, X, threshold=0.5):\n        \"\"\"Predict anomalies (score > threshold).\"\"\"\n        scores = self.anomaly_score(X)\n        return np.where(scores > threshold, -1, 1)",
    "testCases": [],
    "hints": [
      "Isolation trees use random splits on random features",
      "Anomalies have shorter average path lengths",
      "Height limit: ceil(log2(sample_size))",
      "Sample subset of data for each tree (default 256 points)",
      "Anomaly score: 2^(-E[h(x)] / c(n))",
      "Score close to 1 indicates anomaly, close to 0 indicates normal"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-6-13",
    "subjectId": "cs402",
    "topicId": "cs402-topic-6",
    "title": "Implement Autoencoder for Anomaly Detection",
    "difficulty": 4,
    "description": "Build autoencoder neural network for unsupervised anomaly detection.\n\nRequirements:\n- Encoder: compress input to lower-dimensional latent space\n- Decoder: reconstruct input from latent representation\n- Train to minimize reconstruction error\n- Use reconstruction error as anomaly score",
    "starterCode": "import torch\nimport torch.nn as nn\n\nclass Autoencoder(nn.Module):\n    def __init__(self, input_dim, latent_dim):\n        super().__init__()\n        # Define encoder and decoder\n\n    def forward(self, x):\n        # TODO: Implement\n        pass\n\ndef detect_anomalies(model, X, threshold):\n    # TODO: Implement\n    pass",
    "solution": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\nclass Autoencoder(nn.Module):\n    def __init__(self, input_dim, latent_dim):\n        super().__init__()\n\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, latent_dim)\n        )\n\n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, 32),\n            nn.ReLU(),\n            nn.Linear(32, 64),\n            nn.ReLU(),\n            nn.Linear(64, input_dim)\n        )\n\n    def forward(self, x):\n        \"\"\"Forward pass through autoencoder.\"\"\"\n        latent = self.encoder(x)\n        reconstruction = self.decoder(latent)\n        return reconstruction\n\ndef train_autoencoder(model, X_train, epochs=50, lr=0.001):\n    \"\"\"\n    Train autoencoder.\n\n    Args:\n        model: autoencoder model\n        X_train: training data (normal examples)\n        epochs: number of training epochs\n        lr: learning rate\n    \"\"\"\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n\n    X_train_tensor = torch.FloatTensor(X_train)\n\n    for epoch in range(epochs):\n        model.train()\n        optimizer.zero_grad()\n\n        # Forward pass\n        reconstruction = model(X_train_tensor)\n        loss = criterion(reconstruction, X_train_tensor)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n\n        if (epoch + 1) % 10 == 0:\n            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')\n\ndef detect_anomalies(model, X, threshold):\n    \"\"\"\n    Detect anomalies using reconstruction error.\n\n    Args:\n        model: trained autoencoder\n        X: data points\n        threshold: reconstruction error threshold\n\n    Returns:\n        labels (1 = normal, -1 = anomaly)\n        reconstruction errors\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        X_tensor = torch.FloatTensor(X)\n        reconstruction = model(X_tensor)\n\n        # Compute reconstruction error for each sample\n        errors = torch.mean((X_tensor - reconstruction)**2, dim=1)\n        errors = errors.numpy()\n\n    # Flag high error as anomalies\n    labels = np.where(errors < threshold, 1, -1)\n\n    return labels, errors\n\n# Example usage\ninput_dim = 10\nlatent_dim = 3\n\nmodel = Autoencoder(input_dim, latent_dim)\nX_train = np.random.randn(1000, input_dim)\n\ntrain_autoencoder(model, X_train)\n\n# Determine threshold from training data\n_, train_errors = detect_anomalies(model, X_train, threshold=float('inf'))\nthreshold = np.percentile(train_errors, 95)  # 95th percentile\n\n# Detect anomalies in test data\nX_test = np.random.randn(100, input_dim)\nlabels, errors = detect_anomalies(model, X_test, threshold)",
    "testCases": [],
    "hints": [
      "Train autoencoder only on normal data",
      "Encoder compresses to latent dimension (bottleneck)",
      "Decoder reconstructs from latent representation",
      "Use MSE loss for reconstruction error",
      "Anomalies have high reconstruction error",
      "Set threshold based on validation data (e.g., 95th percentile)"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-6-14",
    "subjectId": "cs402",
    "topicId": "cs402-topic-6",
    "title": "Implement Davies-Bouldin Index",
    "difficulty": 2,
    "description": "Compute Davies-Bouldin index to evaluate clustering quality.\n\nRequirements:\n- For each cluster, compute average distance to cluster center\n- Compute cluster similarity (sum of spreads / distance between centers)\n- Lower DB index indicates better clustering\n- Compare different clustering solutions",
    "starterCode": "import numpy as np\n\ndef davies_bouldin_index(X, labels):\n    \"\"\"\n    Compute Davies-Bouldin index.\n\n    Args:\n        X: data points\n        labels: cluster assignments\n\n    Returns:\n        DB index (lower is better)\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import numpy as np\n\ndef davies_bouldin_index(X, labels):\n    \"\"\"\n    Compute Davies-Bouldin index.\n\n    Args:\n        X: data points (n_samples, n_features)\n        labels: cluster assignments\n\n    Returns:\n        DB index (lower is better)\n    \"\"\"\n    n_clusters = len(np.unique(labels))\n\n    # Compute cluster centers\n    centers = np.array([X[labels == k].mean(axis=0)\n                       for k in range(n_clusters)])\n\n    # Compute average distance within each cluster (spread)\n    spreads = np.zeros(n_clusters)\n    for k in range(n_clusters):\n        cluster_points = X[labels == k]\n        if len(cluster_points) > 0:\n            spreads[k] = np.mean([np.linalg.norm(x - centers[k])\n                                 for x in cluster_points])\n\n    # Compute DB index\n    db_scores = np.zeros(n_clusters)\n\n    for i in range(n_clusters):\n        max_ratio = 0\n        for j in range(n_clusters):\n            if i != j:\n                # Distance between centers\n                center_dist = np.linalg.norm(centers[i] - centers[j])\n\n                if center_dist > 0:\n                    # Ratio of spreads to separation\n                    ratio = (spreads[i] + spreads[j]) / center_dist\n                    max_ratio = max(max_ratio, ratio)\n\n        db_scores[i] = max_ratio\n\n    # Average over all clusters\n    return np.mean(db_scores)",
    "testCases": [],
    "hints": [
      "Spread S_i: average distance from points to cluster center",
      "Separation M_ij: distance between cluster centers i and j",
      "Similarity R_ij = (S_i + S_j) / M_ij",
      "For each cluster i, find max R_ij over all other clusters j",
      "DB index is average of these max values",
      "Lower DB index means better separated, compact clusters"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-6-15",
    "subjectId": "cs402",
    "topicId": "cs402-topic-6",
    "title": "Build Mean Shift Clustering",
    "difficulty": 4,
    "description": "Implement mean shift algorithm for clustering.\n\nRequirements:\n- Iteratively shift points toward density maxima\n- Use kernel (Gaussian) for weighting neighbors\n- Merge points that converge to same mode\n- Support bandwidth parameter",
    "starterCode": "import numpy as np\n\nclass MeanShift:\n    def __init__(self, bandwidth=1.0, max_iter=300):\n        self.bandwidth = bandwidth\n        self.max_iter = max_iter\n\n    def fit_predict(self, X):\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\n\nclass MeanShift:\n    def __init__(self, bandwidth=1.0, max_iter=300, tol=1e-3):\n        self.bandwidth = bandwidth\n        self.max_iter = max_iter\n        self.tol = tol\n        self.cluster_centers = None\n\n    def gaussian_kernel(self, distance):\n        \"\"\"Gaussian kernel for weighting.\"\"\"\n        return np.exp(-(distance**2) / (2 * self.bandwidth**2))\n\n    def shift_point(self, point, X):\n        \"\"\"Shift point toward density maximum.\"\"\"\n        for _ in range(self.max_iter):\n            # Compute distances to all points\n            distances = np.linalg.norm(X - point, axis=1)\n\n            # Compute weights using Gaussian kernel\n            weights = self.gaussian_kernel(distances)\n\n            # Compute weighted mean\n            new_point = np.sum(weights[:, np.newaxis] * X, axis=0) / np.sum(weights)\n\n            # Check convergence\n            if np.linalg.norm(new_point - point) < self.tol:\n                break\n\n            point = new_point\n\n        return point\n\n    def fit_predict(self, X):\n        \"\"\"\n        Perform mean shift clustering.\n\n        Args:\n            X: data points (n_samples, n_features)\n\n        Returns:\n            cluster labels\n        \"\"\"\n        n_samples = len(X)\n\n        # Shift each point to find modes\n        modes = np.zeros_like(X)\n        for i in range(n_samples):\n            modes[i] = self.shift_point(X[i].copy(), X)\n\n        # Merge nearby modes\n        cluster_centers = []\n        labels = np.zeros(n_samples, dtype=int)\n\n        for i in range(n_samples):\n            # Check if mode is close to existing cluster center\n            is_new_cluster = True\n\n            for j, center in enumerate(cluster_centers):\n                if np.linalg.norm(modes[i] - center) < self.bandwidth:\n                    labels[i] = j\n                    is_new_cluster = False\n                    break\n\n            if is_new_cluster:\n                cluster_centers.append(modes[i])\n                labels[i] = len(cluster_centers) - 1\n\n        self.cluster_centers = np.array(cluster_centers)\n        return labels",
    "testCases": [],
    "hints": [
      "Mean shift moves points toward highest density region",
      "Use Gaussian kernel to weight nearby points",
      "New position: weighted average of neighbors",
      "Iterate until convergence (small movement)",
      "Points converging to same mode belong to same cluster",
      "Bandwidth controls neighborhood size (similar to KDE)"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-6-16",
    "subjectId": "cs402",
    "topicId": "cs402-topic-6",
    "title": "Implement Spectral Clustering",
    "difficulty": 5,
    "description": "Build spectral clustering using graph Laplacian.\n\nRequirements:\n- Construct similarity graph (RBF kernel)\n- Compute graph Laplacian matrix\n- Find eigenvectors of Laplacian\n- Apply k-means to eigenvector representation\n- Handle non-convex clusters",
    "starterCode": "import numpy as np\n\nclass SpectralClustering:\n    def __init__(self, n_clusters=3, gamma=1.0):\n        self.n_clusters = n_clusters\n        self.gamma = gamma\n\n    def fit_predict(self, X):\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass SpectralClustering:\n    def __init__(self, n_clusters=3, gamma=1.0):\n        self.n_clusters = n_clusters\n        self.gamma = gamma\n\n    def rbf_kernel(self, X):\n        \"\"\"Compute RBF (Gaussian) similarity matrix.\"\"\"\n        sq_dists = np.sum(X**2, axis=1, keepdims=True) +                    np.sum(X**2, axis=1) - 2 * X @ X.T\n        return np.exp(-self.gamma * sq_dists)\n\n    def fit_predict(self, X):\n        \"\"\"\n        Perform spectral clustering.\n\n        Args:\n            X: data points (n_samples, n_features)\n\n        Returns:\n            cluster labels\n        \"\"\"\n        n_samples = len(X)\n\n        # Step 1: Compute similarity matrix\n        W = self.rbf_kernel(X)\n\n        # Step 2: Compute degree matrix\n        D = np.diag(np.sum(W, axis=1))\n\n        # Step 3: Compute normalized graph Laplacian\n        # L = D^(-1/2) * (D - W) * D^(-1/2)\n        D_sqrt_inv = np.diag(1.0 / np.sqrt(np.diag(D) + 1e-10))\n        L = D_sqrt_inv @ (D - W) @ D_sqrt_inv\n\n        # Step 4: Compute eigenvectors\n        eigenvalues, eigenvectors = np.linalg.eigh(L)\n\n        # Step 5: Use first k eigenvectors (smallest eigenvalues)\n        U = eigenvectors[:, :self.n_clusters]\n\n        # Step 6: Normalize rows to unit length\n        U_normalized = U / (np.linalg.norm(U, axis=1, keepdims=True) + 1e-10)\n\n        # Step 7: Apply k-means to the eigenvector representation\n        kmeans = KMeans(n_clusters=self.n_clusters, random_state=42)\n        labels = kmeans.fit_predict(U_normalized)\n\n        return labels",
    "testCases": [],
    "hints": [
      "Construct affinity matrix using RBF kernel: W_ij = exp(-gamma * ||x_i - x_j||^2)",
      "Degree matrix D is diagonal with D_ii = sum_j W_ij",
      "Normalized Laplacian: L = D^(-1/2) * (D - W) * D^(-1/2)",
      "Use eigenvectors corresponding to k smallest eigenvalues",
      "Normalize eigenvector rows before k-means",
      "Spectral clustering can find non-convex clusters"
    ],
    "language": "python"
  },
  {
    "id": "cs402-t7-ex01",
    "subjectId": "cs402",
    "topicId": "cs402-topic-7",
    "title": "Implement Cross-Validation from Scratch",
    "difficulty": 2,
    "description": "Build k-fold cross-validation without sklearn.\n\nRequirements:\n- Split data into k folds\n- Train and evaluate on each fold\n- Return scores for all folds\n- Calculate mean and standard deviation",
    "starterCode": "import numpy as np\n\ndef cross_validate(model, X, y, k=5):\n    # Split into k folds\n    # Train on k-1, test on 1\n    # Return scores\n    pass",
    "solution": "import numpy as np\n\ndef cross_validate(model, X, y, k=5):\n    n = len(X)\n    fold_size = n // k\n    indices = np.arange(n)\n    np.random.shuffle(indices)\n    \n    scores = []\n    for i in range(k):\n        # Split\n        test_idx = indices[i*fold_size:(i+1)*fold_size]\n        train_idx = np.concatenate([indices[:i*fold_size], indices[(i+1)*fold_size:]])\n        \n        X_train, X_test = X[train_idx], X[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n        \n        # Train and evaluate\n        model.fit(X_train, y_train)\n        score = model.score(X_test, y_test)\n        scores.append(score)\n\n    return np.array(scores)",
    "testCases": [],
    "hints": [
      "Divide the dataset into k equal-sized folds",
      "Use shuffled indices to ensure random distribution",
      "For each fold, use it as test set and remaining folds as training set",
      "Store the score for each fold in a list",
      "Return scores as a numpy array for easy statistics calculation"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-7-2",
    "subjectId": "cs402",
    "topicId": "cs402-topic-7",
    "title": "Calculate Precision, Recall, and F1",
    "difficulty": 1,
    "description": "Compute classification metrics from confusion matrix.\n\nRequirements:\n- Calculate precision: TP / (TP + FP)\n- Calculate recall: TP / (TP + FN)\n- Calculate F1 score: 2 * (precision * recall) / (precision + recall)\n- Handle edge cases (division by zero)",
    "starterCode": "import numpy as np\n\ndef calculate_metrics(y_true, y_pred):\n    \"\"\"\n    Calculate precision, recall, and F1 score.\n\n    Args:\n        y_true: true labels (0 or 1)\n        y_pred: predicted labels (0 or 1)\n\n    Returns:\n        precision, recall, f1\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import numpy as np\n\ndef calculate_metrics(y_true, y_pred):\n    \"\"\"\n    Calculate precision, recall, and F1 score.\n\n    Args:\n        y_true: true labels (0 or 1)\n        y_pred: predicted labels (0 or 1)\n\n    Returns:\n        precision, recall, f1\n    \"\"\"\n    # Convert to numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Calculate confusion matrix components\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    tn = np.sum((y_true == 0) & (y_pred == 0))\n\n    # Calculate precision\n    if tp + fp == 0:\n        precision = 0.0\n    else:\n        precision = tp / (tp + fp)\n\n    # Calculate recall\n    if tp + fn == 0:\n        recall = 0.0\n    else:\n        recall = tp / (tp + fn)\n\n    # Calculate F1 score\n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * (precision * recall) / (precision + recall)\n\n    return precision, recall, f1\n\n# Example\ny_true = [1, 1, 0, 1, 0, 1, 0, 0]\ny_pred = [1, 0, 0, 1, 0, 1, 1, 0]\n\np, r, f1 = calculate_metrics(y_true, y_pred)\nprint(f\"Precision: {p:.3f}, Recall: {r:.3f}, F1: {f1:.3f}\")",
    "testCases": [],
    "hints": [
      "TP: correctly predicted positive examples",
      "FP: incorrectly predicted as positive (false alarms)",
      "FN: incorrectly predicted as negative (misses)",
      "Precision: of predicted positives, how many are correct?",
      "Recall: of actual positives, how many did we catch?",
      "F1 is harmonic mean of precision and recall"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-7-3",
    "subjectId": "cs402",
    "topicId": "cs402-topic-7",
    "title": "Build Confusion Matrix",
    "difficulty": 1,
    "description": "Create confusion matrix for multi-class classification.\n\nRequirements:\n- Support arbitrary number of classes\n- Rows represent true labels, columns represent predictions\n- Include counts for each (true, predicted) pair\n- Visualize as heatmap",
    "starterCode": "import numpy as np\n\ndef confusion_matrix(y_true, y_pred, n_classes=None):\n    \"\"\"\n    Compute confusion matrix.\n\n    Args:\n        y_true: true labels\n        y_pred: predicted labels\n        n_classes: number of classes (inferred if None)\n\n    Returns:\n        confusion matrix (n_classes x n_classes)\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef confusion_matrix(y_true, y_pred, n_classes=None):\n    \"\"\"\n    Compute confusion matrix.\n\n    Args:\n        y_true: true labels\n        y_pred: predicted labels\n        n_classes: number of classes (inferred if None)\n\n    Returns:\n        confusion matrix (n_classes x n_classes)\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    if n_classes is None:\n        n_classes = max(np.max(y_true), np.max(y_pred)) + 1\n\n    # Initialize confusion matrix\n    cm = np.zeros((n_classes, n_classes), dtype=int)\n\n    # Fill confusion matrix\n    for true_label, pred_label in zip(y_true, y_pred):\n        cm[true_label, pred_label] += 1\n\n    return cm\n\ndef plot_confusion_matrix(cm, class_names=None):\n    \"\"\"Plot confusion matrix as heatmap.\"\"\"\n    plt.figure(figsize=(8, 6))\n\n    if class_names is None:\n        class_names = [str(i) for i in range(len(cm))]\n\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.title('Confusion Matrix')\n    plt.show()\n\n# Example\ny_true = [0, 1, 2, 0, 1, 2, 1, 2, 0, 1]\ny_pred = [0, 2, 2, 0, 1, 1, 1, 2, 0, 0]\n\ncm = confusion_matrix(y_true, y_pred)\nprint(cm)\nplot_confusion_matrix(cm, ['Class 0', 'Class 1', 'Class 2'])",
    "testCases": [],
    "hints": [
      "Confusion matrix: cm[i][j] = count of true class i predicted as j",
      "Diagonal elements are correct predictions",
      "Off-diagonal elements are misclassifications",
      "Each row sums to total examples of that true class",
      "Use np.zeros to initialize matrix",
      "Iterate through (true, pred) pairs and increment counts"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-7-4",
    "subjectId": "cs402",
    "topicId": "cs402-topic-7",
    "title": "Implement ROC Curve",
    "difficulty": 3,
    "description": "Generate ROC curve and compute AUC.\n\nRequirements:\n- Use predicted probabilities, not hard labels\n- Vary threshold from 0 to 1\n- Calculate TPR and FPR at each threshold\n- Compute Area Under Curve (AUC) using trapezoidal rule",
    "starterCode": "import numpy as np\n\ndef roc_curve(y_true, y_scores):\n    \"\"\"\n    Compute ROC curve.\n\n    Args:\n        y_true: true binary labels\n        y_scores: predicted probabilities\n\n    Returns:\n        fpr, tpr, thresholds\n    \"\"\"\n    # TODO: Implement\n    pass\n\ndef auc(fpr, tpr):\n    \"\"\"Compute area under ROC curve.\"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef roc_curve(y_true, y_scores):\n    \"\"\"\n    Compute ROC curve.\n\n    Args:\n        y_true: true binary labels (0 or 1)\n        y_scores: predicted probabilities\n\n    Returns:\n        fpr, tpr, thresholds\n    \"\"\"\n    y_true = np.array(y_true)\n    y_scores = np.array(y_scores)\n\n    # Get unique thresholds (sorted descending)\n    thresholds = np.sort(np.unique(y_scores))[::-1]\n    thresholds = np.concatenate([[np.inf], thresholds, [-np.inf]])\n\n    # Calculate TPR and FPR for each threshold\n    tpr_list = []\n    fpr_list = []\n\n    n_pos = np.sum(y_true == 1)\n    n_neg = np.sum(y_true == 0)\n\n    for threshold in thresholds:\n        # Predict 1 if score >= threshold\n        y_pred = (y_scores >= threshold).astype(int)\n\n        # Calculate TPR and FPR\n        tp = np.sum((y_true == 1) & (y_pred == 1))\n        fp = np.sum((y_true == 0) & (y_pred == 1))\n\n        tpr = tp / n_pos if n_pos > 0 else 0\n        fpr = fp / n_neg if n_neg > 0 else 0\n\n        tpr_list.append(tpr)\n        fpr_list.append(fpr)\n\n    return np.array(fpr_list), np.array(tpr_list), thresholds\n\ndef auc(fpr, tpr):\n    \"\"\"\n    Compute area under ROC curve using trapezoidal rule.\n\n    Args:\n        fpr: false positive rates\n        tpr: true positive rates\n\n    Returns:\n        area under curve\n    \"\"\"\n    # Sort by fpr\n    sorted_indices = np.argsort(fpr)\n    fpr = fpr[sorted_indices]\n    tpr = tpr[sorted_indices]\n\n    # Trapezoidal integration\n    area = 0\n    for i in range(1, len(fpr)):\n        area += (fpr[i] - fpr[i-1]) * (tpr[i] + tpr[i-1]) / 2\n\n    return area\n\ndef plot_roc_curve(fpr, tpr, auc_score):\n    \"\"\"Plot ROC curve.\"\"\"\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, linewidth=2, label=f'ROC curve (AUC = {auc_score:.3f})')\n    plt.plot([0, 1], [0, 1], 'k--', label='Random classifier')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# Example\ny_true = [0, 0, 1, 1, 0, 1, 0, 1]\ny_scores = [0.1, 0.4, 0.35, 0.8, 0.2, 0.7, 0.3, 0.9]\n\nfpr, tpr, thresholds = roc_curve(y_true, y_scores)\nauc_score = auc(fpr, tpr)\n\nprint(f\"AUC: {auc_score:.3f}\")\nplot_roc_curve(fpr, tpr, auc_score)",
    "testCases": [],
    "hints": [
      "ROC curve: plot TPR vs FPR at different thresholds",
      "TPR = TP / (TP + FN) = recall = sensitivity",
      "FPR = FP / (FP + TN) = 1 - specificity",
      "Start with high threshold (predict all negative), end with low (predict all positive)",
      "Perfect classifier: AUC = 1.0, Random: AUC = 0.5",
      "Use trapezoidal rule for integration: sum of trapezoid areas"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-7-5",
    "subjectId": "cs402",
    "topicId": "cs402-topic-7",
    "title": "Implement Precision-Recall Curve",
    "difficulty": 2,
    "description": "Generate precision-recall curve for imbalanced datasets.\n\nRequirements:\n- Calculate precision and recall at each threshold\n- Plot precision vs recall\n- Compute average precision (AP)\n- Better than ROC for imbalanced classes",
    "starterCode": "import numpy as np\n\ndef precision_recall_curve(y_true, y_scores):\n    \"\"\"\n    Compute precision-recall curve.\n\n    Args:\n        y_true: true binary labels\n        y_scores: predicted probabilities\n\n    Returns:\n        precision, recall, thresholds\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef precision_recall_curve(y_true, y_scores):\n    \"\"\"\n    Compute precision-recall curve.\n\n    Args:\n        y_true: true binary labels (0 or 1)\n        y_scores: predicted probabilities\n\n    Returns:\n        precision, recall, thresholds\n    \"\"\"\n    y_true = np.array(y_true)\n    y_scores = np.array(y_scores)\n\n    # Get unique thresholds\n    thresholds = np.sort(np.unique(y_scores))[::-1]\n    thresholds = np.concatenate([[np.inf], thresholds])\n\n    precision_list = []\n    recall_list = []\n\n    for threshold in thresholds:\n        y_pred = (y_scores >= threshold).astype(int)\n\n        tp = np.sum((y_true == 1) & (y_pred == 1))\n        fp = np.sum((y_true == 0) & (y_pred == 1))\n        fn = np.sum((y_true == 1) & (y_pred == 0))\n\n        # Precision\n        if tp + fp == 0:\n            precision = 1.0  # No predictions, convention\n        else:\n            precision = tp / (tp + fp)\n\n        # Recall\n        if tp + fn == 0:\n            recall = 0.0\n        else:\n            recall = tp / (tp + fn)\n\n        precision_list.append(precision)\n        recall_list.append(recall)\n\n    return np.array(precision_list), np.array(recall_list), thresholds\n\ndef average_precision(precision, recall):\n    \"\"\"\n    Compute average precision (area under PR curve).\n\n    Args:\n        precision: precision values\n        recall: recall values\n\n    Returns:\n        average precision\n    \"\"\"\n    # Sort by recall\n    sorted_indices = np.argsort(recall)\n    recall = recall[sorted_indices]\n    precision = precision[sorted_indices]\n\n    # Compute AP using trapezoidal rule\n    ap = 0\n    for i in range(1, len(recall)):\n        ap += (recall[i] - recall[i-1]) * precision[i]\n\n    return ap\n\ndef plot_pr_curve(precision, recall, ap):\n    \"\"\"Plot precision-recall curve.\"\"\"\n    plt.figure(figsize=(8, 6))\n    plt.plot(recall, precision, linewidth=2, label=f'PR curve (AP = {ap:.3f})')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    plt.legend()\n    plt.grid(True)\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.show()\n\n# Example\ny_true = [0, 0, 1, 1, 0, 1, 0, 1]\ny_scores = [0.1, 0.4, 0.35, 0.8, 0.2, 0.7, 0.3, 0.9]\n\nprecision, recall, thresholds = precision_recall_curve(y_true, y_scores)\nap = average_precision(precision, recall)\n\nprint(f\"Average Precision: {ap:.3f}\")\nplot_pr_curve(precision, recall, ap)",
    "testCases": [],
    "hints": [
      "PR curve plots precision vs recall at different thresholds",
      "More informative than ROC for imbalanced datasets",
      "High precision + high recall = good classifier",
      "Average precision: area under PR curve",
      "Perfect classifier: AP = 1.0",
      "Start with high threshold (high precision, low recall)"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-7-6",
    "subjectId": "cs402",
    "topicId": "cs402-topic-7",
    "title": "Implement Stratified K-Fold",
    "difficulty": 2,
    "description": "Build stratified k-fold cross-validation from scratch.\n\nRequirements:\n- Maintain class distribution in each fold\n- Split data into k stratified folds\n- Ensure each class is represented proportionally\n- Return train/test indices for each fold",
    "starterCode": "import numpy as np\n\nclass StratifiedKFold:\n    def __init__(self, n_splits=5):\n        self.n_splits = n_splits\n\n    def split(self, X, y):\n        \"\"\"\n        Generate train/test indices.\n\n        Args:\n            X: features\n            y: labels\n\n        Yields:\n            train_idx, test_idx for each fold\n        \"\"\"\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\n\nclass StratifiedKFold:\n    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def split(self, X, y):\n        \"\"\"\n        Generate stratified train/test indices.\n\n        Args:\n            X: features (n_samples, n_features)\n            y: labels (n_samples,)\n\n        Yields:\n            train_idx, test_idx for each fold\n        \"\"\"\n        y = np.array(y)\n        n_samples = len(y)\n\n        # Get unique classes and their counts\n        classes, class_counts = np.unique(y, return_counts=True)\n\n        # Shuffle indices within each class\n        if self.shuffle:\n            rng = np.random.RandomState(self.random_state)\n            class_indices = {}\n            for cls in classes:\n                indices = np.where(y == cls)[0]\n                rng.shuffle(indices)\n                class_indices[cls] = indices\n        else:\n            class_indices = {cls: np.where(y == cls)[0] for cls in classes}\n\n        # Create folds\n        for fold_idx in range(self.n_splits):\n            test_indices = []\n            train_indices = []\n\n            for cls in classes:\n                indices = class_indices[cls]\n                n_class_samples = len(indices)\n                fold_size = n_class_samples // self.n_splits\n\n                # Test indices for this fold\n                start_idx = fold_idx * fold_size\n                end_idx = start_idx + fold_size if fold_idx < self.n_splits - 1 else n_class_samples\n                test_indices.extend(indices[start_idx:end_idx])\n\n                # Train indices (all others)\n                train_indices.extend(np.concatenate([\n                    indices[:start_idx],\n                    indices[end_idx:]\n                ]))\n\n            yield np.array(train_indices), np.array(test_indices)\n\n# Example usage\nX = np.random.randn(100, 5)\ny = np.array([0] * 30 + [1] * 70)  # Imbalanced\n\nskf = StratifiedKFold(n_splits=5)\n\nfor fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n    train_labels = y[train_idx]\n    test_labels = y[test_idx]\n\n    print(f\"Fold {fold + 1}:\")\n    print(f\"  Train: {len(train_idx)} samples, Class dist: {np.bincount(train_labels)}\")\n    print(f\"  Test: {len(test_idx)} samples, Class dist: {np.bincount(test_labels)}\")",
    "testCases": [],
    "hints": [
      "Stratified splitting maintains class proportions in each fold",
      "Split each class independently into k parts",
      "For each fold, combine parts from all classes",
      "Essential for imbalanced datasets",
      "Shuffle indices within each class before splitting",
      "Handle uneven divisions by putting remainder in last fold"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-7-7",
    "subjectId": "cs402",
    "topicId": "cs402-topic-7",
    "title": "Implement Grid Search",
    "difficulty": 3,
    "description": "Build grid search for hyperparameter tuning.\n\nRequirements:\n- Try all combinations of hyperparameter values\n- Use cross-validation to evaluate each combination\n- Track best parameters and best score\n- Support any sklearn-compatible model",
    "starterCode": "import numpy as np\n\nclass GridSearchCV:\n    def __init__(self, model, param_grid, cv=5):\n        self.model = model\n        self.param_grid = param_grid\n        self.cv = cv\n\n    def fit(self, X, y):\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\nfrom itertools import product\nfrom sklearn.model_selection import cross_val_score\nimport copy\n\nclass GridSearchCV:\n    def __init__(self, model, param_grid, cv=5, scoring='accuracy'):\n        self.model = model\n        self.param_grid = param_grid\n        self.cv = cv\n        self.scoring = scoring\n        self.best_params_ = None\n        self.best_score_ = None\n        self.best_model_ = None\n        self.cv_results_ = []\n\n    def fit(self, X, y):\n        \"\"\"\n        Perform grid search with cross-validation.\n\n        Args:\n            X: features\n            y: labels\n        \"\"\"\n        # Generate all combinations of parameters\n        param_names = list(self.param_grid.keys())\n        param_values = list(self.param_grid.values())\n        param_combinations = list(product(*param_values))\n\n        best_score = -np.inf\n\n        for param_combo in param_combinations:\n            # Create parameter dictionary\n            params = dict(zip(param_names, param_combo))\n\n            # Create model with these parameters\n            model = copy.deepcopy(self.model)\n            model.set_params(**params)\n\n            # Evaluate with cross-validation\n            scores = cross_val_score(model, X, y, cv=self.cv, scoring=self.scoring)\n            mean_score = np.mean(scores)\n            std_score = np.std(scores)\n\n            # Store results\n            self.cv_results_.append({\n                'params': params,\n                'mean_score': mean_score,\n                'std_score': std_score,\n                'scores': scores\n            })\n\n            # Update best\n            if mean_score > best_score:\n                best_score = mean_score\n                self.best_params_ = params\n                self.best_score_ = mean_score\n\n        # Train best model on full dataset\n        self.best_model_ = copy.deepcopy(self.model)\n        self.best_model_.set_params(**self.best_params_)\n        self.best_model_.fit(X, y)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using best model.\"\"\"\n        return self.best_model_.predict(X)\n\n# Example usage\nfrom sklearn.svm import SVC\n\n# Define parameter grid\nparam_grid = {\n    'C': [0.1, 1, 10],\n    'kernel': ['linear', 'rbf'],\n    'gamma': ['scale', 'auto']\n}\n\n# Create grid search\nmodel = SVC()\ngrid_search = GridSearchCV(model, param_grid, cv=5)\n\n# Generate sample data\nX = np.random.randn(100, 5)\ny = np.random.randint(0, 2, 100)\n\n# Fit\ngrid_search.fit(X, y)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best score: {grid_search.best_score_:.3f}\")",
    "testCases": [],
    "hints": [
      "Use itertools.product to generate all parameter combinations",
      "For each combination, train model with cross-validation",
      "Track mean and std of cross-validation scores",
      "Keep parameters with best mean score",
      "Train final model on full dataset with best parameters",
      "Use copy.deepcopy to avoid modifying original model"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-7-8",
    "subjectId": "cs402",
    "topicId": "cs402-topic-7",
    "title": "Implement Random Search",
    "difficulty": 3,
    "description": "Build random search for hyperparameter tuning.\n\nRequirements:\n- Sample hyperparameters randomly from distributions\n- More efficient than grid search for high-dimensional spaces\n- Support continuous and discrete distributions\n- Track best parameters found",
    "starterCode": "import numpy as np\n\nclass RandomizedSearchCV:\n    def __init__(self, model, param_distributions, n_iter=10, cv=5):\n        self.model = model\n        self.param_distributions = param_distributions\n        self.n_iter = n_iter\n        self.cv = cv\n\n    def fit(self, X, y):\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\nfrom sklearn.model_selection import cross_val_score\nimport copy\n\nclass RandomizedSearchCV:\n    def __init__(self, model, param_distributions, n_iter=10, cv=5,\n                 scoring='accuracy', random_state=None):\n        self.model = model\n        self.param_distributions = param_distributions\n        self.n_iter = n_iter\n        self.cv = cv\n        self.scoring = scoring\n        self.random_state = random_state\n        self.best_params_ = None\n        self.best_score_ = None\n        self.best_model_ = None\n        self.cv_results_ = []\n\n    def sample_params(self, rng):\n        \"\"\"Sample one set of parameters.\"\"\"\n        params = {}\n        for param_name, distribution in self.param_distributions.items():\n            if isinstance(distribution, list):\n                # Discrete choice\n                params[param_name] = rng.choice(distribution)\n            elif hasattr(distribution, 'rvs'):\n                # scipy distribution\n                params[param_name] = distribution.rvs(random_state=rng)\n            elif callable(distribution):\n                # Custom callable\n                params[param_name] = distribution(rng)\n        return params\n\n    def fit(self, X, y):\n        \"\"\"\n        Perform random search with cross-validation.\n\n        Args:\n            X: features\n            y: labels\n        \"\"\"\n        rng = np.random.RandomState(self.random_state)\n        best_score = -np.inf\n\n        for i in range(self.n_iter):\n            # Sample parameters\n            params = self.sample_params(rng)\n\n            # Create model with these parameters\n            model = copy.deepcopy(self.model)\n            try:\n                model.set_params(**params)\n            except ValueError:\n                # Invalid parameter combination, skip\n                continue\n\n            # Evaluate with cross-validation\n            scores = cross_val_score(model, X, y, cv=self.cv, scoring=self.scoring)\n            mean_score = np.mean(scores)\n            std_score = np.std(scores)\n\n            # Store results\n            self.cv_results_.append({\n                'params': params,\n                'mean_score': mean_score,\n                'std_score': std_score,\n                'scores': scores\n            })\n\n            # Update best\n            if mean_score > best_score:\n                best_score = mean_score\n                self.best_params_ = params\n                self.best_score_ = mean_score\n\n            print(f\"Iteration {i+1}/{self.n_iter}: score={mean_score:.3f}, params={params}\")\n\n        # Train best model on full dataset\n        self.best_model_ = copy.deepcopy(self.model)\n        self.best_model_.set_params(**self.best_params_)\n        self.best_model_.fit(X, y)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using best model.\"\"\"\n        return self.best_model_.predict(X)\n\n# Example usage\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import uniform, randint\n\n# Define parameter distributions\nparam_distributions = {\n    'n_estimators': randint(10, 200),\n    'max_depth': randint(3, 20),\n    'min_samples_split': randint(2, 20),\n    'min_samples_leaf': randint(1, 10),\n    'max_features': uniform(0.1, 0.9)\n}\n\n# Create random search\nmodel = RandomForestClassifier()\nrandom_search = RandomizedSearchCV(model, param_distributions,\n                                   n_iter=20, cv=5, random_state=42)\n\n# Generate sample data\nX = np.random.randn(200, 10)\ny = np.random.randint(0, 2, 200)\n\n# Fit\nrandom_search.fit(X, y)\n\nprint(f\"\\nBest parameters: {random_search.best_params_}\")\nprint(f\"Best score: {random_search.best_score_:.3f}\")",
    "testCases": [],
    "hints": [
      "Sample parameters randomly from distributions",
      "Use scipy.stats distributions for continuous parameters",
      "Use lists for discrete categorical parameters",
      "More efficient than grid search for large parameter spaces",
      "Can explore wider range of parameter values",
      "Set random_state for reproducibility"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-7-9",
    "subjectId": "cs402",
    "topicId": "cs402-topic-7",
    "title": "Calculate Bootstrap Confidence Intervals",
    "difficulty": 2,
    "description": "Use bootstrap resampling to estimate confidence intervals.\n\nRequirements:\n- Resample data with replacement\n- Calculate metric on each bootstrap sample\n- Compute confidence interval from bootstrap distribution\n- Support percentile and BCa methods",
    "starterCode": "import numpy as np\n\ndef bootstrap_confidence_interval(X, y, model, metric_fn,\n                                   n_bootstrap=1000, confidence=0.95):\n    \"\"\"\n    Compute bootstrap confidence interval.\n\n    Args:\n        X: features\n        y: labels\n        model: fitted model\n        metric_fn: function to compute metric\n        n_bootstrap: number of bootstrap samples\n        confidence: confidence level\n\n    Returns:\n        (lower_bound, upper_bound)\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import numpy as np\n\ndef bootstrap_confidence_interval(X, y, model, metric_fn,\n                                   n_bootstrap=1000, confidence=0.95,\n                                   random_state=None):\n    \"\"\"\n    Compute bootstrap confidence interval.\n\n    Args:\n        X: features\n        y: labels\n        model: fitted model (already trained)\n        metric_fn: function(y_true, y_pred) -> score\n        n_bootstrap: number of bootstrap samples\n        confidence: confidence level (e.g., 0.95 for 95%)\n\n    Returns:\n        (lower_bound, upper_bound, bootstrap_scores)\n    \"\"\"\n    rng = np.random.RandomState(random_state)\n    n_samples = len(X)\n\n    bootstrap_scores = []\n\n    for _ in range(n_bootstrap):\n        # Resample with replacement\n        indices = rng.choice(n_samples, size=n_samples, replace=True)\n        X_boot = X[indices]\n        y_boot = y[indices]\n\n        # Make predictions\n        y_pred = model.predict(X_boot)\n\n        # Calculate metric\n        score = metric_fn(y_boot, y_pred)\n        bootstrap_scores.append(score)\n\n    bootstrap_scores = np.array(bootstrap_scores)\n\n    # Compute percentile confidence interval\n    alpha = 1 - confidence\n    lower_percentile = (alpha / 2) * 100\n    upper_percentile = (1 - alpha / 2) * 100\n\n    lower_bound = np.percentile(bootstrap_scores, lower_percentile)\n    upper_bound = np.percentile(bootstrap_scores, upper_percentile)\n\n    return lower_bound, upper_bound, bootstrap_scores\n\n# Example usage\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Generate sample data\nX = np.random.randn(100, 5)\ny = np.random.randint(0, 2, 100)\n\n# Train model\nmodel = LogisticRegression()\nmodel.fit(X, y)\n\n# Compute bootstrap CI for accuracy\nlower, upper, scores = bootstrap_confidence_interval(\n    X, y, model, accuracy_score,\n    n_bootstrap=1000, confidence=0.95, random_state=42\n)\n\nprint(f\"Mean accuracy: {np.mean(scores):.3f}\")\nprint(f\"95% CI: [{lower:.3f}, {upper:.3f}]\")\nprint(f\"Standard error: {np.std(scores):.3f}\")",
    "testCases": [],
    "hints": [
      "Bootstrap: resample data with replacement",
      "Create n_bootstrap resamples of same size as original",
      "Calculate metric on each resample",
      "Confidence interval: percentiles of bootstrap distribution",
      "For 95% CI: use 2.5th and 97.5th percentiles",
      "Bootstrap provides estimate of sampling variability"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-7-10",
    "subjectId": "cs402",
    "topicId": "cs402-topic-7",
    "title": "Implement Learning Curves",
    "difficulty": 2,
    "description": "Plot learning curves to diagnose bias vs variance.\n\nRequirements:\n- Train on increasing amounts of data\n- Track training and validation scores\n- Plot scores vs training set size\n- Diagnose overfitting/underfitting",
    "starterCode": "import numpy as np\n\ndef learning_curve(model, X, y, train_sizes, cv=5):\n    \"\"\"\n    Compute learning curve.\n\n    Args:\n        model: sklearn model\n        X: features\n        y: labels\n        train_sizes: array of training set sizes\n        cv: number of CV folds\n\n    Returns:\n        train_scores, val_scores for each size\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import numpy as np\nfrom sklearn.model_selection import ShuffleSplit\nimport matplotlib.pyplot as plt\nimport copy\n\ndef learning_curve(model, X, y, train_sizes, cv=5, random_state=None):\n    \"\"\"\n    Compute learning curve.\n\n    Args:\n        model: sklearn model\n        X: features (n_samples, n_features)\n        y: labels\n        train_sizes: array of training set sizes (or fractions)\n        cv: number of CV folds\n\n    Returns:\n        train_sizes_abs, train_scores, val_scores\n    \"\"\"\n    n_samples = len(X)\n\n    # Convert fractions to absolute numbers\n    if np.max(train_sizes) <= 1.0:\n        train_sizes_abs = (train_sizes * n_samples).astype(int)\n    else:\n        train_sizes_abs = train_sizes.astype(int)\n\n    train_scores_list = []\n    val_scores_list = []\n\n    for train_size in train_sizes_abs:\n        train_scores = []\n        val_scores = []\n\n        # Create CV splitter\n        cv_splitter = ShuffleSplit(n_splits=cv, train_size=train_size,\n                                    random_state=random_state)\n\n        for train_idx, val_idx in cv_splitter.split(X):\n            X_train, X_val = X[train_idx], X[val_idx]\n            y_train, y_val = y[train_idx], y[val_idx]\n\n            # Train model\n            model_copy = copy.deepcopy(model)\n            model_copy.fit(X_train, y_train)\n\n            # Evaluate\n            train_score = model_copy.score(X_train, y_train)\n            val_score = model_copy.score(X_val, y_val)\n\n            train_scores.append(train_score)\n            val_scores.append(val_score)\n\n        train_scores_list.append(train_scores)\n        val_scores_list.append(val_scores)\n\n    return train_sizes_abs, np.array(train_scores_list), np.array(val_scores_list)\n\ndef plot_learning_curve(train_sizes, train_scores, val_scores):\n    \"\"\"Plot learning curves.\"\"\"\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    val_mean = np.mean(val_scores, axis=1)\n    val_std = np.std(val_scores, axis=1)\n\n    plt.figure(figsize=(10, 6))\n\n    # Plot training scores\n    plt.plot(train_sizes, train_mean, 'o-', label='Training score')\n    plt.fill_between(train_sizes,\n                     train_mean - train_std,\n                     train_mean + train_std,\n                     alpha=0.1)\n\n    # Plot validation scores\n    plt.plot(train_sizes, val_mean, 'o-', label='Validation score')\n    plt.fill_between(train_sizes,\n                     val_mean - val_std,\n                     val_mean + val_std,\n                     alpha=0.1)\n\n    plt.xlabel('Training Set Size')\n    plt.ylabel('Score')\n    plt.title('Learning Curves')\n    plt.legend(loc='best')\n    plt.grid(True)\n    plt.show()\n\n# Example usage\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Generate sample data\nX = np.random.randn(500, 10)\ny = np.random.randint(0, 2, 500)\n\n# Compute learning curve\nmodel = DecisionTreeClassifier(max_depth=5)\ntrain_sizes = np.linspace(0.1, 1.0, 10)\nsizes, train_scores, val_scores = learning_curve(model, X, y, train_sizes, cv=5)\n\n# Plot\nplot_learning_curve(sizes, train_scores, val_scores)\n\n# Interpret:\n# - High training score, low val score: overfitting\n# - Both scores low: underfitting\n# - Both scores high and close: good fit",
    "testCases": [],
    "hints": [
      "Train on increasingly large subsets of data",
      "For each subset size, use cross-validation",
      "Plot both training and validation scores",
      "High training score + low validation score = overfitting",
      "Both scores low = underfitting (high bias)",
      "Scores converging at high value = good fit"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-7-11",
    "subjectId": "cs402",
    "topicId": "cs402-topic-7",
    "title": "Implement McNemar Test",
    "difficulty": 3,
    "description": "Use McNemar test to compare two classifiers statistically.\n\nRequirements:\n- Build contingency table of agreements/disagreements\n- Apply McNemar chi-squared test\n- Determine if difference is statistically significant\n- Return p-value and test statistic",
    "starterCode": "import numpy as np\n\ndef mcnemar_test(y_true, y_pred1, y_pred2):\n    \"\"\"\n    Perform McNemar test.\n\n    Args:\n        y_true: true labels\n        y_pred1: predictions from model 1\n        y_pred2: predictions from model 2\n\n    Returns:\n        statistic, p_value\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import numpy as np\nfrom scipy.stats import chi2\n\ndef mcnemar_test(y_true, y_pred1, y_pred2):\n    \"\"\"\n    Perform McNemar test to compare two classifiers.\n\n    Args:\n        y_true: true labels\n        y_pred1: predictions from model 1\n        y_pred2: predictions from model 2\n\n    Returns:\n        statistic, p_value\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred1 = np.array(y_pred1)\n    y_pred2 = np.array(y_pred2)\n\n    # Build contingency table\n    # Rows: model 1 correct/incorrect\n    # Cols: model 2 correct/incorrect\n\n    # Both correct\n    n_11 = np.sum((y_pred1 == y_true) & (y_pred2 == y_true))\n\n    # Model 1 correct, model 2 incorrect\n    n_10 = np.sum((y_pred1 == y_true) & (y_pred2 != y_true))\n\n    # Model 1 incorrect, model 2 correct\n    n_01 = np.sum((y_pred1 != y_true) & (y_pred2 == y_true))\n\n    # Both incorrect\n    n_00 = np.sum((y_pred1 != y_true) & (y_pred2 != y_true))\n\n    print(f\"Contingency table:\")\n    print(f\"  Both correct: {n_11}\")\n    print(f\"  Only M1 correct: {n_10}\")\n    print(f\"  Only M2 correct: {n_01}\")\n    print(f\"  Both incorrect: {n_00}\")\n\n    # McNemar test statistic\n    # Only disagreements matter (n_01 and n_10)\n    if n_10 + n_01 == 0:\n        return 0.0, 1.0  # No disagreements, models identical\n\n    # Chi-squared test with continuity correction\n    statistic = (abs(n_10 - n_01) - 1)**2 / (n_10 + n_01)\n\n    # p-value from chi-squared distribution with 1 df\n    p_value = 1 - chi2.cdf(statistic, df=1)\n\n    return statistic, p_value\n\n# Example usage\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Generate data\nX = np.random.randn(200, 10)\ny = np.random.randint(0, 2, 200)\n\n# Train two models\nmodel1 = DecisionTreeClassifier(max_depth=5, random_state=42)\nmodel2 = RandomForestClassifier(n_estimators=10, random_state=42)\n\nmodel1.fit(X, y)\nmodel2.fit(X, y)\n\n# Predictions\ny_pred1 = model1.predict(X)\ny_pred2 = model2.predict(X)\n\n# McNemar test\nstat, p_value = mcnemar_test(y, y_pred1, y_pred2)\n\nprint(f\"\\nMcNemar statistic: {stat:.3f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\nif p_value < 0.05:\n    print(\"Significant difference between models (p < 0.05)\")\nelse:\n    print(\"No significant difference between models (p >= 0.05)\")",
    "testCases": [],
    "hints": [
      "McNemar test compares paired predictions from two classifiers",
      "Build 2x2 contingency table of correct/incorrect predictions",
      "Test statistic: (|n_01 - n_10| - 1)^2 / (n_01 + n_10)",
      "Only disagreements matter (where one model correct, other incorrect)",
      "Use chi-squared distribution with 1 degree of freedom",
      "Null hypothesis: both models have same error rate"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-7-12",
    "subjectId": "cs402",
    "topicId": "cs402-topic-7",
    "title": "Calculate Calibration Curve",
    "difficulty": 3,
    "description": "Assess probability calibration of classifier.\n\nRequirements:\n- Bin predictions by probability\n- Calculate empirical probability in each bin\n- Plot predicted vs actual probabilities\n- Compute calibration error (ECE)",
    "starterCode": "import numpy as np\n\ndef calibration_curve(y_true, y_proba, n_bins=10):\n    \"\"\"\n    Compute calibration curve.\n\n    Args:\n        y_true: true binary labels\n        y_proba: predicted probabilities\n        n_bins: number of bins\n\n    Returns:\n        bin_edges, empirical_probs, predicted_probs\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef calibration_curve(y_true, y_proba, n_bins=10):\n    \"\"\"\n    Compute calibration curve.\n\n    Args:\n        y_true: true binary labels (0 or 1)\n        y_proba: predicted probabilities\n        n_bins: number of bins\n\n    Returns:\n        bin_boundaries, empirical_probs, predicted_probs, counts\n    \"\"\"\n    y_true = np.array(y_true)\n    y_proba = np.array(y_proba)\n\n    # Create bins\n    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n    bin_indices = np.digitize(y_proba, bin_boundaries[:-1]) - 1\n    bin_indices = np.clip(bin_indices, 0, n_bins - 1)\n\n    # Calculate empirical and predicted probabilities per bin\n    empirical_probs = []\n    predicted_probs = []\n    counts = []\n\n    for i in range(n_bins):\n        mask = bin_indices == i\n        count = np.sum(mask)\n\n        if count > 0:\n            # Empirical probability (fraction of positive examples)\n            empirical_prob = np.mean(y_true[mask])\n            # Mean predicted probability\n            predicted_prob = np.mean(y_proba[mask])\n        else:\n            empirical_prob = 0\n            predicted_prob = 0\n\n        empirical_probs.append(empirical_prob)\n        predicted_probs.append(predicted_prob)\n        counts.append(count)\n\n    return bin_boundaries, np.array(empirical_probs), np.array(predicted_probs), np.array(counts)\n\ndef expected_calibration_error(empirical_probs, predicted_probs, counts):\n    \"\"\"Compute Expected Calibration Error (ECE).\"\"\"\n    total_count = np.sum(counts)\n    ece = np.sum(counts * np.abs(empirical_probs - predicted_probs)) / total_count\n    return ece\n\ndef plot_calibration_curve(bin_boundaries, empirical_probs, predicted_probs):\n    \"\"\"Plot calibration curve.\"\"\"\n    plt.figure(figsize=(8, 8))\n\n    # Plot calibration curve\n    bin_centers = (bin_boundaries[:-1] + bin_boundaries[1:]) / 2\n    plt.plot(predicted_probs, empirical_probs, 'o-', linewidth=2, label='Calibration curve')\n\n    # Plot perfect calibration\n    plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n\n    plt.xlabel('Predicted Probability')\n    plt.ylabel('Empirical Probability')\n    plt.title('Calibration Curve')\n    plt.legend()\n    plt.grid(True)\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.show()\n\n# Example usage\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\n\n# Generate data\nX = np.random.randn(1000, 10)\ny = (X[:, 0] + X[:, 1] > 0).astype(int)\n\n# Train poorly calibrated model (Naive Bayes)\nmodel_uncalibrated = GaussianNB()\nmodel_uncalibrated.fit(X, y)\ny_proba_uncalibrated = model_uncalibrated.predict_proba(X)[:, 1]\n\n# Train well-calibrated model (Logistic Regression)\nmodel_calibrated = LogisticRegression()\nmodel_calibrated.fit(X, y)\ny_proba_calibrated = model_calibrated.predict_proba(X)[:, 1]\n\n# Compute calibration curves\nbins, emp1, pred1, counts1 = calibration_curve(y, y_proba_uncalibrated, n_bins=10)\nece1 = expected_calibration_error(emp1, pred1, counts1)\n\nbins, emp2, pred2, counts2 = calibration_curve(y, y_proba_calibrated, n_bins=10)\nece2 = expected_calibration_error(emp2, pred2, counts2)\n\nprint(f\"Naive Bayes ECE: {ece1:.3f}\")\nprint(f\"Logistic Regression ECE: {ece2:.3f}\")\n\nplot_calibration_curve(bins, emp1, pred1)\nplot_calibration_curve(bins, emp2, pred2)",
    "testCases": [],
    "hints": [
      "Calibration: predicted probabilities match empirical frequencies",
      "Bin predictions by probability (e.g., 0-0.1, 0.1-0.2, ...)",
      "In each bin, compute fraction of positive examples",
      "Plot predicted vs empirical probabilities",
      "Perfect calibration: points lie on diagonal",
      "ECE: average absolute difference weighted by bin size"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-7-13",
    "subjectId": "cs402",
    "topicId": "cs402-topic-7",
    "title": "Implement Bayesian Optimization",
    "difficulty": 5,
    "description": "Build Bayesian optimization for hyperparameter tuning.\n\nRequirements:\n- Use Gaussian Process as surrogate model\n- Implement acquisition function (Expected Improvement)\n- Iteratively select promising hyperparameters\n- Balance exploration vs exploitation",
    "starterCode": "import numpy as np\n\nclass BayesianOptimization:\n    def __init__(self, f, bounds, n_iter=25):\n        self.f = f  # Objective function\n        self.bounds = bounds\n        self.n_iter = n_iter\n\n    def optimize(self):\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel\n\nclass BayesianOptimization:\n    def __init__(self, f, bounds, n_iter=25, n_init=5, random_state=None):\n        \"\"\"\n        Bayesian optimization.\n\n        Args:\n            f: objective function to maximize\n            bounds: list of (min, max) for each parameter\n            n_iter: number of iterations\n            n_init: number of random initialization points\n        \"\"\"\n        self.f = f\n        self.bounds = np.array(bounds)\n        self.n_iter = n_iter\n        self.n_init = n_init\n        self.random_state = random_state\n\n        self.X_observed = []\n        self.y_observed = []\n\n    def expected_improvement(self, X, gp, y_max, xi=0.01):\n        \"\"\"\n        Expected Improvement acquisition function.\n\n        Args:\n            X: points to evaluate\n            gp: fitted Gaussian Process\n            y_max: current best observed value\n            xi: exploration parameter\n\n        Returns:\n            expected improvement values\n        \"\"\"\n        mu, sigma = gp.predict(X, return_std=True)\n        sigma = sigma.reshape(-1, 1)\n\n        # Calculate EI\n        with np.errstate(divide='warn'):\n            Z = (mu - y_max - xi) / sigma\n            ei = (mu - y_max - xi) * norm.cdf(Z) + sigma * norm.pdf(Z)\n            ei[sigma == 0.0] = 0.0\n\n        return ei\n\n    def propose_location(self, gp, y_max):\n        \"\"\"Find point that maximizes acquisition function.\"\"\"\n        min_val = float('inf')\n        min_x = None\n\n        # Try multiple random starting points\n        rng = np.random.RandomState(self.random_state)\n        for _ in range(10):\n            x0 = rng.uniform(self.bounds[:, 0], self.bounds[:, 1])\n\n            # Minimize negative EI\n            res = minimize(\n                lambda x: -self.expected_improvement(x.reshape(1, -1), gp, y_max),\n                x0=x0,\n                bounds=self.bounds,\n                method='L-BFGS-B'\n            )\n\n            if res.fun < min_val:\n                min_val = res.fun\n                min_x = res.x\n\n        return min_x\n\n    def optimize(self):\n        \"\"\"Run Bayesian optimization.\"\"\"\n        rng = np.random.RandomState(self.random_state)\n\n        # Random initialization\n        for _ in range(self.n_init):\n            x = rng.uniform(self.bounds[:, 0], self.bounds[:, 1])\n            y = self.f(x)\n            self.X_observed.append(x)\n            self.y_observed.append(y)\n\n        X = np.array(self.X_observed)\n        y = np.array(self.y_observed).reshape(-1, 1)\n\n        # Bayesian optimization loop\n        for i in range(self.n_iter - self.n_init):\n            # Fit Gaussian Process\n            kernel = ConstantKernel(1.0) * RBF(length_scale=1.0)\n            gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n            gp.fit(X, y)\n\n            # Find next point to evaluate\n            y_max = np.max(y)\n            x_next = self.propose_location(gp, y_max)\n\n            # Evaluate objective\n            y_next = self.f(x_next)\n\n            # Update observations\n            self.X_observed.append(x_next)\n            self.y_observed.append(y_next)\n\n            X = np.array(self.X_observed)\n            y = np.array(self.y_observed).reshape(-1, 1)\n\n            print(f\"Iteration {i+1}/{self.n_iter - self.n_init}: \"\n                  f\"x={x_next}, f(x)={y_next:.4f}, best={np.max(y):.4f}\")\n\n        # Return best found\n        best_idx = np.argmax(y)\n        return self.X_observed[best_idx], self.y_observed[best_idx]\n\n# Example usage: optimize hyperparameters of a model\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\n\n# Generate data\nX_train = np.random.randn(200, 5)\ny_train = np.random.randint(0, 2, 200)\n\ndef objective(params):\n    \"\"\"Objective function: cross-validation score.\"\"\"\n    C, gamma = params\n    model = SVC(C=10**C, gamma=10**gamma)\n    score = cross_val_score(model, X_train, y_train, cv=3).mean()\n    return score\n\n# Optimize\nbounds = [(-3, 3), (-3, 3)]  # log10(C), log10(gamma)\noptimizer = BayesianOptimization(objective, bounds, n_iter=20, random_state=42)\nbest_params, best_score = optimizer.optimize()\n\nprint(f\"\\nBest parameters: C={10**best_params[0]:.3f}, gamma={10**best_params[1]:.3f}\")\nprint(f\"Best score: {best_score:.3f}\")",
    "testCases": [],
    "hints": [
      "Bayesian optimization uses Gaussian Process as surrogate",
      "Acquisition function balances exploration vs exploitation",
      "Expected Improvement: how much better than current best?",
      "Fit GP to observations, maximize acquisition to get next point",
      "More sample-efficient than random/grid search",
      "Good for expensive objective functions (e.g., training deep nets)"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-7-14",
    "subjectId": "cs402",
    "topicId": "cs402-topic-7",
    "title": "Implement Early Stopping",
    "difficulty": 2,
    "description": "Add early stopping to prevent overfitting during training.\n\nRequirements:\n- Monitor validation loss during training\n- Stop if no improvement for patience epochs\n- Restore best weights\n- Support different monitoring criteria",
    "starterCode": "import numpy as np\n\nclass EarlyStopping:\n    def __init__(self, patience=5, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n\n    def __call__(self, val_loss):\n        # TODO: Implement\n        pass",
    "solution": "import numpy as np\n\nclass EarlyStopping:\n    def __init__(self, patience=5, min_delta=0, mode='min'):\n        \"\"\"\n        Early stopping callback.\n\n        Args:\n            patience: number of epochs with no improvement to wait\n            min_delta: minimum change to qualify as improvement\n            mode: 'min' for loss, 'max' for accuracy\n        \"\"\"\n        self.patience = patience\n        self.min_delta = min_delta\n        self.mode = mode\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.best_weights = None\n\n    def is_improvement(self, score):\n        \"\"\"Check if score is an improvement.\"\"\"\n        if self.best_score is None:\n            return True\n\n        if self.mode == 'min':\n            return score < self.best_score - self.min_delta\n        else:  # mode == 'max'\n            return score > self.best_score + self.min_delta\n\n    def __call__(self, score, model_weights=None):\n        \"\"\"\n        Check if training should stop.\n\n        Args:\n            score: validation score (loss or metric)\n            model_weights: current model weights to save\n\n        Returns:\n            should_stop: whether to stop training\n        \"\"\"\n        if self.is_improvement(score):\n            self.best_score = score\n            self.counter = 0\n            if model_weights is not None:\n                self.best_weights = model_weights.copy()\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n\n        return self.early_stop\n\n# Example usage with PyTorch-style training loop\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Simple neural network\nclass SimpleNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Generate data\nX_train = torch.randn(500, 10)\ny_train = torch.randint(0, 2, (500,))\nX_val = torch.randn(100, 10)\ny_val = torch.randint(0, 2, (100,))\n\n# Setup\nmodel = SimpleNN(10, 20, 2)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Early stopping\nearly_stopping = EarlyStopping(patience=5, min_delta=0.001, mode='min')\n\n# Training loop\nfor epoch in range(100):\n    # Training\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train)\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n\n    # Validation\n    model.eval()\n    with torch.no_grad():\n        val_outputs = model(X_val)\n        val_loss = criterion(val_outputs, y_val).item()\n\n    print(f\"Epoch {epoch+1}: train_loss={loss.item():.4f}, val_loss={val_loss:.4f}\")\n\n    # Check early stopping\n    # In PyTorch, you'd save model.state_dict()\n    if early_stopping(val_loss):\n        print(f\"Early stopping triggered at epoch {epoch+1}\")\n        # Restore best weights here\n        break\n\nprint(f\"Best validation loss: {early_stopping.best_score:.4f}\")",
    "testCases": [],
    "hints": [
      "Monitor validation metric during training",
      "Keep track of best score seen so far",
      "Increment counter when no improvement",
      "Stop when counter reaches patience",
      "Save best weights to restore later",
      "Use min_delta to avoid stopping on tiny improvements"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-7-15",
    "subjectId": "cs402",
    "topicId": "cs402-topic-7",
    "title": "Calculate Matthews Correlation Coefficient",
    "difficulty": 2,
    "description": "Compute MCC for binary classification.\n\nRequirements:\n- Calculate from confusion matrix\n- MCC = (TP*TN - FP*FN) / sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN))\n- Returns value between -1 and 1\n- Better than accuracy for imbalanced datasets",
    "starterCode": "import numpy as np\n\ndef matthews_corrcoef(y_true, y_pred):\n    \"\"\"\n    Calculate Matthews Correlation Coefficient.\n\n    Args:\n        y_true: true labels\n        y_pred: predicted labels\n\n    Returns:\n        MCC score\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import numpy as np\n\ndef matthews_corrcoef(y_true, y_pred):\n    \"\"\"\n    Calculate Matthews Correlation Coefficient.\n\n    Args:\n        y_true: true binary labels (0 or 1)\n        y_pred: predicted labels (0 or 1)\n\n    Returns:\n        MCC score (-1 to 1)\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Calculate confusion matrix elements\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    tn = np.sum((y_true == 0) & (y_pred == 0))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n\n    # Calculate MCC\n    numerator = tp * tn - fp * fn\n    denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n\n    if denominator == 0:\n        return 0.0\n\n    mcc = numerator / denominator\n\n    return mcc\n\n# Example usage\ny_true = [1, 1, 0, 1, 0, 1, 0, 0, 1, 0]\ny_pred = [1, 0, 0, 1, 0, 1, 1, 0, 1, 0]\n\nmcc = matthews_corrcoef(y_true, y_pred)\nprint(f\"MCC: {mcc:.3f}\")\n\n# Compare with imbalanced dataset\ny_true_imb = [1] * 95 + [0] * 5\ny_pred_imb = [1] * 100  # Predict all positive\n\nfrom sklearn.metrics import accuracy_score\nacc = accuracy_score(y_true_imb, y_pred_imb)\nmcc_imb = matthews_corrcoef(y_true_imb, y_pred_imb)\n\nprint(f\"\\nImbalanced dataset:\")\nprint(f\"Accuracy: {acc:.3f}\")  # 0.95 (misleading!)\nprint(f\"MCC: {mcc_imb:.3f}\")   # Low (correctly shows poor performance)",
    "testCases": [],
    "hints": [
      "MCC takes into account all four confusion matrix values",
      "Returns value between -1 (total disagreement) and 1 (perfect)",
      "MCC = 0 means random prediction",
      "More robust than accuracy for imbalanced classes",
      "Handle division by zero when denominator is 0",
      "Formula: (TP*TN - FP*FN) / sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN))"
    ],
    "language": "python"
  },
  {
    "id": "cs402-ex-7-16",
    "subjectId": "cs402",
    "topicId": "cs402-topic-7",
    "title": "Implement Nested Cross-Validation",
    "difficulty": 4,
    "description": "Build nested CV for unbiased model selection and evaluation.\n\nRequirements:\n- Outer loop: estimate generalization performance\n- Inner loop: hyperparameter tuning\n- Avoid data leakage from parameter selection\n- Return unbiased performance estimate",
    "starterCode": "import numpy as np\n\ndef nested_cross_validation(model, param_grid, X, y,\n                            outer_cv=5, inner_cv=3):\n    \"\"\"\n    Perform nested cross-validation.\n\n    Args:\n        model: sklearn model\n        param_grid: hyperparameter grid\n        X: features\n        y: labels\n        outer_cv: outer CV folds\n        inner_cv: inner CV folds\n\n    Returns:\n        outer_scores, best_params_per_fold\n    \"\"\"\n    # TODO: Implement\n    pass",
    "solution": "import numpy as np\nfrom sklearn.model_selection import KFold, GridSearchCV\nimport copy\n\ndef nested_cross_validation(model, param_grid, X, y,\n                            outer_cv=5, inner_cv=3, scoring='accuracy'):\n    \"\"\"\n    Perform nested cross-validation.\n\n    Args:\n        model: sklearn model\n        param_grid: hyperparameter grid\n        X: features (n_samples, n_features)\n        y: labels\n        outer_cv: outer CV folds\n        inner_cv: inner CV folds\n        scoring: scoring metric\n\n    Returns:\n        outer_scores, best_params_per_fold\n    \"\"\"\n    outer_kfold = KFold(n_splits=outer_cv, shuffle=True, random_state=42)\n    outer_scores = []\n    best_params_per_fold = []\n\n    for fold_idx, (train_idx, test_idx) in enumerate(outer_kfold.split(X)):\n        print(f\"\\nOuter fold {fold_idx + 1}/{outer_cv}\")\n\n        # Split data\n        X_train, X_test = X[train_idx], X[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        # Inner loop: hyperparameter tuning\n        inner_cv = GridSearchCV(\n            estimator=model,\n            param_grid=param_grid,\n            cv=inner_cv,\n            scoring=scoring,\n            n_jobs=-1\n        )\n\n        inner_cv.fit(X_train, y_train)\n\n        # Best parameters from inner CV\n        best_params = inner_cv.best_params_\n        best_params_per_fold.append(best_params)\n\n        print(f\"  Best params: {best_params}\")\n        print(f\"  Inner CV score: {inner_cv.best_score_:.3f}\")\n\n        # Evaluate on outer test set\n        test_score = inner_cv.score(X_test, y_test)\n        outer_scores.append(test_score)\n\n        print(f\"  Outer test score: {test_score:.3f}\")\n\n    outer_scores = np.array(outer_scores)\n\n    print(f\"\\n{'='*50}\")\n    print(f\"Nested CV Results:\")\n    print(f\"Mean score: {np.mean(outer_scores):.3f} (+/- {np.std(outer_scores):.3f})\")\n    print(f\"Scores: {outer_scores}\")\n\n    return outer_scores, best_params_per_fold\n\n# Example usage\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import make_classification\n\n# Generate data\nX, y = make_classification(n_samples=200, n_features=10,\n                           n_informative=5, random_state=42)\n\n# Define model and parameter grid\nmodel = SVC()\nparam_grid = {\n    'C': [0.1, 1, 10],\n    'kernel': ['linear', 'rbf'],\n    'gamma': ['scale', 'auto']\n}\n\n# Run nested CV\nscores, best_params = nested_cross_validation(\n    model, param_grid, X, y,\n    outer_cv=5, inner_cv=3\n)\n\nprint(f\"\\nBest parameters per fold:\")\nfor i, params in enumerate(best_params):\n    print(f\"  Fold {i+1}: {params}\")",
    "testCases": [],
    "hints": [
      "Nested CV has two loops: outer (evaluation) and inner (tuning)",
      "Outer loop: estimate true generalization performance",
      "Inner loop: select best hyperparameters on training data",
      "Never use test data for hyperparameter selection",
      "Each outer fold may select different best parameters",
      "More computationally expensive but gives unbiased estimate"
    ],
    "language": "python"
  }
]