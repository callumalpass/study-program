[
  {
    "id": "cs402-quiz-2-1",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Regression Fundamentals",
    "questions": [
      {
        "id": "cs402-q16",
        "type": "multiple_choice",
        "prompt": "What is the hypothesis function for linear regression?",
        "options": [
          "h(x) = 1/(1 + e^(-θᵀx))",
          "h(x) = max(0, θᵀx)",
          "h(x) = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₙxₙ",
          "h(x) = sign(θᵀx)"
        ],
        "correctAnswer": 2,
        "explanation": "Linear regression uses a linear combination of features: h(x) = θᵀx where θ are parameters to learn. This represents a hyperplane in feature space."
      },
      {
        "id": "cs402-q17",
        "type": "multiple_choice",
        "prompt": "What loss function is typically used for linear regression?",
        "options": [
          "0-1 loss",
          "Cross-entropy",
          "Mean Squared Error (MSE)",
          "Hinge loss"
        ],
        "correctAnswer": 2,
        "explanation": "Linear regression minimizes MSE: (1/2m)Σ(h(x⁽ⁱ⁾) - y⁽ⁱ⁾)². This is equivalent to maximum likelihood estimation under Gaussian noise assumptions."
      },
      {
        "id": "cs402-q18",
        "type": "multiple_choice",
        "prompt": "What does gradient descent do?",
        "options": [
          "Randomly searches for good parameters",
          "Computes the exact optimal solution analytically",
          "Iteratively updates parameters in the direction that decreases the loss function",
          "Increases the loss function to find maximum"
        ],
        "correctAnswer": 2,
        "explanation": "Gradient descent updates parameters as θⱼ := θⱼ - α(∂J/∂θⱼ), moving downhill on the loss surface. The learning rate α controls step size."
      },
      {
        "id": "cs402-q19",
        "type": "multiple_choice",
        "prompt": "What is the purpose of the sigmoid function in logistic regression?",
        "options": [
          "To handle missing values",
          "To map linear predictions to probability range [0,1]",
          "To increase model accuracy",
          "To speed up training"
        ],
        "correctAnswer": 1,
        "explanation": "The sigmoid function σ(z) = 1/(1+e^(-z)) squashes any real number to (0,1), allowing interpretation as probability. Logistic regression uses h(x) = σ(θᵀx)."
      },
      {
        "id": "cs402-q20",
        "type": "multiple_choice",
        "prompt": "What is L2 regularization (Ridge) also known as?",
        "options": [
          "Lasso",
          "Feature selection",
          "Dropout",
          "Weight decay"
        ],
        "correctAnswer": 3,
        "explanation": "L2 regularization adds λΣθⱼ² to the loss, penalizing large weights. This is equivalent to weight decay in gradient descent: weights shrink toward zero at each step."
      }
    ]
  },
  {
    "id": "cs402-quiz-2-2",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Gradient Descent & Regularization",
    "questions": [
      {
        "id": "cs402-q21",
        "type": "multiple_choice",
        "prompt": "Your gradient descent is oscillating and not converging. What should you do?",
        "options": [
          "Use fewer training iterations",
          "Add more features",
          "Decrease the learning rate",
          "Increase the learning rate"
        ],
        "correctAnswer": 2,
        "explanation": "Oscillation indicates the learning rate is too high, causing overshooting. Decreasing α makes smaller, more stable steps toward the minimum."
      },
      {
        "id": "cs402-q22",
        "type": "multiple_choice",
        "prompt": "When would you use L1 (Lasso) regularization over L2 (Ridge)?",
        "options": [
          "When you have no multicollinearity",
          "When you want feature selection by driving some weights to exactly zero",
          "When you want all weights to be small but non-zero",
          "When all features are important"
        ],
        "correctAnswer": 1,
        "explanation": "L1 regularization (λΣ|θⱼ|) creates sparse solutions with many exact zeros due to its non-differentiable corners, effectively performing feature selection. L2 only shrinks weights."
      },
      {
        "id": "cs402-q23",
        "type": "multiple_choice",
        "prompt": "You are using batch gradient descent and training is very slow. What could speed it up?",
        "options": [
          "Use mini-batch or stochastic gradient descent",
          "Increase the number of features",
          "Decrease the learning rate",
          "Use a more complex model"
        ],
        "correctAnswer": 0,
        "explanation": "Batch GD computes gradients over the entire dataset, which is slow for large datasets. Mini-batch GD uses small batches for faster, noisier updates that often converge faster in practice."
      },
      {
        "id": "cs402-q24",
        "type": "multiple_choice",
        "prompt": "For multiclass logistic regression, which approach uses multiple binary classifiers?",
        "options": [
          "Binary cross-entropy",
          "One-vs-All (OvA)",
          "Softmax regression",
          "Sigmoid activation"
        ],
        "correctAnswer": 1,
        "explanation": "One-vs-All trains K binary classifiers (one per class vs rest), then selects the class with highest confidence. Softmax regression directly models all K classes in one model."
      },
      {
        "id": "cs402-q25",
        "type": "multiple_choice",
        "prompt": "What is the closed-form solution for linear regression called?",
        "options": [
          "Newton's method",
          "Gradient descent",
          "Backpropagation",
          "Normal equation: θ = (XᵀX)⁻¹Xᵀy"
        ],
        "correctAnswer": 3,
        "explanation": "The normal equation provides an analytical solution by setting gradient to zero and solving. It works well for small n but is O(n³) due to matrix inversion, while gradient descent is O(kn²) for k iterations."
      }
    ]
  },
  {
    "id": "cs402-quiz-2-3",
    "subjectId": "cs402",
    "topicId": "cs402-topic-2",
    "title": "Advanced Regression Theory",
    "questions": [
      {
        "id": "cs402-q26",
        "type": "multiple_choice",
        "prompt": "What is the probabilistic interpretation of linear regression?",
        "options": [
          "Bayesian inference with conjugate prior",
          "Minimizing KL divergence",
          "Maximum likelihood estimation assuming Gaussian noise: y = θᵀx + ε where ε ~ N(0,σ²)",
          "Maximum a posteriori estimation with uniform prior"
        ],
        "correctAnswer": 2,
        "explanation": "Under Gaussian noise assumptions, minimizing MSE is equivalent to maximum likelihood estimation. This explains why squared loss is natural for regression."
      },
      {
        "id": "cs402-q27",
        "type": "multiple_choice",
        "prompt": "What is the gradient of logistic regression loss with respect to θⱼ?",
        "options": [
          "Σ(h(x⁽ⁱ⁾) - y⁽ⁱ⁾)²",
          "(1/m)Σ(y⁽ⁱ⁾ - h(x⁽ⁱ⁾))xⱼ⁽ⁱ⁾",
          "(1/m)Σ(h(x⁽ⁱ⁾) - y⁽ⁱ⁾)xⱼ⁽ⁱ⁾",
          "-Σy⁽ⁱ⁾log(h(x⁽ⁱ⁾))"
        ],
        "correctAnswer": 2,
        "explanation": "Remarkably, logistic regression has the same gradient form as linear regression, even though h(x) = σ(θᵀx) is nonlinear. This comes from the cancellation between sigmoid derivative and log-likelihood."
      },
      {
        "id": "cs402-q28",
        "type": "multiple_choice",
        "prompt": "What does the condition number of XᵀX indicate?",
        "options": [
          "The number of features",
          "The optimal learning rate",
          "The rank of the design matrix",
          "Numerical stability of the normal equation; high condition number indicates ill-conditioning"
        ],
        "correctAnswer": 3,
        "explanation": "Condition number κ(XᵀX) = λ_max/λ_min measures sensitivity to numerical errors. High κ (near-singular matrix) makes (XᵀX)⁻¹ unstable. Regularization improves conditioning by adding λI to XᵀX."
      },
      {
        "id": "cs402-q29",
        "type": "multiple_choice",
        "prompt": "What is the relationship between ridge regression and MAP estimation?",
        "options": [
          "Ridge is MAP with Laplace prior",
          "Ridge is MAP with Gaussian prior on weights: p(θ) ~ N(0, σ²I)",
          "Ridge is unrelated to Bayesian inference",
          "Ridge is maximum likelihood estimation"
        ],
        "correctAnswer": 1,
        "explanation": "L2 regularization emerges naturally from Bayesian inference with Gaussian prior on weights. The penalty term λΣθⱼ² corresponds to -log p(θ) for a Gaussian prior."
      },
      {
        "id": "cs402-q30",
        "type": "multiple_choice",
        "prompt": "Why is logistic regression a convex optimization problem?",
        "options": [
          "The negative log-likelihood loss is convex, guaranteeing global minimum",
          "It uses linear features",
          "The sigmoid function is convex",
          "It always converges in one iteration"
        ],
        "correctAnswer": 0,
        "explanation": "The negative log-likelihood J(θ) = -Σ[y log h(x) + (1-y)log(1-h(x))] is convex in θ. Convexity means any local minimum is global, so gradient descent is guaranteed to find the optimal solution."
      }
    ]
  }
]
