[
  {
    "id": "cs402-quiz-5-1",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "CNNs & Computer Vision",
    "questions": [
      {
        "id": "cs402-q61",
        "type": "multiple_choice",
        "prompt": "What is a Convolutional Neural Network (CNN) primarily designed for?",
        "options": [
          "Processing grid-like data such as images",
          "Processing sequential data like text",
          "Processing tabular data",
          "Processing graph-structured data"
        ],
        "correctAnswer": "Processing grid-like data such as images",
        "explanation": "CNNs use convolutional layers with local connectivity and weight sharing, making them ideal for grid-like data (images, videos) where spatial relationships matter."
      },
      {
        "id": "cs402-q62",
        "type": "multiple_choice",
        "prompt": "What does a convolutional layer learn?",
        "options": [
          "Filters (kernels) that detect local patterns in the input",
          "Global statistics of the entire image",
          "Individual pixel values",
          "Image metadata"
        ],
        "correctAnswer": "Filters (kernels) that detect local patterns in the input",
        "explanation": "Each filter in a convolutional layer learns to detect specific local patterns (edges, textures, shapes). Early layers detect simple patterns, deeper layers detect complex patterns."
      },
      {
        "id": "cs402-q63",
        "type": "multiple_choice",
        "prompt": "What is the purpose of pooling layers in CNNs?",
        "options": [
          "Reduce spatial dimensions while retaining important features",
          "Increase model capacity",
          "Add non-linearity",
          "Normalize activations"
        ],
        "correctAnswer": "Reduce spatial dimensions while retaining important features",
        "explanation": "Pooling layers downsample feature maps, reducing computation and providing some translation invariance. Max pooling and average pooling are common strategies."
      },
      {
        "id": "cs402-q64",
        "type": "multiple_choice",
        "prompt": "What architectural pattern do ResNet and other modern CNNs use to train very deep networks?",
        "options": [
          "Skip/residual connections that bypass layers",
          "Larger learning rates",
          "Smaller batch sizes",
          "Removing batch normalization"
        ],
        "correctAnswer": "Skip/residual connections that bypass layers",
        "explanation": "Residual connections (skip connections) allow gradients to flow directly through the network, solving the degradation problem and enabling training of networks with 100+ layers."
      },
      {
        "id": "cs402-q65",
        "type": "multiple_choice",
        "prompt": "What is transfer learning in the context of CNNs?",
        "options": [
          "Using a pre-trained model and fine-tuning it for a new task",
          "Transferring data between training and test sets",
          "Converting between different model architectures",
          "Transferring weights randomly"
        ],
        "correctAnswer": "Using a pre-trained model and fine-tuning it for a new task",
        "explanation": "Transfer learning leverages models pre-trained on large datasets (e.g., ImageNet) and fine-tunes them for specific tasks, dramatically reducing training time and data requirements."
      }
    ]
  },
  {
    "id": "cs402-quiz-5-2",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "RNNs & Sequence Models",
    "questions": [
      {
        "id": "cs402-q66",
        "type": "multiple_choice",
        "prompt": "What is a Recurrent Neural Network (RNN) best suited for?",
        "options": [
          "Sequential data with temporal dependencies",
          "Static images",
          "Tabular data",
          "Graph-structured data"
        ],
        "correctAnswer": "Sequential data with temporal dependencies",
        "explanation": "RNNs have recurrent connections that maintain hidden state across time steps, making them ideal for sequences like text, speech, and time series where order matters."
      },
      {
        "id": "cs402-q67",
        "type": "multiple_choice",
        "prompt": "What problem do LSTM and GRU architectures solve?",
        "options": [
          "Vanishing gradients in long sequences",
          "Overfitting",
          "Slow training speed",
          "High memory usage"
        ],
        "correctAnswer": "Vanishing gradients in long sequences",
        "explanation": "LSTMs and GRUs use gating mechanisms to control information flow, allowing them to maintain long-term dependencies without suffering from vanishing gradients like vanilla RNNs."
      },
      {
        "id": "cs402-q68",
        "type": "multiple_choice",
        "prompt": "What are the three gates in an LSTM cell?",
        "options": [
          "Forget gate, input gate, output gate",
          "Read gate, write gate, erase gate",
          "Entry gate, exit gate, memory gate",
          "Start gate, middle gate, end gate"
        ],
        "correctAnswer": "Forget gate, input gate, output gate",
        "explanation": "LSTM cells use forget gate (what to remove from cell state), input gate (what new information to add), and output gate (what to output based on cell state)."
      },
      {
        "id": "cs402-q69",
        "type": "multiple_choice",
        "prompt": "What key innovation do Transformers use instead of recurrence?",
        "options": [
          "Self-attention mechanism",
          "Larger batch sizes",
          "More layers",
          "Different activation functions"
        ],
        "correctAnswer": "Self-attention mechanism",
        "explanation": "Transformers use self-attention to compute relationships between all positions in parallel, eliminating sequential dependencies and enabling much better parallelization than RNNs."
      },
      {
        "id": "cs402-q70",
        "type": "multiple_choice",
        "prompt": "Why are Transformers more efficient to train than RNNs?",
        "options": [
          "They process all sequence positions in parallel rather than sequentially",
          "They have fewer parameters",
          "They use simpler operations",
          "They require less memory"
        ],
        "correctAnswer": "They process all sequence positions in parallel rather than sequentially",
        "explanation": "Unlike RNNs that process sequences step-by-step, Transformers compute attention over all positions simultaneously, fully utilizing parallel computing hardware like GPUs."
      }
    ]
  },
  {
    "id": "cs402-quiz-5-3",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Transformers & Transfer Learning",
    "questions": [
      {
        "id": "cs402-q71",
        "type": "multiple_choice",
        "prompt": "What is the formula for scaled dot-product attention in Transformers?",
        "options": [
          "Attention(Q,K,V) = softmax(QK^T/√d_k)V",
          "Attention(Q,K,V) = QKV",
          "Attention(Q,K,V) = softmax(Q+K)V",
          "Attention(Q,K,V) = sigmoid(QK^T)V"
        ],
        "correctAnswer": "Attention(Q,K,V) = softmax(QK^T/√d_k)V",
        "explanation": "Scaled dot-product attention computes similarity between queries and keys (QK^T), scales by √d_k for stability, applies softmax, and uses the result to weight values V."
      },
      {
        "id": "cs402-q72",
        "type": "multiple_choice",
        "prompt": "What does the positional encoding in Transformers provide?",
        "options": [
          "Information about the position of tokens in the sequence",
          "Better gradient flow",
          "Faster training",
          "Regularization"
        ],
        "correctAnswer": "Information about the position of tokens in the sequence",
        "explanation": "Since Transformers have no inherent notion of sequence order, positional encodings (often sinusoidal) are added to embeddings to inject positional information."
      },
      {
        "id": "cs402-q73",
        "type": "multiple_choice",
        "prompt": "What training strategy involves pre-training on unlabeled data then fine-tuning on labeled data?",
        "options": [
          "Transfer learning / Pre-training and fine-tuning",
          "Supervised learning",
          "Reinforcement learning",
          "Active learning"
        ],
        "correctAnswer": "Transfer learning / Pre-training and fine-tuning",
        "explanation": "Modern NLP models like BERT and GPT use pre-training on large unlabeled corpora (self-supervised) followed by fine-tuning on specific tasks with smaller labeled datasets."
      },
      {
        "id": "cs402-q74",
        "type": "multiple_choice",
        "prompt": "What is the main difference between BERT and GPT architectures?",
        "options": [
          "BERT is bidirectional (encoder), GPT is unidirectional (decoder)",
          "BERT uses RNNs, GPT uses Transformers",
          "BERT is smaller than GPT",
          "BERT is faster than GPT"
        ],
        "correctAnswer": "BERT is bidirectional (encoder), GPT is unidirectional (decoder)",
        "explanation": "BERT uses the Transformer encoder with bidirectional context (masked language modeling), while GPT uses the decoder with causal (left-to-right) attention for autoregressive generation."
      },
      {
        "id": "cs402-q75",
        "type": "multiple_choice",
        "prompt": "What technique allows models like GPT-3 to perform tasks with few examples?",
        "options": [
          "In-context learning / few-shot learning",
          "Transfer learning",
          "Meta-learning",
          "Curriculum learning"
        ],
        "correctAnswer": "In-context learning / few-shot learning",
        "explanation": "Large language models can perform new tasks by providing examples in the prompt (in-context learning), without updating weights, demonstrating emergent few-shot learning capabilities."
      }
    ]
  }
]
