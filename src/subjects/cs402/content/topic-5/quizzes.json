[
  {
    "id": "cs402-quiz-5-1",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "CNNs & Computer Vision",
    "questions": [
      {
        "id": "cs402-q61",
        "type": "multiple_choice",
        "prompt": "What is a Convolutional Neural Network (CNN) primarily designed for?",
        "options": [
          "Processing sequential data like text",
          "Processing tabular data",
          "Processing grid-like data such as images",
          "Processing graph-structured data"
        ],
        "correctAnswer": 2,
        "explanation": "CNNs use convolutional layers with local connectivity and weight sharing, making them ideal for grid-like data (images, videos) where spatial relationships matter."
      },
      {
        "id": "cs402-q62",
        "type": "multiple_choice",
        "prompt": "What does a convolutional layer learn?",
        "options": [
          "Individual pixel values",
          "Filters (kernels) that detect local patterns in the input",
          "Image metadata",
          "Global statistics of the entire image"
        ],
        "correctAnswer": 1,
        "explanation": "Each filter in a convolutional layer learns to detect specific local patterns (edges, textures, shapes). Early layers detect simple patterns, deeper layers detect complex patterns."
      },
      {
        "id": "cs402-q63",
        "type": "multiple_choice",
        "prompt": "What is the purpose of pooling layers in CNNs?",
        "options": [
          "Increase model capacity",
          "Reduce spatial dimensions while retaining important features",
          "Add non-linearity",
          "Normalize activations"
        ],
        "correctAnswer": 1,
        "explanation": "Pooling layers downsample feature maps, reducing computation and providing some translation invariance. Max pooling and average pooling are common strategies."
      },
      {
        "id": "cs402-q64",
        "type": "multiple_choice",
        "prompt": "What architectural pattern do ResNet and other modern CNNs use to train very deep networks?",
        "options": [
          "Smaller batch sizes",
          "Removing batch normalization",
          "Skip/residual connections that bypass layers",
          "Larger learning rates"
        ],
        "correctAnswer": 2,
        "explanation": "Residual connections (skip connections) allow gradients to flow directly through the network, solving the degradation problem and enabling training of networks with 100+ layers."
      },
      {
        "id": "cs402-q65",
        "type": "multiple_choice",
        "prompt": "What is transfer learning in the context of CNNs?",
        "options": [
          "Using a pre-trained model and fine-tuning it for a new task",
          "Transferring data between training and test sets",
          "Converting between different model architectures",
          "Transferring weights randomly"
        ],
        "correctAnswer": 0,
        "explanation": "Transfer learning leverages models pre-trained on large datasets (e.g., ImageNet) and fine-tunes them for specific tasks, dramatically reducing training time and data requirements."
      }
    ]
  },
  {
    "id": "cs402-quiz-5-2",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "RNNs & Sequence Models",
    "questions": [
      {
        "id": "cs402-q66",
        "type": "multiple_choice",
        "prompt": "What is a Recurrent Neural Network (RNN) best suited for?",
        "options": [
          "Tabular data",
          "Sequential data with temporal dependencies",
          "Graph-structured data",
          "Static images"
        ],
        "correctAnswer": 1,
        "explanation": "RNNs have recurrent connections that maintain hidden state across time steps, making them ideal for sequences like text, speech, and time series where order matters."
      },
      {
        "id": "cs402-q67",
        "type": "multiple_choice",
        "prompt": "What problem do LSTM and GRU architectures solve?",
        "options": [
          "Slow training speed",
          "High memory usage",
          "Overfitting",
          "Vanishing gradients in long sequences"
        ],
        "correctAnswer": 3,
        "explanation": "LSTMs and GRUs use gating mechanisms to control information flow, allowing them to maintain long-term dependencies without suffering from vanishing gradients like vanilla RNNs."
      },
      {
        "id": "cs402-q68",
        "type": "multiple_choice",
        "prompt": "What are the three gates in an LSTM cell?",
        "options": [
          "Start gate, middle gate, end gate",
          "Forget gate, input gate, output gate",
          "Entry gate, exit gate, memory gate",
          "Read gate, write gate, erase gate"
        ],
        "correctAnswer": 1,
        "explanation": "LSTM cells use forget gate (what to remove from cell state), input gate (what new information to add), and output gate (what to output based on cell state)."
      },
      {
        "id": "cs402-q69",
        "type": "multiple_choice",
        "prompt": "What key innovation do Transformers use instead of recurrence?",
        "options": [
          "Self-attention mechanism",
          "Larger batch sizes",
          "More layers",
          "Different activation functions"
        ],
        "correctAnswer": 0,
        "explanation": "Transformers use self-attention to compute relationships between all positions in parallel, eliminating sequential dependencies and enabling much better parallelization than RNNs."
      },
      {
        "id": "cs402-q70",
        "type": "multiple_choice",
        "prompt": "Why are Transformers more efficient to train than RNNs?",
        "options": [
          "They have fewer parameters",
          "They use simpler operations",
          "They require less memory",
          "They process all sequence positions in parallel rather than sequentially"
        ],
        "correctAnswer": 3,
        "explanation": "Unlike RNNs that process sequences step-by-step, Transformers compute attention over all positions simultaneously, fully utilizing parallel computing hardware like GPUs."
      }
    ]
  },
  {
    "id": "cs402-quiz-5-3",
    "subjectId": "cs402",
    "topicId": "cs402-topic-5",
    "title": "Transformers & Transfer Learning",
    "questions": [
      {
        "id": "cs402-q71",
        "type": "multiple_choice",
        "prompt": "What is the formula for scaled dot-product attention in Transformers?",
        "options": [
          "Attention(Q,K,V) = softmax(Q+K)V",
          "Attention(Q,K,V) = sigmoid(QK^T)V",
          "Attention(Q,K,V) = QKV",
          "Attention(Q,K,V) = softmax(QK^T/√d_k)V"
        ],
        "correctAnswer": 3,
        "explanation": "Scaled dot-product attention computes similarity between queries and keys (QK^T), scales by √d_k for stability, applies softmax, and uses the result to weight values V."
      },
      {
        "id": "cs402-q72",
        "type": "multiple_choice",
        "prompt": "What does the positional encoding in Transformers provide?",
        "options": [
          "Better gradient flow",
          "Information about the position of tokens in the sequence",
          "Faster training",
          "Regularization"
        ],
        "correctAnswer": 1,
        "explanation": "Since Transformers have no inherent notion of sequence order, positional encodings (often sinusoidal) are added to embeddings to inject positional information."
      },
      {
        "id": "cs402-q73",
        "type": "multiple_choice",
        "prompt": "What training strategy involves pre-training on unlabeled data then fine-tuning on labeled data?",
        "options": [
          "Transfer learning / Pre-training and fine-tuning",
          "Supervised learning",
          "Reinforcement learning",
          "Active learning"
        ],
        "correctAnswer": 0,
        "explanation": "Modern NLP models like BERT and GPT use pre-training on large unlabeled corpora (self-supervised) followed by fine-tuning on specific tasks with smaller labeled datasets."
      },
      {
        "id": "cs402-q74",
        "type": "multiple_choice",
        "prompt": "What is the main difference between BERT and GPT architectures?",
        "options": [
          "BERT is smaller than GPT",
          "BERT uses RNNs, GPT uses Transformers",
          "BERT is faster than GPT",
          "BERT is bidirectional (encoder), GPT is unidirectional (decoder)"
        ],
        "correctAnswer": 3,
        "explanation": "BERT uses the Transformer encoder with bidirectional context (masked language modeling), while GPT uses the decoder with causal (left-to-right) attention for autoregressive generation."
      },
      {
        "id": "cs402-q75",
        "type": "multiple_choice",
        "prompt": "What technique allows models like GPT-3 to perform tasks with few examples?",
        "options": [
          "Transfer learning",
          "Curriculum learning",
          "In-context learning / few-shot learning",
          "Meta-learning"
        ],
        "correctAnswer": 2,
        "explanation": "Large language models can perform new tasks by providing examples in the prompt (in-context learning), without updating weights, demonstrating emergent few-shot learning capabilities."
      }
    ]
  }
]
