[
  {
    "id": "cs304-t1-ex01",
    "subjectId": "cs304",
    "topicId": "cs304-topic-1",
    "title": "Simple Tokenizer for Arithmetic",
    "difficulty": 1,
    "description": "Implement a basic tokenizer that recognizes numbers, operators (+, -, *, /), and parentheses in arithmetic expressions.",
    "starterCode": "def tokenize(input_str):\n    \"\"\"\n    Tokenize an arithmetic expression.\n    Return a list of tuples: [(token_type, value), ...]\n    Types: NUMBER, PLUS, MINUS, MULTIPLY, DIVIDE, LPAREN, RPAREN\n    \"\"\"\n    # Your code here\n    pass\n\n# Test\nprint(tokenize(\"12 + 34 * (5 - 6)\"))",
    "solution": "def tokenize(input_str):\n    \"\"\"\n    Tokenize an arithmetic expression.\n    Return a list of tuples: [(token_type, value), ...]\n    \"\"\"\n    tokens = []\n    i = 0\n\n    while i < len(input_str):\n        if input_str[i].isspace():\n            i += 1\n            continue\n\n        if input_str[i].isdigit():\n            num = ''\n            while i < len(input_str) and input_str[i].isdigit():\n                num += input_str[i]\n                i += 1\n            tokens.append(('NUMBER', int(num)))\n        elif input_str[i] == '+':\n            tokens.append(('PLUS', '+'))\n            i += 1\n        elif input_str[i] == '-':\n            tokens.append(('MINUS', '-'))\n            i += 1\n        elif input_str[i] == '*':\n            tokens.append(('MULTIPLY', '*'))\n            i += 1\n        elif input_str[i] == '/':\n            tokens.append(('DIVIDE', '/'))\n            i += 1\n        elif input_str[i] == '(':\n            tokens.append(('LPAREN', '('))\n            i += 1\n        elif input_str[i] == ')':\n            tokens.append(('RPAREN', ')'))\n            i += 1\n        else:\n            raise ValueError(f\"Unknown character: {input_str[i]}\")\n\n    return tokens\n\n# Test\nprint(tokenize(\"12 + 34 * (5 - 6)\"))",
    "testCases": [
      {
        "input": "42",
        "expectedOutput": "[('NUMBER', 42)]",
        "isHidden": false,
        "description": "Single number"
      },
      {
        "input": "1 + 2",
        "expectedOutput": "[('NUMBER', 1), ('PLUS', '+'), ('NUMBER', 2)]",
        "isHidden": false,
        "description": "Simple addition"
      },
      {
        "input": "(10 - 5) * 3",
        "expectedOutput": "[('LPAREN', '('), ('NUMBER', 10), ('MINUS', '-'), ('NUMBER', 5), ('RPAREN', ')'), ('MULTIPLY', '*'), ('NUMBER', 3)]",
        "isHidden": true,
        "description": "Complex expression"
      }
    ],
    "hints": [
      "Process the input character by character",
      "Use a while loop with an index variable",
      "Handle multi-digit numbers by accumulating digits"
    ],
    "language": "python"
  },
  {
    "id": "cs304-t1-ex02",
    "subjectId": "cs304",
    "topicId": "cs304-topic-1",
    "title": "Identifier Recognition",
    "difficulty": 2,
    "description": "Extend the tokenizer to recognize identifiers (variable names) that start with a letter or underscore, followed by letters, digits, or underscores.",
    "starterCode": "def tokenize_with_identifiers(input_str):\n    \"\"\"\n    Tokenize input recognizing numbers, operators, and identifiers.\n    Return list of tuples: [(token_type, value), ...]\n    Types: NUMBER, IDENTIFIER, PLUS, MINUS, MULTIPLY, DIVIDE, ASSIGN\n    \"\"\"\n    # Your code here\n    pass\n\n# Test\nprint(tokenize_with_identifiers(\"x = 10 + y2\"))",
    "solution": "def tokenize_with_identifiers(input_str):\n    \"\"\"\n    Tokenize input recognizing numbers, operators, and identifiers.\n    \"\"\"\n    tokens = []\n    i = 0\n\n    while i < len(input_str):\n        if input_str[i].isspace():\n            i += 1\n            continue\n\n        if input_str[i].isdigit():\n            num = ''\n            while i < len(input_str) and input_str[i].isdigit():\n                num += input_str[i]\n                i += 1\n            tokens.append(('NUMBER', int(num)))\n        elif input_str[i].isalpha() or input_str[i] == '_':\n            identifier = ''\n            while i < len(input_str) and (input_str[i].isalnum() or input_str[i] == '_'):\n                identifier += input_str[i]\n                i += 1\n            tokens.append(('IDENTIFIER', identifier))\n        elif input_str[i] == '=':\n            tokens.append(('ASSIGN', '='))\n            i += 1\n        elif input_str[i] == '+':\n            tokens.append(('PLUS', '+'))\n            i += 1\n        elif input_str[i] == '-':\n            tokens.append(('MINUS', '-'))\n            i += 1\n        elif input_str[i] == '*':\n            tokens.append(('MULTIPLY', '*'))\n            i += 1\n        elif input_str[i] == '/':\n            tokens.append(('DIVIDE', '/'))\n            i += 1\n        else:\n            raise ValueError(f\"Unknown character: {input_str[i]}\")\n\n    return tokens\n\n# Test\nprint(tokenize_with_identifiers(\"x = 10 + y2\"))",
    "testCases": [
      {
        "input": "abc",
        "expectedOutput": "[('IDENTIFIER', 'abc')]",
        "isHidden": false,
        "description": "Simple identifier"
      },
      {
        "input": "_var123",
        "expectedOutput": "[('IDENTIFIER', '_var123')]",
        "isHidden": false,
        "description": "Identifier with underscore and digits"
      },
      {
        "input": "x = 10 + y2",
        "expectedOutput": "[('IDENTIFIER', 'x'), ('ASSIGN', '='), ('NUMBER', 10), ('PLUS', '+'), ('IDENTIFIER', 'y2')]",
        "isHidden": true,
        "description": "Assignment expression"
      }
    ],
    "hints": [
      "Check if character is a letter or underscore for identifier start",
      "Use isalpha() and isalnum() methods",
      "Identifiers can contain digits but cannot start with them"
    ],
    "language": "python"
  },
  {
    "id": "cs304-t1-ex03",
    "subjectId": "cs304",
    "topicId": "cs304-topic-1",
    "title": "Keyword Recognition",
    "difficulty": 2,
    "description": "Implement keyword recognition by distinguishing reserved words (if, else, while, return) from regular identifiers.",
    "starterCode": "KEYWORDS = {'if', 'else', 'while', 'return', 'def', 'class'}\n\ndef tokenize_with_keywords(input_str):\n    \"\"\"\n    Tokenize input distinguishing keywords from identifiers.\n    Types: KEYWORD, IDENTIFIER, NUMBER, operators\n    \"\"\"\n    # Your code here\n    pass\n\n# Test\nprint(tokenize_with_keywords(\"if x == 10 return y\"))",
    "solution": "KEYWORDS = {'if', 'else', 'while', 'return', 'def', 'class'}\n\ndef tokenize_with_keywords(input_str):\n    \"\"\"\n    Tokenize input distinguishing keywords from identifiers.\n    \"\"\"\n    tokens = []\n    i = 0\n\n    while i < len(input_str):\n        if input_str[i].isspace():\n            i += 1\n            continue\n\n        if input_str[i].isdigit():\n            num = ''\n            while i < len(input_str) and input_str[i].isdigit():\n                num += input_str[i]\n                i += 1\n            tokens.append(('NUMBER', int(num)))\n        elif input_str[i].isalpha() or input_str[i] == '_':\n            word = ''\n            while i < len(input_str) and (input_str[i].isalnum() or input_str[i] == '_'):\n                word += input_str[i]\n                i += 1\n            if word in KEYWORDS:\n                tokens.append(('KEYWORD', word))\n            else:\n                tokens.append(('IDENTIFIER', word))\n        elif input_str[i:i+2] == '==':\n            tokens.append(('EQUALS', '=='))\n            i += 2\n        elif input_str[i] == '=':\n            tokens.append(('ASSIGN', '='))\n            i += 1\n        elif input_str[i] == '+':\n            tokens.append(('PLUS', '+'))\n            i += 1\n        elif input_str[i] == '-':\n            tokens.append(('MINUS', '-'))\n            i += 1\n        elif input_str[i] == '*':\n            tokens.append(('MULTIPLY', '*'))\n            i += 1\n        elif input_str[i] == '/':\n            tokens.append(('DIVIDE', '/'))\n            i += 1\n        else:\n            raise ValueError(f\"Unknown character: {input_str[i]}\")\n\n    return tokens\n\n# Test\nprint(tokenize_with_keywords(\"if x == 10 return y\"))",
    "testCases": [
      {
        "input": "if",
        "expectedOutput": "[('KEYWORD', 'if')]",
        "isHidden": false,
        "description": "Single keyword"
      },
      {
        "input": "myvar",
        "expectedOutput": "[('IDENTIFIER', 'myvar')]",
        "isHidden": false,
        "description": "Identifier that is not a keyword"
      },
      {
        "input": "if x == 10 return y",
        "expectedOutput": "[('KEYWORD', 'if'), ('IDENTIFIER', 'x'), ('EQUALS', '=='), ('NUMBER', 10), ('KEYWORD', 'return'), ('IDENTIFIER', 'y')]",
        "isHidden": true,
        "description": "Conditional return statement"
      }
    ],
    "hints": [
      "First tokenize as identifier, then check if it is in the KEYWORDS set",
      "Keywords are context-free in most languages",
      "Handle multi-character operators like == before single character ones"
    ],
    "language": "python"
  },
  {
    "id": "cs304-t1-ex04",
    "subjectId": "cs304",
    "topicId": "cs304-topic-1",
    "title": "Regular Expression Matcher",
    "difficulty": 3,
    "description": "Implement a simple regular expression matcher that supports literals, * (zero or more), and . (any character).",
    "starterCode": "def regex_match(pattern, text):\n    \"\"\"\n    Return True if pattern matches text.\n    Supports: literals, . (any char), * (zero or more of previous)\n    \"\"\"\n    # Your code here\n    pass\n\n# Test\nprint(regex_match(\"a*b\", \"aaab\"))  # True\nprint(regex_match(\"a.c\", \"abc\"))   # True\nprint(regex_match(\".*\", \"hello\"))  # True",
    "solution": "def regex_match(pattern, text):\n    \"\"\"\n    Return True if pattern matches text.\n    Supports: literals, . (any char), * (zero or more of previous)\n    \"\"\"\n    if not pattern:\n        return not text\n\n    first_match = bool(text) and (pattern[0] == text[0] or pattern[0] == '.')\n\n    if len(pattern) >= 2 and pattern[1] == '*':\n        # Two possibilities: skip the * pattern, or use it\n        return (regex_match(pattern[2:], text) or\n                (first_match and regex_match(pattern, text[1:])))\n    else:\n        return first_match and regex_match(pattern[1:], text[1:])\n\n# Test\nprint(regex_match(\"a*b\", \"aaab\"))  # True\nprint(regex_match(\"a.c\", \"abc\"))   # True\nprint(regex_match(\".*\", \"hello\"))  # True",
    "testCases": [
      {
        "input": "abc|abc",
        "expectedOutput": "True",
        "isHidden": false,
        "description": "Exact match"
      },
      {
        "input": "a*b|aaab",
        "expectedOutput": "True",
        "isHidden": false,
        "description": "Star quantifier"
      },
      {
        "input": ".*|hello",
        "expectedOutput": "True",
        "isHidden": false,
        "description": "Match anything"
      },
      {
        "input": "a.c|abc",
        "expectedOutput": "True",
        "isHidden": true,
        "description": "Dot wildcard"
      },
      {
        "input": "a*b|b",
        "expectedOutput": "True",
        "isHidden": true,
        "description": "Star with zero matches"
      }
    ],
    "hints": [
      "Use recursion to handle the pattern matching",
      "Handle the * operator by trying both zero and one-or-more matches",
      "Check if the next character is * before consuming current character"
    ],
    "language": "python"
  },
  {
    "id": "cs304-t1-ex05",
    "subjectId": "cs304",
    "topicId": "cs304-topic-1",
    "title": "NFA State Representation",
    "difficulty": 3,
    "description": "Implement a Non-deterministic Finite Automaton (NFA) data structure with epsilon transitions.",
    "starterCode": "class NFA:\n    def __init__(self, states, alphabet, transitions, start, accept):\n        \"\"\"\n        states: set of state names\n        alphabet: set of input symbols\n        transitions: dict {(state, symbol): set of next states}\n        start: start state\n        accept: set of accept states\n        Use None for epsilon transitions\n        \"\"\"\n        # Your code here\n        pass\n\n    def epsilon_closure(self, states):\n        \"\"\"Return set of states reachable via epsilon transitions\"\"\"\n        # Your code here\n        pass\n\n# Test\nnfa = NFA({'q0', 'q1', 'q2'}, {'a', 'b'},\n          {('q0', 'a'): {'q1'}, ('q1', None): {'q2'}},\n          'q0', {'q2'})\nprint(nfa.epsilon_closure({'q1'}))  # Should include q2",
    "solution": "class NFA:\n    def __init__(self, states, alphabet, transitions, start, accept):\n        \"\"\"\n        states: set of state names\n        alphabet: set of input symbols\n        transitions: dict {(state, symbol): set of next states}\n        start: start state\n        accept: set of accept states\n        Use None for epsilon transitions\n        \"\"\"\n        self.states = states\n        self.alphabet = alphabet\n        self.transitions = transitions\n        self.start = start\n        self.accept = accept\n\n    def epsilon_closure(self, states):\n        \"\"\"Return set of states reachable via epsilon transitions\"\"\"\n        closure = set(states)\n        stack = list(states)\n\n        while stack:\n            state = stack.pop()\n            epsilon_states = self.transitions.get((state, None), set())\n            for next_state in epsilon_states:\n                if next_state not in closure:\n                    closure.add(next_state)\n                    stack.append(next_state)\n\n        return closure\n\n# Test\nnfa = NFA({'q0', 'q1', 'q2'}, {'a', 'b'},\n          {('q0', 'a'): {'q1'}, ('q1', None): {'q2'}},\n          'q0', {'q2'})\nprint(nfa.epsilon_closure({'q1'}))  # Should include q2",
    "testCases": [
      {
        "input": "{'q1'}",
        "expectedOutput": "{'q1', 'q2'}",
        "isHidden": false,
        "description": "Epsilon closure from q1"
      },
      {
        "input": "{'q0'}",
        "expectedOutput": "{'q0'}",
        "isHidden": false,
        "description": "No epsilon transitions from q0"
      },
      {
        "input": "{'q1', 'q0'}",
        "expectedOutput": "{'q0', 'q1', 'q2'}",
        "isHidden": true,
        "description": "Closure from multiple states"
      }
    ],
    "hints": [
      "Use a stack or queue to traverse epsilon transitions",
      "Keep track of visited states to avoid infinite loops",
      "Initialize closure with the input states"
    ],
    "language": "python"
  },
  {
    "id": "cs304-t1-ex06",
    "subjectId": "cs304",
    "topicId": "cs304-topic-1",
    "title": "NFA Simulation",
    "difficulty": 3,
    "description": "Implement the accepts method for an NFA that determines if a string is accepted.",
    "starterCode": "class NFA:\n    def __init__(self, states, alphabet, transitions, start, accept):\n        self.states = states\n        self.alphabet = alphabet\n        self.transitions = transitions\n        self.start = start\n        self.accept = accept\n\n    def epsilon_closure(self, states):\n        closure = set(states)\n        stack = list(states)\n        while stack:\n            state = stack.pop()\n            epsilon_states = self.transitions.get((state, None), set())\n            for next_state in epsilon_states:\n                if next_state not in closure:\n                    closure.add(next_state)\n                    stack.append(next_state)\n        return closure\n\n    def accepts(self, input_string):\n        \"\"\"Return True if NFA accepts the input string\"\"\"\n        # Your code here\n        pass\n\n# Test\nnfa = NFA({'q0', 'q1'}, {'a', 'b'},\n          {('q0', 'a'): {'q0', 'q1'}, ('q0', 'b'): {'q0'}},\n          'q0', {'q1'})\nprint(nfa.accepts(\"bba\"))  # True",
    "solution": "class NFA:\n    def __init__(self, states, alphabet, transitions, start, accept):\n        self.states = states\n        self.alphabet = alphabet\n        self.transitions = transitions\n        self.start = start\n        self.accept = accept\n\n    def epsilon_closure(self, states):\n        closure = set(states)\n        stack = list(states)\n        while stack:\n            state = stack.pop()\n            epsilon_states = self.transitions.get((state, None), set())\n            for next_state in epsilon_states:\n                if next_state not in closure:\n                    closure.add(next_state)\n                    stack.append(next_state)\n        return closure\n\n    def accepts(self, input_string):\n        \"\"\"Return True if NFA accepts the input string\"\"\"\n        current_states = self.epsilon_closure({self.start})\n\n        for symbol in input_string:\n            next_states = set()\n            for state in current_states:\n                next_states.update(self.transitions.get((state, symbol), set()))\n            current_states = self.epsilon_closure(next_states)\n\n        return bool(current_states & self.accept)\n\n# Test\nnfa = NFA({'q0', 'q1'}, {'a', 'b'},\n          {('q0', 'a'): {'q0', 'q1'}, ('q0', 'b'): {'q0'}},\n          'q0', {'q1'})\nprint(nfa.accepts(\"bba\"))  # True",
    "testCases": [
      {
        "input": "bba",
        "expectedOutput": "True",
        "isHidden": false,
        "description": "String ending with a"
      },
      {
        "input": "bbb",
        "expectedOutput": "False",
        "isHidden": false,
        "description": "String not ending with a"
      },
      {
        "input": "a",
        "expectedOutput": "True",
        "isHidden": true,
        "description": "Single character accepted"
      }
    ],
    "hints": [
      "Start with epsilon closure of the start state",
      "For each input symbol, compute all possible next states",
      "Check if any final state is in the accept states"
    ],
    "language": "python"
  },
  {
    "id": "cs304-t1-ex07",
    "subjectId": "cs304",
    "topicId": "cs304-topic-1",
    "title": "DFA Implementation",
    "difficulty": 2,
    "description": "Implement a Deterministic Finite Automaton (DFA) with a single transition function.",
    "starterCode": "class DFA:\n    def __init__(self, states, alphabet, transitions, start, accept):\n        \"\"\"\n        transitions: dict {(state, symbol): next_state}\n        Each (state, symbol) maps to exactly one state\n        \"\"\"\n        # Your code here\n        pass\n\n    def accepts(self, input_string):\n        \"\"\"Return True if DFA accepts the input string\"\"\"\n        # Your code here\n        pass\n\n# Test\ndfa = DFA({'q0', 'q1', 'q2'}, {'0', '1'},\n          {('q0', '0'): 'q0', ('q0', '1'): 'q1',\n           ('q1', '0'): 'q0', ('q1', '1'): 'q2',\n           ('q2', '0'): 'q2', ('q2', '1'): 'q2'},\n          'q0', {'q2'})\nprint(dfa.accepts(\"011\"))  # True",
    "solution": "class DFA:\n    def __init__(self, states, alphabet, transitions, start, accept):\n        \"\"\"\n        transitions: dict {(state, symbol): next_state}\n        Each (state, symbol) maps to exactly one state\n        \"\"\"\n        self.states = states\n        self.alphabet = alphabet\n        self.transitions = transitions\n        self.start = start\n        self.accept = accept\n\n    def accepts(self, input_string):\n        \"\"\"Return True if DFA accepts the input string\"\"\"\n        current_state = self.start\n\n        for symbol in input_string:\n            if (current_state, symbol) not in self.transitions:\n                return False\n            current_state = self.transitions[(current_state, symbol)]\n\n        return current_state in self.accept\n\n# Test\ndfa = DFA({'q0', 'q1', 'q2'}, {'0', '1'},\n          {('q0', '0'): 'q0', ('q0', '1'): 'q1',\n           ('q1', '0'): 'q0', ('q1', '1'): 'q2',\n           ('q2', '0'): 'q2', ('q2', '1'): 'q2'},\n          'q0', {'q2'})\nprint(dfa.accepts(\"011\"))  # True",
    "testCases": [
      {
        "input": "011",
        "expectedOutput": "True",
        "isHidden": false,
        "description": "Contains substring 11"
      },
      {
        "input": "010",
        "expectedOutput": "False",
        "isHidden": false,
        "description": "Does not contain 11"
      },
      {
        "input": "1111",
        "expectedOutput": "True",
        "isHidden": true,
        "description": "Multiple 11 substrings"
      }
    ],
    "hints": [
      "DFA has exactly one transition for each (state, symbol) pair",
      "Track a single current state as you process input",
      "Much simpler than NFA - no epsilon closures or multiple states"
    ],
    "language": "python"
  },
  {
    "id": "cs304-t1-ex08",
    "subjectId": "cs304",
    "topicId": "cs304-topic-1",
    "title": "String Literal Tokenization",
    "difficulty": 3,
    "description": "Implement tokenization of string literals with escape sequences (\\n, \\t, \\\\, \\\").",
    "starterCode": "def tokenize_string(input_str):\n    \"\"\"\n    Tokenize a string literal handling escape sequences.\n    Return the actual string value (with escapes processed).\n    Input includes the surrounding quotes.\n    \"\"\"\n    # Your code here\n    pass\n\n# Test\nprint(tokenize_string('\"hello\\nworld\"'))  # \"hello\nworld\"\nprint(tokenize_string('\"say \\\"hi\\\"\"'))   # \"say \"hi\"\"",
    "solution": "def tokenize_string(input_str):\n    \"\"\"\n    Tokenize a string literal handling escape sequences.\n    Return the actual string value (with escapes processed).\n    Input includes the surrounding quotes.\n    \"\"\"\n    if not input_str.startswith('\"') or not input_str.endswith('\"'):\n        raise ValueError(\"String must be enclosed in quotes\")\n\n    content = input_str[1:-1]  # Remove quotes\n    result = []\n    i = 0\n\n    while i < len(content):\n        if content[i] == '\\' and i + 1 < len(content):\n            next_char = content[i + 1]\n            if next_char == 'n':\n                result.append('\n')\n            elif next_char == 't':\n                result.append('\t')\n            elif next_char == '\\':\n                result.append('\\')\n            elif next_char == '\"':\n                result.append('\"')\n            else:\n                raise ValueError(f\"Unknown escape sequence: \\{next_char}\")\n            i += 2\n        else:\n            result.append(content[i])\n            i += 1\n\n    return ''.join(result)\n\n# Test\nprint(repr(tokenize_string('\"hello\\nworld\"')))  # \"hello\nworld\"\nprint(repr(tokenize_string('\"say \\\"hi\\\"\"')))   # \"say \"hi\"\"",
    "testCases": [
      {
        "input": "\"hello\"",
        "expectedOutput": "'hello'",
        "isHidden": false,
        "description": "Simple string"
      },
      {
        "input": "\"hello\\nworld\"",
        "expectedOutput": "'hello\\nworld'",
        "isHidden": false,
        "description": "String with newline escape"
      },
      {
        "input": "\"tab\\there\"",
        "expectedOutput": "'tab\\there'",
        "isHidden": true,
        "description": "String with tab escape"
      },
      {
        "input": "\"quote\\\"mark\"",
        "expectedOutput": "'quote\"mark'",
        "isHidden": true,
        "description": "String with escaped quote"
      }
    ],
    "hints": [
      "Process characters one at a time",
      "When you encounter backslash, check the next character",
      "Map escape sequences to their actual character values"
    ],
    "language": "python"
  },
  {
    "id": "cs304-t1-ex09",
    "subjectId": "cs304",
    "topicId": "cs304-topic-1",
    "title": "Comment Handling",
    "difficulty": 2,
    "description": "Extend tokenizer to skip single-line (//) and multi-line (/* */) comments.",
    "starterCode": "def tokenize_with_comments(input_str):\n    \"\"\"\n    Tokenize input while skipping comments.\n    // starts single-line comment (to end of line)\n    /* */ delimits multi-line comment\n    \"\"\"\n    # Your code here\n    pass\n\n# Test\ncode = '''x = 5 // assign value\ny = /* mid comment */ 10'''\nprint(tokenize_with_comments(code))",
    "solution": "def tokenize_with_comments(input_str):\n    \"\"\"\n    Tokenize input while skipping comments.\n    // starts single-line comment (to end of line)\n    /* */ delimits multi-line comment\n    \"\"\"\n    tokens = []\n    i = 0\n\n    while i < len(input_str):\n        # Skip whitespace\n        if input_str[i].isspace():\n            i += 1\n            continue\n\n        # Check for comments\n        if i + 1 < len(input_str) and input_str[i:i+2] == '//':\n            # Skip to end of line\n            while i < len(input_str) and input_str[i] != '\n':\n                i += 1\n            continue\n\n        if i + 1 < len(input_str) and input_str[i:i+2] == '/*':\n            # Skip to */\n            i += 2\n            while i + 1 < len(input_str) and input_str[i:i+2] != '*/':\n                i += 1\n            i += 2  # Skip */\n            continue\n\n        # Regular tokenization\n        if input_str[i].isdigit():\n            num = ''\n            while i < len(input_str) and input_str[i].isdigit():\n                num += input_str[i]\n                i += 1\n            tokens.append(('NUMBER', int(num)))\n        elif input_str[i].isalpha() or input_str[i] == '_':\n            identifier = ''\n            while i < len(input_str) and (input_str[i].isalnum() or input_str[i] == '_'):\n                identifier += input_str[i]\n                i += 1\n            tokens.append(('IDENTIFIER', identifier))\n        elif input_str[i] == '=':\n            tokens.append(('ASSIGN', '='))\n            i += 1\n        else:\n            i += 1\n\n    return tokens\n\n# Test\ncode = '''x = 5 // assign value\ny = /* mid comment */ 10'''\nprint(tokenize_with_comments(code))",
    "testCases": [
      {
        "input": "x = 5 // comment",
        "expectedOutput": "[('IDENTIFIER', 'x'), ('ASSIGN', '='), ('NUMBER', 5)]",
        "isHidden": false,
        "description": "Single-line comment"
      },
      {
        "input": "a = /* skip */ 1",
        "expectedOutput": "[('IDENTIFIER', 'a'), ('ASSIGN', '='), ('NUMBER', 1)]",
        "isHidden": false,
        "description": "Multi-line comment inline"
      },
      {
        "input": "x = 5\ny = 10",
        "expectedOutput": "[('IDENTIFIER', 'x'), ('ASSIGN', '='), ('NUMBER', 5), ('IDENTIFIER', 'y'), ('ASSIGN', '='), ('NUMBER', 10)]",
        "isHidden": true,
        "description": "Multiple lines no comments"
      }
    ],
    "hints": [
      "Check for comment start sequences before regular tokenization",
      "For //, skip until newline",
      "For /* */, skip until you find */",
      "Use continue to restart the loop after skipping comments"
    ],
    "language": "python"
  },
  {
    "id": "cs304-t1-ex10",
    "subjectId": "cs304",
    "topicId": "cs304-topic-1",
    "title": "Floating Point Numbers",
    "difficulty": 3,
    "description": "Implement tokenization of floating point numbers in scientific notation (e.g., 3.14, 2.5e-3, 1E+10).",
    "starterCode": "def tokenize_float(input_str):\n    \"\"\"\n    Tokenize floating point numbers including scientific notation.\n    Examples: 3.14, 2.5e-3, 1E+10, .5, 5.\n    Return ('FLOAT', float_value)\n    \"\"\"\n    # Your code here\n    pass\n\n# Test\nprint(tokenize_float(\"3.14\"))      # ('FLOAT', 3.14)\nprint(tokenize_float(\"2.5e-3\"))    # ('FLOAT', 0.0025)\nprint(tokenize_float(\"1E+10\"))     # ('FLOAT', 1e10)",
    "solution": "def tokenize_float(input_str):\n    \"\"\"\n    Tokenize floating point numbers including scientific notation.\n    Examples: 3.14, 2.5e-3, 1E+10, .5, 5.\n    Return ('FLOAT', float_value)\n    \"\"\"\n    import re\n\n    # Pattern: optional digits, optional decimal point with digits, optional exponent\n    pattern = r'^([0-9]*.?[0-9]+)([eE][+-]?[0-9]+)?$'\n    match = re.match(pattern, input_str.strip())\n\n    if match:\n        return ('FLOAT', float(input_str))\n    else:\n        raise ValueError(f\"Invalid float: {input_str}\")\n\n# Alternative without regex:\ndef tokenize_float_manual(input_str):\n    s = input_str.strip()\n    i = 0\n\n    # Integer part\n    int_part = ''\n    while i < len(s) and s[i].isdigit():\n        int_part += s[i]\n        i += 1\n\n    # Decimal part\n    dec_part = ''\n    if i < len(s) and s[i] == '.':\n        dec_part = '.'\n        i += 1\n        while i < len(s) and s[i].isdigit():\n            dec_part += s[i]\n            i += 1\n\n    # Must have either integer or decimal digits\n    if not int_part and dec_part == '.':\n        raise ValueError(\"Invalid float\")\n\n    # Exponent part\n    exp_part = ''\n    if i < len(s) and s[i] in 'eE':\n        exp_part = s[i]\n        i += 1\n        if i < len(s) and s[i] in '+-':\n            exp_part += s[i]\n            i += 1\n        while i < len(s) and s[i].isdigit():\n            exp_part += s[i]\n            i += 1\n\n    if i != len(s):\n        raise ValueError(\"Invalid float\")\n\n    return ('FLOAT', float(int_part + dec_part + exp_part))\n\n# Test\nprint(tokenize_float(\"3.14\"))      # ('FLOAT', 3.14)\nprint(tokenize_float(\"2.5e-3\"))    # ('FLOAT', 0.0025)\nprint(tokenize_float(\"1E+10\"))     # ('FLOAT', 1e10)",
    "testCases": [
      {
        "input": "3.14",
        "expectedOutput": "('FLOAT', 3.14)",
        "isHidden": false,
        "description": "Simple decimal"
      },
      {
        "input": "2.5e-3",
        "expectedOutput": "('FLOAT', 0.0025)",
        "isHidden": false,
        "description": "Scientific notation with negative exponent"
      },
      {
        "input": "1E+10",
        "expectedOutput": "('FLOAT', 10000000000.0)",
        "isHidden": true,
        "description": "Scientific notation with positive exponent"
      },
      {
        "input": ".5",
        "expectedOutput": "('FLOAT', 0.5)",
        "isHidden": true,
        "description": "Leading decimal point"
      }
    ],
    "hints": [
      "You can use Python's float() function to parse the final value",
      "Handle optional parts: integer, decimal, exponent",
      "Exponent can have optional + or - sign",
      "Consider using regex or manual character scanning"
    ],
    "language": "python"
  },
  {
    "id": "cs304-t1-ex11",
    "subjectId": "cs304",
    "topicId": "cs304-topic-1",
    "title": "NFA to DFA Conversion - Simple Case",
    "difficulty": 4,
    "description": "Implement the subset construction algorithm to convert a simple NFA to a DFA.",
    "starterCode": "def nfa_to_dfa(nfa):\n    \"\"\"\n    Convert NFA to DFA using subset construction.\n    nfa: dict with 'states', 'alphabet', 'transitions', 'start', 'accept'\n    Return equivalent DFA structure.\n    \"\"\"\n    # Your code here\n    pass\n\n# Test\nnfa = {\n    'states': {'q0', 'q1', 'q2'},\n    'alphabet': {'a', 'b'},\n    'transitions': {\n        ('q0', 'a'): {'q0', 'q1'},\n        ('q0', 'b'): {'q0'},\n        ('q1', 'b'): {'q2'}\n    },\n    'start': 'q0',\n    'accept': {'q2'}\n}\ndfa = nfa_to_dfa(nfa)\nprint(len(dfa['states']))  # Number of DFA states",
    "solution": "def nfa_to_dfa(nfa):\n    \"\"\"\n    Convert NFA to DFA using subset construction.\n    \"\"\"\n    def get_transitions(states, symbol):\n        \"\"\"Get all states reachable from states via symbol\"\"\"\n        result = set()\n        for state in states:\n            result.update(nfa['transitions'].get((state, symbol), set()))\n        return result\n\n    # Start with the start state as a set\n    start_set = frozenset({nfa['start']})\n    dfa_states = {start_set}\n    dfa_transitions = {}\n    unmarked = [start_set]\n\n    while unmarked:\n        current = unmarked.pop()\n\n        for symbol in nfa['alphabet']:\n            next_states = get_transitions(current, symbol)\n            next_set = frozenset(next_states)\n\n            if next_set:  # Only add non-empty sets\n                dfa_transitions[(current, symbol)] = next_set\n\n                if next_set not in dfa_states:\n                    dfa_states.add(next_set)\n                    unmarked.append(next_set)\n\n    # Accept states are those containing an NFA accept state\n    dfa_accept = {s for s in dfa_states\n                  if any(state in nfa['accept'] for state in s)}\n\n    return {\n        'states': dfa_states,\n        'alphabet': nfa['alphabet'],\n        'transitions': dfa_transitions,\n        'start': start_set,\n        'accept': dfa_accept\n    }\n\n# Test\nnfa = {\n    'states': {'q0', 'q1', 'q2'},\n    'alphabet': {'a', 'b'},\n    'transitions': {\n        ('q0', 'a'): {'q0', 'q1'},\n        ('q0', 'b'): {'q0'},\n        ('q1', 'b'): {'q2'}\n    },\n    'start': 'q0',\n    'accept': {'q2'}\n}\ndfa = nfa_to_dfa(nfa)\nprint(len(dfa['states']))  # Number of DFA states",
    "testCases": [
      {
        "input": "nfa1",
        "expectedOutput": "4",
        "isHidden": false,
        "description": "NFA with multiple transitions results in DFA"
      },
      {
        "input": "nfa2",
        "expectedOutput": "3",
        "isHidden": true,
        "description": "Different NFA structure"
      }
    ],
    "hints": [
      "Use frozenset to represent DFA states (sets of NFA states)",
      "Start with the NFA start state as a single-element set",
      "Process each unmarked DFA state by computing transitions",
      "A DFA state is accepting if it contains any NFA accept state"
    ],
    "language": "python"
  },
  {
    "id": "cs304-t1-ex12",
    "subjectId": "cs304",
    "topicId": "cs304-topic-1",
    "title": "Maximal Munch Tokenization",
    "difficulty": 3,
    "description": "Implement maximal munch (longest match) tokenization for recognizing multi-character operators.",
    "starterCode": "def tokenize_maximal_munch(input_str):\n    \"\"\"\n    Tokenize using maximal munch for operators.\n    Operators: ==, !=, <=, >=, <, >, =, +, -, *, /\n    Always prefer longer match (e.g., == over =)\n    \"\"\"\n    # Your code here\n    pass\n\n# Test\nprint(tokenize_maximal_munch(\"x == 5\"))\nprint(tokenize_maximal_munch(\"y != 10\"))",
    "solution": "def tokenize_maximal_munch(input_str):\n    \"\"\"\n    Tokenize using maximal munch for operators.\n    Operators: ==, !=, <=, >=, <, >, =, +, -, *, /\n    Always prefer longer match (e.g., == over =)\n    \"\"\"\n    # Two-character operators must be checked first\n    two_char_ops = {'==': 'EQ', '!=': 'NE', '<=': 'LE', '>=': 'GE'}\n    one_char_ops = {'<': 'LT', '>': 'GT', '=': 'ASSIGN',\n                    '+': 'PLUS', '-': 'MINUS', '*': 'MUL', '/': 'DIV'}\n\n    tokens = []\n    i = 0\n\n    while i < len(input_str):\n        if input_str[i].isspace():\n            i += 1\n            continue\n\n        # Try two-character operators first (maximal munch)\n        if i + 1 < len(input_str) and input_str[i:i+2] in two_char_ops:\n            tokens.append((two_char_ops[input_str[i:i+2]], input_str[i:i+2]))\n            i += 2\n        # Then try one-character operators\n        elif input_str[i] in one_char_ops:\n            tokens.append((one_char_ops[input_str[i]], input_str[i]))\n            i += 1\n        # Numbers\n        elif input_str[i].isdigit():\n            num = ''\n            while i < len(input_str) and input_str[i].isdigit():\n                num += input_str[i]\n                i += 1\n            tokens.append(('NUMBER', int(num)))\n        # Identifiers\n        elif input_str[i].isalpha() or input_str[i] == '_':\n            ident = ''\n            while i < len(input_str) and (input_str[i].isalnum() or input_str[i] == '_'):\n                ident += input_str[i]\n                i += 1\n            tokens.append(('IDENTIFIER', ident))\n        else:\n            raise ValueError(f\"Unknown character: {input_str[i]}\")\n\n    return tokens\n\n# Test\nprint(tokenize_maximal_munch(\"x == 5\"))\nprint(tokenize_maximal_munch(\"y != 10\"))",
    "testCases": [
      {
        "input": "x == 5",
        "expectedOutput": "[('IDENTIFIER', 'x'), ('EQ', '=='), ('NUMBER', 5)]",
        "isHidden": false,
        "description": "Equality operator"
      },
      {
        "input": "y != 10",
        "expectedOutput": "[('IDENTIFIER', 'y'), ('NE', '!='), ('NUMBER', 10)]",
        "isHidden": false,
        "description": "Not equal operator"
      },
      {
        "input": "a = b",
        "expectedOutput": "[('IDENTIFIER', 'a'), ('ASSIGN', '='), ('IDENTIFIER', 'b')]",
        "isHidden": true,
        "description": "Single character assignment"
      },
      {
        "input": "x <= y",
        "expectedOutput": "[('IDENTIFIER', 'x'), ('LE', '<='), ('IDENTIFIER', 'y')]",
        "isHidden": true,
        "description": "Less than or equal"
      }
    ],
    "hints": [
      "Check for longer operators before shorter ones",
      "Use a dictionary for quick operator lookup",
      "Always consume the maximum number of characters possible"
    ],
    "language": "python"
  },
  {
    "id": "cs304-t1-ex13",
    "subjectId": "cs304",
    "topicId": "cs304-topic-1",
    "title": "DFA Minimization - Partition Refinement",
    "difficulty": 5,
    "description": "Implement DFA minimization using the partition refinement algorithm to merge equivalent states.",
    "starterCode": "def minimize_dfa(dfa):\n    \"\"\"\n    Minimize a DFA by merging equivalent states.\n    dfa: dict with 'states', 'alphabet', 'transitions', 'start', 'accept'\n    Return minimized DFA with fewest states.\n    \"\"\"\n    # Your code here\n    pass\n\n# Test\ndfa = {\n    'states': {'q0', 'q1', 'q2', 'q3'},\n    'alphabet': {'a', 'b'},\n    'transitions': {\n        ('q0', 'a'): 'q1', ('q0', 'b'): 'q2',\n        ('q1', 'a'): 'q1', ('q1', 'b'): 'q3',\n        ('q2', 'a'): 'q1', ('q2', 'b'): 'q2',\n        ('q3', 'a'): 'q1', ('q3', 'b'): 'q3'\n    },\n    'start': 'q0',\n    'accept': {'q3'}\n}\nmin_dfa = minimize_dfa(dfa)\nprint(len(min_dfa['states']))",
    "solution": "def minimize_dfa(dfa):\n    \"\"\"\n    Minimize a DFA by merging equivalent states.\n    Uses partition refinement algorithm.\n    \"\"\"\n    states = dfa['states']\n    alphabet = dfa['alphabet']\n    transitions = dfa['transitions']\n    accept = dfa['accept']\n\n    # Initial partition: accepting vs non-accepting states\n    partitions = [accept, states - accept]\n    partitions = [p for p in partitions if p]  # Remove empty sets\n\n    changed = True\n    while changed:\n        changed = False\n        new_partitions = []\n\n        for partition in partitions:\n            # Try to split this partition\n            splits = {}\n\n            for state in partition:\n                # Create signature: which partition each symbol leads to\n                signature = tuple(\n                    next((i for i, p in enumerate(partitions)\n                          if transitions.get((state, symbol)) in p), None)\n                    for symbol in sorted(alphabet)\n                )\n\n                if signature not in splits:\n                    splits[signature] = set()\n                splits[signature].add(state)\n\n            if len(splits) > 1:\n                changed = True\n\n            new_partitions.extend(splits.values())\n\n        partitions = new_partitions\n\n    # Build minimized DFA\n    # Map each state to its partition\n    state_to_partition = {}\n    for i, partition in enumerate(partitions):\n        for state in partition:\n            state_to_partition[state] = i\n\n    # Create new transitions\n    new_transitions = {}\n    for (state, symbol), target in transitions.items():\n        new_state = state_to_partition[state]\n        new_target = state_to_partition[target]\n        new_transitions[(new_state, symbol)] = new_target\n\n    new_start = state_to_partition[dfa['start']]\n    new_accept = {state_to_partition[s] for s in accept}\n\n    return {\n        'states': set(range(len(partitions))),\n        'alphabet': alphabet,\n        'transitions': new_transitions,\n        'start': new_start,\n        'accept': new_accept\n    }\n\n# Test\ndfa = {\n    'states': {'q0', 'q1', 'q2', 'q3'},\n    'alphabet': {'a', 'b'},\n    'transitions': {\n        ('q0', 'a'): 'q1', ('q0', 'b'): 'q2',\n        ('q1', 'a'): 'q1', ('q1', 'b'): 'q3',\n        ('q2', 'a'): 'q1', ('q2', 'b'): 'q2',\n        ('q3', 'a'): 'q1', ('q3', 'b'): 'q3'\n    },\n    'start': 'q0',\n    'accept': {'q3'}\n}\nmin_dfa = minimize_dfa(dfa)\nprint(len(min_dfa['states']))",
    "testCases": [
      {
        "input": "dfa1",
        "expectedOutput": "3",
        "isHidden": false,
        "description": "DFA with redundant states"
      },
      {
        "input": "dfa2",
        "expectedOutput": "2",
        "isHidden": true,
        "description": "DFA that minimizes to 2 states"
      }
    ],
    "hints": [
      "Start with two partitions: accepting and non-accepting states",
      "Repeatedly refine partitions based on transition behavior",
      "Two states are equivalent if they transition to the same partitions",
      "Stop when no partition can be split further"
    ],
    "language": "python"
  },
  {
    "id": "cs304-t1-ex14",
    "subjectId": "cs304",
    "topicId": "cs304-topic-1",
    "title": "Lexical Error Recovery",
    "difficulty": 4,
    "description": "Implement error recovery in a tokenizer that skips invalid characters and reports errors with line/column info.",
    "starterCode": "def tokenize_with_errors(input_str):\n    \"\"\"\n    Tokenize input and report errors without stopping.\n    Return (tokens, errors) where errors is list of:\n    {'line': line_num, 'col': col_num, 'char': bad_char}\n    \"\"\"\n    # Your code here\n    pass\n\n# Test\ncode = '''x = 5\ny @ 10\nz = 15'''\ntokens, errors = tokenize_with_errors(code)\nprint(f\"Tokens: {tokens}\")\nprint(f\"Errors: {errors}\")",
    "solution": "def tokenize_with_errors(input_str):\n    \"\"\"\n    Tokenize input and report errors without stopping.\n    Return (tokens, errors) where errors is list of:\n    {'line': line_num, 'col': col_num, 'char': bad_char}\n    \"\"\"\n    tokens = []\n    errors = []\n    i = 0\n    line = 1\n    col = 1\n\n    while i < len(input_str):\n        start_col = col\n\n        if input_str[i] == '\n':\n            line += 1\n            col = 1\n            i += 1\n            continue\n\n        if input_str[i].isspace():\n            col += 1\n            i += 1\n            continue\n\n        # Numbers\n        if input_str[i].isdigit():\n            num = ''\n            while i < len(input_str) and input_str[i].isdigit():\n                num += input_str[i]\n                i += 1\n                col += 1\n            tokens.append(('NUMBER', int(num)))\n        # Identifiers\n        elif input_str[i].isalpha() or input_str[i] == '_':\n            ident = ''\n            while i < len(input_str) and (input_str[i].isalnum() or input_str[i] == '_'):\n                ident += input_str[i]\n                i += 1\n                col += 1\n            tokens.append(('IDENTIFIER', ident))\n        # Valid operators\n        elif input_str[i] in '=+-*/':\n            tokens.append(('OPERATOR', input_str[i]))\n            i += 1\n            col += 1\n        # Invalid character - report error and skip\n        else:\n            errors.append({\n                'line': line,\n                'col': start_col,\n                'char': input_str[i]\n            })\n            i += 1\n            col += 1\n\n    return tokens, errors\n\n# Test\ncode = '''x = 5\ny @ 10\nz = 15'''\ntokens, errors = tokenize_with_errors(code)\nprint(f\"Tokens: {tokens}\")\nprint(f\"Errors: {errors}\")",
    "testCases": [
      {
        "input": "x = 5",
        "expectedOutput": "(tokens, [])",
        "isHidden": false,
        "description": "No errors"
      },
      {
        "input": "y @ 10",
        "expectedOutput": "(tokens, [{'line': 1, 'col': 3, 'char': '@'}])",
        "isHidden": false,
        "description": "Invalid operator"
      },
      {
        "input": "a #\nb",
        "expectedOutput": "(tokens, [{'line': 1, 'col': 3, 'char': '#'}])",
        "isHidden": true,
        "description": "Error on first line"
      }
    ],
    "hints": [
      "Track line and column numbers as you scan",
      "When encountering an invalid character, record the error and continue",
      "Increment line counter on newline, reset column to 1",
      "Don't stop tokenization on errors - collect them all"
    ],
    "language": "python"
  },
  {
    "id": "cs304-t1-ex15",
    "subjectId": "cs304",
    "topicId": "cs304-topic-1",
    "title": "Context-Sensitive Tokenization",
    "difficulty": 4,
    "description": "Implement tokenization where < can mean either less-than or the start of a template (like C++ templates), depending on context.",
    "starterCode": "def tokenize_context_sensitive(input_str, in_template=False):\n    \"\"\"\n    Tokenize with context-sensitive handling of < and >.\n    When in_template=True, < and > are TEMPLATE_OPEN/CLOSE.\n    Otherwise, they are LT/GT operators.\n    Handle 'template' keyword to enter template context.\n    \"\"\"\n    # Your code here\n    pass\n\n# Test\nprint(tokenize_context_sensitive(\"a < b\"))\nprint(tokenize_context_sensitive(\"template<int>\"))",
    "solution": "def tokenize_context_sensitive(input_str, in_template=False):\n    \"\"\"\n    Tokenize with context-sensitive handling of < and >.\n    When in_template=True, < and > are TEMPLATE_OPEN/CLOSE.\n    Otherwise, they are LT/GT operators.\n    \"\"\"\n    tokens = []\n    i = 0\n    template_depth = 0\n\n    while i < len(input_str):\n        if input_str[i].isspace():\n            i += 1\n            continue\n\n        # Identifiers and keywords\n        if input_str[i].isalpha() or input_str[i] == '_':\n            word = ''\n            while i < len(input_str) and (input_str[i].isalnum() or input_str[i] == '_'):\n                word += input_str[i]\n                i += 1\n\n            if word == 'template':\n                tokens.append(('KEYWORD', 'template'))\n                # Next < will be template open\n                # We'll handle this in < processing\n            else:\n                tokens.append(('IDENTIFIER', word))\n\n        # Numbers\n        elif input_str[i].isdigit():\n            num = ''\n            while i < len(input_str) and input_str[i].isdigit():\n                num += input_str[i]\n                i += 1\n            tokens.append(('NUMBER', int(num)))\n\n        # Context-sensitive < and >\n        elif input_str[i] == '<':\n            # Check if previous token was 'template' keyword or we're in template\n            if (tokens and tokens[-1] == ('KEYWORD', 'template')) or template_depth > 0:\n                tokens.append(('TEMPLATE_OPEN', '<'))\n                template_depth += 1\n            else:\n                tokens.append(('LT', '<'))\n            i += 1\n\n        elif input_str[i] == '>':\n            if template_depth > 0:\n                tokens.append(('TEMPLATE_CLOSE', '>'))\n                template_depth -= 1\n            else:\n                tokens.append(('GT', '>'))\n            i += 1\n\n        else:\n            i += 1\n\n    return tokens\n\n# Test\nprint(tokenize_context_sensitive(\"a < b\"))\nprint(tokenize_context_sensitive(\"template<int>\"))",
    "testCases": [
      {
        "input": "a < b",
        "expectedOutput": "[('IDENTIFIER', 'a'), ('LT', '<'), ('IDENTIFIER', 'b')]",
        "isHidden": false,
        "description": "Less-than operator"
      },
      {
        "input": "template<int>",
        "expectedOutput": "[('KEYWORD', 'template'), ('TEMPLATE_OPEN', '<'), ('IDENTIFIER', 'int'), ('TEMPLATE_CLOSE', '>')]",
        "isHidden": false,
        "description": "Template syntax"
      },
      {
        "input": "template<pair<int>>",
        "expectedOutput": "nested templates",
        "isHidden": true,
        "description": "Nested template parameters"
      }
    ],
    "hints": [
      "Keep track of whether you're inside template brackets",
      "Use a depth counter for nested templates",
      "Check previous token to see if it was the template keyword",
      "This is why C++ is hard to parse!"
    ],
    "language": "python"
  },
  {
    "id": "cs304-t1-ex16",
    "subjectId": "cs304",
    "topicId": "cs304-topic-1",
    "title": "Complete Lexer with Position Tracking",
    "difficulty": 5,
    "description": "Build a complete lexer that tracks file position, handles all previous token types, and returns a token stream with metadata.",
    "starterCode": "class Token:\n    def __init__(self, type, value, line, col, length):\n        self.type = type\n        self.value = value\n        self.line = line\n        self.col = col\n        self.length = length\n\n    def __repr__(self):\n        return f\"Token({self.type}, {self.value!r}, {self.line}:{self.col})\"\n\nclass Lexer:\n    def __init__(self, input_str):\n        self.input = input_str\n        self.pos = 0\n        self.line = 1\n        self.col = 1\n\n    def tokenize(self):\n        \"\"\"Return list of Token objects\"\"\"\n        # Your code here\n        pass\n\n# Test\nlexer = Lexer('x = 42 // comment\\ny = \"hello\"')\ntokens = lexer.tokenize()\nfor tok in tokens:\n    print(tok)",
    "solution": "class Token:\n    def __init__(self, type, value, line, col, length):\n        self.type = type\n        self.value = value\n        self.line = line\n        self.col = col\n        self.length = length\n\n    def __repr__(self):\n        return f\"Token({self.type}, {self.value!r}, {self.line}:{self.col})\"\n\nclass Lexer:\n    def __init__(self, input_str):\n        self.input = input_str\n        self.pos = 0\n        self.line = 1\n        self.col = 1\n\n    def peek(self, offset=0):\n        pos = self.pos + offset\n        return self.input[pos] if pos < len(self.input) else None\n\n    def advance(self):\n        if self.pos < len(self.input):\n            if self.input[self.pos] == '\n':\n                self.line += 1\n                self.col = 1\n            else:\n                self.col += 1\n            self.pos += 1\n\n    def skip_whitespace(self):\n        while self.peek() and self.peek().isspace():\n            self.advance()\n\n    def tokenize(self):\n        tokens = []\n\n        while self.pos < len(self.input):\n            self.skip_whitespace()\n            if self.pos >= len(self.input):\n                break\n\n            start_line, start_col = self.line, self.col\n\n            # Comments\n            if self.peek() == '/' and self.peek(1) == '/':\n                while self.peek() and self.peek() != '\n':\n                    self.advance()\n                continue\n\n            # Strings\n            if self.peek() == '\"':\n                self.advance()\n                value = ''\n                while self.peek() and self.peek() != '\"':\n                    if self.peek() == '\\':\n                        self.advance()\n                        if self.peek() == 'n':\n                            value += '\n'\n                        elif self.peek() == 't':\n                            value += '\t'\n                        elif self.peek():\n                            value += self.peek()\n                        self.advance()\n                    else:\n                        value += self.peek()\n                        self.advance()\n                self.advance()  # closing \"\n                tokens.append(Token('STRING', value, start_line, start_col, self.col - start_col))\n\n            # Numbers\n            elif self.peek() and self.peek().isdigit():\n                value = ''\n                while self.peek() and self.peek().isdigit():\n                    value += self.peek()\n                    self.advance()\n                tokens.append(Token('NUMBER', int(value), start_line, start_col, self.col - start_col))\n\n            # Identifiers\n            elif self.peek() and (self.peek().isalpha() or self.peek() == '_'):\n                value = ''\n                while self.peek() and (self.peek().isalnum() or self.peek() == '_'):\n                    value += self.peek()\n                    self.advance()\n                tokens.append(Token('IDENTIFIER', value, start_line, start_col, self.col - start_col))\n\n            # Operators\n            elif self.peek() == '=' and self.peek(1) == '=':\n                self.advance()\n                self.advance()\n                tokens.append(Token('EQUALS', '==', start_line, start_col, 2))\n            elif self.peek() == '=':\n                self.advance()\n                tokens.append(Token('ASSIGN', '=', start_line, start_col, 1))\n            elif self.peek() in '+-*/':\n                op = self.peek()\n                self.advance()\n                tokens.append(Token('OPERATOR', op, start_line, start_col, 1))\n            else:\n                self.advance()\n\n        return tokens\n\n# Test\nlexer = Lexer('x = 42 // comment\\ny = \"hello\"')\ntokens = lexer.tokenize()\nfor tok in tokens:\n    print(tok)",
    "testCases": [
      {
        "input": "x = 42",
        "expectedOutput": "3 tokens",
        "isHidden": false,
        "description": "Simple assignment"
      },
      {
        "input": "x = 42 // comment",
        "expectedOutput": "3 tokens (comment ignored)",
        "isHidden": false,
        "description": "Assignment with comment"
      },
      {
        "input": "y = \"hello\"",
        "expectedOutput": "3 tokens with string",
        "isHidden": true,
        "description": "String literal"
      }
    ],
    "hints": [
      "Use a Lexer class to maintain state (position, line, column)",
      "Implement helper methods like peek(), advance(), skip_whitespace()",
      "Create Token objects with full position metadata",
      "Handle all token types from previous exercises",
      "Track line and column accurately through newlines"
    ],
    "language": "python"
  }
]
