[
  {
    "id": "cs406-t1-ex01",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Simple Reflex Agent for Thermostat",
    "difficulty": 1,
    "description": "Implement a simple reflex agent that controls a thermostat.\n\nYour implementation should:\n- Read temperature from a sensor\n- Turn on heating if temperature < 18°C\n- Turn on cooling if temperature > 24°C\n- Turn off both otherwise\n- Log all state changes",
    "starterCode": "class ThermostatAgent:\n    def __init__(self, target_low=18, target_high=24):\n        self.target_low = target_low\n        self.target_high = target_high\n        self.heating_on = False\n        self.cooling_on = False\n\n    def perceive(self, temperature: float) -> dict:\n        # Return perception\n        pass\n\n    def decide_action(self, percept: dict) -> str:\n        # Decide what action to take\n        pass\n\n    def execute(self, action: str):\n        # Execute the action\n        pass\n\n# Example usage:\n# agent = ThermostatAgent()\n# agent.perceive(15.0)  # Cold\n# agent.perceive(22.0)  # Just right\n# agent.perceive(26.0)  # Hot",
    "solution": "class ThermostatAgent:\n    def __init__(self, target_low=18, target_high=24):\n        self.target_low = target_low\n        self.target_high = target_high\n        self.heating_on = False\n        self.cooling_on = False\n        self.log = []\n\n    def perceive(self, temperature: float) -> dict:\n        return {\n            'temperature': temperature,\n            'too_cold': temperature < self.target_low,\n            'too_hot': temperature > self.target_high,\n            'comfortable': self.target_low <= temperature <= self.target_high\n        }\n\n    def decide_action(self, percept: dict) -> str:\n        if percept['too_cold']:\n            return 'HEAT_ON'\n        elif percept['too_hot']:\n            return 'COOL_ON'\n        else:\n            return 'OFF'\n\n    def execute(self, action: str):\n        if action == 'HEAT_ON':\n            if not self.heating_on:\n                self.heating_on = True\n                self.cooling_on = False\n                self.log.append('Heating ON')\n        elif action == 'COOL_ON':\n            if not self.cooling_on:\n                self.heating_on = False\n                self.cooling_on = True\n                self.log.append('Cooling ON')\n        else:  # OFF\n            if self.heating_on or self.cooling_on:\n                self.heating_on = False\n                self.cooling_on = False\n                self.log.append('System OFF')\n\n    def run(self, temperature: float):\n        percept = self.perceive(temperature)\n        action = self.decide_action(percept)\n        self.execute(action)\n        return action\n\n# Test\nagent = ThermostatAgent()\nprint(agent.run(15.0))  # HEAT_ON\nprint(agent.run(22.0))  # OFF\nprint(agent.run(26.0))  # COOL_ON\nprint('Log:', agent.log)",
    "testCases": [
      {
        "input": "agent.run(15.0)",
        "isHidden": false,
        "description": "Test cold temperature (below 18°C)"
      },
      {
        "input": "agent.run(22.0)",
        "isHidden": false,
        "description": "Test comfortable temperature (18-24°C)"
      },
      {
        "input": "agent.run(26.0)",
        "isHidden": false,
        "description": "Test hot temperature (above 24°C)"
      }
    ],
    "hints": [
      "Start by implementing the perceive() method to create a dictionary with temperature information",
      "The decide_action() method should check percept values and return appropriate action strings",
      "Use boolean flags (heating_on, cooling_on) to track current state and avoid duplicate log entries"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex02",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Model-Based Agent with State",
    "difficulty": 2,
    "description": "Implement a model-based vacuum cleaner agent that maintains state.\n\nThe agent should:\n- Track which locations it has visited\n- Remember which locations are clean\n- Move to dirty locations efficiently\n- Handle a 2D grid environment",
    "starterCode": "class VacuumAgent:\n    def __init__(self, grid_size=(3, 3)):\n        self.grid_size = grid_size\n        self.position = (0, 0)\n        # TODO: Initialize internal state\n\n    def update_state(self, percept):\n        # Update internal model of world\n        pass\n\n    def choose_action(self):\n        # Choose action based on state\n        pass\n\n# Grid: 0=clean, 1=dirty\n# Actions: 'SUCK', 'UP', 'DOWN', 'LEFT', 'RIGHT'",
    "solution": "class VacuumAgent:\n    def __init__(self, grid_size=(3, 3)):\n        self.grid_size = grid_size\n        self.position = (0, 0)\n        self.world_state = {}  # (x,y) -> 'clean'/'dirty'/'unknown'\n        self.visited = set()\n\n        # Initialize all as unknown\n        for i in range(grid_size[0]):\n            for j in range(grid_size[1]):\n                self.world_state[(i, j)] = 'unknown'\n\n    def update_state(self, percept):\n        # percept = {'location': (x,y), 'dirty': bool}\n        loc = percept['location']\n        self.position = loc\n        self.visited.add(loc)\n\n        if percept['dirty']:\n            self.world_state[loc] = 'dirty'\n        else:\n            self.world_state[loc] = 'clean'\n\n    def choose_action(self):\n        x, y = self.position\n\n        # If current location dirty, suck\n        if self.world_state[self.position] == 'dirty':\n            self.world_state[self.position] = 'clean'\n            return 'SUCK'\n\n        # Find nearest dirty or unknown location\n        target = self.find_target()\n\n        if target is None:\n            return 'DONE'  # All clean\n\n        # Move toward target\n        tx, ty = target\n        if tx < x and x > 0:\n            return 'LEFT'\n        elif tx > x and x < self.grid_size[0] - 1:\n            return 'RIGHT'\n        elif ty < y and y > 0:\n            return 'DOWN'\n        elif ty > y and y < self.grid_size[1] - 1:\n            return 'UP'\n\n        return 'DONE'\n\n    def find_target(self):\n        # Find nearest dirty or unknown location\n        for dx in range(max(self.grid_size)):\n            for dy in range(max(self.grid_size)):\n                # Check all locations at Manhattan distance dx+dy\n                for loc in self.get_locations_at_distance(dx + dy):\n                    if self.world_state.get(loc, 'unknown') in ['dirty', 'unknown']:\n                        return loc\n        return None\n\n    def get_locations_at_distance(self, dist):\n        x, y = self.position\n        locs = []\n        for i in range(self.grid_size[0]):\n            for j in range(self.grid_size[1]):\n                if abs(i - x) + abs(j - y) == dist:\n                    locs.append((i, j))\n        return locs\n\n# Test\nagent = VacuumAgent((3, 3))\nagent.update_state({'location': (0, 0), 'dirty': True})\nprint(agent.choose_action())  # SUCK\nagent.update_state({'location': (0, 0), 'dirty': False})\nprint(agent.choose_action())  # Move to explore",
    "testCases": [
      {
        "input": "agent.choose_action() when location is dirty",
        "isHidden": false,
        "description": "Test agent sucks when location is dirty"
      },
      {
        "input": "agent.choose_action() after cleaning",
        "isHidden": false,
        "description": "Test agent moves to explore after cleaning"
      },
      {
        "input": "agent.world_state tracking",
        "isHidden": false,
        "description": "Test internal state is maintained correctly"
      }
    ],
    "hints": [
      "Use a dictionary to store the world state, mapping (x,y) positions to clean/dirty/unknown",
      "The find_target() method should search for the nearest dirty or unknown location using Manhattan distance",
      "Remember to update the world_state after sucking to mark the location as clean"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex03",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Goal-Based Agent for Route Planning",
    "difficulty": 2,
    "description": "Implement a goal-based agent that plans routes to reach a destination.\n\nYour implementation should:\n- Maintain a world model (map)\n- Define goals (target locations)\n- Plan sequences of actions to reach goals\n- Adapt to changing environments",
    "starterCode": "class RouteAgent:\n    def __init__(self, world_map, start, goal):\n        self.world_map = world_map\n        self.position = start\n        self.goal = goal\n        self.plan = []\n\n    def make_plan(self):\n        # Create plan to reach goal\n        pass\n\n    def execute_plan(self):\n        # Execute the plan step by step\n        pass\n\n# World map: graph with nodes and edges\n# world_map = {'A': ['B', 'C'], 'B': ['D'], ...}",
    "solution": "from collections import deque\n\nclass RouteAgent:\n    def __init__(self, world_map, start, goal):\n        self.world_map = world_map\n        self.position = start\n        self.goal = goal\n        self.plan = []\n\n    def make_plan(self):\n        \"\"\"Use BFS to find path from current position to goal.\"\"\"\n        if self.position == self.goal:\n            return []\n\n        queue = deque([(self.position, [self.position])])\n        visited = set()\n\n        while queue:\n            node, path = queue.popleft()\n\n            if node in visited:\n                continue\n\n            visited.add(node)\n\n            if node == self.goal:\n                self.plan = path[1:]  # Exclude starting position\n                return self.plan\n\n            for neighbor in self.world_map.get(node, []):\n                if neighbor not in visited:\n                    queue.append((neighbor, path + [neighbor]))\n\n        return None  # No path found\n\n    def execute_plan(self):\n        \"\"\"Execute plan step by step.\"\"\"\n        if not self.plan:\n            return \"Already at goal or no plan\"\n\n        steps = []\n        for next_pos in self.plan:\n            steps.append(f\"Move from {self.position} to {next_pos}\")\n            self.position = next_pos\n\n        return steps\n\n    def replan_if_needed(self, blocked_nodes):\n        \"\"\"Replan if current plan goes through blocked nodes.\"\"\"\n        if any(node in blocked_nodes for node in self.plan):\n            # Remove blocked nodes from map temporarily\n            temp_map = {k: [v for v in vals if v not in blocked_nodes]\n                       for k, vals in self.world_map.items()}\n\n            old_map = self.world_map\n            self.world_map = temp_map\n\n            result = self.make_plan()\n\n            self.world_map = old_map\n            return result is not None\n\n        return True\n\n# Test\nworld_map = {\n    'A': ['B', 'C'],\n    'B': ['D', 'E'],\n    'C': ['F'],\n    'D': ['G'],\n    'E': ['G'],\n    'F': ['G'],\n    'G': []\n}\n\nagent = RouteAgent(world_map, 'A', 'G')\nprint(\"Initial position:\", agent.position)\nprint(\"Goal:\", agent.goal)\n\nplan = agent.make_plan()\nprint(\"Plan:\", plan)\n\nsteps = agent.execute_plan()\nprint(\"Execution:\")\nfor step in steps:\n    print(f\"  {step}\")\n\nprint(\"Final position:\", agent.position)",
    "testCases": [
      {
        "input": "agent.make_plan()",
        "isHidden": false,
        "description": "Test agent creates valid plan to goal"
      },
      {
        "input": "agent.execute_plan()",
        "isHidden": false,
        "description": "Test plan execution reaches goal"
      },
      {
        "input": "agent.replan_if_needed(blocked_nodes)",
        "isHidden": false,
        "description": "Test agent replans when obstacles appear"
      }
    ],
    "hints": [
      "Use BFS to find the shortest path from current position to goal",
      "Store the plan as a sequence of positions to visit",
      "Execute the plan by moving through each position in order"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex04",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Utility-Based Agent for Resource Allocation",
    "difficulty": 3,
    "description": "Implement a utility-based agent that allocates resources to maximize utility.\n\nYour implementation should:\n- Define utility function for different outcomes\n- Evaluate expected utility of actions\n- Choose action that maximizes expected utility\n- Handle uncertain outcomes",
    "starterCode": "class ResourceAgent:\n    def __init__(self, resources):\n        self.resources = resources\n\n    def utility(self, allocation):\n        # Calculate utility of resource allocation\n        pass\n\n    def choose_action(self, possible_actions):\n        # Choose action with maximum expected utility\n        pass\n\n# Example: Allocate budget between projects\n# Each project has cost and uncertain return",
    "solution": "import random\n\nclass ResourceAgent:\n    def __init__(self, resources):\n        self.resources = resources\n\n    def utility(self, allocation):\n        \"\"\"\n        Calculate utility of resource allocation.\n        allocation: dict mapping project -> amount\n\n        Utility increases with allocation but with diminishing returns.\n        \"\"\"\n        total_utility = 0\n\n        for project, amount in allocation.items():\n            # Diminishing returns: sqrt(amount) * project_value\n            project_value = {\n                'A': 100,\n                'B': 150,\n                'C': 80\n            }.get(project, 50)\n\n            total_utility += (amount ** 0.5) * project_value\n\n        return total_utility\n\n    def expected_utility(self, action, outcomes, probabilities):\n        \"\"\"\n        Calculate expected utility of action.\n        action: resource allocation\n        outcomes: list of possible results\n        probabilities: probability of each outcome\n        \"\"\"\n        expected = 0\n        for outcome, prob in zip(outcomes, probabilities):\n            # Apply outcome multiplier to allocation\n            adjusted = {k: v * outcome for k, v in action.items()}\n            expected += prob * self.utility(adjusted)\n\n        return expected\n\n    def choose_action(self, possible_actions, outcomes=None, probabilities=None):\n        \"\"\"Choose action with maximum expected utility.\"\"\"\n        if outcomes is None:\n            # Deterministic case\n            best_action = None\n            best_utility = float('-inf')\n\n            for action in possible_actions:\n                # Check if allocation is valid (within resources)\n                if sum(action.values()) <= self.resources:\n                    util = self.utility(action)\n                    if util > best_utility:\n                        best_utility = util\n                        best_action = action\n\n            return best_action, best_utility\n        else:\n            # Stochastic case\n            best_action = None\n            best_expected = float('-inf')\n\n            for action in possible_actions:\n                if sum(action.values()) <= self.resources:\n                    expected = self.expected_utility(action, outcomes, probabilities)\n                    if expected > best_expected:\n                        best_expected = expected\n                        best_action = action\n\n            return best_action, best_expected\n\n# Test\nagent = ResourceAgent(resources=100)\n\n# Possible allocations\npossible_actions = [\n    {'A': 50, 'B': 50, 'C': 0},\n    {'A': 60, 'B': 20, 'C': 20},\n    {'A': 30, 'B': 30, 'C': 40},\n    {'A': 0, 'B': 100, 'C': 0},\n    {'A': 40, 'B': 40, 'C': 20}\n]\n\nprint(\"Resource Allocation Agent\")\nprint(f\"Total resources: {agent.resources}\")\nprint()\n\n# Deterministic case\nprint(\"Deterministic utility:\")\nbest_action, best_util = agent.choose_action(possible_actions)\nprint(f\"Best allocation: {best_action}\")\nprint(f\"Utility: {best_util:.2f}\")\nprint()\n\n# Stochastic case (uncertain outcomes)\nprint(\"Stochastic case (uncertain project success):\")\noutcomes = [0.5, 1.0, 1.5]  # Project could underperform, meet, or exceed expectations\nprobabilities = [0.2, 0.5, 0.3]\n\nbest_action, best_expected = agent.choose_action(\n    possible_actions,\n    outcomes,\n    probabilities\n)\nprint(f\"Best allocation: {best_action}\")\nprint(f\"Expected utility: {best_expected:.2f}\")",
    "testCases": [
      {
        "input": "agent.utility(allocation)",
        "isHidden": false,
        "description": "Test utility calculation for allocation"
      },
      {
        "input": "agent.choose_action(possible_actions)",
        "isHidden": false,
        "description": "Test agent chooses action maximizing utility"
      },
      {
        "input": "agent.expected_utility(action, outcomes, probabilities)",
        "isHidden": false,
        "description": "Test expected utility with uncertainty"
      }
    ],
    "hints": [
      "Utility function should capture preferences (e.g., diminishing returns)",
      "Expected utility = sum of (probability × utility) over all possible outcomes",
      "Choose the action with highest expected utility among valid options"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex05",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Learning Agent with Experience",
    "difficulty": 3,
    "description": "Implement a learning agent that improves from experience.\n\nYour implementation should:\n- Maintain performance history\n- Learn from successes and failures\n- Update internal model based on experience\n- Improve decision-making over time",
    "starterCode": "class LearningAgent:\n    def __init__(self):\n        self.experience = []\n        self.model = {}\n\n    def act(self, state):\n        # Choose action based on current model\n        pass\n\n    def learn(self, state, action, reward):\n        # Update model based on experience\n        pass\n\n    def performance(self):\n        # Return average reward\n        pass",
    "solution": "import random\n\nclass LearningAgent:\n    def __init__(self, actions, learning_rate=0.1, epsilon=0.1):\n        self.actions = actions\n        self.experience = []\n\n        # Q-values: (state, action) -> expected reward\n        self.q_values = {}\n\n        self.learning_rate = learning_rate\n        self.epsilon = epsilon  # Exploration rate\n\n    def get_q_value(self, state, action):\n        \"\"\"Get Q-value for state-action pair.\"\"\"\n        return self.q_values.get((state, action), 0.0)\n\n    def act(self, state):\n        \"\"\"Choose action using epsilon-greedy policy.\"\"\"\n        # Exploration: random action\n        if random.random() < self.epsilon:\n            return random.choice(self.actions)\n\n        # Exploitation: best known action\n        best_action = None\n        best_value = float('-inf')\n\n        for action in self.actions:\n            q_val = self.get_q_value(state, action)\n            if q_val > best_value:\n                best_value = q_val\n                best_action = action\n\n        # If all Q-values are 0, choose randomly\n        if best_action is None:\n            best_action = random.choice(self.actions)\n\n        return best_action\n\n    def learn(self, state, action, reward, next_state=None):\n        \"\"\"Update Q-value based on experience.\"\"\"\n        self.experience.append((state, action, reward))\n\n        # Current Q-value\n        current_q = self.get_q_value(state, action)\n\n        # Simple update: moving average toward observed reward\n        if next_state is None:\n            # Terminal state\n            target = reward\n        else:\n            # Q-learning update: reward + best future Q-value\n            best_next_q = max(\n                (self.get_q_value(next_state, a) for a in self.actions),\n                default=0.0\n            )\n            target = reward + best_next_q\n\n        # Update Q-value\n        new_q = current_q + self.learning_rate * (target - current_q)\n        self.q_values[(state, action)] = new_q\n\n    def performance(self):\n        \"\"\"Return average reward over all experience.\"\"\"\n        if not self.experience:\n            return 0.0\n\n        total_reward = sum(reward for _, _, reward in self.experience)\n        return total_reward / len(self.experience)\n\n    def get_policy(self):\n        \"\"\"Extract learned policy.\"\"\"\n        policy = {}\n        states = set(state for (state, _), _ in self.q_values.items())\n\n        for state in states:\n            best_action = None\n            best_value = float('-inf')\n\n            for action in self.actions:\n                q_val = self.get_q_value(state, action)\n                if q_val > best_value:\n                    best_value = q_val\n                    best_action = action\n\n            policy[state] = best_action\n\n        return policy\n\n# Test with simple grid world\nactions = ['up', 'down', 'left', 'right']\nagent = LearningAgent(actions, learning_rate=0.1, epsilon=0.2)\n\nprint(\"Learning Agent Simulation\")\nprint(\"=\"*50)\n\n# Simulate episodes\n# State: (x, y), Goal: (5, 5), Reward: -1 per step, +10 at goal\n\ndef get_next_state(state, action):\n    \"\"\"Get next state given action.\"\"\"\n    x, y = state\n    if action == 'up':\n        return (x, min(y + 1, 5))\n    elif action == 'down':\n        return (x, max(y - 1, 0))\n    elif action == 'left':\n        return (max(x - 1, 0), y)\n    elif action == 'right':\n        return (min(x + 1, 5), y)\n    return state\n\ndef get_reward(state):\n    \"\"\"Get reward for reaching state.\"\"\"\n    if state == (5, 5):\n        return 10.0\n    return -1.0\n\n# Train for multiple episodes\nnum_episodes = 100\nfor episode in range(num_episodes):\n    state = (0, 0)\n    steps = 0\n    max_steps = 50\n\n    while state != (5, 5) and steps < max_steps:\n        action = agent.act(state)\n        next_state = get_next_state(state, action)\n        reward = get_reward(next_state)\n\n        agent.learn(state, action, reward, next_state)\n\n        state = next_state\n        steps += 1\n\n    if (episode + 1) % 20 == 0:\n        print(f\"Episode {episode + 1}: Avg reward = {agent.performance():.2f}\")\n\nprint()\nprint(f\"Final average reward: {agent.performance():.2f}\")\nprint(f\"Total experiences: {len(agent.experience)}\")\n\n# Show learned policy for some states\nprint(\"\\nLearned policy (sample states):\")\npolicy = agent.get_policy()\nfor state in [(0, 0), (2, 2), (4, 4), (5, 4)]:\n    if state in policy:\n        print(f\"  State {state}: {policy[state]}\")",
    "testCases": [
      {
        "input": "agent.act(state)",
        "isHidden": false,
        "description": "Test agent chooses actions (exploration vs exploitation)"
      },
      {
        "input": "agent.learn(state, action, reward)",
        "isHidden": false,
        "description": "Test learning updates Q-values"
      },
      {
        "input": "agent.performance()",
        "isHidden": false,
        "description": "Test performance improves over time"
      }
    ],
    "hints": [
      "Use Q-learning to learn value of state-action pairs",
      "Epsilon-greedy policy balances exploration (random) and exploitation (best known)",
      "Update Q-values using: Q(s,a) ← Q(s,a) + α[r + max Q(s',a') - Q(s,a)]"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex06",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Multi-Agent System - Collaborative Agents",
    "difficulty": 3,
    "description": "Implement multiple agents that collaborate to achieve a shared goal.\n\nYour implementation should:\n- Coordinate multiple agents\n- Share information between agents\n- Divide tasks among agents\n- Achieve goals faster through collaboration",
    "starterCode": "class CollaborativeAgent:\n    def __init__(self, agent_id, shared_knowledge):\n        self.id = agent_id\n        self.shared_knowledge = shared_knowledge\n\n    def perceive(self, environment):\n        pass\n\n    def communicate(self, message):\n        pass\n\n    def act(self):\n        pass",
    "solution": "class SharedKnowledge:\n    \"\"\"Shared knowledge base for collaborative agents.\"\"\"\n    def __init__(self):\n        self.discovered_items = set()\n        self.agent_positions = {}\n        self.assigned_tasks = {}\n\n    def add_discovery(self, item, location):\n        \"\"\"Add newly discovered item.\"\"\"\n        self.discovered_items.add((item, location))\n\n    def update_position(self, agent_id, position):\n        \"\"\"Update agent position.\"\"\"\n        self.agent_positions[agent_id] = position\n\n    def assign_task(self, agent_id, task):\n        \"\"\"Assign task to agent.\"\"\"\n        self.assigned_tasks[agent_id] = task\n\n    def get_unassigned_items(self):\n        \"\"\"Get items not yet assigned to any agent.\"\"\"\n        assigned = set(task for task in self.assigned_tasks.values() if task)\n        return [item for item in self.discovered_items if item not in assigned]\n\nclass CollaborativeAgent:\n    def __init__(self, agent_id, position, shared_knowledge):\n        self.id = agent_id\n        self.position = position\n        self.shared_knowledge = shared_knowledge\n        self.holding = None\n        self.target = None\n\n    def perceive(self, environment):\n        \"\"\"Perceive environment and share discoveries.\"\"\"\n        # Check current location for items\n        if self.position in environment:\n            item = environment[self.position]\n            self.shared_knowledge.add_discovery(item, self.position)\n\n        # Update own position\n        self.shared_knowledge.update_position(self.id, self.position)\n\n    def communicate(self, message):\n        \"\"\"Process message from another agent.\"\"\"\n        if message['type'] == 'request_help':\n            return self.offer_help(message)\n        elif message['type'] == 'task_complete':\n            return self.acknowledge_completion(message)\n\n    def select_task(self):\n        \"\"\"Select next task based on shared knowledge.\"\"\"\n        # Check if already have a task\n        if self.target:\n            return self.target\n\n        # Get unassigned items\n        unassigned = self.shared_knowledge.get_unassigned_items()\n\n        if not unassigned:\n            return None\n\n        # Choose nearest unassigned item\n        best_item = None\n        best_distance = float('inf')\n\n        for item, location in unassigned:\n            distance = abs(location[0] - self.position[0]) + abs(location[1] - self.position[1])\n            if distance < best_distance:\n                best_distance = distance\n                best_item = (item, location)\n\n        if best_item:\n            self.shared_knowledge.assign_task(self.id, best_item)\n            self.target = best_item\n\n        return self.target\n\n    def act(self):\n        \"\"\"Execute action toward current goal.\"\"\"\n        if not self.target:\n            self.select_task()\n\n        if not self.target:\n            return f\"Agent {self.id}: No tasks available\"\n\n        item, location = self.target\n\n        # Move toward target\n        if self.position != location:\n            # Simple movement (Manhattan)\n            x, y = self.position\n            tx, ty = location\n\n            if x < tx:\n                self.position = (x + 1, y)\n            elif x > tx:\n                self.position = (x - 1, y)\n            elif y < ty:\n                self.position = (x, y + 1)\n            elif y > ty:\n                self.position = (x, y - 1)\n\n            self.shared_knowledge.update_position(self.id, self.position)\n            return f\"Agent {self.id}: Moving to {location}\"\n        else:\n            # Reached target, pick up item\n            self.holding = item\n            self.target = None\n            self.shared_knowledge.assign_task(self.id, None)\n            return f\"Agent {self.id}: Collected {item}\"\n\n    def offer_help(self, message):\n        \"\"\"Offer to help another agent.\"\"\"\n        return {\n            'agent_id': self.id,\n            'available': self.target is None,\n            'position': self.position\n        }\n\n    def acknowledge_completion(self, message):\n        \"\"\"Acknowledge task completion.\"\"\"\n        return f\"Agent {self.id}: Acknowledged completion by Agent {message['agent_id']}\"\n\n# Test multi-agent collaboration\nprint(\"Multi-Agent Collaboration Simulation\")\nprint(\"=\"*50)\n\n# Environment: items at locations\nenvironment = {\n    (2, 3): 'ItemA',\n    (5, 1): 'ItemB',\n    (1, 5): 'ItemC',\n    (7, 7): 'ItemD',\n    (3, 8): 'ItemE'\n}\n\n# Shared knowledge\nshared = SharedKnowledge()\n\n# Create agents at different positions\nagents = [\n    CollaborativeAgent('Agent1', (0, 0), shared),\n    CollaborativeAgent('Agent2', (5, 5), shared),\n    CollaborativeAgent('Agent3', (8, 0), shared)\n]\n\nprint(f\"Environment has {len(environment)} items to collect\")\nprint(f\"Number of agents: {len(agents)}\")\nprint()\n\n# Simulate\ncollected = 0\nmax_steps = 50\n\nfor step in range(max_steps):\n    print(f\"Step {step + 1}:\")\n\n    # All agents perceive\n    for agent in agents:\n        agent.perceive(environment)\n\n    # All agents act\n    for agent in agents:\n        action = agent.act()\n        print(f\"  {action}\")\n\n        if \"Collected\" in action:\n            collected += 1\n\n    # Check if all items collected\n    if collected >= len(environment):\n        print(f\"\\nAll items collected in {step + 1} steps!\")\n        break\n\n    print()\n\nprint(f\"\\nFinal state:\")\nprint(f\"Items collected: {collected}/{len(environment)}\")\nfor agent in agents:\n    print(f\"  {agent.id} at {agent.position}, holding: {agent.holding}\")",
    "testCases": [
      {
        "input": "agent.perceive(environment)",
        "isHidden": false,
        "description": "Test agent perceives and shares information"
      },
      {
        "input": "agent.select_task()",
        "isHidden": false,
        "description": "Test agents coordinate to avoid duplicate work"
      },
      {
        "input": "agent.act()",
        "isHidden": false,
        "description": "Test agents work together to complete all tasks"
      }
    ],
    "hints": [
      "Use shared knowledge structure for communication between agents",
      "Agents should claim tasks to avoid duplication",
      "Each agent selects nearest unassigned task for efficiency"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex07",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Reactive Agent with Subsumption Architecture",
    "difficulty": 2,
    "description": "Implement a reactive agent using subsumption architecture with layered behaviors.\n\nYour implementation should:\n- Define multiple behavior layers\n- Implement priority-based arbitration\n- Higher layers can override lower layers\n- React quickly to environment changes",
    "starterCode": "class Behavior:\n    def __init__(self, name, priority):\n        self.name = name\n        self.priority = priority\n\n    def is_active(self, percepts):\n        # Check if behavior should activate\n        pass\n\n    def action(self, percepts):\n        # Return action to execute\n        pass\n\nclass SubsumptionAgent:\n    def __init__(self):\n        self.behaviors = []\n\n    def add_behavior(self, behavior):\n        pass\n\n    def select_action(self, percepts):\n        pass",
    "solution": "class Behavior:\n    \"\"\"Base class for behaviors in subsumption architecture.\"\"\"\n    def __init__(self, name, priority):\n        self.name = name\n        self.priority = priority\n\n    def is_active(self, percepts):\n        \"\"\"Check if behavior conditions are met.\"\"\"\n        raise NotImplementedError\n\n    def action(self, percepts):\n        \"\"\"Return action to execute.\"\"\"\n        raise NotImplementedError\n\nclass AvoidObstacle(Behavior):\n    \"\"\"High priority: avoid obstacles.\"\"\"\n    def __init__(self):\n        super().__init__(\"AvoidObstacle\", priority=3)\n\n    def is_active(self, percepts):\n        return percepts.get('obstacle_ahead', False)\n\n    def action(self, percepts):\n        # Turn away from obstacle\n        if percepts.get('obstacle_left', False):\n            return 'turn_right'\n        else:\n            return 'turn_left'\n\nclass MoveToGoal(Behavior):\n    \"\"\"Medium priority: move toward goal.\"\"\"\n    def __init__(self):\n        super().__init__(\"MoveToGoal\", priority=2)\n\n    def is_active(self, percepts):\n        return percepts.get('goal_detected', False)\n\n    def action(self, percepts):\n        goal_dir = percepts.get('goal_direction')\n        if goal_dir == 'left':\n            return 'turn_left'\n        elif goal_dir == 'right':\n            return 'turn_right'\n        else:\n            return 'move_forward'\n\nclass Wander(Behavior):\n    \"\"\"Low priority: random exploration.\"\"\"\n    def __init__(self):\n        super().__init__(\"Wander\", priority=1)\n\n    def is_active(self, percepts):\n        return True  # Always active as fallback\n\n    def action(self, percepts):\n        import random\n        return random.choice(['move_forward', 'turn_left', 'turn_right'])\n\nclass SubsumptionAgent:\n    \"\"\"Agent using subsumption architecture.\"\"\"\n    def __init__(self):\n        self.behaviors = []\n\n    def add_behavior(self, behavior):\n        \"\"\"Add behavior to agent.\"\"\"\n        self.behaviors.append(behavior)\n        # Sort by priority (highest first)\n        self.behaviors.sort(key=lambda b: b.priority, reverse=True)\n\n    def select_action(self, percepts):\n        \"\"\"Select action from highest priority active behavior.\"\"\"\n        for behavior in self.behaviors:\n            if behavior.is_active(percepts):\n                action = behavior.action(percepts)\n                return action, behavior.name\n\n        return 'stop', 'None'\n\n# Test subsumption agent\nprint(\"Subsumption Architecture Agent\")\nprint(\"=\"*50)\n\n# Create agent\nagent = SubsumptionAgent()\n\n# Add behaviors (order doesn't matter, sorted by priority)\nagent.add_behavior(Wander())\nagent.add_behavior(MoveToGoal())\nagent.add_behavior(AvoidObstacle())\n\nprint(\"Behaviors (by priority):\")\nfor b in agent.behaviors:\n    print(f\"  {b.name} (priority {b.priority})\")\nprint()\n\n# Test scenarios\nscenarios = [\n    {\n        'name': 'Obstacle ahead',\n        'percepts': {'obstacle_ahead': True, 'obstacle_left': False, 'goal_detected': True, 'goal_direction': 'forward'}\n    },\n    {\n        'name': 'Goal visible, no obstacles',\n        'percepts': {'obstacle_ahead': False, 'goal_detected': True, 'goal_direction': 'left'}\n    },\n    {\n        'name': 'No goal, no obstacles',\n        'percepts': {'obstacle_ahead': False, 'goal_detected': False}\n    },\n    {\n        'name': 'Goal right, obstacle ahead',\n        'percepts': {'obstacle_ahead': True, 'obstacle_left': True, 'goal_detected': True, 'goal_direction': 'right'}\n    }\n]\n\nfor scenario in scenarios:\n    action, behavior = agent.select_action(scenario['percepts'])\n    print(f\"Scenario: {scenario['name']}\")\n    print(f\"  Active behavior: {behavior}\")\n    print(f\"  Action: {action}\")\n    print()\n\n# Simulation\nprint(\"=\"*50)\nprint(\"Simulation\")\nprint(\"=\"*50)\n\nposition = (0, 0)\ngoal = (5, 5)\nobstacles = [(2, 2), (3, 2), (4, 3)]\n\nfor step in range(15):\n    # Create percepts based on position\n    percepts = {}\n\n    # Check obstacle ahead (simplified)\n    x, y = position\n    obstacle_positions = [\n        (x+1, y), (x, y+1), (x-1, y), (x, y-1)\n    ]\n    percepts['obstacle_ahead'] = any(pos in obstacles for pos in obstacle_positions)\n    percepts['obstacle_left'] = (x-1, y) in obstacles\n\n    # Check goal\n    percepts['goal_detected'] = True\n    gx, gy = goal\n    if gx > x:\n        percepts['goal_direction'] = 'right'\n    elif gx < x:\n        percepts['goal_direction'] = 'left'\n    elif gy > y:\n        percepts['goal_direction'] = 'forward'\n    else:\n        percepts['goal_direction'] = 'forward'\n\n    # Select action\n    action, behavior = agent.select_action(percepts)\n\n    print(f\"Step {step+1}: Position {position}, Behavior: {behavior}, Action: {action}\")\n\n    # Update position (simplified)\n    if action == 'move_forward' and not percepts['obstacle_ahead']:\n        if position[1] < goal[1]:\n            position = (position[0], position[1] + 1)\n        elif position[0] < goal[0]:\n            position = (position[0] + 1, position[1])\n    elif action == 'turn_right':\n        if position[0] < goal[0]:\n            position = (position[0] + 1, position[1])\n    elif action == 'turn_left':\n        if position[1] < goal[1]:\n            position = (position[0], position[1] + 1)\n\n    if position == goal:\n        print(f\"\\nGoal reached at step {step+1}!\")\n        break",
    "testCases": [
      {
        "input": "agent.select_action(percepts)",
        "isHidden": false,
        "description": "Test agent selects highest priority active behavior"
      },
      {
        "input": "behavior.is_active(percepts)",
        "isHidden": false,
        "description": "Test behavior activation conditions"
      },
      {
        "input": "agent with obstacle",
        "isHidden": false,
        "description": "Test avoid behavior overrides move-to-goal"
      }
    ],
    "hints": [
      "Subsumption: higher priority behaviors suppress lower ones",
      "Check behaviors in priority order, use first active one",
      "Avoid behavior should have highest priority for safety"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex08",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Agent Environment Interaction Loop",
    "difficulty": 2,
    "description": "Implement a complete agent-environment interaction simulation.\n\nYour implementation should:\n- Define environment with states and transitions\n- Implement agent with perception and action\n- Run interaction loop\n- Track performance metrics",
    "starterCode": "class Environment:\n    def __init__(self, initial_state):\n        self.state = initial_state\n\n    def get_percepts(self):\n        pass\n\n    def apply_action(self, action):\n        pass\n\nclass Agent:\n    def perceive(self, percepts):\n        pass\n\n    def decide(self):\n        pass\n\ndef run_simulation(env, agent, steps):\n    pass",
    "solution": "import random\n\nclass GridEnvironment:\n    \"\"\"Simple grid world environment.\"\"\"\n    def __init__(self, size, goal, obstacles):\n        self.size = size\n        self.goal = goal\n        self.obstacles = obstacles\n        self.agent_pos = (0, 0)\n        self.steps = 0\n        self.done = False\n\n    def get_percepts(self):\n        \"\"\"Return percepts for agent.\"\"\"\n        return {\n            'position': self.agent_pos,\n            'goal': self.goal,\n            'obstacles': self.obstacles,\n            'at_goal': self.agent_pos == self.goal,\n            'steps': self.steps\n        }\n\n    def apply_action(self, action):\n        \"\"\"Apply action and return reward.\"\"\"\n        if self.done:\n            return 0\n\n        x, y = self.agent_pos\n\n        # Apply movement\n        if action == 'up':\n            new_pos = (x, min(y + 1, self.size - 1))\n        elif action == 'down':\n            new_pos = (x, max(y - 1, 0))\n        elif action == 'left':\n            new_pos = (max(x - 1, 0), y)\n        elif action == 'right':\n            new_pos = (min(x + 1, self.size - 1), y)\n        else:\n            new_pos = self.agent_pos\n\n        # Check if valid (not obstacle)\n        if new_pos not in self.obstacles:\n            self.agent_pos = new_pos\n\n        self.steps += 1\n\n        # Calculate reward\n        if self.agent_pos == self.goal:\n            self.done = True\n            return 10  # Goal reached\n        elif self.agent_pos in self.obstacles:\n            return -5  # Hit obstacle\n        else:\n            return -1  # Step cost\n\nclass SimpleAgent:\n    \"\"\"Simple goal-directed agent.\"\"\"\n    def __init__(self):\n        self.percepts = None\n\n    def perceive(self, percepts):\n        \"\"\"Store percepts.\"\"\"\n        self.percepts = percepts\n\n    def decide(self):\n        \"\"\"Decide action based on percepts.\"\"\"\n        if self.percepts is None:\n            return 'stop'\n\n        if self.percepts['at_goal']:\n            return 'stop'\n\n        # Simple strategy: move toward goal\n        pos = self.percepts['position']\n        goal = self.percepts['goal']\n        obstacles = self.percepts['obstacles']\n\n        x, y = pos\n        gx, gy = goal\n\n        # Try to move toward goal\n        if gx > x:\n            if (x + 1, y) not in obstacles:\n                return 'right'\n        elif gx < x:\n            if (x - 1, y) not in obstacles:\n                return 'left'\n\n        if gy > y:\n            if (x, y + 1) not in obstacles:\n                return 'up'\n        elif gy < y:\n            if (x, y - 1) not in obstacles:\n                return 'down'\n\n        # If blocked, try random move\n        return random.choice(['up', 'down', 'left', 'right'])\n\ndef run_simulation(env, agent, max_steps=100):\n    \"\"\"Run agent-environment interaction loop.\"\"\"\n    total_reward = 0\n    history = []\n\n    for step in range(max_steps):\n        # Get percepts\n        percepts = env.get_percepts()\n\n        # Agent perceives\n        agent.perceive(percepts)\n\n        # Agent decides\n        action = agent.decide()\n\n        # Apply action to environment\n        reward = env.apply_action(action)\n        total_reward += reward\n\n        # Record history\n        history.append({\n            'step': step,\n            'position': percepts['position'],\n            'action': action,\n            'reward': reward\n        })\n\n        # Check if done\n        if env.done:\n            print(f\"Goal reached in {step + 1} steps!\")\n            break\n\n    return {\n        'total_reward': total_reward,\n        'steps': env.steps,\n        'success': env.done,\n        'history': history\n    }\n\n# Test simulation\nprint(\"Agent-Environment Simulation\")\nprint(\"=\"*50)\n\n# Create environment\nenv = GridEnvironment(\n    size=6,\n    goal=(5, 5),\n    obstacles=[(2, 2), (2, 3), (3, 2)]\n)\n\n# Create agent\nagent = SimpleAgent()\n\nprint(f\"Environment size: {env.size}x{env.size}\")\nprint(f\"Goal: {env.goal}\")\nprint(f\"Obstacles: {env.obstacles}\")\nprint(f\"Starting position: {env.agent_pos}\")\nprint()\n\n# Run simulation\nresults = run_simulation(env, agent, max_steps=100)\n\nprint()\nprint(\"Results:\")\nprint(f\"  Success: {results['success']}\")\nprint(f\"  Total steps: {results['steps']}\")\nprint(f\"  Total reward: {results['total_reward']}\")\nprint()\n\nprint(\"Path taken:\")\nfor i, entry in enumerate(results['history'][:20]):  # Show first 20 steps\n    print(f\"  Step {entry['step'] + 1}: {entry['position']} -> {entry['action']} (reward: {entry['reward']})\")\n\nif len(results['history']) > 20:\n    print(f\"  ... ({len(results['history']) - 20} more steps)\")",
    "testCases": [
      {
        "input": "env.get_percepts()",
        "isHidden": false,
        "description": "Test environment provides percepts"
      },
      {
        "input": "env.apply_action(action)",
        "isHidden": false,
        "description": "Test environment updates based on action"
      },
      {
        "input": "run_simulation(env, agent, steps)",
        "isHidden": false,
        "description": "Test complete simulation runs correctly"
      }
    ],
    "hints": [
      "Perception-action loop: percepts -> decision -> action -> update environment",
      "Environment maintains state and returns percepts to agent",
      "Agent chooses actions based on percepts, environment applies them"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex09",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Performance Measure for Agent Evaluation",
    "difficulty": 2,
    "description": "Implement performance measures to evaluate agent effectiveness.\n\nYour implementation should:\n- Define multiple performance metrics\n- Track agent performance over time\n- Compare different agent strategies\n- Visualize performance data",
    "starterCode": "class PerformanceMeasure:\n    def __init__(self):\n        pass\n\n    def record(self, state, action, reward):\n        pass\n\n    def evaluate(self):\n        pass\n\n    def compare_agents(self, agent_results):\n        pass",
    "solution": "class PerformanceMeasure:\n    \"\"\"Track and evaluate agent performance.\"\"\"\n    def __init__(self, metrics=None):\n        if metrics is None:\n            metrics = ['total_reward', 'steps', 'success_rate']\n\n        self.metrics = metrics\n        self.history = []\n        self.episodes = []\n        self.current_episode = {\n            'rewards': [],\n            'actions': [],\n            'steps': 0,\n            'success': False\n        }\n\n    def record(self, state, action, reward):\n        \"\"\"Record single step.\"\"\"\n        self.history.append({\n            'state': state,\n            'action': action,\n            'reward': reward\n        })\n\n        self.current_episode['rewards'].append(reward)\n        self.current_episode['actions'].append(action)\n        self.current_episode['steps'] += 1\n\n    def end_episode(self, success=False):\n        \"\"\"Mark episode as complete.\"\"\"\n        self.current_episode['success'] = success\n        self.current_episode['total_reward'] = sum(self.current_episode['rewards'])\n        self.episodes.append(self.current_episode.copy())\n\n        # Reset for next episode\n        self.current_episode = {\n            'rewards': [],\n            'actions': [],\n            'steps': 0,\n            'success': False\n        }\n\n    def evaluate(self):\n        \"\"\"Calculate performance metrics.\"\"\"\n        if not self.episodes:\n            return {}\n\n        metrics = {}\n\n        # Total reward\n        metrics['total_reward'] = sum(ep['total_reward'] for ep in self.episodes)\n        metrics['avg_reward_per_episode'] = metrics['total_reward'] / len(self.episodes)\n\n        # Steps\n        metrics['total_steps'] = sum(ep['steps'] for ep in self.episodes)\n        metrics['avg_steps_per_episode'] = metrics['total_steps'] / len(self.episodes)\n\n        # Success rate\n        successes = sum(1 for ep in self.episodes if ep['success'])\n        metrics['success_rate'] = successes / len(self.episodes)\n        metrics['num_episodes'] = len(self.episodes)\n        metrics['num_successes'] = successes\n\n        # Efficiency (reward per step)\n        metrics['efficiency'] = metrics['total_reward'] / max(metrics['total_steps'], 1)\n\n        return metrics\n\n    def compare_agents(self, agent_results):\n        \"\"\"Compare multiple agents.\"\"\"\n        comparison = {}\n\n        for agent_name, results in agent_results.items():\n            comparison[agent_name] = results.evaluate()\n\n        return comparison\n\n    def print_summary(self):\n        \"\"\"Print performance summary.\"\"\"\n        metrics = self.evaluate()\n\n        print(\"Performance Summary\")\n        print(\"=\"*50)\n        print(f\"Episodes: {metrics.get('num_episodes', 0)}\")\n        print(f\"Success rate: {metrics.get('success_rate', 0):.1%}\")\n        print(f\"Avg reward per episode: {metrics.get('avg_reward_per_episode', 0):.2f}\")\n        print(f\"Avg steps per episode: {metrics.get('avg_steps_per_episode', 0):.1f}\")\n        print(f\"Efficiency (reward/step): {metrics.get('efficiency', 0):.3f}\")\n\n    def plot_learning_curve(self):\n        \"\"\"Plot learning curve (rewards over episodes).\"\"\"\n        if not self.episodes:\n            print(\"No episodes to plot\")\n            return\n\n        episode_numbers = list(range(1, len(self.episodes) + 1))\n        rewards = [ep['total_reward'] for ep in self.episodes]\n\n        # Simple text-based plot\n        print(\"\\nLearning Curve (Total Reward per Episode)\")\n        print(\"=\"*50)\n\n        max_reward = max(rewards) if rewards else 1\n        min_reward = min(rewards) if rewards else 0\n\n        for i, (ep_num, reward) in enumerate(zip(episode_numbers[:20], rewards[:20])):\n            # Normalize to 0-40 characters\n            if max_reward != min_reward:\n                bar_len = int(40 * (reward - min_reward) / (max_reward - min_reward))\n            else:\n                bar_len = 20\n\n            bar = '#' * bar_len\n            print(f\"Ep {ep_num:3d}: {bar} ({reward:.1f})\")\n\n        if len(episode_numbers) > 20:\n            print(f\"... ({len(episode_numbers) - 20} more episodes)\")\n\n# Test performance measurement\nprint(\"Performance Measurement Example\")\nprint(\"=\"*50)\n\n# Simulate two agents with different strategies\npm_agent1 = PerformanceMeasure()\npm_agent2 = PerformanceMeasure()\n\n# Agent 1: Conservative (slower but steady)\nprint(\"Agent 1: Conservative strategy\")\nfor episode in range(10):\n    for step in range(15):\n        reward = -1  # Step cost\n        pm_agent1.record(None, 'move', reward)\n\n    # Reaches goal\n    pm_agent1.record(None, 'reach_goal', 10)\n    pm_agent1.end_episode(success=True)\n\nprint()\npm_agent1.print_summary()\n\n# Agent 2: Aggressive (faster but riskier)\nprint(\"\\nAgent 2: Aggressive strategy\")\nfor episode in range(10):\n    # Sometimes fails (hits obstacle)\n    if episode % 3 == 0:\n        for step in range(5):\n            pm_agent2.record(None, 'move', -1)\n        pm_agent2.record(None, 'hit_obstacle', -5)\n        pm_agent2.end_episode(success=False)\n    else:\n        # Reaches goal quickly\n        for step in range(8):\n            pm_agent2.record(None, 'move', -1)\n        pm_agent2.record(None, 'reach_goal', 10)\n        pm_agent2.end_episode(success=True)\n\nprint()\npm_agent2.print_summary()\n\n# Compare agents\nprint(\"\\n\" + \"=\"*50)\nprint(\"Agent Comparison\")\nprint(\"=\"*50)\n\ncomparison = pm_agent1.compare_agents({\n    'Conservative': pm_agent1,\n    'Aggressive': pm_agent2\n})\n\nfor agent_name, metrics in comparison.items():\n    print(f\"\\n{agent_name}:\")\n    print(f\"  Success rate: {metrics['success_rate']:.1%}\")\n    print(f\"  Avg reward: {metrics['avg_reward_per_episode']:.2f}\")\n    print(f\"  Avg steps: {metrics['avg_steps_per_episode']:.1f}\")\n    print(f\"  Efficiency: {metrics['efficiency']:.3f}\")\n\n# Show learning curve\npm_agent1.plot_learning_curve()",
    "testCases": [
      {
        "input": "pm.record(state, action, reward)",
        "isHidden": false,
        "description": "Test recording agent steps"
      },
      {
        "input": "pm.evaluate()",
        "isHidden": false,
        "description": "Test computing performance metrics"
      },
      {
        "input": "pm.compare_agents(agent_results)",
        "isHidden": false,
        "description": "Test comparing multiple agents"
      }
    ],
    "hints": [
      "Track multiple metrics: total reward, success rate, efficiency",
      "Record episodes separately to measure learning over time",
      "Compare agents using normalized metrics like reward per step"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex10",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Rational Agent Decision Making",
    "difficulty": 2,
    "description": "Implement a rational agent that selects actions to maximize expected utility.\n\nYour implementation should:\n- Evaluate utility of possible outcomes\n- Calculate expected utility for each action\n- Select action with highest expected utility\n- Handle probabilistic outcomes",
    "starterCode": "class RationalAgent:\n    def __init__(self, utility_function):\n        self.utility = utility_function\n\n    def expected_utility(self, action, outcomes):\n        # Calculate expected utility of action\n        pass\n\n    def choose_action(self, possible_actions):\n        # Choose action maximizing expected utility\n        pass",
    "solution": "class RationalAgent:\n    \"\"\"Agent that maximizes expected utility.\"\"\"\n    def __init__(self, utility_function):\n        self.utility = utility_function\n        self.history = []\n\n    def expected_utility(self, action, outcomes, probabilities):\n        \"\"\"\n        Calculate expected utility of action.\n\n        action: action identifier\n        outcomes: list of possible outcome states\n        probabilities: probability of each outcome\n        \"\"\"\n        expected = 0.0\n\n        for outcome, prob in zip(outcomes, probabilities):\n            utility_val = self.utility(outcome)\n            expected += prob * utility_val\n\n        return expected\n\n    def choose_action(self, possible_actions, outcome_model):\n        \"\"\"\n        Choose action maximizing expected utility.\n\n        possible_actions: list of available actions\n        outcome_model: function(action) -> (outcomes, probabilities)\n        \"\"\"\n        best_action = None\n        best_expected = float('-inf')\n\n        for action in possible_actions:\n            outcomes, probabilities = outcome_model(action)\n            expected = self.expected_utility(action, outcomes, probabilities)\n\n            if expected > best_expected:\n                best_expected = expected\n                best_action = action\n\n        self.history.append({\n            'action': best_action,\n            'expected_utility': best_expected\n        })\n\n        return best_action, best_expected\n\n    def performance_summary(self):\n        \"\"\"Summary of decision history.\"\"\"\n        if not self.history:\n            return \"No decisions made\"\n\n        total_expected = sum(h['expected_utility'] for h in self.history)\n        avg_expected = total_expected / len(self.history)\n\n        return {\n            'decisions': len(self.history),\n            'avg_expected_utility': avg_expected,\n            'total_expected_utility': total_expected\n        }\n\n# Example: Investment decision agent\n\ndef investment_utility(outcome):\n    \"\"\"Utility function for money (with risk aversion).\"\"\"\n    # Square root for diminishing marginal utility\n    money = outcome['money']\n    return money ** 0.5\n\n# Create agent\nagent = RationalAgent(investment_utility)\n\nprint(\"Rational Agent - Investment Decisions\")\nprint(\"=\"*50)\n\n# Define outcome models for different actions\ndef outcome_model(action):\n    \"\"\"Model outcomes and probabilities for each action.\"\"\"\n    if action == 'safe_investment':\n        # Safe: guaranteed 5% return\n        outcomes = [\n            {'money': 105}\n        ]\n        probabilities = [1.0]\n\n    elif action == 'moderate_risk':\n        # Moderate: 60% chance of 15% return, 40% chance of 0% return\n        outcomes = [\n            {'money': 115},\n            {'money': 100}\n        ]\n        probabilities = [0.6, 0.4]\n\n    elif action == 'high_risk':\n        # High risk: 40% chance of 50% return, 60% chance of -20% loss\n        outcomes = [\n            {'money': 150},\n            {'money': 80}\n        ]\n        probabilities = [0.4, 0.6]\n\n    else:\n        outcomes = [{'money': 100}]\n        probabilities = [1.0]\n\n    return outcomes, probabilities\n\n# Available actions\nactions = ['safe_investment', 'moderate_risk', 'high_risk']\n\nprint(\"Initial capital: $100\")\nprint(\"\\nEvaluating investment options:\")\nprint()\n\n# Evaluate each action\nfor action in actions:\n    outcomes, probs = outcome_model(action)\n    expected = agent.expected_utility(action, outcomes, probs)\n\n    print(f\"{action}:\")\n    print(f\"  Possible outcomes:\")\n    for outcome, prob in zip(outcomes, probs):\n        print(f\"    ${outcome['money']} (prob: {prob:.1%})\")\n    print(f\"  Expected utility: {expected:.2f}\")\n    print()\n\n# Agent chooses best action\nbest_action, best_utility = agent.choose_action(actions, outcome_model)\n\nprint(\"=\"*50)\nprint(f\"Agent chooses: {best_action}\")\nprint(f\"Expected utility: {best_utility:.2f}\")\nprint()\n\n# Multiple decisions\nprint(\"=\"*50)\nprint(\"Multiple Decision Scenario\")\nprint(\"=\"*50)\n\ndecisions = []\nfor i in range(5):\n    action, utility = agent.choose_action(actions, outcome_model)\n    decisions.append(action)\n    print(f\"Decision {i+1}: {action} (EU: {utility:.2f})\")\n\nsummary = agent.performance_summary()\nprint(f\"\\nSummary:\")\nprint(f\"  Total decisions: {summary['decisions']}\")\nprint(f\"  Avg expected utility: {summary['avg_expected_utility']:.2f}\")\n\n# Test with different utility function (risk-seeking)\nprint(\"\\n\" + \"=\"*50)\nprint(\"Risk-Seeking Agent (logarithmic utility)\")\nprint(\"=\"*50)\n\ndef risk_seeking_utility(outcome):\n    \"\"\"Risk-seeking utility function.\"\"\"\n    import math\n    money = outcome['money']\n    # Exponential for risk-seeking behavior\n    return money ** 1.5\n\nagent2 = RationalAgent(risk_seeking_utility)\n\nfor action in actions:\n    outcomes, probs = outcome_model(action)\n    expected = agent2.expected_utility(action, outcomes, probs)\n    print(f\"{action}: EU = {expected:.2f}\")\n\nbest_action2, best_utility2 = agent2.choose_action(actions, outcome_model)\nprint(f\"\\nRisk-seeking agent chooses: {best_action2}\")",
    "testCases": [
      {
        "input": "agent.expected_utility(action, outcomes, probabilities)",
        "isHidden": false,
        "description": "Test expected utility calculation"
      },
      {
        "input": "agent.choose_action(possible_actions, outcome_model)",
        "isHidden": false,
        "description": "Test agent selects action with highest expected utility"
      },
      {
        "input": "different utility functions",
        "isHidden": false,
        "description": "Test different utility functions lead to different choices"
      }
    ],
    "hints": [
      "Expected utility = sum of (probability × utility) over all outcomes",
      "Compare expected utility of all actions, choose highest",
      "Utility function encodes agent preferences (risk-averse vs risk-seeking)"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex11",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Agent State Representation",
    "difficulty": 2,
    "description": "Implement different state representations for agents.\n\nYour implementation should:\n- Atomic state representation\n- Factored state representation (feature-based)\n- Structured state representation (objects and relations)\n- Convert between representations",
    "starterCode": "class AtomicState:\n    pass\n\nclass FactoredState:\n    pass\n\nclass StructuredState:\n    pass\n\ndef convert_atomic_to_factored(atomic_state):\n    pass\n\ndef convert_factored_to_structured(factored_state):\n    pass",
    "solution": "class AtomicState:\n    \"\"\"Atomic state: single indivisible identifier.\"\"\"\n    def __init__(self, state_id):\n        self.state_id = state_id\n\n    def __repr__(self):\n        return f\"AtomicState({self.state_id})\"\n\n    def __eq__(self, other):\n        return isinstance(other, AtomicState) and self.state_id == other.state_id\n\n    def __hash__(self):\n        return hash(self.state_id)\n\nclass FactoredState:\n    \"\"\"Factored state: set of variable-value pairs.\"\"\"\n    def __init__(self, variables):\n        self.variables = variables  # dict: variable -> value\n\n    def __repr__(self):\n        vars_str = \", \".join(f\"{k}={v}\" for k, v in self.variables.items())\n        return f\"FactoredState({vars_str})\"\n\n    def __eq__(self, other):\n        return isinstance(other, FactoredState) and self.variables == other.variables\n\n    def __hash__(self):\n        return hash(tuple(sorted(self.variables.items())))\n\n    def get(self, variable, default=None):\n        return self.variables.get(variable, default)\n\n    def set(self, variable, value):\n        self.variables[variable] = value\n\nclass StructuredState:\n    \"\"\"Structured state: objects with properties and relations.\"\"\"\n    def __init__(self):\n        self.objects = {}  # object_id -> properties dict\n        self.relations = []  # list of (relation_name, obj1, obj2)\n\n    def add_object(self, obj_id, properties):\n        self.objects[obj_id] = properties\n\n    def add_relation(self, relation_name, obj1, obj2):\n        self.relations.append((relation_name, obj1, obj2))\n\n    def get_object_property(self, obj_id, property_name):\n        return self.objects.get(obj_id, {}).get(property_name)\n\n    def query_relations(self, relation_name):\n        return [(o1, o2) for rel, o1, o2 in self.relations if rel == relation_name]\n\n    def __repr__(self):\n        objs_str = f\"{len(self.objects)} objects\"\n        rels_str = f\"{len(self.relations)} relations\"\n        return f\"StructuredState({objs_str}, {rels_str})\"\n\ndef convert_atomic_to_factored(atomic_state, state_mapping):\n    \"\"\"\n    Convert atomic state to factored representation.\n    state_mapping: dict mapping state_id -> variables dict\n    \"\"\"\n    variables = state_mapping.get(atomic_state.state_id, {})\n    return FactoredState(variables.copy())\n\ndef convert_factored_to_atomic(factored_state, reverse_mapping):\n    \"\"\"\n    Convert factored state to atomic representation.\n    reverse_mapping: dict mapping variables tuple -> state_id\n    \"\"\"\n    key = tuple(sorted(factored_state.variables.items()))\n    state_id = reverse_mapping.get(key, f\"state_{hash(key)}\")\n    return AtomicState(state_id)\n\ndef convert_factored_to_structured(factored_state):\n    \"\"\"Convert factored state to structured representation.\"\"\"\n    structured = StructuredState()\n\n    # Extract objects from variables\n    # Assume variables like \"robot_x\", \"robot_y\", \"box_x\", \"box_y\"\n\n    objects = {}\n    for var, val in factored_state.variables.items():\n        parts = var.split('_')\n        if len(parts) >= 2:\n            obj_name = parts[0]\n            property_name = '_'.join(parts[1:])\n\n            if obj_name not in objects:\n                objects[obj_name] = {}\n            objects[obj_name][property_name] = val\n        else:\n            # Global property\n            if 'global' not in objects:\n                objects['global'] = {}\n            objects['global'][var] = val\n\n    # Add objects to structured state\n    for obj_name, properties in objects.items():\n        structured.add_object(obj_name, properties)\n\n    # Infer relations based on properties\n    # Example: if robot and box have same position, they're \"at\" same location\n    obj_names = list(objects.keys())\n    for i, obj1 in enumerate(obj_names):\n        for obj2 in obj_names[i+1:]:\n            if obj1 == 'global' or obj2 == 'global':\n                continue\n\n            # Check if same location\n            if objects[obj1].get('x') == objects[obj2].get('x') and                objects[obj1].get('y') == objects[obj2].get('y'):\n                structured.add_relation('same_location', obj1, obj2)\n\n    return structured\n\n# Test different representations\nprint(\"State Representation Examples\")\nprint(\"=\"*50)\n\n# Atomic state\nprint(\"\\n1. Atomic State:\")\natomic = AtomicState(\"state_42\")\nprint(f\"   {atomic}\")\n\n# Factored state\nprint(\"\\n2. Factored State:\")\nfactored = FactoredState({\n    'robot_x': 3,\n    'robot_y': 2,\n    'box_x': 3,\n    'box_y': 2,\n    'goal_x': 5,\n    'goal_y': 5,\n    'holding': True\n})\nprint(f\"   {factored}\")\nprint(f\"   Robot position: ({factored.get('robot_x')}, {factored.get('robot_y')})\")\nprint(f\"   Holding: {factored.get('holding')}\")\n\n# Structured state\nprint(\"\\n3. Structured State:\")\nstructured = StructuredState()\nstructured.add_object('robot', {'x': 3, 'y': 2, 'holding': True})\nstructured.add_object('box', {'x': 3, 'y': 2, 'weight': 10})\nstructured.add_object('goal', {'x': 5, 'y': 5})\nstructured.add_relation('at', 'robot', 'box')\nstructured.add_relation('holding', 'robot', 'box')\n\nprint(f\"   {structured}\")\nprint(f\"   Objects:\")\nfor obj_id, props in structured.objects.items():\n    print(f\"     {obj_id}: {props}\")\nprint(f\"   Relations:\")\nfor rel in structured.relations:\n    print(f\"     {rel[0]}({rel[1]}, {rel[2]})\")\n\n# Conversions\nprint(\"\\n\" + \"=\"*50)\nprint(\"State Conversions\")\nprint(\"=\"*50)\n\n# Atomic to Factored\nprint(\"\\nAtomic -> Factored:\")\nstate_mapping = {\n    \"state_42\": {'x': 5, 'y': 3, 'energy': 100}\n}\natomic = AtomicState(\"state_42\")\nfactored_from_atomic = convert_atomic_to_factored(atomic, state_mapping)\nprint(f\"   {atomic} -> {factored_from_atomic}\")\n\n# Factored to Structured\nprint(\"\\nFactored -> Structured:\")\nfactored = FactoredState({\n    'robot_x': 2,\n    'robot_y': 3,\n    'box_x': 2,\n    'box_y': 3,\n})\nstructured_from_factored = convert_factored_to_structured(factored)\nprint(f\"   {factored}\")\nprint(f\"   -> {structured_from_factored}\")\nprint(f\"   Objects: {list(structured_from_factored.objects.keys())}\")\nprint(f\"   Relations: {structured_from_factored.relations}\")\n\n# Compare representations\nprint(\"\\n\" + \"=\"*50)\nprint(\"Representation Comparison\")\nprint(\"=\"*50)\nprint(\"\\nAtomic State:\")\nprint(\"  + Simplest representation\")\nprint(\"  + Easy to implement\")\nprint(\"  - No structure for reasoning\")\nprint(\"  - Exponential number of states\")\n\nprint(\"\\nFactored State:\")\nprint(\"  + Compact representation\")\nprint(\"  + Easy to query properties\")\nprint(\"  + Good for feature-based learning\")\nprint(\"  - No explicit object relations\")\n\nprint(\"\\nStructured State:\")\nprint(\"  + Explicit objects and relations\")\nprint(\"  + Supports complex reasoning\")\nprint(\"  + Natural for many domains\")\nprint(\"  - More complex to implement\")",
    "testCases": [
      {
        "input": "factored_state.get(variable)",
        "isHidden": false,
        "description": "Test factored state variable access"
      },
      {
        "input": "structured_state.query_relations(name)",
        "isHidden": false,
        "description": "Test structured state relation queries"
      },
      {
        "input": "convert_factored_to_structured(factored)",
        "isHidden": false,
        "description": "Test conversion between representations"
      }
    ],
    "hints": [
      "Atomic: single identifier, simplest but least informative",
      "Factored: variable-value pairs, good for feature-based reasoning",
      "Structured: objects and relations, most expressive"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex12",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "PEAS Analysis for Agent Design",
    "difficulty": 1,
    "description": "Perform PEAS analysis (Performance, Environment, Actuators, Sensors) for different agent types.\n\nYour implementation should:\n- Define PEAS for various agent types\n- Validate PEAS specifications\n- Compare different agent designs\n- Generate agent specifications from PEAS",
    "starterCode": "class PEASSpecification:\n    def __init__(self, performance, environment, actuators, sensors):\n        pass\n\n    def validate(self):\n        # Check if specification is complete\n        pass\n\n    def compare(self, other_spec):\n        # Compare two PEAS specifications\n        pass",
    "solution": "class PEASSpecification:\n    \"\"\"PEAS specification for agent design.\"\"\"\n    def __init__(self, agent_type, performance, environment, actuators, sensors):\n        self.agent_type = agent_type\n        self.performance = performance  # list of performance measures\n        self.environment = environment  # dict describing environment\n        self.actuators = actuators  # list of actuators\n        self.sensors = sensors  # list of sensors\n\n    def validate(self):\n        \"\"\"Check if specification is complete and consistent.\"\"\"\n        errors = []\n\n        if not self.performance:\n            errors.append(\"No performance measures defined\")\n\n        if not self.environment:\n            errors.append(\"Environment not specified\")\n\n        if not self.actuators:\n            errors.append(\"No actuators defined\")\n\n        if not self.sensors:\n            errors.append(\"No sensors defined\")\n\n        # Check environment properties\n        required_props = ['observable', 'deterministic', 'episodic', 'static', 'discrete', 'agents']\n        for prop in required_props:\n            if prop not in self.environment:\n                errors.append(f\"Environment property '{prop}' not specified\")\n\n        return len(errors) == 0, errors\n\n    def compare(self, other_spec):\n        \"\"\"Compare two PEAS specifications.\"\"\"\n        comparison = {\n            'agent_types': (self.agent_type, other_spec.agent_type),\n            'differences': []\n        }\n\n        # Compare performance measures\n        perf_diff = set(self.performance) ^ set(other_spec.performance)\n        if perf_diff:\n            comparison['differences'].append(f\"Performance measures differ: {perf_diff}\")\n\n        # Compare actuators\n        act_diff = set(self.actuators) ^ set(other_spec.actuators)\n        if act_diff:\n            comparison['differences'].append(f\"Actuators differ: {act_diff}\")\n\n        # Compare sensors\n        sens_diff = set(self.sensors) ^ set(other_spec.sensors)\n        if sens_diff:\n            comparison['differences'].append(f\"Sensors differ: {sens_diff}\")\n\n        # Compare environment\n        for key in self.environment:\n            if key in other_spec.environment:\n                if self.environment[key] != other_spec.environment[key]:\n                    comparison['differences'].append(\n                        f\"Environment {key}: {self.environment[key]} vs {other_spec.environment[key]}\"\n                    )\n\n        return comparison\n\n    def __repr__(self):\n        return f\"PEAS({self.agent_type})\"\n\n    def print_specification(self):\n        \"\"\"Print detailed PEAS specification.\"\"\"\n        print(f\"\\nPEAS Specification: {self.agent_type}\")\n        print(\"=\"*60)\n\n        print(\"\\nPerformance Measures:\")\n        for pm in self.performance:\n            print(f\"  - {pm}\")\n\n        print(\"\\nEnvironment:\")\n        for key, val in self.environment.items():\n            print(f\"  - {key}: {val}\")\n\n        print(\"\\nActuators:\")\n        for act in self.actuators:\n            print(f\"  - {act}\")\n\n        print(\"\\nSensors:\")\n        for sens in self.sensors:\n            print(f\"  - {sens}\")\n\n# Example PEAS specifications\n\n# 1. Autonomous Vacuum Cleaner\nvacuum_peas = PEASSpecification(\n    agent_type=\"Autonomous Vacuum Cleaner\",\n    performance=[\n        \"Amount of dirt cleaned\",\n        \"Energy efficiency\",\n        \"Coverage area\",\n        \"Time to complete cleaning\"\n    ],\n    environment={\n        'observable': 'partially',  # Can sense immediate surroundings\n        'deterministic': 'stochastic',  # Dirt appears unpredictably\n        'episodic': False,  # Continuous cleaning\n        'static': False,  # Dirt can appear while cleaning\n        'discrete': True,  # Grid-based world\n        'agents': 'single',  # Usually one vacuum\n        'properties': ['Floor type', 'Obstacles', 'Dirt distribution']\n    },\n    actuators=[\n        \"Wheels (forward, backward, turn)\",\n        \"Vacuum motor\",\n        \"Brush\",\n        \"Charging contact\"\n    ],\n    sensors=[\n        \"Dirt detector\",\n        \"Bump sensor\",\n        \"Cliff detector\",\n        \"Battery level sensor\",\n        \"Camera (optional)\"\n    ]\n)\n\n# 2. Self-Driving Car\ncar_peas = PEASSpecification(\n    agent_type=\"Self-Driving Car\",\n    performance=[\n        \"Safety (no accidents)\",\n        \"Destination reached\",\n        \"Travel time\",\n        \"Fuel efficiency\",\n        \"Passenger comfort\",\n        \"Traffic law compliance\"\n    ],\n    environment={\n        'observable': 'partially',  # Limited by sensors\n        'deterministic': 'stochastic',  # Other drivers unpredictable\n        'episodic': False,  # Continuous driving\n        'static': False,  # Dynamic traffic\n        'discrete': False,  # Continuous space and time\n        'agents': 'multi',  # Many other vehicles\n        'properties': ['Roads', 'Traffic', 'Weather', 'Pedestrians']\n    },\n    actuators=[\n        \"Steering\",\n        \"Accelerator\",\n        \"Brake\",\n        \"Turn signals\",\n        \"Horn\"\n    ],\n    sensors=[\n        \"Cameras\",\n        \"Lidar\",\n        \"Radar\",\n        \"GPS\",\n        \"IMU (Inertial Measurement Unit)\",\n        \"Wheel encoders\",\n        \"Ultrasonic sensors\"\n    ]\n)\n\n# 3. Chess-Playing Agent\nchess_peas = PEASSpecification(\n    agent_type=\"Chess Player\",\n    performance=[\n        \"Win/loss/draw outcome\",\n        \"Move quality\",\n        \"Time per move\",\n        \"Rating improvement\"\n    ],\n    environment={\n        'observable': 'fully',  # Complete board state visible\n        'deterministic': 'deterministic',  # Rules are deterministic\n        'episodic': True,  # Each game is independent\n        'static': False,  # Opponent makes moves\n        'discrete': True,  # Discrete board and moves\n        'agents': 'two',  # Two players\n        'properties': ['Chess board', '8x8 grid', 'Standard rules']\n    },\n    actuators=[\n        \"Move pieces\"\n    ],\n    sensors=[\n        \"Board state sensor\"\n    ]\n)\n\n# 4. Medical Diagnosis Agent\nmedical_peas = PEASSpecification(\n    agent_type=\"Medical Diagnosis System\",\n    performance=[\n        \"Diagnostic accuracy\",\n        \"False positive/negative rates\",\n        \"Time to diagnosis\",\n        \"Patient outcome improvement\"\n    ],\n    environment={\n        'observable': 'partially',  # Limited by tests available\n        'deterministic': 'stochastic',  # Disease progression uncertain\n        'episodic': True,  # Each patient case separate\n        'static': False,  # Patient condition changes\n        'discrete': False,  # Continuous measurements\n        'agents': 'multi',  # Doctors, other systems\n        'properties': ['Patient symptoms', 'Test results', 'Medical history']\n    },\n    actuators=[\n        \"Request tests\",\n        \"Propose diagnosis\",\n        \"Suggest treatment\",\n        \"Alert doctor\"\n    ],\n    sensors=[\n        \"Patient symptoms input\",\n        \"Lab test results\",\n        \"Imaging data\",\n        \"Vital signs\",\n        \"Medical history database\"\n    ]\n)\n\n# Display specifications\nprint(\"PEAS Specifications for Different Agent Types\")\nprint(\"=\"*60)\n\nfor peas in [vacuum_peas, car_peas, chess_peas, medical_peas]:\n    peas.print_specification()\n\n    is_valid, errors = peas.validate()\n    if is_valid:\n        print(\"\\n✓ Specification is valid and complete\")\n    else:\n        print(\"\\n✗ Specification issues:\")\n        for error in errors:\n            print(f\"  - {error}\")\n\n# Compare specifications\nprint(\"\\n\" + \"=\"*60)\nprint(\"Comparing Specifications\")\nprint(\"=\"*60)\n\ncomparison = vacuum_peas.compare(car_peas)\nprint(f\"\\nComparing: {comparison['agent_types'][0]} vs {comparison['agent_types'][1]}\")\nif comparison['differences']:\n    print(\"\\nKey differences:\")\n    for diff in comparison['differences']:\n        print(f\"  - {diff}\")\nelse:\n    print(\"\\nNo significant differences found\")\n\n# Environment type analysis\nprint(\"\\n\" + \"=\"*60)\nprint(\"Environment Type Analysis\")\nprint(\"=\"*60)\n\nagents = [vacuum_peas, car_peas, chess_peas, medical_peas]\n\nprint(\"\\n{:<30} {:<15} {:<15} {:<10} {:<10}\".format(\n    \"Agent\", \"Observable\", \"Deterministic\", \"Episodic\", \"Agents\"\n))\nprint(\"-\" * 80)\n\nfor peas in agents:\n    env = peas.environment\n    print(\"{:<30} {:<15} {:<15} {:<10} {:<10}\".format(\n        peas.agent_type[:28],\n        env['observable'],\n        env['deterministic'],\n        str(env['episodic']),\n        env['agents']\n    ))",
    "testCases": [
      {
        "input": "peas.validate()",
        "isHidden": false,
        "description": "Test PEAS specification validation"
      },
      {
        "input": "peas.compare(other_peas)",
        "isHidden": false,
        "description": "Test comparing two PEAS specifications"
      },
      {
        "input": "peas.print_specification()",
        "isHidden": false,
        "description": "Test printing complete PEAS details"
      }
    ],
    "hints": [
      "PEAS: Performance measure, Environment, Actuators, Sensors",
      "Performance: how agent success is measured",
      "Environment: properties like observable, deterministic, episodic",
      "Actuators: how agent affects environment",
      "Sensors: how agent perceives environment"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex13",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Agent Communication Language (ACL)",
    "difficulty": 3,
    "description": "Implement basic agent communication using ACL primitives.\n\nYour implementation should:\n- Define message types (inform, request, query)\n- Implement message passing between agents\n- Handle speech acts and performatives\n- Coordinate multi-agent tasks through communication",
    "starterCode": "class Message:\n    def __init__(self, sender, receiver, performative, content):\n        pass\n\nclass CommunicatingAgent:\n    def __init__(self, agent_id):\n        self.id = agent_id\n        self.inbox = []\n\n    def send(self, message):\n        pass\n\n    def receive(self):\n        pass\n\n    def handle_message(self, message):\n        pass",
    "solution": "from collections import deque\nimport time\n\nclass Message:\n    \"\"\"ACL message with performative and content.\"\"\"\n    def __init__(self, sender, receiver, performative, content, reply_to=None):\n        self.sender = sender\n        self.receiver = receiver\n        self.performative = performative  # inform, request, query, agree, refuse, etc.\n        self.content = content\n        self.reply_to = reply_to  # message being replied to\n        self.timestamp = time.time()\n\n    def __repr__(self):\n        return f\"Message({self.sender}->{self.receiver}: {self.performative}({self.content}))\"\n\nclass MessageBroker:\n    \"\"\"Central message broker for agent communication.\"\"\"\n    def __init__(self):\n        self.agents = {}  # agent_id -> agent\n        self.message_log = []\n\n    def register_agent(self, agent):\n        self.agents[agent.id] = agent\n\n    def send_message(self, message):\n        \"\"\"Route message to receiver.\"\"\"\n        self.message_log.append(message)\n\n        if message.receiver in self.agents:\n            self.agents[message.receiver].receive(message)\n            return True\n        elif message.receiver == 'broadcast':\n            # Broadcast to all except sender\n            for agent_id, agent in self.agents.items():\n                if agent_id != message.sender:\n                    agent.receive(message)\n            return True\n        else:\n            print(f\"Warning: Agent {message.receiver} not found\")\n            return False\n\nclass CommunicatingAgent:\n    \"\"\"Agent capable of ACL communication.\"\"\"\n    def __init__(self, agent_id, broker):\n        self.id = agent_id\n        self.broker = broker\n        self.inbox = deque()\n        self.knowledge_base = {}  # stores received information\n        self.pending_requests = {}  # request_id -> original message\n\n    def send(self, receiver, performative, content, reply_to=None):\n        \"\"\"Send message to another agent.\"\"\"\n        message = Message(self.id, receiver, performative, content, reply_to)\n        self.broker.send_message(message)\n        return message\n\n    def receive(self, message):\n        \"\"\"Receive message (called by broker).\"\"\"\n        self.inbox.append(message)\n\n    def process_messages(self):\n        \"\"\"Process all messages in inbox.\"\"\"\n        responses = []\n\n        while self.inbox:\n            message = self.inbox.popleft()\n            response = self.handle_message(message)\n            if response:\n                responses.append(response)\n\n        return responses\n\n    def handle_message(self, message):\n        \"\"\"Handle incoming message based on performative.\"\"\"\n        print(f\"[{self.id}] Received: {message.performative} from {message.sender}: {message.content}\")\n\n        if message.performative == 'inform':\n            return self.handle_inform(message)\n        elif message.performative == 'request':\n            return self.handle_request(message)\n        elif message.performative == 'query':\n            return self.handle_query(message)\n        elif message.performative == 'agree':\n            return self.handle_agree(message)\n        elif message.performative == 'refuse':\n            return self.handle_refuse(message)\n        elif message.performative == 'confirm':\n            return self.handle_confirm(message)\n        else:\n            print(f\"[{self.id}] Unknown performative: {message.performative}\")\n            return None\n\n    def handle_inform(self, message):\n        \"\"\"Handle inform message (receiving information).\"\"\"\n        # Store information in knowledge base\n        if isinstance(message.content, dict):\n            self.knowledge_base.update(message.content)\n        else:\n            self.knowledge_base[f\"info_from_{message.sender}\"] = message.content\n\n        # Acknowledge receipt\n        return self.send(message.sender, 'confirm', f\"Received: {message.content}\", message)\n\n    def handle_request(self, message):\n        \"\"\"Handle request message.\"\"\"\n        # Check if we can fulfill request\n        request = message.content\n\n        if self.can_fulfill(request):\n            # Agree to request\n            response = self.send(message.sender, 'agree', f\"Will fulfill: {request}\", message)\n            # Actually fulfill it\n            result = self.fulfill_request(request)\n            self.send(message.sender, 'inform', result, message)\n            return response\n        else:\n            # Refuse request\n            return self.send(message.sender, 'refuse', f\"Cannot fulfill: {request}\", message)\n\n    def handle_query(self, message):\n        \"\"\"Handle query message.\"\"\"\n        query = message.content\n\n        # Look up in knowledge base\n        result = self.knowledge_base.get(query, \"Unknown\")\n\n        return self.send(message.sender, 'inform', {query: result}, message)\n\n    def handle_agree(self, message):\n        \"\"\"Handle agreement to our request.\"\"\"\n        print(f\"[{self.id}] {message.sender} agreed to: {message.content}\")\n        return None\n\n    def handle_refuse(self, message):\n        \"\"\"Handle refusal of our request.\"\"\"\n        print(f\"[{self.id}] {message.sender} refused: {message.content}\")\n        return None\n\n    def handle_confirm(self, message):\n        \"\"\"Handle confirmation message.\"\"\"\n        print(f\"[{self.id}] Confirmed by {message.sender}: {message.content}\")\n        return None\n\n    def can_fulfill(self, request):\n        \"\"\"Check if agent can fulfill request.\"\"\"\n        # Simple implementation: check if we have relevant knowledge\n        if isinstance(request, dict):\n            task = request.get('task', '')\n            return task in ['compute', 'search', 'inform']\n        return True\n\n    def fulfill_request(self, request):\n        \"\"\"Fulfill a request and return result.\"\"\"\n        if isinstance(request, dict):\n            task = request.get('task')\n            if task == 'compute':\n                return {'result': 'computation_done'}\n            elif task == 'search':\n                return {'result': 'search_complete'}\n        return {'result': 'task_completed'}\n\n# Test agent communication\nprint(\"Agent Communication Language (ACL) Example\")\nprint(\"=\"*60)\n\n# Create broker and agents\nbroker = MessageBroker()\n\nagent1 = CommunicatingAgent('Agent1', broker)\nagent2 = CommunicatingAgent('Agent2', broker)\nagent3 = CommunicatingAgent('Agent3', broker)\n\nbroker.register_agent(agent1)\nbroker.register_agent(agent2)\nbroker.register_agent(agent3)\n\nprint(\"\\nScenario 1: Information sharing\")\nprint(\"-\"*60)\n\n# Agent1 informs Agent2 of some information\nagent1.send('Agent2', 'inform', {'temperature': 25, 'humidity': 60})\nagent2.process_messages()\n\n# Agent2 queries Agent3\nagent2.send('Agent3', 'inform', {'location': 'lab', 'status': 'active'})\nagent3.process_messages()\n\nprint(\"\\nScenario 2: Request handling\")\nprint(\"-\"*60)\n\n# Agent1 requests Agent2 to perform a task\nagent1.send('Agent2', 'request', {'task': 'compute', 'data': [1, 2, 3]})\nagent2.process_messages()\nagent1.process_messages()  # Process agree/refuse and result\n\nprint(\"\\nScenario 3: Query\")\nprint(\"-\"*60)\n\n# Agent1 queries Agent3\nagent1.send('Agent3', 'query', 'location')\nagent3.process_messages()\nagent1.process_messages()  # Process response\n\nprint(\"\\nScenario 4: Broadcast\")\nprint(\"-\"*60)\n\n# Agent1 broadcasts to all\nagent1.send('broadcast', 'inform', {'announcement': 'System update at 10pm'})\nagent2.process_messages()\nagent3.process_messages()\n\nprint(\"\\nKnowledge Bases:\")\nprint(\"-\"*60)\nprint(f\"Agent1: {agent1.knowledge_base}\")\nprint(f\"Agent2: {agent2.knowledge_base}\")\nprint(f\"Agent3: {agent3.knowledge_base}\")\n\nprint(f\"\\nTotal messages sent: {len(broker.message_log)}\")",
    "testCases": [
      {
        "input": "agent.send(receiver, performative, content)",
        "isHidden": false,
        "description": "Test sending ACL messages"
      },
      {
        "input": "agent.handle_message(message)",
        "isHidden": false,
        "description": "Test handling different performatives"
      },
      {
        "input": "broker.send_message(message)",
        "isHidden": false,
        "description": "Test message routing through broker"
      }
    ],
    "hints": [
      "ACL performatives: inform (share info), request (ask to do something), query (ask question)",
      "Use message broker pattern for routing messages between agents",
      "Agents should respond appropriately based on performative type"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex14",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Belief-Desire-Intention (BDI) Agent",
    "difficulty": 4,
    "description": "Implement a BDI agent architecture with beliefs, desires, and intentions.\n\nYour implementation should:\n- Maintain belief base (world model)\n- Define desires (goals)\n- Form intentions (committed plans)\n- Update beliefs from percepts\n- Revise intentions based on beliefs",
    "starterCode": "class BDIAgent:\n    def __init__(self):\n        self.beliefs = {}\n        self.desires = []\n        self.intentions = []\n\n    def update_beliefs(self, percepts):\n        pass\n\n    def generate_options(self):\n        pass\n\n    def filter_intentions(self):\n        pass\n\n    def execute_intention(self):\n        pass",
    "solution": "from collections import deque\n\nclass Belief:\n    \"\"\"Represents agent's belief about the world.\"\"\"\n    def __init__(self, predicate, value, confidence=1.0):\n        self.predicate = predicate\n        self.value = value\n        self.confidence = confidence  # 0.0 to 1.0\n        self.timestamp = 0\n\n    def __repr__(self):\n        return f\"Belief({self.predicate}={self.value}, conf={self.confidence:.2f})\"\n\nclass Desire:\n    \"\"\"Represents agent's goal/desire.\"\"\"\n    def __init__(self, goal, priority=1.0):\n        self.goal = goal\n        self.priority = priority\n\n    def __repr__(self):\n        return f\"Desire({self.goal}, priority={self.priority:.1f})\"\n\nclass Intention:\n    \"\"\"Represents agent's intention (committed plan).\"\"\"\n    def __init__(self, goal, plan):\n        self.goal = goal\n        self.plan = deque(plan)  # sequence of actions\n        self.completed = False\n\n    def next_action(self):\n        \"\"\"Get next action in plan.\"\"\"\n        if self.plan:\n            return self.plan[0]\n        return None\n\n    def execute_step(self):\n        \"\"\"Execute next step of plan.\"\"\"\n        if self.plan:\n            return self.plan.popleft()\n        self.completed = True\n        return None\n\n    def is_complete(self):\n        \"\"\"Check if intention is complete.\"\"\"\n        return len(self.plan) == 0\n\n    def __repr__(self):\n        return f\"Intention({self.goal}, {len(self.plan)} steps remaining)\"\n\nclass BDIAgent:\n    \"\"\"Belief-Desire-Intention agent architecture.\"\"\"\n    def __init__(self, name=\"BDI-Agent\"):\n        self.name = name\n        self.beliefs = {}  # predicate -> Belief\n        self.desires = []  # list of Desire\n        self.intentions = []  # list of Intention\n        self.time = 0\n\n    def update_beliefs(self, percepts):\n        \"\"\"Update beliefs based on percepts (sense-think-act cycle).\"\"\"\n        print(f\"[{self.name}] Updating beliefs from percepts...\")\n\n        for predicate, value in percepts.items():\n            if predicate in self.beliefs:\n                # Update existing belief\n                old_value = self.beliefs[predicate].value\n                self.beliefs[predicate].value = value\n                self.beliefs[predicate].timestamp = self.time\n                print(f\"  Updated: {predicate} = {value} (was {old_value})\")\n            else:\n                # New belief\n                self.beliefs[predicate] = Belief(predicate, value)\n                self.beliefs[predicate].timestamp = self.time\n                print(f\"  New: {predicate} = {value}\")\n\n    def add_desire(self, goal, priority=1.0):\n        \"\"\"Add new desire/goal.\"\"\"\n        desire = Desire(goal, priority)\n        self.desires.append(desire)\n        print(f\"[{self.name}] New desire: {desire}\")\n\n    def generate_options(self):\n        \"\"\"Generate possible options (plans) for desires.\"\"\"\n        options = []\n\n        for desire in self.desires:\n            # Check if desire is achievable given current beliefs\n            if self.is_achievable(desire):\n                plan = self.plan_for_desire(desire)\n                if plan:\n                    options.append((desire, plan))\n\n        return options\n\n    def is_achievable(self, desire):\n        \"\"\"Check if desire is achievable given current beliefs.\"\"\"\n        goal = desire.goal\n\n        # Simple check: can we reach the goal?\n        if 'blocked' in self.beliefs and self.beliefs['blocked'].value:\n            return False  # Can't achieve anything if blocked\n\n        return True\n\n    def plan_for_desire(self, desire):\n        \"\"\"Generate plan to achieve desire.\"\"\"\n        goal = desire.goal\n\n        # Simple planning based on goal type\n        if goal == 'reach_location':\n            target = self.beliefs.get('target_location')\n            if target:\n                return ['move_toward_target', 'move_toward_target', 'arrive']\n\n        elif goal == 'collect_resource':\n            if self.beliefs.get('resource_location'):\n                return ['move_to_resource', 'pickup_resource', 'return']\n\n        elif goal == 'avoid_danger':\n            if self.beliefs.get('danger_present', Belief('danger_present', False)).value:\n                return ['move_away', 'find_safe_location', 'wait']\n\n        return []\n\n    def filter_intentions(self, options):\n        \"\"\"Filter options to select intentions (deliberation).\"\"\"\n        print(f\"[{self.name}] Filtering {len(options)} options...\")\n\n        # Remove completed intentions\n        self.intentions = [i for i in self.intentions if not i.is_complete()]\n\n        # If no current intentions, select from options\n        if not self.intentions and options:\n            # Select highest priority desire\n            options.sort(key=lambda x: x[0].priority, reverse=True)\n            desire, plan = options[0]\n\n            intention = Intention(desire.goal, plan)\n            self.intentions.append(intention)\n            print(f\"  Selected intention: {intention}\")\n\n        # Revise intentions based on changed beliefs\n        self.revise_intentions()\n\n    def revise_intentions(self):\n        \"\"\"Revise intentions based on new beliefs.\"\"\"\n        # Check if intentions are still valid\n        to_remove = []\n\n        for intention in self.intentions:\n            # Check if goal is already achieved\n            if self.is_goal_achieved(intention.goal):\n                print(f\"  Goal achieved: {intention.goal}\")\n                to_remove.append(intention)\n\n            # Check if intention is impossible now\n            elif not self.is_intention_possible(intention):\n                print(f\"  Intention no longer possible: {intention.goal}\")\n                to_remove.append(intention)\n\n        for intention in to_remove:\n            self.intentions.remove(intention)\n\n    def is_goal_achieved(self, goal):\n        \"\"\"Check if goal is achieved.\"\"\"\n        if goal == 'reach_location':\n            return self.beliefs.get('at_target', Belief('at_target', False)).value\n        elif goal == 'collect_resource':\n            return self.beliefs.get('has_resource', Belief('has_resource', False)).value\n        return False\n\n    def is_intention_possible(self, intention):\n        \"\"\"Check if intention is still possible given beliefs.\"\"\"\n        if 'blocked' in self.beliefs and self.beliefs['blocked'].value:\n            return False\n        return True\n\n    def execute_intention(self):\n        \"\"\"Execute current intention.\"\"\"\n        if not self.intentions:\n            print(f\"[{self.name}] No intentions to execute\")\n            return None\n\n        intention = self.intentions[0]\n        action = intention.execute_step()\n\n        if action:\n            print(f\"[{self.name}] Executing: {action}\")\n            return action\n        else:\n            print(f\"[{self.name}] Intention complete: {intention.goal}\")\n            self.intentions.remove(intention)\n            return None\n\n    def run_cycle(self, percepts):\n        \"\"\"Run one BDI cycle.\"\"\"\n        print(f\"\\n{'='*60}\")\n        print(f\"[{self.name}] BDI Cycle {self.time}\")\n        print(f\"{'='*60}\")\n\n        # 1. Update beliefs from percepts\n        self.update_beliefs(percepts)\n\n        # 2. Generate options from desires\n        options = self.generate_options()\n\n        # 3. Filter/select intentions\n        self.filter_intentions(options)\n\n        # 4. Execute intention\n        action = self.execute_intention()\n\n        self.time += 1\n\n        return action\n\n    def print_state(self):\n        \"\"\"Print current BDI state.\"\"\"\n        print(f\"\\n[{self.name}] Current State:\")\n        print(f\"  Beliefs:\")\n        for belief in self.beliefs.values():\n            print(f\"    {belief}\")\n        print(f\"  Desires:\")\n        for desire in self.desires:\n            print(f\"    {desire}\")\n        print(f\"  Intentions:\")\n        for intention in self.intentions:\n            print(f\"    {intention}\")\n\n# Test BDI agent\nprint(\"Belief-Desire-Intention (BDI) Agent\")\nprint(\"=\"*60)\n\n# Create agent\nagent = BDIAgent(\"Explorer\")\n\n# Add desires\nagent.add_desire('reach_location', priority=2.0)\nagent.add_desire('collect_resource', priority=1.5)\nagent.add_desire('avoid_danger', priority=3.0)\n\n# Simulation\nscenarios = [\n    {\n        'step': 1,\n        'percepts': {\n            'target_location': (10, 10),\n            'current_location': (0, 0),\n            'blocked': False,\n            'at_target': False\n        }\n    },\n    {\n        'step': 2,\n        'percepts': {\n            'target_location': (10, 10),\n            'current_location': (5, 5),\n            'blocked': False,\n            'at_target': False,\n            'resource_location': (5, 6)\n        }\n    },\n    {\n        'step': 3,\n        'percepts': {\n            'target_location': (10, 10),\n            'current_location': (8, 8),\n            'blocked': False,\n            'at_target': False,\n            'danger_present': True\n        }\n    },\n    {\n        'step': 4,\n        'percepts': {\n            'target_location': (10, 10),\n            'current_location': (10, 10),\n            'blocked': False,\n            'at_target': True,\n            'danger_present': False\n        }\n    }\n]\n\nfor scenario in scenarios:\n    action = agent.run_cycle(scenario['percepts'])\n    agent.print_state()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"BDI Architecture Summary\")\nprint(\"=\"*60)\nprint(\"\\nBeliefs: Agent's model of the world\")\nprint(\"  - Updated from percepts\")\nprint(\"  - Can be uncertain (confidence)\")\nprint(\"\\nDesires: Agent's goals\")\nprint(\"  - Have priorities\")\nprint(\"  - May conflict\")\nprint(\"\\nIntentions: Committed plans\")\nprint(\"  - Selected from options\")\nprint(\"  - Executed step-by-step\")\nprint(\"  - Revised when beliefs change\")",
    "testCases": [
      {
        "input": "agent.update_beliefs(percepts)",
        "isHidden": false,
        "description": "Test belief update from percepts"
      },
      {
        "input": "agent.generate_options()",
        "isHidden": false,
        "description": "Test generating options from desires"
      },
      {
        "input": "agent.filter_intentions(options)",
        "isHidden": false,
        "description": "Test selecting intentions from options"
      },
      {
        "input": "agent.execute_intention()",
        "isHidden": false,
        "description": "Test executing intention steps"
      }
    ],
    "hints": [
      "Beliefs: agent's knowledge about world state",
      "Desires: goals agent wants to achieve",
      "Intentions: plans agent commits to executing",
      "BDI cycle: update beliefs -> generate options -> filter -> execute"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex15",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Environment Types and Agent Design",
    "difficulty": 2,
    "description": "Implement agents optimized for different environment types.\n\nYour implementation should:\n- Handle fully vs partially observable environments\n- Adapt to deterministic vs stochastic environments\n- Work in episodic vs sequential environments\n- Function in single-agent vs multi-agent environments",
    "starterCode": "class EnvironmentAdapter:\n    def __init__(self, env_type):\n        self.env_type = env_type\n\n    def design_agent(self):\n        # Design agent appropriate for environment type\n        pass\n\n    def select_architecture(self):\n        # Select agent architecture based on environment\n        pass",
    "solution": "class Environment:\n    \"\"\"Base environment class.\"\"\"\n    def __init__(self, properties):\n        self.properties = properties\n        # Properties: observable, deterministic, episodic, static, discrete, agents\n\n    def __repr__(self):\n        props_str = \", \".join(f\"{k}={v}\" for k, v in self.properties.items())\n        return f\"Environment({props_str})\"\n\nclass Agent:\n    \"\"\"Base agent class.\"\"\"\n    def __init__(self, name, architecture):\n        self.name = name\n        self.architecture = architecture\n\n    def __repr__(self):\n        return f\"Agent({self.name}, {self.architecture})\"\n\nclass EnvironmentAdapter:\n    \"\"\"Adapt agent design to environment properties.\"\"\"\n    def __init__(self):\n        pass\n\n    def analyze_environment(self, env):\n        \"\"\"Analyze environment properties.\"\"\"\n        props = env.properties\n\n        analysis = {\n            'observability': 'full' if props.get('observable') == 'fully' else 'partial',\n            'determinism': 'deterministic' if props.get('deterministic') == 'deterministic' else 'stochastic',\n            'episodic': props.get('episodic', False),\n            'static': props.get('static', False),\n            'discrete': props.get('discrete', True),\n            'agents': props.get('agents', 'single')\n        }\n\n        return analysis\n\n    def design_agent(self, env):\n        \"\"\"Design agent appropriate for environment.\"\"\"\n        analysis = self.analyze_environment(env)\n\n        print(f\"\\nDesigning agent for environment:\")\n        print(f\"  Observability: {analysis['observability']}\")\n        print(f\"  Determinism: {analysis['determinism']}\")\n        print(f\"  Episodic: {analysis['episodic']}\")\n        print(f\"  Static: {analysis['static']}\")\n        print(f\"  Agents: {analysis['agents']}\")\n        print()\n\n        # Select architecture\n        architecture = self.select_architecture(analysis)\n\n        # Select required capabilities\n        capabilities = self.select_capabilities(analysis)\n\n        # Design considerations\n        considerations = self.design_considerations(analysis)\n\n        return {\n            'architecture': architecture,\n            'capabilities': capabilities,\n            'considerations': considerations\n        }\n\n    def select_architecture(self, analysis):\n        \"\"\"Select agent architecture based on environment.\"\"\"\n        if analysis['observability'] == 'partial' and analysis['determinism'] == 'stochastic':\n            # Complex environment: need sophisticated reasoning\n            if analysis['episodic']:\n                return \"Utility-based agent\"\n            else:\n                return \"Learning agent with probabilistic reasoning\"\n\n        elif analysis['observability'] == 'full' and analysis['determinism'] == 'deterministic':\n            # Simple environment: can use simpler architecture\n            if analysis['episodic']:\n                return \"Simple reflex agent\"\n            else:\n                return \"Model-based agent\"\n\n        elif analysis['observability'] == 'partial':\n            # Need to maintain state\n            return \"Model-based agent\"\n\n        elif analysis['determinism'] == 'stochastic':\n            # Need to handle uncertainty\n            return \"Utility-based agent\"\n\n        else:\n            return \"Goal-based agent\"\n\n    def select_capabilities(self, analysis):\n        \"\"\"Select required capabilities.\"\"\"\n        capabilities = []\n\n        if analysis['observability'] == 'partial':\n            capabilities.append(\"State estimation\")\n            capabilities.append(\"Sensor fusion\")\n\n        if analysis['determinism'] == 'stochastic':\n            capabilities.append(\"Probabilistic reasoning\")\n            capabilities.append(\"Risk assessment\")\n\n        if not analysis['episodic']:\n            capabilities.append(\"Planning\")\n            capabilities.append(\"Memory\")\n\n        if not analysis['static']:\n            capabilities.append(\"Continuous monitoring\")\n            capabilities.append(\"Replanning\")\n\n        if analysis['agents'] == 'multi':\n            capabilities.append(\"Communication\")\n            capabilities.append(\"Coordination\")\n            capabilities.append(\"Game-theoretic reasoning\")\n\n        return capabilities\n\n    def design_considerations(self, analysis):\n        \"\"\"Design considerations based on environment.\"\"\"\n        considerations = []\n\n        # Observability\n        if analysis['observability'] == 'partial':\n            considerations.append(\"Maintain belief state over possible worlds\")\n            considerations.append(\"Active sensing strategy may be beneficial\")\n        else:\n            considerations.append(\"Can use simple state representation\")\n\n        # Determinism\n        if analysis['determinism'] == 'stochastic':\n            considerations.append(\"Use probabilistic action models\")\n            considerations.append(\"Consider expected utility, not just goal achievement\")\n        else:\n            considerations.append(\"Can use deterministic planning algorithms\")\n\n        # Episodic vs Sequential\n        if analysis['episodic']:\n            considerations.append(\"No need to plan ahead\")\n            considerations.append(\"Each episode independent\")\n        else:\n            considerations.append(\"Actions affect future states\")\n            considerations.append(\"Need lookahead and planning\")\n\n        # Static vs Dynamic\n        if not analysis['static']:\n            considerations.append(\"Environment changes while agent deliberates\")\n            considerations.append(\"Fast decision-making important\")\n            considerations.append(\"May need reactive components\")\n        else:\n            considerations.append(\"Can take time to deliberate\")\n\n        # Single vs Multi-agent\n        if analysis['agents'] == 'multi':\n            considerations.append(\"Model other agents' behavior\")\n            considerations.append(\"Potential for cooperation or competition\")\n        else:\n            considerations.append(\"Environment predictable (except randomness)\")\n\n        return considerations\n\n# Test environment adapter\nprint(\"Environment-Based Agent Design\")\nprint(\"=\"*60)\n\nadapter = EnvironmentAdapter()\n\n# Test different environment types\n\n# 1. Chess\nprint(\"\\n\" + \"=\"*60)\nprint(\"Environment 1: Chess\")\nprint(\"=\"*60)\n\nchess_env = Environment({\n    'observable': 'fully',\n    'deterministic': 'deterministic',\n    'episodic': True,\n    'static': False,  # Opponent moves\n    'discrete': True,\n    'agents': 'two'\n})\n\ndesign = adapter.design_agent(chess_env)\nprint(f\"Recommended architecture: {design['architecture']}\")\nprint(f\"\\nRequired capabilities:\")\nfor cap in design['capabilities']:\n    print(f\"  - {cap}\")\nprint(f\"\\nDesign considerations:\")\nfor con in design['considerations']:\n    print(f\"  - {con}\")\n\n# 2. Self-driving car\nprint(\"\\n\" + \"=\"*60)\nprint(\"Environment 2: Self-Driving Car\")\nprint(\"=\"*60)\n\ncar_env = Environment({\n    'observable': 'partially',\n    'deterministic': 'stochastic',\n    'episodic': False,\n    'static': False,\n    'discrete': False,\n    'agents': 'multi'\n})\n\ndesign = adapter.design_agent(car_env)\nprint(f\"Recommended architecture: {design['architecture']}\")\nprint(f\"\\nRequired capabilities:\")\nfor cap in design['capabilities']:\n    print(f\"  - {cap}\")\nprint(f\"\\nDesign considerations:\")\nfor con in design['considerations']:\n    print(f\"  - {con}\")\n\n# 3. Robotic vacuum\nprint(\"\\n\" + \"=\"*60)\nprint(\"Environment 3: Robotic Vacuum\")\nprint(\"=\"*60)\n\nvacuum_env = Environment({\n    'observable': 'partially',\n    'deterministic': 'stochastic',\n    'episodic': False,\n    'static': False,  # Dirt appears\n    'discrete': True,\n    'agents': 'single'\n})\n\ndesign = adapter.design_agent(vacuum_env)\nprint(f\"Recommended architecture: {design['architecture']}\")\nprint(f\"\\nRequired capabilities:\")\nfor cap in design['capabilities']:\n    print(f\"  - {cap}\")\nprint(f\"\\nDesign considerations:\")\nfor con in design['considerations']:\n    print(f\"  - {con}\")\n\n# 4. Medical diagnosis\nprint(\"\\n\" + \"=\"*60)\nprint(\"Environment 4: Medical Diagnosis\")\nprint(\"=\"*60)\n\nmedical_env = Environment({\n    'observable': 'partially',\n    'deterministic': 'stochastic',\n    'episodic': True,  # Each patient separate\n    'static': False,  # Patient condition changes\n    'discrete': False,\n    'agents': 'multi'  # Doctors, other systems\n})\n\ndesign = adapter.design_agent(medical_env)\nprint(f\"Recommended architecture: {design['architecture']}\")\nprint(f\"\\nRequired capabilities:\")\nfor cap in design['capabilities']:\n    print(f\"  - {cap}\")\nprint(f\"\\nDesign considerations:\")\nfor con in design['considerations']:\n    print(f\"  - {con}\")\n\n# Summary table\nprint(\"\\n\" + \"=\"*60)\nprint(\"Architecture Selection Summary\")\nprint(\"=\"*60)\n\nenvironments = [\n    (\"Chess\", chess_env),\n    (\"Self-Driving Car\", car_env),\n    (\"Robotic Vacuum\", vacuum_env),\n    (\"Medical Diagnosis\", medical_env)\n]\n\nprint(\"\\n{:<20} {:<15} {:<15} {:<25}\".format(\n    \"Environment\", \"Observable\", \"Deterministic\", \"Architecture\"\n))\nprint(\"-\"*80)\n\nfor name, env in environments:\n    analysis = adapter.analyze_environment(env)\n    architecture = adapter.select_architecture(analysis)\n    print(\"{:<20} {:<15} {:<15} {:<25}\".format(\n        name[:18],\n        analysis['observability'],\n        analysis['determinism'],\n        architecture[:23]\n    ))",
    "testCases": [
      {
        "input": "adapter.analyze_environment(env)",
        "isHidden": false,
        "description": "Test environment analysis"
      },
      {
        "input": "adapter.select_architecture(analysis)",
        "isHidden": false,
        "description": "Test architecture selection"
      },
      {
        "input": "adapter.design_agent(env)",
        "isHidden": false,
        "description": "Test complete agent design"
      }
    ],
    "hints": [
      "Partially observable: need state estimation and memory",
      "Stochastic: need probabilistic reasoning and utility",
      "Sequential (not episodic): need planning and lookahead",
      "Multi-agent: need coordination and communication"
    ],
    "language": "python"
  },
  {
    "id": "cs406-t1-ex16",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Agent Rationality and Bounded Rationality",
    "difficulty": 3,
    "description": "Implement agents with different rationality assumptions.\n\nYour implementation should:\n- Implement perfectly rational agent (unlimited computation)\n- Implement boundedly rational agent (limited time/resources)\n- Compare performance under resource constraints\n- Demonstrate satisficing vs optimizing behavior",
    "starterCode": "class RationalAgent:\n    def decide(self, state, actions):\n        # Perfect rationality: find optimal action\n        pass\n\nclass BoundedRationalAgent:\n    def decide(self, state, actions, time_limit):\n        # Bounded rationality: find good enough action\n        pass\n\ndef compare_rationality(rational, bounded):\n    pass",
    "solution": "import time\nimport random\n\nclass RationalAgent:\n    \"\"\"Perfectly rational agent (unlimited computation).\"\"\"\n    def __init__(self, utility_function):\n        self.utility = utility_function\n        self.decisions = []\n\n    def decide(self, state, actions):\n        \"\"\"\n        Find optimal action by evaluating all possibilities.\n        This may take a long time for complex problems.\n        \"\"\"\n        start_time = time.time()\n\n        best_action = None\n        best_utility = float('-inf')\n\n        # Evaluate all actions exhaustively\n        for action in actions:\n            # Simulate outcome\n            outcome = self.simulate_outcome(state, action)\n            utility = self.utility(outcome)\n\n            if utility > best_utility:\n                best_utility = utility\n                best_action = action\n\n        elapsed_time = time.time() - start_time\n\n        self.decisions.append({\n            'action': best_action,\n            'utility': best_utility,\n            'time': elapsed_time,\n            'actions_evaluated': len(actions)\n        })\n\n        return best_action, best_utility\n\n    def simulate_outcome(self, state, action):\n        \"\"\"Simulate outcome of action.\"\"\"\n        # Simple simulation\n        return {\n            'state': state,\n            'action': action,\n            'reward': random.gauss(action.get('expected_reward', 0), 1.0)\n        }\n\nclass BoundedRationalAgent:\n    \"\"\"Boundedly rational agent (limited computation).\"\"\"\n    def __init__(self, utility_function, satisficing_threshold=0.8):\n        self.utility = utility_function\n        self.satisficing_threshold = satisficing_threshold\n        self.decisions = []\n\n    def decide(self, state, actions, time_limit=None, max_evaluations=None):\n        \"\"\"\n        Find good enough action within resource constraints.\n        Uses satisficing: stop when action exceeds threshold.\n        \"\"\"\n        start_time = time.time()\n\n        best_action = None\n        best_utility = float('-inf')\n        evaluations = 0\n\n        # Shuffle actions for randomized search\n        actions_to_try = list(actions)\n        random.shuffle(actions_to_try)\n\n        for action in actions_to_try:\n            # Check resource constraints\n            if time_limit and (time.time() - start_time) > time_limit:\n                break\n\n            if max_evaluations and evaluations >= max_evaluations:\n                break\n\n            # Evaluate action\n            outcome = self.simulate_outcome(state, action)\n            utility = self.utility(outcome)\n            evaluations += 1\n\n            if utility > best_utility:\n                best_utility = utility\n                best_action = action\n\n            # Satisficing: stop if good enough\n            if utility >= self.satisficing_threshold * self.max_possible_utility():\n                break\n\n        elapsed_time = time.time() - start_time\n\n        self.decisions.append({\n            'action': best_action,\n            'utility': best_utility,\n            'time': elapsed_time,\n            'actions_evaluated': evaluations\n        })\n\n        return best_action, best_utility\n\n    def simulate_outcome(self, state, action):\n        \"\"\"Simulate outcome of action.\"\"\"\n        return {\n            'state': state,\n            'action': action,\n            'reward': random.gauss(action.get('expected_reward', 0), 1.0)\n        }\n\n    def max_possible_utility(self):\n        \"\"\"Estimate maximum possible utility.\"\"\"\n        return 1.0  # Normalize to [0, 1]\n\nclass AdaptiveAgent:\n    \"\"\"Agent that adapts rationality to situation.\"\"\"\n    def __init__(self, utility_function):\n        self.utility = utility_function\n        self.decisions = []\n\n    def decide(self, state, actions, urgency=0.5):\n        \"\"\"\n        Adapt decision strategy based on urgency.\n        Low urgency: more deliberation (rational)\n        High urgency: quick decision (bounded)\n        \"\"\"\n        start_time = time.time()\n\n        # Adapt evaluation strategy based on urgency\n        if urgency < 0.3:\n            # Low urgency: evaluate many actions\n            max_evals = len(actions)\n        elif urgency < 0.7:\n            # Medium urgency: evaluate subset\n            max_evals = len(actions) // 2\n        else:\n            # High urgency: quick heuristic\n            max_evals = 3\n\n        best_action = None\n        best_utility = float('-inf')\n\n        # Prioritize actions by heuristic\n        actions_sorted = sorted(\n            actions,\n            key=lambda a: a.get('expected_reward', 0),\n            reverse=True\n        )\n\n        for i, action in enumerate(actions_sorted[:max_evals]):\n            outcome = self.simulate_outcome(state, action)\n            utility = self.utility(outcome)\n\n            if utility > best_utility:\n                best_utility = utility\n                best_action = action\n\n        elapsed_time = time.time() - start_time\n\n        self.decisions.append({\n            'action': best_action,\n            'utility': best_utility,\n            'time': elapsed_time,\n            'actions_evaluated': min(max_evals, len(actions)),\n            'urgency': urgency\n        })\n\n        return best_action, best_utility\n\n    def simulate_outcome(self, state, action):\n        \"\"\"Simulate outcome of action.\"\"\"\n        return {\n            'state': state,\n            'action': action,\n            'reward': random.gauss(action.get('expected_reward', 0), 1.0)\n        }\n\ndef utility_function(outcome):\n    \"\"\"Simple utility function based on reward.\"\"\"\n    reward = outcome.get('reward', 0)\n    # Normalize to [0, 1]\n    return (reward + 10) / 20.0\n\ndef compare_rationality(rational, bounded, adaptive, num_trials=10):\n    \"\"\"Compare different rationality approaches.\"\"\"\n\n    print(\"\\nComparing Rationality Approaches\")\n    print(\"=\"*60)\n\n    for trial in range(num_trials):\n        # Generate decision problem\n        state = {'trial': trial}\n        num_actions = 20 + trial * 5  # Increasing complexity\n\n        actions = [\n            {'id': i, 'expected_reward': random.uniform(-5, 5)}\n            for i in range(num_actions)\n        ]\n\n        # Rational agent (no constraints)\n        rational.decide(state, actions)\n\n        # Bounded rational agent (limited evaluations)\n        bounded.decide(state, actions, max_evaluations=10)\n\n        # Adaptive agent (medium urgency)\n        adaptive.decide(state, actions, urgency=0.5)\n\n    # Print results\n    print(f\"\\nResults after {num_trials} trials:\")\n    print(\"-\"*60)\n\n    agents = [\n        ('Rational', rational),\n        ('Bounded Rational', bounded),\n        ('Adaptive', adaptive)\n    ]\n\n    for name, agent in agents:\n        decisions = agent.decisions\n        avg_utility = sum(d['utility'] for d in decisions) / len(decisions)\n        avg_time = sum(d['time'] for d in decisions) / len(decisions)\n        avg_evals = sum(d['actions_evaluated'] for d in decisions) / len(decisions)\n\n        print(f\"\\n{name}:\")\n        print(f\"  Avg utility: {avg_utility:.4f}\")\n        print(f\"  Avg time: {avg_time*1000:.2f} ms\")\n        print(f\"  Avg evaluations: {avg_evals:.1f}\")\n\n    return agents\n\n# Test different rationality levels\nprint(\"Agent Rationality Comparison\")\nprint(\"=\"*60)\n\n# Create agents\nrational_agent = RationalAgent(utility_function)\nbounded_agent = BoundedRationalAgent(utility_function, satisficing_threshold=0.7)\nadaptive_agent = AdaptiveAgent(utility_function)\n\n# Compare performance\nagents = compare_rationality(rational_agent, bounded_agent, adaptive_agent, num_trials=10)\n\n# Detailed analysis\nprint(\"\\n\" + \"=\"*60)\nprint(\"Rationality Trade-offs\")\nprint(\"=\"*60)\n\nprint(\"\\nPerfect Rationality:\")\nprint(\"  + Always finds optimal action\")\nprint(\"  - Computationally expensive\")\nprint(\"  - May be too slow for real-time decisions\")\n\nprint(\"\\nBounded Rationality:\")\nprint(\"  + Fast decision making\")\nprint(\"  + Satisficing: finds 'good enough' solutions\")\nprint(\"  - May miss optimal actions\")\nprint(\"  + More realistic model of human/agent behavior\")\n\nprint(\"\\nAdaptive Rationality:\")\nprint(\"  + Adapts to situation demands\")\nprint(\"  + Balances speed and optimality\")\nprint(\"  + Useful in varying time pressure\")\n\n# Visualize satisficing\nprint(\"\\n\" + \"=\"*60)\nprint(\"Satisficing Behavior\")\nprint(\"=\"*60)\n\nstate = {}\nactions = [\n    {'id': 1, 'expected_reward': 3.0},\n    {'id': 2, 'expected_reward': 5.0},\n    {'id': 3, 'expected_reward': 8.0},  # Optimal\n    {'id': 4, 'expected_reward': 7.5},\n    {'id': 5, 'expected_reward': 4.0},\n]\n\nprint(\"\\nAvailable actions:\")\nfor action in actions:\n    print(f\"  Action {action['id']}: expected reward = {action['expected_reward']}\")\n\n# Bounded agent with high threshold\nbounded_high = BoundedRationalAgent(utility_function, satisficing_threshold=0.9)\naction, utility = bounded_high.decide(state, actions, max_evaluations=5)\nprint(f\"\\nBounded agent (high threshold): chose action {action['id'] if action else 'None'}\")\n\n# Bounded agent with low threshold\nbounded_low = BoundedRationalAgent(utility_function, satisficing_threshold=0.6)\naction, utility = bounded_low.decide(state, actions, max_evaluations=5)\nprint(f\"Bounded agent (low threshold): chose action {action['id'] if action else 'None'}\")\n\n# Rational agent\naction, utility = rational_agent.decide(state, actions)\nprint(f\"Rational agent: chose action {action['id'] if action else 'None'}\")",
    "testCases": [
      {
        "input": "rational_agent.decide(state, actions)",
        "isHidden": false,
        "description": "Test rational agent finds optimal action"
      },
      {
        "input": "bounded_agent.decide(state, actions, time_limit)",
        "isHidden": false,
        "description": "Test bounded agent with resource constraints"
      },
      {
        "input": "compare_rationality(rational, bounded)",
        "isHidden": false,
        "description": "Test performance comparison"
      }
    ],
    "hints": [
      "Perfect rationality: evaluate all options, choose best (may be slow)",
      "Bounded rationality: evaluate limited options, satisfice (faster)",
      "Satisficing: stop when solution is \"good enough\" vs optimal",
      "Trade-off: decision quality vs computational resources"
    ],
    "language": "python"
  }
]
