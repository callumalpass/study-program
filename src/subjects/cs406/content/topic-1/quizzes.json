[
  {
    "id": "cs406-quiz-1a",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Topic 1 - Quiz Fundamentals",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "What is the Turing test designed to evaluate?",
        "options": [
          "The processing speed of a computer",
          "Whether a machine has consciousness",
          "The memory capacity of an AI system",
          "Whether a machine can think like a human based on behavioral equivalence"
        ],
        "correctAnswer": 3,
        "explanation": "The Turing test evaluates whether a machine can exhibit behavior indistinguishable from a human in conversation, focusing on external behavior rather than internal processes. It was proposed by Alan Turing in 1950 as a practical alternative to the question 'Can machines think?'"
      },
      {
        "id": "q2",
        "type": "multiple_choice",
        "prompt": "What characterizes a rational agent?",
        "options": [
          "It thinks like a human",
          "It never makes mistakes",
          "It maximizes expected performance measure given percept sequence and knowledge",
          "It always makes perfect decisions"
        ],
        "correctAnswer": 2,
        "explanation": "A rational agent does the best it can with available information to maximize its performance measure, not necessarily achieving perfect outcomes. Rationality depends on the agent's percept sequence, prior knowledge, available actions, and performance measure."
      },
      {
        "id": "q3",
        "type": "true_false",
        "prompt": "A fully observable environment means the agent's sensors can detect all aspects relevant to the choice of action at each time step.",
        "correctAnswer": true,
        "explanation": "In a fully observable environment, the agent has complete access to the state of the environment at each point in time. This contrasts with partially observable environments where sensors may be noisy, inaccurate, or incomplete."
      },
      {
        "id": "q4",
        "type": "fill_blank",
        "prompt": "In the PEAS framework, the 'E' stands for _____.",
        "correctAnswer": "Environment",
        "explanation": "PEAS stands for Performance measure, Environment, Actuators, and Sensors. The Environment component describes the task environment in which the agent operates, including all relevant aspects of the world the agent must navigate."
      },
      {
        "id": "q5",
        "type": "multiple_choice",
        "prompt": "Which type of agent maintains an internal model of the world?",
        "options": [
          "Simple reflex agent",
          "Random agent",
          "Reactive agent",
          "Model-based reflex agent"
        ],
        "correctAnswer": 3,
        "explanation": "Model-based reflex agents maintain an internal state that tracks aspects of the world not evident in the current percept. This internal model helps the agent handle partial observability by keeping track of unseen parts of the world."
      }
    ]
  },
  {
    "id": "cs406-quiz-1b",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Topic 1 - Quiz Application",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "Which environment property means the next state depends only on current state and action?",
        "options": [
          "Deterministic",
          "Observable",
          "Episodic",
          "Static"
        ],
        "correctAnswer": 0,
        "explanation": "In a deterministic environment, the next state is completely determined by the current state and action executed, with no randomness. Stochastic environments involve uncertainty where actions may have multiple possible outcomes."
      },
      {
        "id": "q2",
        "type": "multiple_choice",
        "prompt": "What is the main advantage of a utility-based agent over a goal-based agent?",
        "options": [
          "It can handle trade-offs between conflicting objectives",
          "It doesn't need a model of the world",
          "It requires less computation",
          "It always finds optimal solutions"
        ],
        "correctAnswer": 0,
        "explanation": "Utility-based agents use utility functions to compare states, enabling rational trade-offs between multiple objectives rather than binary goal achievement. This allows for more nuanced decision-making when multiple goals may conflict or when quality of outcome matters."
      },
      {
        "id": "q3",
        "type": "code_output",
        "prompt": "What is the output of this simple reflex agent?",
        "codeSnippet": "def reflex_agent(percept):\n    if percept == 'dirty':\n        return 'suck'\n    elif percept == 'clean':\n        return 'move'\n    \naction = reflex_agent('dirty')\nprint(action)",
        "correctAnswer": "suck",
        "explanation": "The simple reflex agent maps percepts directly to actions using condition-action rules. When the percept is 'dirty', it returns the action 'suck'. This is the simplest type of agent architecture."
      },
      {
        "id": "q4",
        "type": "true_false",
        "prompt": "An episodic environment means each episode depends on the actions taken in previous episodes.",
        "correctAnswer": false,
        "explanation": "Actually, episodic environments are the opposite - each episode is independent and doesn't depend on previous episodes. Sequential environments are where current decisions affect future decisions. For example, a spam classifier sees each email independently (episodic), while chess is sequential."
      },
      {
        "id": "q5",
        "type": "fill_blank",
        "prompt": "A _____ agent selects actions based on the current percept, ignoring percept history.",
        "correctAnswer": "simple reflex",
        "explanation": "Simple reflex agents select actions based only on the current percept using condition-action rules, without considering percept history. They work well in fully observable environments but struggle with partial observability."
      }
    ]
  },
  {
    "id": "cs406-quiz-1c",
    "subjectId": "cs406",
    "topicId": "cs406-topic-1",
    "title": "Topic 1 - Quiz Mastery",
    "questions": [
      {
        "id": "q1",
        "type": "multiple_choice",
        "prompt": "In the Chinese Room argument, what does Searle argue?",
        "options": [
          "Syntax alone is neither sufficient for nor constitutive of semantics",
          "Computers can achieve true understanding through symbol manipulation",
          "Natural language processing is impossible",
          "The Turing test is the best measure of intelligence"
        ],
        "correctAnswer": 0,
        "explanation": "Searle argues that manipulating symbols according to rules (syntax) does not create genuine understanding (semantics), challenging strong AI claims. The thought experiment shows that passing the Turing test doesn't necessarily imply understanding or consciousness."
      },
      {
        "id": "q2",
        "type": "code_output",
        "prompt": "What performance measure value does this goal-based agent calculate?",
        "codeSnippet": "def calculate_performance(states):\n    goal_state = 'clean'\n    achieved = [s for s in states if s == goal_state]\n    return len(achieved) / len(states)\n\nstates = ['dirty', 'clean', 'clean', 'dirty', 'clean']\nperformance = calculate_performance(states)\nprint(f\"{performance:.1f}\")",
        "correctAnswer": "0.6",
        "explanation": "The performance measure calculates the proportion of states that match the goal (clean). Out of 5 states, 3 are clean, giving 3/5 = 0.6 or 60% goal achievement rate."
      },
      {
        "id": "q3",
        "type": "multiple_choice",
        "prompt": "Which agent type uses learning to improve its performance over time?",
        "options": [
          "Goal-based agent",
          "Simple reflex agent",
          "Model-based agent",
          "Learning agent"
        ],
        "correctAnswer": 3,
        "explanation": "Learning agents have a learning element that modifies the performance element based on feedback from a critic, allowing the agent to improve over time. This architecture includes learning, performance, critic, and problem generator components."
      },
      {
        "id": "q4",
        "type": "fill_blank",
        "prompt": "An agent that operates in an environment where the environment can change while the agent is deliberating is called a _____ environment.",
        "correctAnswer": "dynamic",
        "explanation": "Dynamic environments can change while the agent is thinking, requiring the agent to keep looking at the world during deliberation. Static environments don't change during deliberation, making decision-making simpler. Semidynamic environments don't change but the agent's performance score does."
      },
      {
        "id": "q5",
        "type": "true_false",
        "prompt": "A rational agent must be omniscient (know the actual outcome of its actions).",
        "correctAnswer": false,
        "explanation": "Rationality is not the same as omniscience. A rational agent makes the best decision based on available information and percepts, but cannot be expected to know things it hasn't perceived. Rationality maximizes expected performance, not actual performance."
      }
    ]
  }
]
