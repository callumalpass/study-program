---
title: "Hidden Markov Models"
slug: "hidden-markov-models"
description: "Hidden Markov Models for temporal reasoning, including forward algorithm, Viterbi algorithm, and applications in sequence modeling"
---

# Hidden Markov Models

Hidden Markov Models (HMMs) are probabilistic models for sequential data where the system being modeled is assumed to be a Markov process with hidden (unobserved) states. HMMs are fundamental to many AI applications including speech recognition, natural language processing, bioinformatics, and activity recognition. They provide an elegant framework for reasoning about time series when the true state of the system is not directly observable.

## Introduction to HMMs

An HMM models a sequence of observations generated by a sequence of hidden states. The model assumes:

1. **Markov Property**: The current state depends only on the previous state, not the entire history
2. **Output Independence**: Observations depend only on the current state

**Components of an HMM:**

- **States** $S = \{s_1, s_2, ..., s_N\}$: Hidden states (not directly observed)
- **Observations** $O = \{o_1, o_2, ..., o_M\}$: Observable outputs
- **Initial Probabilities** $\pi_i = P(X_1 = s_i)$: Probability of starting in state $s_i$
- **Transition Probabilities** $a_{ij} = P(X_{t+1} = s_j | X_t = s_i)$: Probability of transitioning from state $s_i$ to $s_j$
- **Emission Probabilities** $b_j(o_k) = P(O_t = o_k | X_t = s_j)$: Probability of observing $o_k$ in state $s_j$

**Compact Notation:** An HMM is specified by $\lambda = (A, B, \pi)$ where:
- $A$ is the transition matrix
- $B$ is the emission matrix
- $\pi$ is the initial state distribution

## HMM Structure and Assumptions

The joint probability of a state sequence $X_{1:T}$ and observation sequence $O_{1:T}$ factors as:

$$P(X_{1:T}, O_{1:T}) = \pi_{X_1} \cdot \prod_{t=1}^{T} b_{X_t}(O_t) \cdot \prod_{t=1}^{T-1} a_{X_t, X_{t+1}}$$

This factorization dramatically simplifies computation by breaking a complex joint distribution into local conditional dependencies.

**Example: Weather Model**

Consider a simple HMM for weather:
- **Hidden states**: Sunny, Rainy
- **Observations**: Happy, Grumpy (person's mood)
- **Assumption**: Weather is hidden, we only observe mood

```python
import numpy as np

class HiddenMarkovModel:
    """Hidden Markov Model implementation."""

    def __init__(self, states, observations, transition_prob, emission_prob, initial_prob):
        """
        Initialize HMM.

        Args:
            states: List of state names
            observations: List of observation symbols
            transition_prob: N x N transition matrix
            emission_prob: N x M emission matrix
            initial_prob: Initial state distribution (length N)
        """
        self.states = states
        self.observations = observations
        self.A = np.array(transition_prob)  # Transition matrix
        self.B = np.array(emission_prob)    # Emission matrix
        self.pi = np.array(initial_prob)    # Initial distribution

        self.N = len(states)  # Number of states
        self.M = len(observations)  # Number of observation symbols

    def state_to_index(self, state):
        """Convert state name to index."""
        return self.states.index(state)

    def obs_to_index(self, obs):
        """Convert observation to index."""
        return self.observations.index(obs)


# Example: Weather HMM
states = ['Sunny', 'Rainy']
observations = ['Happy', 'Grumpy']

# Transition probabilities: A[i][j] = P(state_j | state_i)
transition_prob = [
    [0.7, 0.3],  # Sunny -> Sunny: 0.7, Sunny -> Rainy: 0.3
    [0.4, 0.6]   # Rainy -> Sunny: 0.4, Rainy -> Rainy: 0.6
]

# Emission probabilities: B[i][j] = P(obs_j | state_i)
emission_prob = [
    [0.8, 0.2],  # Sunny -> Happy: 0.8, Sunny -> Grumpy: 0.2
    [0.3, 0.7]   # Rainy -> Happy: 0.3, Rainy -> Grumpy: 0.7
]

# Initial state distribution
initial_prob = [0.6, 0.4]  # P(Sunny) = 0.6, P(Rainy) = 0.4

hmm = HiddenMarkovModel(states, observations, transition_prob, emission_prob, initial_prob)
print(f"HMM created with {hmm.N} states and {hmm.M} observation types")
```

## Three Fundamental Problems

HMMs address three key inference problems:

**1. Evaluation Problem**: Given observation sequence $O_{1:T}$ and model $\lambda$, compute $P(O_{1:T}|\lambda)$
- **Solution**: Forward algorithm

**2. Decoding Problem**: Given observations $O_{1:T}$ and model $\lambda$, find most likely state sequence
- **Solution**: Viterbi algorithm

**3. Learning Problem**: Given observations, learn parameters $\lambda = (A, B, \pi)$
- **Solution**: Baum-Welch algorithm (EM for HMMs)

## Forward Algorithm

The forward algorithm efficiently computes $P(O_{1:T}|\lambda)$ using dynamic programming.

**Forward Variable:**

$$\alpha_t(i) = P(O_{1:t}, X_t = s_i | \lambda)$$

This represents the probability of observing the first $t$ observations and being in state $s_i$ at time $t$.

**Recursion:**

1. **Initialization**: $\alpha_1(i) = \pi_i \cdot b_i(O_1)$
2. **Recursion**: $\alpha_{t+1}(j) = \left[\sum_{i=1}^{N} \alpha_t(i) \cdot a_{ij}\right] \cdot b_j(O_{t+1})$
3. **Termination**: $P(O_{1:T}|\lambda) = \sum_{i=1}^{N} \alpha_T(i)$

**Complexity**: $O(N^2 T)$ instead of $O(N^T)$ for naive enumeration.

```python
def forward_algorithm(hmm, observations):
    """
    Compute forward probabilities using dynamic programming.

    Args:
        hmm: HiddenMarkovModel instance
        observations: Sequence of observations

    Returns:
        alpha: Forward probability matrix (T x N)
        log_likelihood: log P(observations | model)
    """
    T = len(observations)
    N = hmm.N

    # Initialize forward probabilities
    alpha = np.zeros((T, N))

    # Convert observations to indices
    obs_indices = [hmm.obs_to_index(o) for o in observations]

    # Base case: t = 0
    alpha[0, :] = hmm.pi * hmm.B[:, obs_indices[0]]

    # Recursive case
    for t in range(1, T):
        for j in range(N):
            # Sum over all possible previous states
            alpha[t, j] = np.sum(alpha[t-1, :] * hmm.A[:, j]) * hmm.B[j, obs_indices[t]]

    # Total probability is sum over final states
    likelihood = np.sum(alpha[T-1, :])
    log_likelihood = np.log(likelihood) if likelihood > 0 else -np.inf

    return alpha, log_likelihood


# Example: Compute probability of observation sequence
obs_sequence = ['Happy', 'Happy', 'Grumpy']
alpha, log_prob = forward_algorithm(hmm, obs_sequence)
print(f"P(observations) = {np.exp(log_prob):.6f}")
print(f"Log-likelihood = {log_prob:.6f}")
```

## Backward Algorithm

The backward algorithm complements the forward algorithm, enabling efficient computation of smoothing probabilities.

**Backward Variable:**

$$\beta_t(i) = P(O_{t+1:T} | X_t = s_i, \lambda)$$

This represents the probability of observing the remaining observations given we're in state $s_i$ at time $t$.

**Recursion:**

1. **Initialization**: $\beta_T(i) = 1$ for all $i$
2. **Recursion**: $\beta_t(i) = \sum_{j=1}^{N} a_{ij} \cdot b_j(O_{t+1}) \cdot \beta_{t+1}(j)$

**Smoothing:** Combining forward and backward gives:

$$P(X_t = s_i | O_{1:T}, \lambda) = \frac{\alpha_t(i) \cdot \beta_t(i)}{\sum_{j=1}^{N} \alpha_t(j) \cdot \beta_t(j)}$$

```python
def backward_algorithm(hmm, observations):
    """
    Compute backward probabilities.

    Args:
        hmm: HiddenMarkovModel instance
        observations: Sequence of observations

    Returns:
        beta: Backward probability matrix (T x N)
    """
    T = len(observations)
    N = hmm.N

    # Initialize backward probabilities
    beta = np.zeros((T, N))

    # Convert observations to indices
    obs_indices = [hmm.obs_to_index(o) for o in observations]

    # Base case: t = T-1
    beta[T-1, :] = 1.0

    # Recursive case (going backwards)
    for t in range(T-2, -1, -1):
        for i in range(N):
            beta[t, i] = np.sum(hmm.A[i, :] * hmm.B[:, obs_indices[t+1]] * beta[t+1, :])

    return beta


def smoothing(hmm, observations):
    """
    Compute smoothed state probabilities P(X_t | O_1:T).

    Args:
        hmm: HiddenMarkovModel instance
        observations: Sequence of observations

    Returns:
        gamma: Smoothed probabilities (T x N)
    """
    alpha, _ = forward_algorithm(hmm, observations)
    beta = backward_algorithm(hmm, observations)

    # Compute gamma = P(X_t | observations)
    gamma = alpha * beta
    gamma = gamma / gamma.sum(axis=1, keepdims=True)

    return gamma


# Example: Compute smoothed probabilities
gamma = smoothing(hmm, obs_sequence)
print("Smoothed state probabilities:")
for t, probs in enumerate(gamma):
    print(f"  t={t}: Sunny={probs[0]:.3f}, Rainy={probs[1]:.3f}")
```

## Viterbi Algorithm

The Viterbi algorithm finds the most likely state sequence that could have generated the observations.

**Viterbi Variable:**

$$\delta_t(i) = \max_{X_{1:t-1}} P(X_{1:t-1}, X_t = s_i, O_{1:t} | \lambda)$$

This represents the probability of the most likely path ending in state $s_i$ at time $t$.

**Recursion:**

1. **Initialization**: $\delta_1(i) = \pi_i \cdot b_i(O_1)$, $\psi_1(i) = 0$
2. **Recursion**:
   - $\delta_t(j) = \max_i [\delta_{t-1}(i) \cdot a_{ij}] \cdot b_j(O_t)$
   - $\psi_t(j) = \arg\max_i [\delta_{t-1}(i) \cdot a_{ij}]$ (backpointer)
3. **Termination**: $P^* = \max_i \delta_T(i)$, $X_T^* = \arg\max_i \delta_T(i)$
4. **Backtrack**: $X_t^* = \psi_{t+1}(X_{t+1}^*)$

```python
def viterbi_algorithm(hmm, observations):
    """
    Find most likely state sequence using Viterbi algorithm.

    Args:
        hmm: HiddenMarkovModel instance
        observations: Sequence of observations

    Returns:
        path: Most likely state sequence
        prob: Probability of that sequence
    """
    T = len(observations)
    N = hmm.N

    # Initialize arrays
    delta = np.zeros((T, N))
    psi = np.zeros((T, N), dtype=int)

    # Convert observations to indices
    obs_indices = [hmm.obs_to_index(o) for o in observations]

    # Initialization: t = 0
    delta[0, :] = hmm.pi * hmm.B[:, obs_indices[0]]
    psi[0, :] = 0

    # Recursion
    for t in range(1, T):
        for j in range(N):
            # Find most likely previous state
            prob_from_i = delta[t-1, :] * hmm.A[:, j]
            delta[t, j] = np.max(prob_from_i) * hmm.B[j, obs_indices[t]]
            psi[t, j] = np.argmax(prob_from_i)

    # Termination: find best final state
    best_path_prob = np.max(delta[T-1, :])
    best_last_state = np.argmax(delta[T-1, :])

    # Backtrack to find best path
    path = np.zeros(T, dtype=int)
    path[T-1] = best_last_state

    for t in range(T-2, -1, -1):
        path[t] = psi[t+1, path[t+1]]

    # Convert indices to state names
    state_path = [hmm.states[i] for i in path]

    return state_path, best_path_prob


# Example: Find most likely weather sequence
best_path, prob = viterbi_algorithm(hmm, obs_sequence)
print(f"\nMost likely state sequence for {obs_sequence}:")
print(f"  States: {best_path}")
print(f"  Probability: {prob:.6f}")
```

## Applications of HMMs

**1. Speech Recognition:**
- States: Phonemes or words
- Observations: Acoustic features (MFCCs)
- Task: Decode speech to text

**2. Part-of-Speech Tagging:**
- States: POS tags (noun, verb, adjective, etc.)
- Observations: Words
- Task: Tag each word with its grammatical role

**3. Bioinformatics:**
- States: Gene structure elements (exon, intron, promoter)
- Observations: DNA sequences
- Task: Predict gene locations

**4. Activity Recognition:**
- States: Activities (walking, running, sitting)
- Observations: Sensor readings (accelerometer, gyroscope)
- Task: Identify current activity

```python
# Example: POS tagging setup
pos_states = ['NOUN', 'VERB', 'ADJ']
words = ['dog', 'runs', 'fast']

# In real application, these would be learned from tagged corpus
pos_transition = [
    [0.3, 0.5, 0.2],  # NOUN -> ...
    [0.4, 0.1, 0.5],  # VERB -> ...
    [0.6, 0.2, 0.2]   # ADJ -> ...
]

# Large emission matrix mapping states to vocabulary
# (simplified - real systems have thousands of words)
print("HMMs are widely used in NLP for sequence labeling tasks")
```

## Key Takeaways

1. **HMMs** model sequential data with hidden states and observable outputs
2. **Markov assumption** limits state dependencies to previous state only, enabling efficient computation
3. **Three fundamental problems**: evaluation (forward), decoding (Viterbi), and learning (Baum-Welch)
4. **Forward algorithm** computes observation likelihood in $O(N^2T)$ time using dynamic programming
5. **Backward algorithm** enables smoothing - computing state probabilities given all observations
6. **Viterbi algorithm** finds the single most likely state sequence (decoding)
7. **Smoothing vs filtering vs prediction**: Past data (smoothing) gives better estimates than online filtering
8. **Applications** span speech recognition, NLP, bioinformatics, and time series analysis
9. **Limitations**: First-order Markov assumption may be too restrictive; observations must be discrete or Gaussian
10. **Extensions**: Higher-order HMMs, hierarchical HMMs, and continuous-observation HMMs address limitations

Hidden Markov Models provide a principled probabilistic framework for temporal reasoning with hidden state. Their efficient algorithms and solid theoretical foundation make them indispensable for sequence modeling tasks across AI applications. Modern deep learning approaches like RNNs and Transformers have supplanted HMMs in some domains, but HMMs remain valuable for interpretability, small data regimes, and problems where probabilistic reasoning is essential.
