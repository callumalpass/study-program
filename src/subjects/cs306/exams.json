[
  {
    "id": "cs306-midterm",
    "subjectId": "cs306",
    "title": "CS306 Midterm Examination",
    "questions": [
      {
        "id": "cs306-mid-q1",
        "type": "multiple_choice",
        "prompt": "What is the correct order of stages in the graphics rendering pipeline?",
        "options": [
          "Fragment Processing → Vertex Processing → Rasterization → Output Merging",
          "Rasterization → Vertex Processing → Fragment Processing → Output Merging",
          "Vertex Processing → Fragment Processing → Rasterization → Output Merging",
          "Vertex Processing → Rasterization → Fragment Processing → Output Merging"
        ],
        "correctAnswer": 3,
        "explanation": "The graphics pipeline processes vertices first, converts them to fragments through rasterization, processes each fragment, and finally merges the output to the framebuffer."
      },
      {
        "id": "cs306-mid-q2",
        "type": "multiple_choice",
        "prompt": "Which coordinate system is typically used immediately after model transformations?",
        "options": [
          "Screen space",
          "World space",
          "Clip space",
          "View space"
        ],
        "correctAnswer": 1,
        "explanation": "After model transformations, vertices are in world space. They then undergo view transformation to camera space, projection to clip space, and finally viewport transformation to screen space."
      },
      {
        "id": "cs306-mid-q3",
        "type": "multiple_choice",
        "prompt": "What is the primary purpose of the vertex shader in modern graphics pipelines?",
        "options": [
          "To determine pixel colors",
          "To transform vertex positions and attributes",
          "To perform texture sampling",
          "To handle user input"
        ],
        "correctAnswer": 1,
        "explanation": "The vertex shader is responsible for transforming vertex positions through the model-view-projection matrix and processing vertex attributes like normals and texture coordinates."
      },
      {
        "id": "cs306-mid-q4",
        "type": "multiple_choice",
        "prompt": "What does the z-buffer (depth buffer) store?",
        "options": [
          "Normal vectors for lighting",
          "Color values for each pixel",
          "Depth information for visibility testing",
          "Texture coordinates"
        ],
        "correctAnswer": 2,
        "explanation": "The z-buffer stores depth values for each pixel to determine which surfaces are visible and should be rendered, solving the hidden surface problem."
      },
      {
        "id": "cs306-mid-q5",
        "type": "multiple_choice",
        "prompt": "In the graphics pipeline, what happens during primitive assembly?",
        "options": [
          "Individual vertices are grouped into geometric primitives like triangles",
          "Pixels are colored based on interpolated values",
          "Matrices are multiplied together",
          "Textures are loaded into memory"
        ],
        "correctAnswer": 0,
        "explanation": "Primitive assembly takes the transformed vertices and organizes them into geometric primitives (points, lines, or triangles) for the rasterization stage."
      },
      {
        "id": "cs306-mid-q6",
        "type": "multiple_choice",
        "prompt": "What is clipping in the graphics pipeline?",
        "options": [
          "Applying textures to surfaces",
          "Determining which surfaces are visible",
          "Removing primitives or parts of primitives outside the view frustum",
          "Converting 3D coordinates to 2D screen coordinates"
        ],
        "correctAnswer": 2,
        "explanation": "Clipping removes geometry that falls outside the viewing volume (frustum) to avoid rendering unnecessary primitives and ensure correct behavior at the edges of the view."
      },
      {
        "id": "cs306-mid-q7",
        "type": "multiple_choice",
        "prompt": "What type of transformation preserves parallel lines and ratios of distances along lines?",
        "options": [
          "Affine transformation",
          "Perspective transformation",
          "Non-linear transformation",
          "Conformal transformation"
        ],
        "correctAnswer": 0,
        "explanation": "Affine transformations (translation, rotation, scaling, shearing) preserve parallelism and ratios of distances. Perspective transformations do not preserve these properties."
      },
      {
        "id": "cs306-mid-q8",
        "type": "multiple_choice",
        "prompt": "Why are homogeneous coordinates used in computer graphics?",
        "options": [
          "To increase rendering speed",
          "To simplify texture mapping",
          "To represent translations as matrix multiplications",
          "To reduce memory usage"
        ],
        "correctAnswer": 2,
        "explanation": "Homogeneous coordinates (adding a 4th component) allow translations to be represented as matrix multiplications, enabling all transformations to be combined through matrix multiplication."
      },
      {
        "id": "cs306-mid-q9",
        "type": "multiple_choice",
        "prompt": "What is the result of composing transformation matrices T1 followed by T2?",
        "options": [
          "T2 - T1",
          "T1 + T2",
          "T2 × T1",
          "T1 × T2"
        ],
        "correctAnswer": 2,
        "explanation": "In standard column-vector notation, to apply transformation T1 first and then T2, we compute T2 × T1. The rightmost matrix is applied first."
      },
      {
        "id": "cs306-mid-q10",
        "type": "multiple_choice",
        "prompt": "What does a scaling matrix with negative values achieve?",
        "options": [
          "Reflection across an axis",
          "Rotation",
          "Translation",
          "Shearing"
        ],
        "correctAnswer": 0,
        "explanation": "Negative scaling factors flip the object across the corresponding axis, creating a reflection or mirror image."
      },
      {
        "id": "cs306-mid-q11",
        "type": "multiple_choice",
        "prompt": "What property must a rotation matrix satisfy?",
        "options": [
          "All elements must be positive",
          "It must be diagonal",
          "Its determinant must be zero",
          "It must be orthogonal (its transpose equals its inverse)"
        ],
        "correctAnswer": 3,
        "explanation": "Rotation matrices are orthogonal, meaning R^T = R^(-1). This property ensures that rotations preserve lengths and angles."
      },
      {
        "id": "cs306-mid-q12",
        "type": "multiple_choice",
        "prompt": "In a 4×4 transformation matrix, where is the translation information stored?",
        "options": [
          "The bottom-right element",
          "The top-left 3×3 submatrix",
          "The fourth column (or row, depending on convention)",
          "The diagonal elements"
        ],
        "correctAnswer": 2,
        "explanation": "In a 4×4 homogeneous transformation matrix, the translation components (tx, ty, tz) are stored in the fourth column (column-major) or fourth row (row-major)."
      },
      {
        "id": "cs306-mid-q13",
        "type": "multiple_choice",
        "prompt": "What is gimbal lock in 3D rotations?",
        "options": [
          "An error in matrix multiplication",
          "A technique for faster rotation calculations",
          "Loss of one degree of freedom when two rotation axes align",
          "A method to lock rotation angles"
        ],
        "correctAnswer": 2,
        "explanation": "Gimbal lock occurs in Euler angle representations when two of the three rotation axes align, causing the loss of one rotational degree of freedom. Quaternions can avoid this problem."
      },
      {
        "id": "cs306-mid-q14",
        "type": "multiple_choice",
        "prompt": "What are the three key vectors needed to define a camera's view transformation?",
        "options": [
          "Near plane, far plane, and field of view",
          "Position, rotation, and scale",
          "X-axis, Y-axis, and Z-axis",
          "Eye position, look-at point, and up vector"
        ],
        "correctAnswer": 3,
        "explanation": "A view transformation is typically defined by the camera position (eye), the point it's looking at, and an up vector to determine the camera's orientation."
      },
      {
        "id": "cs306-mid-q15",
        "type": "multiple_choice",
        "prompt": "What is the main difference between orthographic and perspective projection?",
        "options": [
          "Orthographic projection requires a z-buffer",
          "Orthographic projection is faster to compute",
          "Perspective projection can only render triangles",
          "Perspective projection includes foreshortening; orthographic does not"
        ],
        "correctAnswer": 3,
        "explanation": "Perspective projection creates realistic depth cues where distant objects appear smaller (foreshortening). Orthographic projection preserves parallel lines and sizes regardless of distance."
      },
      {
        "id": "cs306-mid-q16",
        "type": "multiple_choice",
        "prompt": "What does the field of view (FOV) parameter control in perspective projection?",
        "options": [
          "The resolution of the rendered image",
          "The camera's movement speed",
          "The distance to the near clipping plane",
          "The angular extent of the visible scene"
        ],
        "correctAnswer": 3,
        "explanation": "Field of view determines how wide the camera's view is. A larger FOV creates a wider, more distorted view (wide-angle lens effect), while a smaller FOV creates a narrower, more telescopic view."
      },
      {
        "id": "cs306-mid-q17",
        "type": "multiple_choice",
        "prompt": "What is the viewing frustum?",
        "options": [
          "The area of the screen where rendering occurs",
          "The buffer used for depth testing",
          "The rectangular box containing all objects in the scene",
          "The truncated pyramid-shaped volume of visible space in perspective projection"
        ],
        "correctAnswer": 3,
        "explanation": "The viewing frustum is a truncated pyramid defined by the near plane, far plane, and field of view. Objects outside this volume are clipped."
      },
      {
        "id": "cs306-mid-q18",
        "type": "multiple_choice",
        "prompt": "After perspective division (dividing by w), what coordinate space are vertices in?",
        "options": [
          "Screen space",
          "Normalized Device Coordinates (NDC)",
          "World space",
          "View space"
        ],
        "correctAnswer": 1,
        "explanation": "Perspective division converts clip space coordinates to Normalized Device Coordinates (NDC), typically ranging from -1 to 1 in each dimension, independent of screen resolution."
      },
      {
        "id": "cs306-mid-q19",
        "type": "multiple_choice",
        "prompt": "What is the purpose of the near and far clipping planes?",
        "options": [
          "To control the camera's position",
          "To define the depth range of the visible volume",
          "To set the screen resolution",
          "To determine the lighting intensity"
        ],
        "correctAnswer": 1,
        "explanation": "The near and far clipping planes define the minimum and maximum depth values for rendering. Objects closer than near or farther than far are clipped and not rendered."
      },
      {
        "id": "cs306-mid-q20",
        "type": "multiple_choice",
        "prompt": "What is the primary purpose of rasterization?",
        "options": [
          "Applying textures to surfaces",
          "Converting geometric primitives into discrete pixels",
          "Transforming vertices",
          "Calculating lighting effects"
        ],
        "correctAnswer": 1,
        "explanation": "Rasterization is the process of determining which pixels are covered by geometric primitives (triangles, lines, points) and generating fragments for those pixels."
      },
      {
        "id": "cs306-mid-q21",
        "type": "multiple_choice",
        "prompt": "What is barycentric interpolation used for in triangle rasterization?",
        "options": [
          "Determining if a point is inside a triangle",
          "Interpolating vertex attributes across the triangle surface",
          "Sorting triangles by depth",
          "Calculating triangle area"
        ],
        "correctAnswer": 1,
        "explanation": "Barycentric coordinates allow smooth interpolation of vertex attributes (colors, normals, texture coordinates) across the triangle surface for each fragment."
      },
      {
        "id": "cs306-mid-q22",
        "type": "multiple_choice",
        "prompt": "What problem does antialiasing address in rasterization?",
        "options": [
          "Slow rendering performance",
          "Jagged edges (aliasing artifacts) on rendered primitives",
          "Missing texture details",
          "Incorrect depth ordering"
        ],
        "correctAnswer": 1,
        "explanation": "Antialiasing techniques reduce jagged, stair-step edges (aliasing) that occur when continuous geometric shapes are sampled at discrete pixel locations."
      },
      {
        "id": "cs306-mid-q23",
        "type": "multiple_choice",
        "prompt": "In scanline rasterization, what are edge equations used for?",
        "options": [
          "Calculating triangle normals",
          "Sorting triangles by depth",
          "Determining which pixels are inside a triangle",
          "Applying texture coordinates"
        ],
        "correctAnswer": 2,
        "explanation": "Edge equations (or edge functions) evaluate whether a point is on the positive or negative side of a triangle edge, allowing efficient inside/outside testing for each pixel."
      },
      {
        "id": "cs306-mid-q24",
        "type": "multiple_choice",
        "prompt": "What is multisampling antialiasing (MSAA)?",
        "options": [
          "Applying blur to the final image",
          "Sampling multiple locations within each pixel for better edge quality",
          "Rendering the scene multiple times at different resolutions",
          "Using multiple textures per surface"
        ],
        "correctAnswer": 1,
        "explanation": "MSAA evaluates coverage at multiple sample points within each pixel, then combines the results to produce smoother edges while maintaining most of the performance of single-sample rendering."
      },
      {
        "id": "cs306-mid-q25",
        "type": "multiple_choice",
        "prompt": "What does the fill convention solve in rasterization?",
        "options": [
          "How to calculate pixel colors",
          "The order in which triangles are rendered",
          "How to interpolate colors across surfaces",
          "Which pixels to fill when a pixel center lies exactly on a triangle edge"
        ],
        "correctAnswer": 3,
        "explanation": "The fill convention (like the top-left rule) provides consistent rules for handling edge cases where pixel centers fall exactly on triangle edges, preventing gaps or double-coverage between adjacent triangles."
      },
      {
        "id": "cs306-mid-q26",
        "type": "multiple_choice",
        "prompt": "What is perspective-correct interpolation?",
        "options": [
          "Interpolation of colors without depth information",
          "Linear interpolation of depth values",
          "Interpolation that accounts for perspective division to correctly interpolate attributes in 3D",
          "Interpolation performed in screen space only"
        ],
        "correctAnswer": 2,
        "explanation": "Perspective-correct interpolation properly accounts for the nonlinear effect of perspective projection when interpolating attributes like texture coordinates, ensuring they appear correct in 3D space."
      }
    ]
  },
  {
    "id": "cs306-final",
    "subjectId": "cs306",
    "title": "CS306 Final Examination",
    "questions": [
      {
        "id": "cs306-final-q1",
        "type": "multiple_choice",
        "prompt": "Which stage of the graphics pipeline converts primitives into fragments?",
        "options": [
          "Output merging",
          "Rasterization",
          "Fragment processing",
          "Vertex processing"
        ],
        "correctAnswer": 1,
        "explanation": "Rasterization converts geometric primitives (like triangles) into fragments, which are potential pixels with associated attributes."
      },
      {
        "id": "cs306-final-q2",
        "type": "multiple_choice",
        "prompt": "What is the composite transformation matrix that converts from model space directly to clip space called?",
        "options": [
          "Model-View-Projection (MVP) matrix",
          "World transformation matrix",
          "View matrix",
          "Normal matrix"
        ],
        "correctAnswer": 0,
        "explanation": "The MVP matrix is the product of the model, view, and projection matrices, transforming vertices from model space directly to clip space in a single operation."
      },
      {
        "id": "cs306-final-q3",
        "type": "multiple_choice",
        "prompt": "What happens during the viewport transformation?",
        "options": [
          "Clip space coordinates are divided by w",
          "Vertices are transformed by the model matrix",
          "World coordinates are transformed to view coordinates",
          "NDC coordinates are mapped to screen pixel coordinates"
        ],
        "correctAnswer": 3,
        "explanation": "The viewport transformation maps Normalized Device Coordinates (typically -1 to 1) to actual screen pixel coordinates based on the viewport dimensions."
      },
      {
        "id": "cs306-final-q4",
        "type": "multiple_choice",
        "prompt": "Why are quaternions often preferred over Euler angles for rotations?",
        "options": [
          "They avoid gimbal lock and interpolate smoothly",
          "They require less memory",
          "They are easier to understand",
          "They render faster"
        ],
        "correctAnswer": 0,
        "explanation": "Quaternions avoid gimbal lock problems inherent in Euler angles and provide smooth interpolation for animations through spherical linear interpolation (slerp)."
      },
      {
        "id": "cs306-final-q5",
        "type": "multiple_choice",
        "prompt": "In backface culling, which triangles are typically removed?",
        "options": [
          "Triangles outside the view frustum",
          "Triangles with negative depth",
          "Triangles facing away from the camera",
          "Triangles smaller than one pixel"
        ],
        "correctAnswer": 2,
        "explanation": "Backface culling removes triangles whose normals point away from the camera. For closed objects, these back-facing triangles are not visible and can be discarded to improve performance."
      },
      {
        "id": "cs306-final-q6",
        "type": "multiple_choice",
        "prompt": "What are the three main components of the Phong reflection model?",
        "options": [
          "Direct, indirect, and emissive",
          "Red, green, and blue",
          "Position, normal, and color",
          "Ambient, diffuse, and specular"
        ],
        "correctAnswer": 3,
        "explanation": "The Phong model combines ambient lighting (constant background), diffuse reflection (matte surfaces), and specular reflection (shiny highlights) to approximate realistic lighting."
      },
      {
        "id": "cs306-final-q7",
        "type": "multiple_choice",
        "prompt": "What does the diffuse component in Phong shading depend on?",
        "options": [
          "The distance from the camera",
          "The angle between the surface normal and light direction",
          "Only the light intensity",
          "The angle between the view direction and reflection direction"
        ],
        "correctAnswer": 1,
        "explanation": "Diffuse reflection follows Lambert's cosine law: intensity is proportional to the cosine of the angle between the surface normal and light direction (N · L)."
      },
      {
        "id": "cs306-final-q8",
        "type": "multiple_choice",
        "prompt": "What does the specular exponent (shininess) control in Phong shading?",
        "options": [
          "The overall brightness of the surface",
          "The shadow intensity",
          "The color of the surface",
          "The size and sharpness of specular highlights"
        ],
        "correctAnswer": 3,
        "explanation": "Higher specular exponents create smaller, sharper highlights (more mirror-like), while lower values create larger, softer highlights (less shiny surfaces)."
      },
      {
        "id": "cs306-final-q9",
        "type": "multiple_choice",
        "prompt": "What is the main difference between Phong shading and Blinn-Phong shading?",
        "options": [
          "Blinn-Phong only works with point lights",
          "Phong shading doesn't include specular highlights",
          "Blinn-Phong uses a halfway vector instead of the reflection vector",
          "Phong shading is physically accurate"
        ],
        "correctAnswer": 2,
        "explanation": "Blinn-Phong uses the halfway vector (between light and view directions) instead of computing the reflection vector, which is more efficient and can produce more pleasing results."
      },
      {
        "id": "cs306-final-q10",
        "type": "multiple_choice",
        "prompt": "What is the difference between flat shading and smooth shading?",
        "options": [
          "Flat only works with triangles; smooth works with any polygon",
          "Flat is faster but smooth is more accurate",
          "Flat uses one normal per polygon; smooth interpolates normals across the surface",
          "Flat uses ambient lighting; smooth uses diffuse lighting"
        ],
        "correctAnswer": 2,
        "explanation": "Flat shading uses a single normal for the entire polygon, creating faceted appearance. Smooth shading (Gouraud or Phong) interpolates vertex normals across the surface for a smooth look."
      },
      {
        "id": "cs306-final-q11",
        "type": "multiple_choice",
        "prompt": "In Gouraud shading, when are lighting calculations performed?",
        "options": [
          "At each vertex, then colors are interpolated across the polygon",
          "At every pixel across the surface",
          "Once per polygon",
          "During the texture mapping stage"
        ],
        "correctAnswer": 0,
        "explanation": "Gouraud shading computes lighting at vertices and interpolates the resulting colors across the polygon. This is faster than per-pixel lighting but can miss highlights."
      },
      {
        "id": "cs306-final-q12",
        "type": "multiple_choice",
        "prompt": "What does per-pixel (Phong) shading interpolate across the polygon surface?",
        "options": [
          "Light positions",
          "Normal vectors",
          "Depth values",
          "Final colors"
        ],
        "correctAnswer": 1,
        "explanation": "Per-pixel (Phong) shading interpolates normal vectors across the surface and performs full lighting calculations at each pixel, producing more accurate highlights than Gouraud shading."
      },
      {
        "id": "cs306-final-q13",
        "type": "multiple_choice",
        "prompt": "What is a normal map used for in shading?",
        "options": [
          "Defining light directions",
          "Adding surface detail by perturbing normals without adding geometry",
          "Specifying material colors",
          "Storing vertex positions"
        ],
        "correctAnswer": 1,
        "explanation": "Normal maps store perturbed normal vectors that modify lighting calculations to simulate surface details like bumps and wrinkles without increasing polygon count."
      },
      {
        "id": "cs306-final-q14",
        "type": "multiple_choice",
        "prompt": "What color space are normal map values typically stored in?",
        "options": [
          "Tangent space",
          "World space",
          "View space",
          "Screen space"
        ],
        "correctAnswer": 0,
        "explanation": "Tangent-space normal maps store normals relative to the surface's local coordinate system, making them reusable on different objects and orientations."
      },
      {
        "id": "cs306-final-q15",
        "type": "multiple_choice",
        "prompt": "What is the main advantage of physically-based rendering (PBR) over traditional shading models?",
        "options": [
          "More realistic and consistent lighting under different conditions",
          "Faster rendering performance",
          "Simpler implementation",
          "Less memory usage"
        ],
        "correctAnswer": 0,
        "explanation": "PBR uses physically-based principles (energy conservation, Fresnel effects, microfacet theory) to produce consistent, realistic results across various lighting environments."
      },
      {
        "id": "cs306-final-q16",
        "type": "multiple_choice",
        "prompt": "In PBR, what do the metallic and roughness parameters control?",
        "options": [
          "Metallic controls brightness; roughness controls color",
          "Metallic controls reflectivity type; roughness controls surface smoothness",
          "Metallic controls transparency; roughness controls refraction",
          "Metallic controls shadows; roughness controls highlights"
        ],
        "correctAnswer": 1,
        "explanation": "Metallic determines whether the surface reflects like a metal (colored reflections) or dielectric (white reflections). Roughness controls how smooth or rough the microsurface is."
      },
      {
        "id": "cs306-final-q17",
        "type": "multiple_choice",
        "prompt": "What is the Fresnel effect in rendering?",
        "options": [
          "Colors become more saturated at distance",
          "Textures appear sharper when viewed closely",
          "Reflectivity increases at grazing angles",
          "Shadows become softer near edges"
        ],
        "correctAnswer": 2,
        "explanation": "The Fresnel effect describes how reflectivity increases when viewing a surface at shallow (grazing) angles. Even matte surfaces become more reflective at these angles."
      },
      {
        "id": "cs306-final-q18",
        "type": "multiple_choice",
        "prompt": "What are texture coordinates (UV coordinates)?",
        "options": [
          "The position of textures in world space",
          "The pixel dimensions of a texture image",
          "Color values stored at each pixel",
          "Parametric coordinates that map points on a surface to locations in a texture"
        ],
        "correctAnswer": 3,
        "explanation": "UV coordinates are 2D parametric coordinates (typically ranging 0-1) that define how a 2D texture image maps onto a 3D surface."
      },
      {
        "id": "cs306-final-q19",
        "type": "multiple_choice",
        "prompt": "What is texture filtering used for?",
        "options": [
          "Removing textures from memory",
          "Converting between color spaces",
          "Determining texture color when texture coordinates don't align with texel centers",
          "Compressing texture data"
        ],
        "correctAnswer": 2,
        "explanation": "Texture filtering (like nearest neighbor or bilinear filtering) determines how to compute a color when sampling the texture at coordinates that fall between texel centers."
      },
      {
        "id": "cs306-final-q20",
        "type": "multiple_choice",
        "prompt": "What is the difference between nearest neighbor and bilinear texture filtering?",
        "options": [
          "Bilinear only works with square textures",
          "Nearest neighbor picks the closest texel; bilinear interpolates between four texels",
          "Nearest neighbor is more accurate but slower",
          "Nearest neighbor requires mipmaps"
        ],
        "correctAnswer": 1,
        "explanation": "Nearest neighbor sampling selects the single closest texel (fast but pixelated). Bilinear filtering interpolates between the four nearest texels for smoother results."
      },
      {
        "id": "cs306-final-q21",
        "type": "multiple_choice",
        "prompt": "What problem do mipmaps solve?",
        "options": [
          "Textures appearing too bright",
          "UV coordinate wrapping",
          "Aliasing and performance issues when textures are minified",
          "Running out of texture memory"
        ],
        "correctAnswer": 2,
        "explanation": "Mipmaps are pre-filtered, progressively lower-resolution versions of a texture. They reduce aliasing artifacts and improve performance when textures are viewed at a distance."
      },
      {
        "id": "cs306-final-q22",
        "type": "multiple_choice",
        "prompt": "What is trilinear filtering?",
        "options": [
          "Bilinear filtering between two mipmap levels plus interpolation between the levels",
          "Filtering in three color channels separately",
          "Using three texture samples per pixel",
          "Filtering based on three light sources"
        ],
        "correctAnswer": 0,
        "explanation": "Trilinear filtering performs bilinear filtering on two adjacent mipmap levels and then linearly interpolates between them, eliminating visible transitions between mipmap levels."
      },
      {
        "id": "cs306-final-q23",
        "type": "multiple_choice",
        "prompt": "What is anisotropic filtering designed to handle?",
        "options": [
          "High-resolution textures",
          "Multiple overlapping textures",
          "Textures viewed at oblique angles where detail stretches in one direction",
          "Textures with transparent regions"
        ],
        "correctAnswer": 2,
        "explanation": "Anisotropic filtering takes multiple texture samples along the direction of anisotropy (elongation), producing sharper textures when surfaces are viewed at shallow angles."
      },
      {
        "id": "cs306-final-q24",
        "type": "multiple_choice",
        "prompt": "What does texture wrapping mode determine?",
        "options": [
          "The color space of the texture",
          "How texture coordinates outside the 0-1 range are handled",
          "How textures are compressed",
          "The resolution of the texture"
        ],
        "correctAnswer": 1,
        "explanation": "Wrapping modes (repeat, clamp, mirror) define what happens when UV coordinates fall outside the standard 0-1 range, controlling texture tiling and edge behavior."
      },
      {
        "id": "cs306-final-q25",
        "type": "multiple_choice",
        "prompt": "What is a cube map typically used for?",
        "options": [
          "Terrain height information",
          "Shadow mapping",
          "Environment mapping and reflections",
          "Character skin textures"
        ],
        "correctAnswer": 2,
        "explanation": "Cube maps store six square textures arranged as the faces of a cube, commonly used for skyboxes and environment reflections where sampling direction is a 3D vector."
      },
      {
        "id": "cs306-final-q26",
        "type": "multiple_choice",
        "prompt": "What is texture atlasing?",
        "options": [
          "Combining multiple textures into a single larger texture",
          "Generating mipmaps automatically",
          "Compressing texture data",
          "Animating texture coordinates"
        ],
        "correctAnswer": 0,
        "explanation": "Texture atlasing packs multiple smaller textures into one large texture, reducing the number of texture switches during rendering and improving performance."
      },
      {
        "id": "cs306-final-q27",
        "type": "multiple_choice",
        "prompt": "What information does a displacement map store?",
        "options": [
          "Height information that actually moves vertex positions",
          "Color information for surfaces",
          "Normal perturbations for lighting",
          "Transparency values"
        ],
        "correctAnswer": 0,
        "explanation": "Unlike normal maps which only affect lighting, displacement maps actually modify the geometry by moving vertices based on height values, creating real surface detail."
      },
      {
        "id": "cs306-final-q28",
        "type": "multiple_choice",
        "prompt": "What is the fundamental principle of ray tracing?",
        "options": [
          "Rasterizing triangles to pixels",
          "Tracing rays from the camera through pixels to find intersections with scene geometry",
          "Interpolating vertex colors",
          "Sorting objects by depth"
        ],
        "correctAnswer": 1,
        "explanation": "Ray tracing works by casting rays from the camera through each pixel into the scene, finding what objects they intersect, and computing the color based on those intersections."
      },
      {
        "id": "cs306-final-q29",
        "type": "multiple_choice",
        "prompt": "What is the primary advantage of ray tracing over rasterization?",
        "options": [
          "Faster rendering speed",
          "Simpler implementation",
          "Accurate reflections, refractions, and shadows with global illumination",
          "Lower memory usage"
        ],
        "correctAnswer": 2,
        "explanation": "Ray tracing naturally handles complex light transport including realistic reflections, refractions, shadows, and global illumination effects that are difficult or impossible with traditional rasterization."
      },
      {
        "id": "cs306-final-q30",
        "type": "multiple_choice",
        "prompt": "In ray-sphere intersection, what equation must be solved?",
        "options": [
          "A quadratic equation derived from substituting the ray equation into the sphere equation",
          "A linear system of equations",
          "A differential equation",
          "A matrix equation"
        ],
        "correctAnswer": 0,
        "explanation": "Ray-sphere intersection involves substituting the parametric ray equation into the implicit sphere equation, resulting in a quadratic equation whose solutions give intersection distances."
      },
      {
        "id": "cs306-final-q31",
        "type": "multiple_choice",
        "prompt": "What is a shadow ray in ray tracing?",
        "options": [
          "The primary ray from the camera",
          "A ray that bounces off shadowed surfaces",
          "A ray cast from a surface point toward a light source to test for occlusion",
          "A ray that creates shadows on other objects"
        ],
        "correctAnswer": 2,
        "explanation": "Shadow rays are cast from surface points toward light sources. If the ray intersects an object before reaching the light, the point is in shadow."
      },
      {
        "id": "cs306-final-q32",
        "type": "multiple_choice",
        "prompt": "What is recursive ray tracing?",
        "options": [
          "Spawning secondary rays for reflections and refractions that recursively generate more rays",
          "Tracing rays in a loop until convergence",
          "Using recursion to organize scene geometry",
          "Repeating the rendering process multiple times"
        ],
        "correctAnswer": 0,
        "explanation": "Recursive ray tracing generates secondary rays (reflection, refraction) at intersection points, which themselves can generate more rays, creating a recursive tree of ray paths."
      },
      {
        "id": "cs306-final-q33",
        "type": "multiple_choice",
        "prompt": "What is the purpose of acceleration structures like BVH (Bounding Volume Hierarchy) in ray tracing?",
        "options": [
          "To sort objects by material type",
          "To quickly reject large groups of objects that a ray doesn't intersect",
          "To parallelize ray tracing across multiple processors",
          "To compress scene geometry"
        ],
        "correctAnswer": 1,
        "explanation": "BVH and similar structures organize geometry hierarchically in bounding volumes, allowing ray tracers to quickly skip large groups of objects without testing individual intersections."
      },
      {
        "id": "cs306-final-q34",
        "type": "multiple_choice",
        "prompt": "What is Snell's law used for in ray tracing?",
        "options": [
          "Determining shadow intensity",
          "Calculating reflection angles",
          "Computing surface normals",
          "Computing the direction of refracted rays when light passes through transparent materials"
        ],
        "correctAnswer": 3,
        "explanation": "Snell's law (n1 sin θ1 = n2 sin θ2) relates the angles and indices of refraction when light passes between materials, used to compute refraction direction in ray tracing."
      },
      {
        "id": "cs306-final-q35",
        "type": "multiple_choice",
        "prompt": "What is path tracing?",
        "options": [
          "A method for finding the shortest path through a scene",
          "Drawing paths that objects follow in animation",
          "Tracing rays along predetermined paths",
          "A Monte Carlo ray tracing method that traces random paths for global illumination"
        ],
        "correctAnswer": 3,
        "explanation": "Path tracing randomly samples light paths through the scene, accumulating many samples to converge on the correct global illumination solution, producing highly realistic lighting."
      },
      {
        "id": "cs306-final-q36",
        "type": "multiple_choice",
        "prompt": "What is the rendering equation?",
        "options": [
          "The equation for calculating depth values",
          "A formula for interpolating vertex attributes",
          "An integral equation describing how light bounces and accumulates in a scene",
          "The formula for projecting 3D points to 2D"
        ],
        "correctAnswer": 2,
        "explanation": "The rendering equation, introduced by Kajiya, is an integral equation that describes the total light reflected from a point as a sum of emitted and reflected light from all directions."
      },
      {
        "id": "cs306-final-q37",
        "type": "multiple_choice",
        "prompt": "What is importance sampling in Monte Carlo ray tracing?",
        "options": [
          "Biasing random samples toward directions likely to contribute more to the final result",
          "Sampling only the most important objects in the scene",
          "Taking more samples for important pixels",
          "Sampling textures at higher resolution"
        ],
        "correctAnswer": 0,
        "explanation": "Importance sampling concentrates samples in directions that contribute more to the result (e.g., toward light sources), reducing noise and improving convergence speed."
      },
      {
        "id": "cs306-final-q38",
        "type": "multiple_choice",
        "prompt": "What is ambient occlusion in ray tracing?",
        "options": [
          "The ambient light component of the Phong model",
          "Approximating indirect lighting by testing how exposed each point is to the sky",
          "Shadow rays that miss all light sources",
          "Occlusion culling for performance optimization"
        ],
        "correctAnswer": 1,
        "explanation": "Ambient occlusion casts rays in random directions to determine how occluded a point is. Crevices and corners receive less ambient light, adding depth and realism."
      },
      {
        "id": "cs306-final-q39",
        "type": "multiple_choice",
        "prompt": "What is Russian roulette in path tracing?",
        "options": [
          "Randomly selecting which objects to render",
          "Randomly terminating ray paths with probability based on contribution to reduce computation",
          "Random selection of camera positions",
          "A method for randomly placing lights in a scene"
        ],
        "correctAnswer": 1,
        "explanation": "Russian roulette randomly terminates rays with some probability (adjusting contribution accordingly), preventing infinite recursion while maintaining unbiased results on average."
      },
      {
        "id": "cs306-final-q40",
        "type": "multiple_choice",
        "prompt": "What is the difference between hard shadows and soft shadows?",
        "options": [
          "Soft shadows only occur indoors",
          "Hard shadows are faster to compute",
          "Hard shadows have sharp edges (point lights); soft shadows have gradual transitions (area lights)",
          "Hard shadows are darker than soft shadows"
        ],
        "correctAnswer": 2,
        "explanation": "Hard shadows result from point light sources and have sharp, well-defined edges. Soft shadows result from area lights and have gradual penumbra regions."
      },
      {
        "id": "cs306-final-q41",
        "type": "multiple_choice",
        "prompt": "What is caustics in computer graphics?",
        "options": [
          "The cause of rendering artifacts",
          "Focused light patterns created by reflection or refraction through curved surfaces",
          "Lighting effects in fog or smoke",
          "Chemical effects on materials"
        ],
        "correctAnswer": 1,
        "explanation": "Caustics are concentrated light patterns formed when light rays converge after reflecting off or refracting through curved surfaces, like the patterns at the bottom of a swimming pool."
      },
      {
        "id": "cs306-final-q42",
        "type": "multiple_choice",
        "prompt": "What is photon mapping?",
        "options": [
          "A two-pass algorithm that traces photons from lights and uses them to compute indirect illumination",
          "Creating maps of scene geometry for lighting",
          "Storing light intensity in texture maps",
          "Mapping camera rays to light sources"
        ],
        "correctAnswer": 0,
        "explanation": "Photon mapping first traces photons from light sources and stores them where they hit surfaces. In a second pass, these stored photons are used to efficiently compute indirect lighting and caustics."
      }
    ]
  }
]
